[
  {
    "id": "9ecf4b49a77c8857acf06e5bf2067301",
    "url_s": "https://opensourceconnections.com/blog/2006/06/29/rolling-out-new-blog/",
    "title": "Rolling out new blog",
    "content": [
      "Weve discussed setting up a company-wide blog for a while now, and today I decided to just go ahead and do it. We looked at some of the powerful ones, but realized that simplicity is all we need, and therefore SimpleLog is an obvious answer!",
      "Look for postings about what is going on at OpenSource Connections."
    ],
    "summary_t": ""
  },
  {
    "id": "e26a363a450eab7b66763f4c6383f506",
    "url_s": "https://opensourceconnections.com/blog/2006/06/30/preparation-for-oscon/",
    "title": "Preparation for OSCON",
    "content": [
      "While we did not think that we could sign up, show up, and generate business, there are many separate factors that we must consider to make sure that our first big exhibition is a success.",
      "Media: As a company that has always focused on word-of-mouth referrals and marketing for our business, we have not developed a robust set of marketing materials. Thus, we needed to come up with our exhibit backdrop, fliers, press releases, and determine what it was that we were going to display. If you can afford it, it is worthwhile to use a couple of different resources. First, a graphic designer. Find someone who has designed for exhibits before and who has a good understanding of your target audience. If your reaction to this is \"what is my target audience,\" then you have more work to do before you can get to this stage. Second, a copy writer. If you cannot capture the essence of what you do and get the reader of your materials to want to engage in further conversations in 20 words or less, rewrite it. Most technical people do not have the language savvy and skills to do this; a few hours with a professional copywriter will help you. We have used Proofrock Copy and been very happy with their results.",
      "Message: You cant be everything to everyone. Even if you offer several packages, trying to sell all of them will likely result in failing to close on potential deals because of lack of focus. We have chosen to focus on two main areas: strategic engagements and Agile Development training. The strategic engagements are designed for our clients to develop the appropriate roadmap for getting their IT systems to the point to where they can provide the information that decision makers need to drive business forward. Agile Development training is, for the most part, self-explanatory. Its still more of a buzzword than a practice, and many teams have tried and failed to incorporate Agile because of lack of appropriate training and methodologies. While we have other offerings, such as laboratory information management systems and content management systems, we would rather focus on 200 of the 3,000 attendees than cast a wide net and hope to snag a random hit.",
      "Extras: Just signing up for an exhibit and renting/buying your exhibit standup materials doesnt cover everything. Dont forget that youre going to have to rent carpet, trash cans, cleaning crew, podia, tables, chairs, etc. If youve never done an exhibition before, network with people who have to see what their costs were. If you cant find someone who has, then take your estimates and double them. Additionally, dont spend yourself down to nearly nothing to exhibit. There will be follow-up work and engagements (one hopes) after the convention, and you will need the capital to fund your follow-up sales efforts and initial engagement work.",
      "Were going to have some exciting things at OSCON. We look forward to meeting all of you there!"
    ],
    "summary_t": ""
  },
  {
    "id": "aece04f9361deaf8630ef4a9177f1351",
    "url_s": "https://opensourceconnections.com/blog/2006/08/03/reflections-on-oscon/",
    "title": "Reflections on OSCON",
    "content": [
      "It appears to me that, after having talked to countless people at OSCON, there is still a significant disconnect between IT and the rest of the business.",
      "I say this because IT representatives had a pretty good idea of what they wanted to do and why. However, when it came to getting someone to pony up the coin to make the changes necessary to move forward, IT representatives, for the most part, were at a loss for how to shake out the money tree.",
      "The biggest reason is that business views IT as a cost center, and so spending on IT needs to be minimized as much as possible. When I asked what the criteria for making decisions was, the inevitable answer was money. The bottom line loomed so large that it drowned out any other competing interest. This is short-sighted and reflective of a company trying to make this quarters numbers without looking at how its going to make next quarters numbers, much less next years numbers.",
      "To me, this is analogous to building a house out of particle board. Sure, once the house is done, the total cost of building the house will be extremely low. It will also be a cheap house and will serve no habitable purpose.",
      "Even for companies that are willing to spend to improve their IT systems, often, they did not know the business purpose they were trying to achieve. They did not know where the weaknesses in process were that the appropriate application of technology could improve. Furthermore, they did not consider total cost of ownership rather than up front price. I liken this to putting caulk around a leak. It might work. But it might also be only a temporary fix and cost more in the long run. Only good solid analysis of the problem and true understanding of a set of requirements can determine if what youre seeking is truly the answer that you want.",
      "Finally, there is an appetite for knowledge about Agile Development, but seemingly few resources available. We gave away Andy Hunts book Practices of an Agile Developer, that can help teams and developers get started down the path.",
      "All in all, the convention was time well spent. We met a lot of people and got in touch with what was going on in the open source community. We will definitely plan on being at OSCON 2007, and we will make sure we leave enough time to do more discovery of the Columbia River gorge!"
    ],
    "summary_t": ""
  },
  {
    "id": "f80508c342dc5702b5dffb38c5adde6e",
    "url_s": "https://opensourceconnections.com/blog/2007/10/09/is-your-development-proudly-found-elsewhere/",
    "title": "Is Your Development Proudly Found Elsewhere?",
    "content": [
      "\"Proudly Found Elsewhere\" is the research and development mantra of Procter and Gamble, one of the worlds largest consumer products companies. The company boasts 7,500 scientist dedicated to R&D, focused on finding new innovations, new breakthroughs, and new products for P&Gs consumer products lines. Yet, 50% of their lines of business which generate more than $1 billion in revenues came from innovations outside of P&G. As the Harvard Business Review reported, it became a simple matter of math: 200 smart engineers who did not work for P&G for every one that did.",
      "Many companies suffer from the myopia of the \"not invented here\" syndrome. They cannot come to grips with the idea that they cannot possibly accomplish everything internally and that the best ideas come from solutions other than the \"in house\" solution. They often get passed by and build the products and provide the services that experienced a peak shown many annual reports before.",
      "This happens to large IT departments as well. These organizations believe that they can build a custom application for every need of the business. Rather than utilizing the knowledge and expertise of others, they fall prey to the notion that if they didnt build it, it cant be good. Instead of using a few external resources with specific knowledge and stitching together existing tools and platforms in a few months, these departments drive their developers into a quixotic quest to build the perfect application from scratch.",
      "As of today, there are 159,599 registered projects, and, more importantly, 1,698,785 registered users on SourceForge. While not all of them are developers, many are. How many organizations offer a development staff even an order of magnitude less than 1.7 million? Perhaps the United States government, and certainly not a commercial organization. Fail to leverage that base of innovation at your own risk.",
      "As for us, we would rather stand on the shoulders of giants and use the innovation and work of others and craft it to our own needs than to tilt at windmills and try to build it all ourselves.",
      "About OpenSource Connections",
      "OpenSource Connections is a custom development consulting group that provides enterprise-level support for open source solutions. Focusing on convergent information systems, OpenSource Connections helps companies find ways to optimize information and resources.",
      "Our team consists of industry-recognized experts in the areas of open source and Agile Development methodology. OpenSource Connections was instrumental in the development of major open source systems such as Apache, Maven, and Ant. We also train in the area of Agile development methodology – an approach that improves software project results through team interaction. OpenSource Connections received a 2005 Webbie award for superior website design and integration."
    ],
    "summary_t": ""
  },
  {
    "id": "121ac6f582cb6a434b5e5f4fbf07e7d8",
    "url_s": "https://opensourceconnections.com/blog/2007/10/09/michael-herndon-will-be-participating-at-agile-open-california/",
    "title": "Michael Herndon will be participating at Agile Open California",
    "content": [
      "Agile Open California will be ramping up Oct 23, and Michael will be sharing our approach to Agile from the trenches."
    ],
    "summary_t": ""
  },
  {
    "id": "07987e988d71d3ecf8ac902e2e98a30b",
    "url_s": "https://opensourceconnections.com/blog/2007/10/10/millipore/",
    "title": "Millipore",
    "content": [
      "OpenSource Connections helped Millipore upgrade its laboratory information management system (LIMS) utilizing a mix of onsite and offsite resources. OpenSource Connections provided testing resources and development resources to work onsite in Dundee, Scotland with laboratory end users and integrated its development team in Brazil to produce a product that had to be approved by corporate leaders at Millipores headquarters in Billerica, Massachusetts.\nThe Scotland-based lab analysts and the US-based scientists heading the projects required an easier and clearer way to track shared data. The LIMS allowed for the easy exchange of information related to the testing of various drug compounds on kinases for Millipores drug manufacturing customers. The LIMS rolled up test data, due dates, customer requirements, such as test setup and layout, and results. The LIMS also allowed for the specification of business rules, which when applied to test result data, automated a great deal of the quality assurance work. Finally, the LIMS facilitates the exportation of data to Excel, and builds charts in Adobe PDF, the customers preferred formats."
    ],
    "summary_t": ""
  },
  {
    "id": "7cf6e8db7618a3dd6499c7e82b90efb6",
    "url_s": "https://opensourceconnections.com/blog/2007/10/10/mitre-blueprint/",
    "title": "MITRE–Blueprint",
    "content": [
      "The MITRE Air Force Center in Bedford, Massachusetts was contacted to build a tool allowing Air Force Program Managers the ability to see funding sources for each program and potential duplication of efforts between programs. They sought help from OpenSource Connections to build a graphical visualization of the connections between programs, as well as the funding streams for each."
    ],
    "summary_t": ""
  },
  {
    "id": "a4bd32c23dac8c717097cbfe9004978f",
    "url_s": "https://opensourceconnections.com/blog/2007/10/10/university-of-virginia-library/",
    "title": "BlackLightDL at the University of Virginia Library",
    "content": [
      "The University of Virginia sought a solution to make all text documents in their Digital Collection Library text-searchable. Containing thousands of documents, the search engine needed to be capable of a rapid and complete search without crashing the back end. UVA selected OpenSource Connections as their partner on this project because of our work in the open source world and academias preference for free and open software. Their solution was BlacklightDL. The back end runs on Solr, an open source enterprise search server based on the Lucene Java search library, with XML/HTTP and JSON APIs, hit highlighting, faceted search, caching, replication, and a web administration interface. The end-user interface is built on Ruby on Rails (RoR), allowing a rapid solution to UVAs data overload. The end result is a license-free solution that will enable thousands of student, faculty and staff users to quickly search UVAs entire Digital Collection Library library from their PC.",
      "Dr. Mike Bergman gives a detailed overview of the application in \"A Not so Long Wave at BlackLight.\""
    ],
    "summary_t": ""
  },
  {
    "id": "78d25ddab1a78fa7a30642dbee77c260",
    "url_s": "https://opensourceconnections.com/blog/2007/10/11/hacking-cool-techniques-for-visual-studio-2008-javascript-intellisense/",
    "title": "Hacking (Cool Techniques) for Visual Studio 2008 Javascript Intellisense.",
    "content": [
      "So I’ve been in the lab working on a javascript work flow for Visual Studio 2008. I know you might being thinking of a work flow engine cause you’re a crazed developer, but I’m actually talking about the whole project/flow of development for doing controls, widgets, libraries and such for javascript in a visual studio type environment.",
      "This is all experimental at this point, thus subject to change. I’ve put in some bugs/suggestions in regards to intellisense to the visual studio team, of course quite a few were closed \"by design\".  But thats ok, I can be the bigger developer and find ways around it .  First off, the xml comments and structure of getting intellisense is pretty constrained.  So the best way I can think of at this point, to get around this without having verbose Javascript, is to have a skeleton file for visual studio to use for intellisense versus, using the actual code file.",
      "Yeah this requires extra files, but at least you’re getting the benefit of inline documentation and the classy \"control + space-bar\" action of autocomplete.  Basically if you use something like the Prototype Library, a class might look like the following…..",
      "Amplify.UI.ContextMenu = Class.create(Amplify.UI.Panel, {\n\n    initialize: function(element, options) {\n        this.setOptions(options);\n        this.load(element);\n    },\n\n    // other methods here...\n\n    _onClick: function(e) {\n        this.dispatcher = e.element();\n        e.stop();\n        this.setPosition(e);\n        this.show(e);\n        Event.observe(document, 'click', this.hideHandler);\n    }\n});",
      "Well to work some magic in getting this work work nicely in visual studio it would need to do the following….",
      "Amplify.UI.ContextMenu = function(element, options) {\n       /// <summary> ....\n       /// <param name=\"element\">element </param>\n       /// <param name=\"options\">options </param>\n}\n\nAmplify.UI.ContextMenu.prototype = {\n\n        initialize: function(element, options) {\n        /// <summary> ....\n            /// <param name=\"element\">element </param>\n            /// <param name=\"options\">options </param>\n\n    },\n\n    // other methods here...\n\n    _onClick: function(e) {\n        /// <summary> ....\n            /// <param name=\"e\">Event </param>\n    }\n});\n\nAmplify.UI.ContextMenu.__class = true;\nAmplify.UI.ContextMenu.__type = \"Amplify.UI.ContextMenu\";\nAmplify.UI.ContextMenu.__baseType = Amplify.UI.Panel;\n\nAmplify.setParent(Amplify.UI.ContextMenu, Amplify.UI.Panel);\n\n// Amplify.setParent does the following... which adds methods from the parent if not overridden...\n//for(var property in inheritedType.prototype) {\n//  if(!type.prototype[property])\n//          type.prototype[property] = inheritedType.prototype[property];\n//  }",
      "As you can see… the syntax is a bit bloated, so separating this into its own file isn’t a bad thing, not to mention, you can use these files for Ajax Doc, in order to create msdn style documentation, without having to keep the comments inside your actual code files. However you’ll also see what I’m doing to create the inheritance of javascript objects for Visual Studio to use that stems from the Asp.Net Ajax Javascript format already in place.",
      "Now that we have this in place, we need to be able to run these two code places on the same page without a lot of copy and paste for something the homemade Specification library that Im using for BDD style development.  So we need to make some adjustments and use a base .aspx page for specifications, with the following code in it.",
      "public class SpecPage : System.Web.UI.Page\n{\n    protected override void OnPreRender(EventArgs e)\n    {\n        base.OnPreRender(e);\n        Control[] controls = new Control[this.Header.Controls.Count];\n        this.Header.Controls.CopyTo(controls, 0);\n        this.Header.Controls.Clear();\n\n        foreach (Control control in controls)\n        {\n            if (control is LiteralControl)\n            {\n                LiteralControl lit = (LiteralControl)control;\n                if (lit.Text.Contains(\"script\"))\n                {\n                    lit.Text = lit.Text.Replace(\"intellisense.\", \"\");\n                }\n            }\n\n            this.Header.Controls.Add(control);\n        }\n    }\n}",
      "Now that we have something like this place we can use the intellisense files to create BDD stub and write our real code, which might look like the following….",
      "Spec.CreateDoc();\nSpec.Define(\n    new Functionality(\n        \"Amplify\", //name\n        \"Amplify.js is the core script that contains only centeralized functions needed constantly\",\n        function () {\n            Spec.Expects(typeof($log), ToBe.Defined, \"$log\",\n                \"$log serves as helper method for logging messages to a div \\\n                 without the need to constantly open up alert boxes.\"\n            );\n\n            // more stubs there....\n\n            Spec.ExpectsAfter(\n                function() { //the path is changed...\n                    Amplify.changePath(\"/Amplify.Js/javascripts/amplify-prototype/\");\n                },\n                [\n                    new Conditional((Amplify.Scripts._scriptPath == \"/Amplify.Js/javascripts/amplify-prototype/\"), \"the script path should be '/javascript/amplify-prototype/'\")\n                ],\n                \"When you use 'Amplify.changePath()' the new path should be set.\"\n            );\n\n            // There should be a better way of doing this.....\n            Amplify.using(\"spec-test\");\n            Amplify.invokeOnLoad(function() {\n                Amplify.Tests.newValue = \"newValue\";\n                Spec.Expects(typeof(Amplify.Tests), ToBe.Defined, \"Amplify.Tests\",\n                    \"Amplify.Tests should now be defined after Ampllify.using('spec-test') is called\"\n                );\n            });\n\n        }\n    )\n);\n\nSpec.Run();",
      "When I have the api done and finish the initial documentation, I’ll put together a code tool for helping to keep documentation and stubs in sync between the javascript files in order to help this process along.   Not to mention creating a file for the Prototype Library that Visual Studio can use, will be a beast in itself to complete.",
      "currently listening to.. Foo Fighters Echoes, Silence, Patience & Grace The Pretender",
      "Tags: .Net, Amplify, BHAG, Javascript"
    ],
    "summary_t": ""
  },
  {
    "id": "fe003cb0e532d7b5be3e6179f087597f",
    "url_s": "https://opensourceconnections.com/blog/2007/10/23/agile-californication-agile-open-conference-in-california/",
    "title": "Agile Californication (Agile Open Conference in California)",
    "content": [
      "I’m currently sitting my temporary dungeon (hotel room) trying to verbalize and regurgitate all the things from the last 2 days before I hit town one last night before reality pulls me back to the east coast.  The agile conference (http://www.agileopencalifornia.com) was definitely the very cool open spaces format, with people relatively in various parts of the software/networking field.  Much of the conference seemed to revolve around finding ways of making the agile methodology viral so that it spreads better inside of concrete business entities resistant to change (sounds almost religious).  However, that being said, just about everyone was down to earth and shared common values that included caring for the people in these companies, more so than converting someone to a methodology, which is refreshing.",
      "Some of the sessions were tapped by people who were trying to figure out this whole agile thing and how it relates to them, while others, especially on the second day, seemed to dig into the meatier types of information.  Probably my fav 2 sessions were \"agile inside a start up company\" and \"transition to agile on a large scale\".  There are many facets to both of these topics as both are company wide and it impacts everyone, including the people in the mail room to the CEO or other decision makers of the company.  The latter of the two had a lot of good insight for consultants who have a firm belief in the power of agile and how much it can benefit companies and their development infrastructure, life cycles, and even people’s moral.",
      "Probably one of the biggest concepts Ill take away from the conference is to evualuate the readiness of a company to adopt a new methodogly. As a consultant you want to be successful, so part of your system should include the ability evaluate the internal parts of a company, see what is going on first hand and then determine if what they are asking is doable and beneficital.  Yes agile will stabalize your company if done right, but timing is everything and you don’t want to do something like this at a crucial time that could make or break your company.",
      "For some insight into some of these sessions, go to   http://agileopencalifornia.com/wiki/index.php?title=Proceedings.",
      "Tags: agile, open, california"
    ],
    "summary_t": ""
  },
  {
    "id": "815afa790a133a75bbde9bf3cf7dbb12",
    "url_s": "https://opensourceconnections.com/blog/2007/10/25/user-interaction-the-dont-make-me-think-design-principle-3/",
    "title": "User Interaction & The `Dont Make Me Think Design Principle.",
    "content": [
      "The `Dont make me think design principle comes from the concept that any user interaction should be obvious to a degree that it does not require critical thinking skills to find the secret pop up menu, and presents information in such a way that a user doesnt have to decipher the information, read glyphs, or call tech support in order to understand it.  Basically, the user doesn’t have to think, because what you have designed is intuitive.",
      "Im currently Pittsburgh right now, the morning after a red eye flight, because United Airlines had violated the key concept of dont make me think design. Sure it is ultimately my responsibility, since I should have arrived at the airport earlier just in case a situation had&nbsp;arisen and&nbsp;I needed more time. The email/flight confirmation&nbsp;that&nbsp;I had received said United Flight, there was nothing about the return flight home being provided by us air at a total different terminal on the other side of the airport. In fact us air was in small print below United Flight 4264′. There was no indication that the user should log into Us Airs website or some kind of bold warning or notation that the flight had another provider. Granted this was technically my fault, most users/clients/customer are not going to be as understanding as I am. After all, `Time is money, or at least a very valued commidity that is hard to come by.",
      "In fact when on the flight up, united sent an e-mail reminder with the flight information the day before the flight. There was no such e-mail for the flight home, nor was it on the website, however there should have been some obvious indicator that would lead you to Us Airs website to let you know who in fact owned the ticket and where you could log in online in order to do an e-checkin.",
      "It is a genius ploy really. United takes the ticket money for both flights, to which it can earn interest on, then at the end of month, it pays Us Air for that passengers flight home. The bad part is, its not obvious to the end user, therefore its not a win/win for the air company.",
      "User Interaction, whether by the Interface design or by the nature of the work flow should be done in such a way that the user naturally flows and does not have to apply critical thinking skills to something that should take less time, not more time. Otherwise youre taking away from the \"value added\" part of the business, leaving yourself open to competitors to take advantage of this weakness.",
      "This theory also applies to the user interface design of an application. A user should never have to spend 20 minutes to find a button or hidden pop up menu. Everything should have a clear indication of its purpose to the point a user does not have to think, they already know what to do or can easily deduct what to do by using some kind of walk through. Communication is key, so how are you communicating to your users?",
      "Tags: Dont+Make+Me+Think, User+Interaction, User+Interface, United+Airlines"
    ],
    "summary_t": ""
  },
  {
    "id": "4ec27e751c891c1035e74a80a595f424",
    "url_s": "https://opensourceconnections.com/blog/2007/11/06/amplifyjs-pre-alpha-ready-to-play-with/",
    "title": "Amplify.JS (pre alpha) ready to play with",
    "content": [
      "http://www.codeaccessory.net/2007/11/amplifyjs-pre-alpha-ready-to-play-with.aspx",
      "If you’ve been itching to play with some javascript and visual studio 2008 (beta). I have a pre-alpha project for you to play with. The intellisense works in visual studio and there is even a set up for Ajax Doc and SandCastle Helper File Builder.  It took a bit of tweaking and testing, but here it is.",
      "https://labs.opensourceconnections.com/svn/amplify/amplify.js/trunk/",
      "currently listening to.. u2 – elevation",
      "Tags: Amplify.JS, Ajax+Doc, SandCastle, Javascript"
    ],
    "summary_t": ""
  },
  {
    "id": "809898e99de012bd53c769abf5a7c628",
    "url_s": "https://opensourceconnections.com/blog/2007/11/06/check-out-the-concept-of-the-aspnet-mvc-framework/",
    "title": "Check out the concept of the ASP.NET MVC Framework",
    "content": [
      "http://weblogs.asp.net/scottgu/archive/2007/10/14/asp-net-mvc-framework.aspx",
      "ASP.NET MVC Framework – ScottGus Blog.",
      "Looks like the rails frenzy has upped some more interesting things from Microsoft, Iâ€™ve seen posts and whispers of this since april of this year when looking up postings of MVP type style of programing in asp.net, but now it looks like its a legit project by the asp.net team. If you look at the project structure, it seems like a breed of the old asp.net websites, with a rails like folder/project structure. Itâ€™s definitely not a total copy, but at least they are supporting multiple testing frameworks, like nunit, mbunit aside from just the core testing suite from team system.",
      "currently listening to.. Mute Math Mutemath – RESET (Advance Release) Peculiar People",
      "Tags: MVC, asp.net, Model+View+Presenter, Model+View+Controller"
    ],
    "summary_t": ""
  },
  {
    "id": "33a5d40a26b552e541fd6f3ee312bdf4",
    "url_s": "https://opensourceconnections.com/blog/2006/08/04/eric-pugh-to-present-at-vptc-2007-trends-seminar/",
    "title": "Eric Pugh To Present At VPTC 2007 Trends Seminar",
    "content": [
      "Eric Pugh will be a guest panelist at the September 2006 VPTC meeting. He will be covering trends in technology that we can expect to see in 2007. More details to follow."
    ],
    "summary_t": ""
  },
  {
    "id": "ea9fc9a42631b2c477e98fd2b5a7f63d",
    "url_s": "https://opensourceconnections.com/blog/2007/11/08/multiple-field-custom-validations-in-rails/",
    "title": "Multiple Field custom validations in Rails",
    "content": [
      "I just took a spin around the bowls of ActiveRecord Validations trying to create a validator that would check if one, and only one, field of a set of fields is non blank on an ActiveRecord object. While I know I could have put the logic into an overridden validation method, I wanted to use the nice declarative syntax of validate_either_or :attribute_a, :attribute_b syntax with an eye to reusing it in the future.",
      "I thought what I could do was mimic the existing validates_confirmation_of code and iterate through each attribute looking for one that is not nil. Set a variable, and then if I hit a second nil, add an error. After cycling through the attributes, if none had been found not nil, then also add an exception:",
      "found_non_blank_already = false\nvalidates_each(attr_names, configuration) do |record, attr_name, value|\nunless value.blank?\nif found_non_blank_already\nrecord.errors.add(:base, configuration[:message])\nelse\nfound_non_blank_already = true\nend\nend\nend\n\nif !found_non_blank_already\n# currently failing to compile\n#record.errors.add(:base, configuration[:message])\nend\nend",
      "However, what I discovered was that if I have an ActiveRecord object declare like this:",
      "validate_either_or :discount_amount, :discount_percentage",
      "and call it multiple times like this:",
      "promo = Promo.new(:discount_percentage => nil, :discount_amount => 5)\npromo.valid?\npromo = Promo.new(:discount_percentage => 25, :discount_amount => nil)\npromo.valid?",
      "The second time the valid call fails validation because the line found_non_blank_already = false executed only once when the validation class is first loaded, and when you call .valid?, only the method validates_each chunk gets executed! So the second promo.valid? starts out thinking it has already hit a non null, and fails on the very first attribute! Clearly, the pattern used by most of the validate_* methods use is meant to validate each field passed in individually in isolation.",
      "I then dug into what validates_each does and duplicated a lot of what it did, ending up with:",
      "def self.validate_either_or(*attrs)\nconfiguration = { :message => \"one of #{attrs.to_sentence :connector => `or} must be set\", :on => :save }\nconfiguration.update(attrs.pop) if attrs.last.is_a?(Hash)\n\noptions = configuration\nsend(validation_method(options[:on] || :save)) do |record|\nfound_non_blank_already = false\nattrs.each do |attr|\nvalue = record.send(attr)\nunless value.blank?\n\nif found_non_blank_already\nrecord.errors.add(:base, configuration[:message])\nelse\nfound_non_blank_already = true\nend\nend\nend\nif !found_non_blank_already\nrecord.errors.add(:base, configuration[:message])\nend\nend\nend",
      "There still seems to be a lot of black magic going on, for instance, the line send(validation_method(options[:on] || :save)) do |record| works, but I cant figure out how it works, or where the validation_method comes from. If you want to test the code, here is what my specification looks like:",
      "it \"should allow either a discount_amount OR a discount_percentage, but not both\" do\npromo = Promo.new(:discount_percentage => 25, :discount_amount => 5)\npromo.should_not be_valid\npromo.errors.should have(1).error_on(:base)\n\npromo = Promo.new(:discount_percentage => 25, :discount_amount => nil)\npromo.should be_valid\npromo.errors.should have(0).error_on(:base)\nend",
      "Oh, and thanks to Jay Fields for his post on to_sentence, it made the error message look nice and pretty!"
    ],
    "summary_t": ""
  },
  {
    "id": "5e1e305ffe12b7b4a3eb445156545a68",
    "url_s": "https://opensourceconnections.com/blog/2007/12/10/eric-pugh-and-matt-jenks-present-agile-development-in-greenfield-and-brownfield-environments/",
    "title": "Eric Pugh and Matt Jenks present \"Agile Development in Greenfield and Brownfield Environments\"",
    "content": [
      "During the December 11th and 12th at the Open Technology conference sponsored by the Association for Enterprise Integration Eric Pugh of OpenSource Connections and Matt Jenks of SAIC will be speaking on Agile Development in Greenfield and Brownfield Environments."
    ],
    "summary_t": ""
  },
  {
    "id": "ce6872e2d62e005a840e2d475e11b36d",
    "url_s": "https://opensourceconnections.com/blog/2007/12/10/linked-in-finally-links-into-the-rest-of-the-world/",
    "title": "LinkedIn Finally Links Into the Rest of the World",
    "content": [
      "An article in this mornings Reuters reported that LinkedIn has finally decided to open up its closed system to the rest of the world by creating a developer API and opening itself up to Business Week. This is a critical move for LinkedIn to keep up with other social networking sites, as, previously, its information was proprietary and difficult to access. The website, a business networking site, played on the idea of six degrees of separation by showing a persons contacts and their contacts contacts, ad infinitum, but requiring a paid subscription to get beyond a second degree of separation. While it is uncertain how much revenue LinkedIn was making through paid subscriptions, the exposures and inventory that they could create through opening up their information seems to be much higher.",
      "As Don Tapscott and Anthony Williams pointed out in Wikinomics, by opening up information for repurposing, a company or website creates the opportunity to have much more exposure than it could by maintaining a closed, proprietary system. As we have previously discussed, no company can hope to keep all of its needed resources internally, and by creating developer APIs, LinkedIn has now opened itself up to the creativity of developers and consumers who want to use the information and present it in ways that LinkedIn does not, has not, and probably has not thought of.",
      "Is LinkedIn going to cannibalize its own revenue by opening itself up? Only time will tell, but I think that it will not; instead, I think that it will find its revenues dramatically increase. By creating APIs and appropriate terms and conditions for the use of the data, LinkedIn will generate many more pageviews than it has before, and the advertising revenues will far outpace its subscription revenues. Subscription revenues may indeed go up because of the increased exposure. Furthermore, more consumers will be able to use LinkedIns information, making it a more valuable brand. The stumbling block will be if the closed nature of LinkedIn has made it lose customers that it will not be able to regain, but, fortunately for LinkedIn and its users, I do not think that it is too late…LinkedIns open API will prevent it from jumping the shark."
    ],
    "summary_t": ""
  },
  {
    "id": "63ff43255546886c57e06f210ada1074",
    "url_s": "https://opensourceconnections.com/blog/2007/12/16/windows-tabbed-command-line-client-wpf/",
    "title": "Windows Tabbed Command Line Client (WPF)",
    "content": [
      "I spend much of my time in the land of windows, and for that I often hear the egotistical rantings of mac users, with such phrases as \"if you were using a mac, you wouldnt have that problem\". However, even though you could use parallels or even vmwares fushion for the mac, i still shy away from getting one and trying to do c# development on it, since I dont know too many clients ready for mono.",
      "However there is an array of tools for pc that are in line with the mac, e-texteditor which is the windows clone of textmate, launchy similar to quicksilver, and Im currently working on a tabbed wpf command line client similar to ITerm (at least in the regards of using tabs).",
      "",
      "The client works, but it still has a way to go like adding simple things like tab complete, arrow up or down for commands, etc, but i think the way to go is wpf, i can even add buttons and random things like that inside the output or color code it later, not to mention skinning or the ability to create macro commands from exiting commands. Current you can press control + T for a new tab, or control + W to close it.",
      "Also it would be nice to add opening up sql command lines or cygwin or even windows powershell in new tabs. I also want to add skinning and opacity options as well.",
      "The project is currently using part of framework, Amplify.Net, Im building for my BHAG and as soon as sourcefource and codeplex. approve me, Ill have the source code up for consumption."
    ],
    "summary_t": ""
  },
  {
    "id": "290da001ec258018ccac442a02e06c34",
    "url_s": "https://opensourceconnections.com/blog/2007/12/16/xbrl-as-the-escape-from-spreadsheet-hell/",
    "title": "XBRL As the Escape From Spreadsheet Hell?",
    "content": [
      "In a previous role, I was responsible for, among other things, creating business cases to justify future expenditures as well as for budgeting for forward fiscal years. My colleagues and I would consistently send spreadsheets back and forth with financial projections, expense forecasts, and other financial data. Usually, each user would take information from just portions of a received spreadsheet, drop it into another spreadsheet, do calculations, and e-mail the updated spreadsheet back. Eventually, version control and data integrity became issues.",
      "We have seen cases in the financial services industry where reporting of assets, returns, fees, and positions occurs through the use of a spreadsheet-based system. Often, data entry occurs in multiple spreadsheets for the same information, and then that information is converted to some other form besides a spreadsheet for consumption. These cases, particularly where financial reporting goes to consumers outside of the company, are crying out for the use of XBRL.",
      "",
      "XBRL, or eXtensible Business Reporting Language, is a language for the electronic communication financial and business information. It is a type of XML language, a standard means of communicating information between users. By tagging information with commonly used business standards, such as EBITDA, users of financial information can send that information back and forth with each other without having to define and find where in the document the information is located.",
      "The use of XBRL as a means of transmitting financial information provides several advantages:",
      "The ability to define taxonomies",
      "While certain standards such as GAAP are common, there are others which may be subject to individual definition. Having a common playbook of calculation standards, for example, will help to ensure that definitions are agreed upon beforehand and are stored in a common location which is easily referenced.",
      "Reduction of data entry",
      "By using XBRL to enter information, subsequent users of the information can come to a common repository where the data is stored and defined to get the information rather than needing to enter it into their own applications or spreadsheets.",
      "Integrity of financial data",
      "By agreeing to common definitions for both types of data and calculations as well as having a single place of entry for financial data, users have a greater likelihood of receiving acceptable data because the possibility of human error has been reduced.",
      "Consumption by other financial applications",
      "Applications ranging from EDGAR to Microsoft Excel and Access can utilize XBRL-tagged data. The same does not necessarily hold true for spreadsheets or other custom applications.",
      "The Federal Financial Institutions Examination Council issued a report on the benefits of XBRL-based reporting. The listed benefits included:",
      "Cleaner data\n  More accurate data\n  Increased productivity, and\n  Greater efficiency",
      "If your company is stuck in spreadsheet hell, not only are your employees not as productive as they could be, but the data your clients rely upon may not be entirely accurate. Perhaps its time to look at shifting to a XBRL standard before you get left behind."
    ],
    "summary_t": ""
  },
  {
    "id": "f7459605873751d66ba88c45819322a4",
    "url_s": "https://opensourceconnections.com/blog/2007/12/20/software-as-a-service-hasnt-caught-on-everywhere/",
    "title": "Software as a Service Hasn’t Caught On Everywhere",
    "content": [
      "Sramana Mitra does a great job of summarizing several blog articles and discussions about companies that have pursued software as a service (SaaS) and the Extended Enterprise (EE, or, as he calls it, EE 3.0). You can find his summary here. Interestingly, he follows up this series with a snippet of how some of the SaaS stocks have performed over the past quarter. While its hard to draw long-term conclusions in a quarter, investing in those companies a quarter ago would have yielded nice returns.",
      "What is interesting to me, though, is Oracle, while decrying software as a service, still managed to raise its year over year quarterly earnings by 35%. This says something, I believe, about switching costs, whether real or perceived. While open source solutions exist comparable to several Oracle products, users have a hard time going away from what they know. Unless an application looks, feels, and acts just like the one that they work on today, there will be some level of resistance to change.",
      "Providers of software as a service cant just expect the user to come over to a new product because the application is free or cheaper. Going with what a user knows has some value, even if it is not monetary, and a key to gaining more adoption is to show the ease of switching and the gentle learning curve. Making the transition as easy as possible (a one or zero click switch with no retraining would be ideal, wouldnt it?) and having evangelists in an adopting enterprise will increase the chances of adoption.",
      "Otherwise, as Oracles financial results show, people tend to stick with what they know."
    ],
    "summary_t": ""
  },
  {
    "id": "54e1ef1e9a5cd9ca5a6a3ec1e6ba35d9",
    "url_s": "https://opensourceconnections.com/blog/2008/01/02/is-java-the-new-cobol-2/",
    "title": "Is Java the new Cobol?",
    "content": [
      "InfoWorld has written an article declaring that Java is the new Cobol, and that its the most underreported tech story of 2007. Heres the article",
      "What do you think?",
      "Personally, most of my experience is .NET and so I agree with some of the comments in the article about .NET being certainly a language of the future and the increasingly common choice for businesses. But it seems to me that its a bit of a grandiose claim to make that Java is as outdated as Cobol. Theres plenty of room in the tech world for multiple major languages, and the clients needs, rather than some trend, should determine which one is appropriate for a project. Thats why we have expertise in many different languages and projects going in a variety of technologies.",
      "Theres plenty of room for .NET, Java, and Ruby on Rails. Cant we all just get along? ;^)"
    ],
    "summary_t": ""
  },
  {
    "id": "f0dd979ca628ed980678b3b03dbf3481",
    "url_s": "https://opensourceconnections.com/blog/2008/01/11/arin-sime-to-chair-cbic-tech-awards-committee-2/",
    "title": "Arin Sime To Chair CBIC Tech Awards Committee",
    "content": [
      "Arin Sime has been asked to chair the Tech Awards Committee of the Charlottesville Business Innovation Council. The CBIC Awards Gala is one of the three marquee events for CBIC each year, with awards going to local companies that demonstrate innovation. Last years winners are listed here. Arin will be responsible for coordinating the nomination process, finding researchers to interview companies, and coordinating the judging process. He will be responsible for reconstituting a process of selection and adding rigor to the judging process. As one of the three marquee events, and the largest fund raiser for CBIC, he has been put into a position of great responsibility and trust.",
      "Please join us in congratulating Arin on his selection."
    ],
    "summary_t": ""
  },
  {
    "id": "6d25a47f9af4e9a1ed2f5ae81fbece81",
    "url_s": "https://opensourceconnections.com/blog/2008/01/18/questions-to-ask-every-client-2/",
    "title": "Questions to ask every client",
    "content": [
      "There are certain things you should ask every client at the start of a project, in order to help make that project go smoothly. This is especially true if its your first project with a client, but also if its a new project with an existing client.",
      "Dont be lulled into a false sense of security by believing that you understand the system well enough to get started or to make a detailed estimate. If the client provides you with excellent documentation at the beginning of the project, dont let that lull you into a false sense of security either. They may have documented one aspect of the system very well, but completely forgotten about another equally important aspect because they either just forgot about it, or perhaps its such a part of their daily lives that they dont realize an outsider might not think of it.",
      "Heres my initial list of questions to ask every client at the beginning of a project – what else would you ask?",
      "General questions:",
      "What sort of user security does the application need?\n  How will the system be deployed? (ie, web farm, public or private network, some combination)\n  What other existing systems must this interface with? How do those interfaces work and can you provide sample code?\n  What administration functions will be needed to manage the website?\n  Have the end users seen your proposals for this system? How can we get them involved in the development process?\n  Who is responsible for deployment of the new system?\n  In what ways do you anticipate this system will grow in the future, and how can we build in flexibility now for future enhancements?\n  What documentation are you looking for us to write? Identify which documents need to be written up front before development (if any), and which should not be written until after the code is complete.\n  What documentation are you providing us on this system?",
      "Questions relating to a legacy system (if one exists):",
      "What is the database structure of the legacy system? What aspects of that design have worked well and what havent?\n  What aspects of the legacy system are most troublesome for the end user and have to be changed?\n  What aspects of the legacy system cannot change?\n  How will conversion from the legacy system to the new system occur? All at once, or piece by piece migration of aspects of the system?\n  Does data from the legacy system need to be migrated to the new system? Who is responsible for that?\n  Can you provide a copy of the legacy database so we can examine the data in it? If thats not possible, can you provide sample records from the legacy database?",
      "What other questions do you ask a client when starting a project?"
    ],
    "summary_t": ""
  },
  {
    "id": "c77195d74d536843cf4d805042751f8d",
    "url_s": "https://opensourceconnections.com/blog/2008/01/24/in-the-lab-amplifyactiverecord-porting-ruby-to-c-2/",
    "title": "In the Lab:  Amplify.ActiveRecord (porting ruby to c#)",
    "content": [
      "Ive been secretly working on an active record type of port as a part of the Amplify Framework. The release of the ASP.Net MVC framework along with an MVC/MVP/Rails has really pushed me in that direction. So far there is both Subsonic and Castles MonoRail which ports a significant amount of code into .Net. However a lot of Rails like paradigms are somewhat lost as everything seems to be extremely object oriented versus using strings or even api calls. (Though MonoRail does have a very cool Inflector class which is a port of methods like pluralize & humanize, etc.",
      "So basically Im prototyping using Linq as the base for making queries at this point, and plan to then use the same API to do an adapter that uses ADO.Net, and hopefully this will go hand in hand with the new ASP.Net MVC framework. If you want to play with the alpha stage bits (with no warranties of code compiling at any given time as this is a prototype) then feel free to download from the projects new home.",
      "svn checkout http://amplify-net.googlecode.com/svn/trunk/ amplify-net-read-only",
      "Below is code from what you might find in subsonic, which does have a very cool query object, but still takes more of a .Net approach, when using the API.",
      "ProductCollection products = new ProductCollection().Where(\"categoryID\",      1).OrderByAsc(\"listOrder\").Load();\nProductCollection products = new ProductCollection().Load(new Query(\"Products\").ExecuteReader());",
      "So my goal is to do more of a direct port that has methods like .New(), .Create(), .Find() and using convention over things like heavy strongly typed objects. Im probably going to make use of .Net 3.5 since the ASP.Net MVC is using 3.5. Im also going to make these objects decorated so that you can do something like the following verses using reflection or having to set/get every property.",
      "Products list = Product.Find(new Selection().Where(\"Name Like '%?'\", \"A\").SortBy(\"listOrder ASC\"));//ado.net sql\nProduct product = Product.Find(12);\nProduct product = Product.Find(\"Name = '?' AND Age = ?\", \"michael\", 28));\nProduct product = Product.Find(\"by_Name_and_Age\", \"michael\", 28));\nvar x = Product.Find(new Options().Where(\"Name.StartsWith(@0) || Name.StartsWith(@1)\", \"a\", \"b\").SortBy(\"Name DESC\"));  //using System.Linq.Dymanic\n\n// in the page_load\nds.Inserted += new ObjectDataSourceStatusEventHandler(ds_Inserted);\nds.Updated += new ObjectDataSourceStatusEventHandler(ds_Updated);\n\nvoid ds_Inserted(object sender, ObjectDataSourceStatusEventArgs e)     {\n    list.Add(Person.Create(e.OutputParameters));\n}\n\nvoid ds_Updated(object sender, ObjectDataSourceStatusEventArgs e)     {\n    Person person = GetPerson();\n    person.Merge(e.OutputParameters); //IDictionary\n    person.Save();\n\n    person[\"Name\"] = \"Michael\"; // or\n    person.Age = 28;\n    person.Save();\n\n    person.Merge(new Hashtable() {\n        {\"Name\", \"Bob\"},\n        {\"Age\", 10}\n    });\n    person.Save();\n}\n\n// or in the new MVC framework\n// <a href=\"http://weblogs.asp.net/scottgu/archive/2007/12/09/asp-net-MVC-framework-part-4-handling-form-edit-and-post-scenarios.aspx\">click here to see what MVC looks like with linq</a>\n[ControllerAction]\npublic void Create()      {\n    Person.Create(Request.Form); //IDictionary\n    RedirectToAction(new { Action = \"Category\", ID = production.CategoryID});\n}",
      "So you can see above that the objects have both strongly type properties and a dictionary style accessors as well so you could easily map objects using a foreach statement (which the .Merge will do) versus having to use reflection, which is also true when pulling data from a datareader.",
      "Other goal will be to make the objects utilize Linq and replace the code that SQLMetal currently generates, so designing Linq compatibility and working on a code generator is also in the works. Since Im using .Net 3.5, I can abuse static methods in order to make writing CodeDom a lil easier."
    ],
    "summary_t": ""
  },
  {
    "id": "306fa54de10b27745bddab03a9bfb839",
    "url_s": "https://opensourceconnections.com/blog/2006/08/04/why-open-source-works/",
    "title": "Why Open Source Works",
    "content": [
      "Last night I experienced again why open source works. I was beating my head getting sorting to work in Ruport (Ruby Reporting Package), and instead of opening a support ticket I posted to the mailing list:",
      "http://lists.stonecode.org/pipermail/ruport-stonecode.org/2006-August/thread.html#227",
      "`Nuff said."
    ],
    "summary_t": ""
  },
  {
    "id": "0a39da6c2bb985a96005d067027dec15",
    "url_s": "https://opensourceconnections.com/blog/2008/01/28/5-ways-open-source-benefits-the-government-2/",
    "title": "5 Ways Open Source Benefits the Government",
    "content": [
      "My partner Eric Pugh recently spoke at the Association For Enterprise Integrations 3rd Department of Defense Open Conference. He and Matt Jenks from SAIC covered the topic \"Best Practices in the Use of Open Source In Government.\" While it is, in my opinion, good that the government is moving towards embracing open source (Jim Laurent of Sun notes: \"Its clear from attending this conference again (this is my third time) that there is no avoiding the use of open source tools in the Federal Government. Whether it is something as simple as glassfish and openssh or more advanced technologies like the UltraSPARC T1 and T2 processors, open source is everywhere in the DoD.\"), I was surprised at the relative lower level of discussion in most of the presentations, defining what open source is and is not, etc. The chasm between the government and open source community was most apparent to me when Jim Stogdill of Gestalt asked the audience how many people had attended OSCON or a BarCamp, and only a couple of hands aside from Gestalt representatives went up.",
      "The tone of most of the questions in the audience during the keynotes was focused on what the open source community could (with the tone hinting towards should) do for the government. The only way for the government to get engagement from the open source community is for the government to engage the open source community first. Otherwise, it is missing the motivation of the open source community completely.",
      "Here are five ways that the U.S. -government could benefit from greater adoption of open source technologies:",
      "Reuse Utilizing open source platforms represents being able to leverage hundreds, thousands, and oftentimes millions of man hours of development work to build from. Rather than needing to build everything from scratch, oftentimes, by using open source platforms, software development projects become more focused on bolting together packages and then customizing them to achieve the end result. We make the analogy to building a house. In building a house, the cheapest labor is found working on framing and foundation work. While crucial to the success of the house, the art does not lie in the basic infrastructure. The distinction and differentiation to a house is found in the carpentry and finish work, where the artisans have the most impact in the outcome of the house. The same holds true for software development. Most projects can piece together the infrastructure from existing open source projects, leaving the artisans to do the custom development to build on the deployed infrastructure. There is no need to pay high-cost developers and consultants to build the basics, particularly when they already exist and are tried and true.\n  **Auditability **With proprietary systems, the buyer has to assume that the product does what the vendor says its going to do because there is no way, aside from extensive post-purchasing use case testing, to ensure that it will do 100% of the things that are promised. While contract law does provide some protection, it does not regain lost time. With the use of open source code, though, the recipient can completely audit the functionality of what is being delivered instead of opening up a .exe file and hoping for the best. Perhaps this would be a good use of the SETA`s time?\n  InnovationThe United States has maintained a competitive economic and military edge because of innovation. However, as Jeff Jarvis reports from Davos, the rest of the world is catching on and catching up. One of the commenters in the innovation discussion in Davos noted that the open source community has been robust, but not particularly innovative, while the U.S. Department of Defense has been, historically, the cauldron of innovation. Open sourcing allows the creation of a fire brigade, where people take ideas, add a little, and pass them on. By taking the sparks of the best ideas that come from the government and open sourcing them, the path to commercialization and incremental improvement becomes much easier.\n  More eyes on the problem should mean fewer bugsAn inverse correlation exists between the number of bugs and the number of eyes looking at code. Even the U.S. government, as big as it is, cannot command the resources of the world, and, therefore, cannot ensure that all of its homegrown code is bug free. The more people who play with code, look at it, and test it, the more likely it is that the bugs will appear…and be fixed.\n  SecurityIt is very hard to hide a backdoor in code that is open. While exposing code which was once proprietary does open it up to greater vulnerabilities in the short term, in the long term, because the code is visible to everyone, so are the intrusion attempts. David Wheeler writes an excellent article on the inherent security improvements of open source. The only true way to guarantee full security is to see the code itself to verify that no backdoors or vulnerabilities exist.",
      "In no way do I promote the notion that the government should open source everything, nor should it open source the things which it could stand to gain in a wily nily manner. Just as in commercial enterprises, it should keep proprietary the things which give it a competitive advantage, although it could certainly open source those to a closed community. However, it should look much more closely into open sourcing the \"infrastructure\" pieces which do not add a competitive advantage. If the government currently pays for a piece of functionality which it would not care about if a foreign government used the same vendor, then it should look at open sourcing. It might even be able to grab best practices from elsewhere, including other governments."
    ],
    "summary_t": ""
  },
  {
    "id": "fae51fbd0f9d4d1cec54e0e40fffc5dc",
    "url_s": "https://opensourceconnections.com/blog/2008/01/29/what-does-suns-mysql-acquisition-mean-for-government-it-usage/",
    "title": "What Does Sun’s MySQL Acquisition Mean For Government IT Usage?",
    "content": [
      "On January 16, Sun announced the pending purchase of MySQL AB, the company which owns the MySQL database. While MySQL is quite popular among developers, it has yet to gain significant traction in the enterprise level community.",
      "One market where this is very apparent is in the government IT sector. Oracle databases, and appurtenant products and service contracts and vendors, rule the roost. One simple metric of measuring this success is to search current and active postings on the government€™s procurement clearinghouse, FedBizOpps. A simple text search for Oracle under the Procurement Classification Code D (Information technology services, including telecommunications services) of both active and archived documents yields 814 results, while the same search for MySQL yields just 21 results.",
      "",
      "This is mainly due to an entrenched and embedded sales force that Oracle deploys to the Beltway. The sales force implies, rightly so, a deep team that is capable of providing as-needed support for the non-power users, which describes the vast majority of government users. MySQL, on the other hand, has limited support and does not have the bench to even begin to propose to the government that it could offer the support that the government desires.",
      "Sun, on the other hand, has both a deep sales force and a deep support bench. The Sun sales force can begin making the pitch on first day after the closing of the acquisition that, among other things:",
      "The code for MySQL is free since it is open source, so the cost is in support, rather than in the licensing for the out of the box product that is required to use Oracle. MySQL is cheaper (and probably by close to an order of magnitude) than Oracle whether paying just for support, or, even more advantageously, if the MySQL expertise is internal\n  \n  \n    MySQL is more commonly known and used in the broader development community than Oracle\n  \n  \n    MySQL will run more efficiently on all of those Sun machines than Oracle will\n  \n  \n    Oracle has one vendor and a lot of authorized resellers. MySQL has a vastly greater potential vendor pool",
      "In the long term, this acquisition, if executed astutely, should break the stranglehold that Oracle, and to a lesser extent, Microsoft SQL Server, have on the government market."
    ],
    "summary_t": ""
  },
  {
    "id": "edf45a4f3af702c1d7e3ae27ba88c95d",
    "url_s": "https://opensourceconnections.com/blog/2008/01/30/4-ways-to-help-the-government-adopt-open-source-technologies/",
    "title": "4 Ways To Help the Government Adopt Open Source Technologies",
    "content": [
      "Bob Sutor, Vice President of Open Source and Standards at IBM, issued a challenge to the open source community for 2008. One of his ten challenges to the community was:",
      "\"Help governments adopt free and open source-friendly IT policies that permit maximal apples-to-apples comparisons of FOSS and proprietary software with regard to relative value for total cost of ownership, local business generation, and innovation of technology for the social good.\"",
      "It is difficult to convince anyone of an apples to apples comparison when a proponent, in this case, the sales forces of proprietary software, sits at the ear of the decision-maker issuing a stream of \"yes, but\" rebuttals.",
      "Thus, in order for the FOSS community to convince the government of its value, it will need to take several steps to level the playing field.",
      "**Find champions who are respected in the community of government decision-makers **The vendors of proprietary software already have those champions and they are already within the decision cycle. Few open source systems have such a champion. It will likely take a consortium of the major players (IBM is already doing a good job in leading the charge, and others can join in) in one unified voice to make significant headway.\n  **Speak the language of the decision makers **The most common rallying cry for making comparisons to proprietary software is total cost of ownership. The story neither begins nor ends with TCO. Issues like security, reliability, and support weigh heavily on the minds of CIOs and CTOs in the government, and often, they are more important than cost.\n  Make it easy for those in the government to become educated about FOSSThere is still a significant amount of ignorance about what FOSS actually means. The perception is that open source means anyone can get into the code, make changes, and run gleefully away snickering about the great virus they just inserted. Education about what open source software is and the process of open source projects will help decision makers see what they are and they arent getting. Another audience to target is the government legal community. A myriad of licenses exist, although certainly no more than the vast array of EULAs the government agrees to in buying proprietary software licenses, so educating government lawyers on the obligations of these licenses will lower the barriers to entry for the decision makers.\n  Engage an honest brokerEveryone has an agenda. It is inevitable. FOSS proponents will want to push FOSS at the expense of proprietary vendors, and vice versa. Its what salespeople are hired to do. Instead, look to an organization like MITRE, which states as one of its missions: \"applying emerging, state-of-the-art technologies to real-world problems.\" MITRE seeks the best possible outcome for the government, not a vendor, so getting the government to engage an organization like MITRE to create the comparison criteria will most likely result in a framework which the government can and will trust.",
      "Just as with any decision maker, unless an objective set of criteria exist, the government is probably going to go with the vendor that has the best sales team. Given that sales teams for open source projects are not numerous, the best alternative, as Bob Sutor has pointed out, is for the open source community to help develop a set of objective criteria to make equivalent comparisons between FOSS and COTS solutions."
    ],
    "summary_t": ""
  },
  {
    "id": "b4d334a728792f6826eba53bae9b7ecd",
    "url_s": "https://opensourceconnections.com/blog/2008/02/01/other-governments-adopt-open-sourceis-the-us-government-missing-the-boat/",
    "title": "Other Governments Adopt Open Source…Is the U.S. Government Missing the Boat?",
    "content": [
      "When we presented at the AFEI DoD Open conference in November, we noticed that there was a lot of talk about the Department of Defense adopting open source technologies. Having not attended DoD Opens 1 and 2, I cannot say if there was progress towards actual adoption rather than a trend of continued talk and little action, but I was surprised at the basic level of education that seemed to need to still occur.",
      "Comparatively speaking, it seems that perhaps the U.S. government is behind the times in open source adoption. Australian agencies view adoption of open source as a positive trend. The Dutch government will adopt open source software as a standard by April 2008. The government of VietNam has \"recommended government agencies use open source software when using state moneys to implement information technology projects.\" Local governments in Europe are moving to open source, as they realize the constraints of licensing, including this example of an expiring license in Thanet (thanks to the article \"Open Source For Local Government Debate\" for pointing out the example).",
      "As John Weathersby of the Open Source Software Institute points out in this interview, federal government adoption of open source software has a positive trickle down effect on state and local governments, but the flow is still just a trickle. The adoption is not as vibrant as the FOSS community believes it should be, and only now are many of the open source initiatives ramping up their efforts to expose the benefits of FOSS to the government (and in particular, the Department of Defense).",
      "Why are smaller governments seemingly ahead of the game in open source adoption when the U.S. government lags behind? I believe the comparisons between small and large businesses are apt here:",
      "Open source technologies have a lower total cost of ownership  When a small business or government has to make purchasing decisions, then opportunity costs weigh in less heavily than actual costs. In the case of small governments, limited revenues means having to forego the big iron system and try to find a cheaper equivalent. Oftentimes, open source platforms and projects make this choice possible.\n  Smaller organizations are usually more nimble than larger organizations  With fewer layers of bureaucracy to get through, smaller organizations can more quickly adopt new and changing technologies. When a new open source project comes out or starts to gain adoption, a smaller organization can move to it more nimbly and widespread adoption becomes much easier.\n  Vendor lock  Larger organizations get the attention of the sales representatives who then push for long-term licenses and contractual agreements. Smaller organizations do not get such attention, and, therefore, have a lower switching cost because the effort of retaining those customers does not justify the benefits. Furthermore, as organizations use more and more non-proprietary solutions, the vendor lock issue becomes equivalently reduced.\n  A lack of resources to invent internally  Smaller organizations cannot dedicate staff and resources to build home-grown solutions to their needs. They must adapt and adopt because of the costs required to build something internally. Thus, from an earlier time, they are more comfortable with the notion of open source systems and are not as reliant on external vendors to provide packaged solutions.",
      "So, is the U.S. government missing the boat? Not yet. It is, after all, a large organization with vast resources. However, over time, failure to adopt will mean that the U.S. has to pay incrementally more for the same outcomes that others get through the use of FOSS. In a time when U.S. hegemony is starting to come under challenge from other growing economies, the government can ill afford to be inefficient.\n."
    ],
    "summary_t": ""
  },
  {
    "id": "464cdbd152cef3b958c9d274ce4319b1",
    "url_s": "https://opensourceconnections.com/blog/2008/02/04/come-visit-opensource-connections-at-fose-2008/",
    "title": "Come Visit OpenSource Connections At FOSE 2008",
    "content": [
      "OpenSource Connections will be exhibiting at FOSE 2008. Come visit us at Booth number 2741. Wed love to speak with you and find out how we could help you with your software development and open sourcing needs.",
      "",
      "What is FOSE? FOSE is the most comprehensive and important event for government technology professionals. With over 20,000 attendees, FOSE covers managing and implementing technology and utilizing technology to improve government operations. If youre going to be attending FOSE, let us know!"
    ],
    "summary_t": ""
  },
  {
    "id": "c53d493934a4373e215a2b1bc4ae523c",
    "url_s": "https://opensourceconnections.com/blog/2008/02/04/open-source-vs-proprietary-software-in-the-federal-government/",
    "title": "Open Source vs. Proprietary Software in the Federal Government",
    "content": [
      "Since I started with OpenSource Connections in July 2007, I have seen what an uphill battle it can be to get the various federal agencies to even consider an open source solution, let alone request one via a solicitation. This was somewhat surprising to me; during my five years with Microsoft, we were convinced that open source solutions like Linux were eating \"our\" lunch. On the other hand, it is not at all surprising given the marketing machines that back Big IT firms like Microsoft, Oracle, and Dell. Open source solutions with their shoe string marketing budgets struggle just to enter the federal market competition. That said, there are a lot of small IT consulting firms like mine who are putting up a good fight.",
      "To document any changes in this space, I have decided to track the results. Each week I will scan the solicitations, synopses and sources sought on Federal Business Opportunities (aka \"FedBizOpps\") the governments central repository for all procurements over $25,000. Each week, my methodology will be to run a series of queries with the following parameters:",
      "Dates: the previous week\n  NAICS Codes: 541511 – 541519 (essentially all the computer-related NAICS codes)\n  Presolicitation, Combined Synopsis, All Synopsis, and Sources Sought",
      "I will then query that pool for the total number of postings, and compare the total to how many mention open source keywords by name (Apache, MySQL, PostgreSQL, Ruby, PHP, Java, Spring, Tapestry, Hibernate, Grails, Jboss, Liferay, Linux). Finally, to remove some of the human-error factor, I will scan the results for duplicates. (ie: FedBizOpps returns multiple results tied to a single project, such as modifications or amendments to the original project which have not substantially changed the original synopsis.) Enough about my methodology. Onto the results for the first week: January 27th – February 2nd, 2008. Of a total 57 solicitations, synopses and sources sought posted last week, nine mentioned either Microsoft or Oracle by name. Only six mentioned any of my 13 open source terms. A single entry accounted for five hits across the spectrum of my open source terms. (The Department of Health and Human Services is looking for someone to contract as a Senior IT Architect.) It also returned a hit for both Microsoft and Oracle. Well, at least DHHS is looking for a well-rounded individual. This gives a total of 10.5% of last weeks postings contained an open source term. Lets see if we can do better next week. I invite comments and suggestions regarding my methodology and keywords.",
      "1/27-2/2 \n    \n  \n\n  \n    \n      Microsoft \n    \n\n    \n      5 \n    \n  \n\n  \n    \n      Oracle \n    \n\n    \n      4 \n    \n  \n\n  \n    \n      Apache \n    \n\n    \n       \n    \n  \n\n  \n    \n      MySQL \n    \n\n    \n      1 \n    \n  \n\n  \n    \n      PostgreSQL \n    \n\n    \n       \n    \n  \n\n  \n    \n      Ruby on Rails \n    \n\n    \n      1 \n    \n  \n\n  \n    \n      PHP \n    \n\n    \n       \n    \n  \n\n  \n    \n      Java \n    \n\n    \n      2 \n    \n  \n\n  \n    \n      Spring \n    \n\n    \n       \n    \n  \n\n  \n    \n      Tapestry \n    \n\n    \n       \n    \n  \n\n  \n    \n      Hibernate \n    \n\n    \n       \n    \n  \n\n  \n    \n      Grails \n    \n\n    \n       \n    \n  \n\n  \n    \n      Jboss \n    \n\n    \n      1 \n    \n  \n\n  \n    \n      Liferay \n    \n\n    \n       \n    \n  \n\n  \n    \n      Linux \n    \n\n    \n      1 \n    \n  \n\n  \n    \n      Total \n    \n\n    \n      57 \n    \n  \n\n  \n    \n      OS Total \n    \n\n    \n      6 \n    \n  \n\n  \n    \n      Open % \n    \n\n    \n      10.53%",
      "",
      "",
      "",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "f6d028af39bdb1bde443fb8fbfa41cc2",
    "url_s": "https://opensourceconnections.com/blog/2008/02/04/the-transportation-security-administration-may-be-hijacked-by-web-20/",
    "title": "The Transportation Security Administration May Be Hijacked By Web 2.0",
    "content": [
      "For those of you who have griped about a long line for screening at an airport, or, alternatively, have been pleasantly surprised at the swift passage through the screening system, the Transportation Security Administration has now opened up a blog site for people to voice their opinions. The TSA seeks to create a two-way dialog with the public by opening up the system for comments. By doing this, though, the Transportation Security Administration has now opened itself up to a potential flood of complaints.",
      "Giving your users a forum for complaints is certainly an appropriate means of communicating with them and seeking feedback; however, such an open and minimally unmoderated forum opens the TSA up to the dangers of being forced to respond to relatively minor complaints and having its agenda hijacked by a determined few who have specific issues that they wish to push.",
      "We experienced the same social networking problem when we developed HealthStories in conjunction with MITRE. The idea behind the site was to compile patients experiences at hospitals to be able to determine trends and to get a batch of seminal stories to take to the Health and Human Services Administration to identify where technology could assist hospitals in delivering better health care. What we quickly discovered was that we needed some mechanism to determine the legitimate complaints as opposed to the people who had an axe to grind. We introduced a rating system to allow other users to vote on the relevance of contributed stories. With such a rating system, the greater community would be able to more quickly hone in on the most relevant issues of the hospital experience.",
      "The TSA has begun the first step towards making the blog more successful–moderating the comments. In an ideal world, the blog would be unmoderated, but if that were the case here, the website administrators and bloggers would be swamped in trying to deal with agendas and answering comments which do not justify responses. Now, they can choose the relevant and salient comments and respond to them.",
      "The TSA, though, must be careful in its moderation lest it be accused of selection bias. It has to make sure to give voice to all sides and to respond appropriately to the warts as well as the beauties lest the commenting (and the interactive aspect of the blog) become useless.",
      "It must also make sure that the focus of the blog is to communicate the behind-the-scenes thinking regarding the TSA and reasoning why people have to suffer the inconveniences that they do. In order to do that, it needs to have bloggers with some authority. The current blogger list does not inspire me with confidence that they speak for the TSA or have insider insight that can help the public to understand whats happening in the back rooms. Bloggers gain credence by establishing credibility and by being experts. Perhaps if the bios focused on more things than \"I like ice cream,\" people would tend to respect the writing.",
      "The idea is a good one. The government can use the Internet to become more responsive to its constituents. However, it needs to have voices of authority doing the interaction, and the government needs to be very careful in managing the interactions to ensure that a very small but vocal minority doesnt hijack the entire conversation.",
      "[Edit: February 5, 2008 9:21 AM] Jake McKee gives a great list of actions the TSA should take to make the site more user-friendly.  Were in the midst of redesigning the Charlottesville-Albemarle Airport Authoritys website, and this article provides a great set of guidelines for us to think about as well."
    ],
    "summary_t": ""
  },
  {
    "id": "8de7ba251e3eba10dae70212e760579a",
    "url_s": "https://opensourceconnections.com/blog/2008/02/05/jim-nist-will-at-citcon/",
    "title": "Jim Nist will attend CITcon",
    "content": [
      "Jim Nist will be among a select group of participants at the North American edition of the Continuous Integration and Testing Conference in Denver April 4th and 5th. CITcon is the premier conference focusing on CI and Testing, but last year also covered a lot of ground discussing topics such as applying Agile development techniques and dynamic versus statically typed languages."
    ],
    "summary_t": ""
  },
  {
    "id": "24597de6dc5874a13c6cee8de2314775",
    "url_s": "https://opensourceconnections.com/blog/2008/02/05/myspace-latest-to-realize-innovation-and-widgets-cant-always-be-internally-developed/",
    "title": "MySpace Latest To Realize Innovation And Widgets Cant Always Be Internally Developed",
    "content": [
      "Social networking giant MySpace announced today that it would allow third-party developers to build applications on its wildly popular website. While not completely open sourcing the platform, it will allow APIs, JavaScript, MDPs, and will utilize \"Kaja,\" a Google-developed technology to prevent vulnerabilities in code, particularly JavaScript.",
      "MySpace seems to be reacting to the wave of opening up software development kits that its competitors, such as Second Life and Facebook, have been trending towards. While its impossible to say that opening up itself for SDKs aided Facebook, the traffic growth has been noteworthy since May, when it allowed third party applications to be developed.",
      "",
      "Looking back further, we can see that the rate of growth increased around that time.",
      "",
      "MySpace sees that keeping proprietary applications isnt the key to success; its building a solid foundation and then letting others build and grow that foundation. This should be a boon for MySpace and traffic should subsequently increase as a result of opening itself up to allow others to build on the platform. Its a path that Google, Facebook, and Second Life have all gone down successfully; the biggest question is not if it will succeed, but, rather, why it took so long."
    ],
    "summary_t": ""
  },
  {
    "id": "aa3b1226b64612562526f98100790a3b",
    "url_s": "https://opensourceconnections.com/blog/2008/02/05/ruby-like-hash-in-c-sharp/",
    "title": "Ruby-like Hash in C# (sharp)",
    "content": [
      "One of the things I like about ruby is that you get more done with less typing, and though c# is a powerful language, Ive been wishing for a ruby like hash and even array initializers. Ive been spending some time doing an ruby on rails active record port to c#, and lately ive been porting over the rails migration ( I even have a prototype working with SQL 2005). However, one thing heavily missing from the c# recipe is the Ruby like Hash for doing migrations. So after some research on the web, I found Alex Henderdon & Andrey Shchekins ruby like hash function for c#.  However this didnt exactly take this concept all the way as it could have, so after some tweaks to a Dictionary>string, object< class, simple ruby like hash for C# now exists and works fairly nicely, as you can see from the code example below. (Taken from the tests project).",
      "<",
      "p>",
      "Hash h = Hash.New(Name => \"Michael Herndon\", Age => 10, Url => \"www.myspace.com\");\n     h[\"Name\"].ShouldBe(\"Michael Herndon\");\n     h[\"Age\"].ShouldBe(10);\n     h[\"Url\"].ShouldBe(\"www.myspace.com\");",
      "Ive also packed up Andrew Peters Inflector.Net using C#s mixins. Infector.Net is based off of the class rail uses to do things like camelize, pluralize etc. Also Ive put together some other rails like mixins like .Gsup, .Merge, .Each. Collect, .Join, for people to enjoy. If you want to take a peak at code, its on the amplify-net googlecode project inside the folder Framework/Amplify.Linq folder.",
      "svn checkout https://amplify-net.googlecode.com/svn/trunk/ amplify-net\n\n     url: <a href=\"http://code.google.com/p/amplify-net/\">http://code.google.com/p/amplify-net</a>",
      "Tags: Ruby, Ruby+Hash, C#, C+Sharp, Inflector.Net, Amplify, Linq, Lambda, Amplify.Net"
    ],
    "summary_t": "One of the things I like about ruby is that you get more done with less typing, and though c# is a powerful language, I’ve been wishing for a ruby like hash ..."
  },
  {
    "id": "ae5c5401230d54426e8b07c1401900f3",
    "url_s": "https://opensourceconnections.com/blog/2006/08/09/mandatory-upgrade-of-rails-115/",
    "title": "Mandatory Upgrade of Rails 1.1.5!",
    "content": [
      "Saw a post on Riding Rails about an extremely dangerous security hole in Rails. Read the full warning from DHH here.",
      "For me to update the software for this blog meant:",
      "sudo gem update rails\ncap restart",
      "See you at RubyJam tonight!"
    ],
    "summary_t": ""
  },
  {
    "id": "ce9cf268112184914c564885ca17d8a5",
    "url_s": "https://opensourceconnections.com/blog/2008/02/05/whats-your-favorite-podcast/",
    "title": "Whats your favorite podcast?",
    "content": [
      "As part of some research for a potential project involving podcasting, Id like to know what your favorite podcasts are. Do you have certain podcasts you regularly listen to? Are they using a unique format or application that you havent seen in other podcasts? Id like to check out other peoples favorites so I can consider their applications and delivery methods for this potential project. Im also interested in the ways that podcasts interact with their listeners and guests, and improving that link if possible. Personally, I listen to several podcasts pretty regularly now while I work:",
      "Adam Currys Daily Source Code\n  .NET Rocks\n  World Soccer Daily",
      "In the Charlottesville area, we also have several interesting local podcasts. Sean Tubbs built the Charlottesville Podcasting Network which includes podcasts of a number of great radio shows in our area such as \"Charlottesville-Right Now\" with Coy Barefoot Brian Wheeler and Sean Tubbs also do a lot of podcasting of area government meetings through the website Charlottesville Tomorrow which are interesting regardless of your political perspective on the issues discussed at those meetings. Theres also a number of sites that list the most popular podcasts out there: Podcast Alley iTunes Top Podcasts of the Day (scroll down to see it) PodFeed Top 100 Whats on your ipod or internet stream? What unique podcast applications are you aware of? <div class=\"bjtags\">\n  Tags: podcast\n</div>"
    ],
    "summary_t": ""
  },
  {
    "id": "a9c0183374a486469e5302516ace1a6e",
    "url_s": "https://opensourceconnections.com/blog/2008/02/06/commonwealth-scholars/",
    "title": "Commonwealth Scholars",
    "content": [
      "On February 20th, Iâ€™ll be among 15 other area professionals presenting to 8th graders at Buford Middle School in Charlottesville, as part of the Commonwealth Scholars program. This is the Virginia version of a nationwide program to encourage rising high school students to consider taking a rigorous course load to better enable to them to get into college or be more prepared for the global workforce.",
      "Basically, each of us presenting will go to a classroom that day and give a standard power point presentation provided to us, but interjecting our own stories and experiences along the way, and taking questions from the kids. This program is in its second year in Virginia, and is being presented to 8th graders at all the middle schools in Charlottesville, Albemarle, and some other area counties.",
      "It should be fun and Iâ€™m looking forward to the 20th!"
    ],
    "summary_t": ""
  },
  {
    "id": "7ca38ad21d5e2385957ccaa9c3d02a9d",
    "url_s": "https://opensourceconnections.com/blog/2008/02/10/css-fury-part-1-the-starasterisk-selector/",
    "title": "Css Fury : Part 1, The * (star/asterisk) Selector",
    "content": [
      "CSS 2.1 which is supported by Firefox, safari, and almost completely supported by IE 7, has what is known as the selector syntax. However IE 6 only supports a minimal amount of the CSS selectors, with that being said, you should use selectors wisely in such a way that degrades gracefully in older browsers.",
      "Even though CSS have been around for a while, there are people are still using tables for layout design. However the ones forwarding the web, with use of a div layout, are finding new ways and combinations of CSS which results in very cool results. For instance, zeroing out every element on the page with a few lines of code. This does work in IE 6.",
      "* {\npadding: 0;\nmargin: 0;\n}",
      "The reason this is so significant is that browsers have different predefined padding, margins for various elements like h1, p, blockquote, ul, li etc. Even IE6 has that evil 3 pixel space between divs. What the above code does is zero the padding and margin on every element for the entire page, thus starting all browsers elements on common ground and forcing you to design a page that has a cross browser look and feel by setting the padding and margins manually.",
      "The * selector (known as the universal selector) matches any single element in the DOM (document object model). So lets say you want to select all spans, even grandchildren inside of a div. It would look like the following.",
      "div * span {\nbackground: #ff0000;\n}",
      "If you use CSS technically youre, already using the universal selector since when you type just a class or id, it implies the * before it. So when you see something like the following…",
      "span {\nbackground: #ff0000;\n}\n\n.error {\nbackground: #ff0000;\n}\n\n#error-message {\nbackground: #ff0000;\n}",
      "What the parser really sees is ….",
      "* span {\nbackground: #ff0000;\n}\n\n* .error {\nbackground: #ff0000;\n}\n\n* #error-message {\nbackground: #ff0000;\n}"
    ],
    "summary_t": ""
  },
  {
    "id": "13bb820db0041ab96cc058ca90892d3a",
    "url_s": "https://opensourceconnections.com/blog/2008/02/14/agile-documentation-and-the-government/",
    "title": "Agile Documentation and the Government",
    "content": [
      "Since I am posting this on Valentines Day, its appropriate that Ive chosen to discuss one of a developers great loves: Documentation.",
      "Okay, so the truth is most developers really dont enjoy writing documentation. Its much more fun to write code. And Agile development methods discourage writing a lot of documentation, with very good reason. Agile focuses on \"Working Software over Comprehensive Documentation\".",
      "In \"Agile Principles, Patterns, and Practices in C#\", authors Robert Martin and Micah Martin declare:",
      "\"Martins First Law of Documentation: Produce no document unless its need is immediate and significant.\"",
      "Scott Ambler has written a very interesting, and dare I say, big document about \"Agile/Lean Documentation: Strategies for Agile Software Development\". He points out that theres nothing wrong with doing documentation, even in an agile environment, and that sometimes it is the best way to communicate with external parts of your team or clients who you dont see every day. But he makes a long list of critical points that emphasize your project should not be driven by documentation. Some of my favorites from his article:",
      "* The fundamental issue is communication, not documentation. * Documentation should be concise: overviews/roadmaps are generally preferred over detailed documentation. * The benefit of having documentation must be greater than the cost of creating and maintaining it. * Ask whether you NEED the documentation, not whether you want it. * Take an evolutionary approach to documentation development, seeking and then acting on feedback on a regular basis.",
      "Perhaps the most important point he makes, and agile methodologies emphasize, is that \"Comprehensive documentation does not ensure project success, in fact, it increases your chance of failure.\" Amblers point seems to be that you should spend your time refactoring your code, and providing a solid test suite so that you have executable requirements and your code can be easily verified.",
      "Thats all true, and I would also add that excessive documentation can lead to project failure especially if you spend too much time up front on a project writing documentation about how youre going to eventually write your code. I believe thats what Ambler is getting at when he says you should \"Document stable things, not speculative things.\"",
      "However, there are situations where a lot of documentation is required, and so we need to consider how to integrate that with Agile environments.",
      "One simple situation is when you are a consultant on a project, and the client does not have a lot of in house expertise in the technology you used for the project. Agile methodology says you should focus on communication within your team and that is more effective than documentation. That is the ideal situation when there are other technical people to communicate with, but especially as a consultant, your clients will often require certain documents so they can keep the system going after you leave.",
      "Another situation is working with government agencies. Some agencies are going to require certain list of documents be written for every project. When a corporate client makes a requirement like that, you may be able to convince them not all of it is necessary. But in the case of some government agencies, they may be required by law or an oversight agency to always produce certain documents.",
      "In those situations, all you can do as a consultant is to maximize your efficiency in development while still meeting their documentation requirements. Following these tips may help when you have to integrate large volumes of documentation into an Agile project:",
      "At the beginning of the project, find out from the client exactly what documentation they require\n  Go through the list and identify which ones will be helpful early in the project and will facilitate communication with the client before code is written. For example, a system overview diagram or high level use case may be helpful to ensure everyone understands the high level operations you are trying to achieve with the code. But there is not much value in coming up with a detailed class diagram before you write iterative code which would instantly make that diagram obsolete.\n  Keep the up front critical documentation to a bare minimum (a couple of days maximum), and quickly get to the point of writing and demoing even very basic code to the client.\n  Save the other documentation which the client requires until the end of the project, and if possible until after your first release to their customers. Then you can easily create a document like a class diagram without having to worry about the code changing as much."
    ],
    "summary_t": ""
  },
  {
    "id": "8a3042e190286a5ad42a61dad7eb71d5",
    "url_s": "https://opensourceconnections.com/blog/2008/02/14/open-source-vs-proprietary-software-in-the-federal-government-week-2/",
    "title": "Open Source vs. Proprietary Software in the Federal Government- Week 2",
    "content": [
      "The results for the week February 3rd-9th, 2008 are unfortunately worse than last weeks. Of a total 51 solicitations, synopses and sources sought posted last week, five mentioned either Microsoft or Oracle by name. Four mentioned my open source terms. Again, the Department of Health and Human Services Senior IT Architect contractor job opening accounted for most of the open source hits, as well as both Microsoft and Oracle hits. This means only 7.8% of last weeks postings contained an open source term. However, we could look on the positive side and say that across all the solicitations, synopses and sources sought, proprietary software only got one more mention than open source. Well see where this trend takes us next week. As always, I invite comments and suggestions.",
      "1/27-2/2 \n    \n\n    \n      2/3-2/9 \n    \n  \n\n  \n    \n      Microsoft \n    \n\n    \n      5 \n    \n\n    \n      3 \n    \n  \n\n  \n    \n      Oracle \n    \n\n    \n      4 \n    \n\n    \n      2 \n    \n  \n\n  \n    \n      Apache \n    \n\n    \n      0 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      MySQL \n    \n\n    \n      1 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      PostgreSQL \n    \n\n    \n      0 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      Ruby on Rails \n    \n\n    \n      1 \n    \n\n    \n      1 \n    \n  \n\n  \n    \n      PHP \n    \n\n    \n      0 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      Java \n    \n\n    \n      2 \n    \n\n    \n      1 \n    \n  \n\n  \n    \n      Spring \n    \n\n    \n      0 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      Tapestry \n    \n\n    \n      0 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      Hibernate \n    \n\n    \n      0 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      Grails \n    \n\n    \n      0 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      Jboss \n    \n\n    \n      1 \n    \n\n    \n      1 \n    \n  \n\n  \n    \n      Liferay \n    \n\n    \n      0 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      Linux \n    \n\n    \n      1 \n    \n\n    \n      0 \n    \n  \n\n  \n    \n      Total \n    \n\n    \n      57 \n    \n\n    \n      51 \n    \n  \n\n  \n    \n      OS Total \n    \n\n    \n      6 \n    \n\n    \n      4 \n    \n  \n\n  \n    \n      Open % \n    \n\n    \n      10.53%\n    \n\n    \n      7.84%",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "12fc20763f481d354ad7c69f30a5e148",
    "url_s": "https://opensourceconnections.com/blog/2008/02/14/three-hours-with-vista/",
    "title": "Three Hours With Vista",
    "content": [
      "Normally Id buy a laptop online or snipe one off eBay. But I needed one quick. So off to Best Buy with Scott Stults. A quick evaluation reveals a disappointing lack of choice, despite over 30 models on display. Nonetheless I find a Toshiba A215-S6816 with good specs for the price. For the record – AMD Turion x64 2.2 GHz, 3GB memory, 250 GB hard drive, and some other built in devices. All controlled by Vista Home Edition Premium.",
      "Later that night…..",
      "OK time to get this machine ready for work. First boot. Lordy, theres 30+ icons on desktop and the systray is four inches wide. Its a shame to because I love a fresh OS install. I hate the practice, but the desktop was more like billboard than a work surface. Pongo, Napster, et. al. had trial versions preinstalled. Even worse, most of them started during startup. Thanks Best Buy. Time to get my uninstall on. Holy smokes, whats the User Access Control? Am I not an administrator of this machine. I can see why Apple makes fun of this. I turn it off, but every so often I get a reminder that I turned it off. The first uninstall goes bad, it fails halfway through. Now every reboot takes longer because the half installed program cant start and Vista wants to fix it. Crimony.",
      "OK lets get on my network. Heres where Vista gets freaky. First off my WEP key isnt working. Am I entering it wrong? Its possible since its 26 characters long. OK, Ill plug in some cat5, browse to the 192.168.0.1 and cut & paste the key. Briefly Vista recognizes both connections, but then they both die. I unplug the LAN line, look the wireless connection has found the access point again. WEP key still not working. On another machine I burn the WEP key to CD and sneaker-net it to the laptop. Still no workie. Inexplicably, on the umpteenth try it does work. So Ive got a wireless connection. Or do I? Basically the wireless connection drops every 10 minutes or so. Reconnecting means entering the WEP key again and again and again. Sometimes the WEP key works on the first try, sometimes I have to cut & paste it ten times. And neither connection works when cat5 is plugged in.",
      "Back to XP…..",
      "I call Scott, he gives me the latitude to change the OS. I hop on one of my home machines to start collecting XP drivers for the laptop. Strange, Toshibas website only list Vista drivers for this machine. Keep digging. I discover this laptop is sold as a Toshiba, Best Buy and Vista exclusive, i.e. sold only at Best Buy and only with Vista Home Edition Premium. All your base are belong to us. Youd think a machine designated as such would work superbly with Vista. I start reading the forums. Im not alone. As I suspected, in Europe the same machine is sold, but with a different model number, but the same exact chipsets. Bingo. The Toshiba Europe website has all the drivers I need. Well almost, my model has a different video card. Further forum reading leads me to Ruud Ketelaars who wrote Modder.Net. Which cleverly modifies the existing Vista drivers to work with XP. So I install the Vista video drivers and modify them with Modder.net. Victory.",
      "I was worried that my wireless networking troubles were hardware related. But the wireless connection performs flawlessly with XP.",
      "The laptop is now what I was hoping for, quick boot, zippy performance, sharp display…."
    ],
    "summary_t": ""
  },
  {
    "id": "8d807fbf38d328eee6fc70d77d405b45",
    "url_s": "https://opensourceconnections.com/blog/2008/02/16/learn-about-openid-at-betech-2/",
    "title": "Learn about OpenID at beTech!",
    "content": [
      "This Wednesday, February 20th, I will present on OpenID (using Ruby on Rails specifically), a decentralized single-sign-on system now used by Google, Yahoo, Microsoft, and the rest of the world. See http://en.wikipedia.org/wiki/OpenID for all the juicy tidbits/history/etc.",
      "When/Where:",
      "Wednesday, February 20th\n  \n  \n    2:00pm-3:30pm\n  \n  \n    electronic classroom in the Science and Engineering Library, Clark Hall.\n(http://www.virginia.edu/webmap/ACentralGrounds.html)"
    ],
    "summary_t": ""
  },
  {
    "id": "b3f3fa23cd261b57b600791a6120aa2d",
    "url_s": "https://opensourceconnections.com/blog/2008/02/17/edemocracy-barcamp-2/",
    "title": "eDemocracy barCamp",
    "content": [
      "March 1 â€\" 2 2008 will be the first ever e-Democracy barCamp, and will be held in Washington DC. Iâ€™m looking forward to attending that weekend, and Iâ€™ll try to attend the friday night kickoff as well. There should be a lot interesting discussions about Web 2.0 and politics, and networking with some very interesting people. eDemocracy is certainly an area with a lot of innovation, and something that Iâ€™m sure OpenSource Connections can provide development value to some organizations with innovative ideas. One thing Iâ€™m keen to talk about are ways to enhance podcasting, and brainstorm with other attendees about the ways that can impact their activism."
    ],
    "summary_t": ""
  },
  {
    "id": "f2f56fe10455141d4e70e688efd8c0f3",
    "url_s": "https://opensourceconnections.com/blog/2008/02/18/does-the-federal-government-suffer-from-it-vendor-lock/",
    "title": "Does the Federal Government Suffer From IT Vendor Lock?",
    "content": [
      "It is a well-known marketing maxim that if a vendor can raise the barrier to exit for a customer, then the vendor can charge incrementally higher costs. If buyers are locked into a vendor because of the belief that either the vendor is the only one who can provide the service/product, or, alternatively, that the costs of switching to a new vendor are too high to justify moving, then free markets are, effectively, eliminated for the buyer.",
      "After being excoriated in the 1980s for excessive spending such as the $436 hammer, the U.S. federal government worked to open up its contracting practices to provide more clarity and, more importantly, more competition. This, in turn, should have made the process of government acquisition more effective and raised the return on U.S. taxpayer dollars. In most cases, this has worked, and the process is much better than it was in the 1980s; however, work remains to be done.",
      "In this article, I shall conduct an analysis of government purchasing between February 18, 2004 and February 6, 2008 in three specific NAICS codes: 541511 (Custom Computer Programming Services), 541512 (Computer System Design Services), and 541519 (Other Computer Related Services) with the intent of seeing if the government suffers from excess vendor lock and look at ways that, if it does suffer from vendor lock, it can ameliorate those issues.",
      "A cursory glance at presolicitations, special notices, synopses, and awards posted on Federal Business Opportunities (otherwise known as \"FedBizOpps\") shows that 2,266 out of 8,997 postings, or 25.2% of all postings for the aforementioned categories and NAICS contain the term \"sole source.\" Naturally, one would expect that not all of these hits returns an actual sole source award or justification, so further digging is necessary. I have taken a sample of 76 of the 2,267 postings to look at the actual posting, which will yield a 95% confidence level with a 11.05% confidence interval. In other words, barring other biases, 95% of the time, results shown from this sample will have a +/- 11.05% margin of error. Its not enough for definite certainty, but provides directional correctness, and suffices for this research.",
      "First off, as a consultant, I feel obligated to add in caveats about the research I am doing.",
      "Compared to the level of analytical rigor necessary to draw unassailable conclusions, this effort falls short. It is meant to be \"quick and dirty\" and directionally correct, and to draw attention to what may be a potential shortcoming in government procurement policies and procedures.\n  \n    There are certainly times when sole sourcing is justified. These include, but are not limited to:\n  \n  When a need can truly only be filled by one vendor\n  The level of capital expenditure to fulfill is extremely high, and has already been spent (an example of this might be the use of an existing and monopoly satellite system)\n  The need is not readily met by open sourcing (conventional or non-conventional means)\n  The government is looking to meet set-aside goals, and one set-aside category filling vendor is identified as capable of fulfilling the need",
      "Whether or not a procurement should have been open sourced is open to interpretation, and I am providing my own interpretation. I tried to use the above for guidelines, and I readily admit that my approach is both normative and likely biased. Again, this is not meant to have unassailable rigor, but, rather, is meant to serve as a baseline and initial approach for trying to answer the question, as I can find no noteworthy research which addresses the problem.Of the 76 I analyzed, 72 were actual sole source justified requests for proposal/announcements. The other four were either intents to sole source after open competition, or in the case of one, a sole source to introduce a new vendor into competition. Of the 72 analyzed, I found that 44, or 61.1% were justified because of specific knowledge necessary to execute, existing wartime continuity, or because of a set-aside justification.",
      "What this means to me is that there are many opportunities for procurement specialists in the federal government to improve their procurement methodology to avoid vendor lock in the first place. Many of these set-asides were maintenance agreements for proprietary systems, some of which were commodity or non-domain specific areas (e.g. travel and relocation services, asset management). If more work is conducted up front to identify long-term impacts of procurement decisions, then many of these situations can be avoided. It is also up to industry to identify areas where vendors can compete. Sources sought announcements go out frequently, and this is the governments method for market research. When only one vendor responds, then its hard for the procurement officers NOT to sole source.",
      "The entire list is available for download here.",
      "I will comment more on this research in subsequent postings. If you have any comments or questions about my methodology, feel free to leave a comment."
    ],
    "summary_t": ""
  },
  {
    "id": "39473f8a3ecae030527d7b335230e14a",
    "url_s": "https://opensourceconnections.com/blog/2008/02/21/openid-slides-posted/",
    "title": "OpenID Slides posted",
    "content": [
      "A big thank you to the beTech community for coming out in force yesterday to talk about OpenID! I know I learned some things about this fascintating new technology, and I really enjoyed sharing what I know.",
      "David Moody wrote up some great notes, so I wont repeat them. If anyone wants a copy of the slides I posted them on SlideShare.net"
    ],
    "summary_t": ""
  },
  {
    "id": "bbede07e9a204b3074806c241bbe4f89",
    "url_s": "https://opensourceconnections.com/blog/2006/08/15/osc-wins-3rd-place-at-ruportday/",
    "title": "OSC wins 3rd Place at RuportDay",
    "content": [
      "Okay, so its not the SuperBowl, but I won 3rd place during the 2006 Ruport Day!",
      "Ruport Day was an attempt to pull together a bunch of folks and hammer out new features, documentation, and tutorials, with cash prizes for 1st through 3rd places.",
      "The wrap-up email is here.",
      "I joined the crew around 8:30 PM, and put in a couple hours, learning enough about Ruport to contribute some ideas, read a lot of Ruby source, and put in a small recipe.",
      "Ruport is the first project that Ive contributed to since doing development in Ruby, and they are a great group, very supportive."
    ],
    "summary_t": ""
  },
  {
    "id": "64397869a46421a3927e8392c9443260",
    "url_s": "https://opensourceconnections.com/blog/2008/02/21/some-opensource-ideas-for-net/",
    "title": "Some OpenSource Ideas for .Net",
    "content": [
      "Have you ever felt there is just simply not enough time in the day to get around to doing everything that needs to be done. Its possible that you may be reading this blog posting totally clueless about what is going on .Net (Microsofts solution to \"write it anything you want, c#, ruby, vb.net and it works on windows platform\"). Well the big MS has dumped a ton of new technologies(while some are totally not new concepts) and Projects and there is more to come. From Linq (Language Integrated Queries) to WPF which all vector based graphics over the typical pixel based windows forms, to Asp.Nets MVC add on project (which is very rails like, no surprise there) to SilverLight, and even supporting projects like Iron Ruby. And there is no slowing down in sight.</p>",
      "And yet with all the cool things to try out, there are still quite a few niches and gaping holes in all this. If youre reading this and possibly have a hard time of keeping yourself busy and want something worth while to do. Try working on or even starting an opensource .net base project. I know that Iron Ruby could use some help. Also I know of plenty of gaps in the dot net world that could definitely be filled in.",
      "Code generation Tool – that doesnt look or act like it was built 10 years ago and in .net it would be nice to have one that makes use of\ncode dom which rules out things like \"my generation\". Also having th7 new mixin methods would help decrease the amount of code needed to use use code dom, plus there is need for multiple database supported as well as reading XML or even documents like the new ADO.NET entity XML files. </ul>\n  .Net Yaml Parser – yes there are some, but not one that stands out. I know that Iron Ruby is really in need of one as well.\n  .Net BDD/Spec Framework – yes there is Nspec which is now apart of NBehave but it looks either really behind or stagnated.\n  Wpf/WinForms Unit Testing Framework – yeah again there is something for this, but its nunitforms which also seems a bit stagnated.\n  Web/Win/Wpf Controls – one really tightly made suite that would give companies like Infragistics, Telerik, and dev express, good competition.\n  .Net Subversion Bindings - could really use some help.\n  **.Net Audio/Music/Mp3 etc Library **- It would be nice to not have to embed windows media player as the typical way of playing audio files other than .wavs in .Net",
      "currently listening to.. Janus Stark  Great Adventure Cigar  Every LIttle Thing Counts",
      "Tags: OpenSource, .Net, Yaml, BDD, Code+Generation"
    ],
    "summary_t": ""
  },
  {
    "id": "bbeb3ab868f37cf971299acd37f128e7",
    "url_s": "https://opensourceconnections.com/blog/2008/02/22/css-fury-part-2-adjacent-selectors-transverse-dom-with-css/",
    "title": "Css Fury: Part 2 Adjacent Selectors, Transverse Dom With CSS",
    "content": [
      "Read Css Fury: Part 1 the star/asterisk selector",
      "One of the \"wicked cool\" but rarely used parts of CSS selectors is the Adjacent/Sibling Selector . IE 6 does not support this selector. However IE 7, Firefox, and Safari browsers do support this feature. So using this selector can be a good way of using CSS styles to enhance the visual experience while degrading gracefully in older browsers (which gives the user a good reason to update to one that does support CSS 2.1, without affecting the functionality of a site).",
      "An Adjacent Selector basically allows you to select sibling nodes within a given node of the DOM. An example of what an adjacent selector looks like is below.",
      "p + p {\n    background: #ccc;\n}",
      "<p>I am paragraph one with a normal background. </p>\n<p>I am paragraph two with a slightly smoke gray background. </p>",
      "Often times designers rely too heavily on programmers to help with things like using a modulus in order to create alternating styles/classes on rows of a grid or list. In a web site you might see something like if((index % 2) == 0) row.CssClass += \" alt\"; Well you can do something like this using only CSS within reason, using adjacent selectors. Now do keep in mind, that since CSS is all about cascading and inheritance, that whatever you set on the alternating columns has to be accounted for in the other style set. Meaning if you want to alternate a background color you need to explicitly set the color on both the odd and even rows like below.",
      "ul li,\nul li + li + li,\nul li + li + li + li + li {\n     background: #ccc;\n}\n\nul li + li,\nul li + li + li + li,\nul li + li + li + li + li + li {\n    background: #999;\n}",
      "The above would render something like the list below.",
      "<li style=\"background: #ccc\">\n  row 1\n</li>\n<li style=\"background: #999\">\n  row 2\n</li>\n<li style=\"background: #ccc\">\n  row 3\n</li>\n<li style=\"background: #999\">\n  row 4\n</li>\n<li style=\"background: #ccc\">\n  row 5\n</li>\n<li style=\"background: #999\">\n  row 6\n</li>",
      "Granted you would not want to do this for something that has more than 10 rows or changes variably, however it does present a solution if you have to style a site that you have limited or no control over the output of the html and you are not allowed to change the semantic markup of a page.",
      "currently listening to.. Grits",
      "The Art of Translation",
      "Here We Go"
    ],
    "summary_t": ""
  },
  {
    "id": "f91c83df1b676e99b2ae733f65432932",
    "url_s": "https://opensourceconnections.com/blog/2008/02/25/converting-tabs-to-spaces-for-cvs/",
    "title": "Converting tabs to spaces for CVS",
    "content": [
      "This seems like a problem I run into infrequently enough that I forget how I worked around it previously. Some organizations implement CVS pre-commit checks that disallow hard tabs in source files. I use Eclipse for editing Java and it’s relatively easy to reformat code according to rules, but those rules ignore hard tabs within comments while this particular CVS pre-commit check did not.",
      "Below are the steps I used to check in code riddled with hard tabs. This was done using a typical Cygwin installation on Windows, but should be pretty close to what would be required on a Linux installation.",
      "Record the errors from cvs commit\n\n    cvs commit -m 'some message' > commit-log.txt 2>&1\n    \n  \n  \n    Grab just the hard tab error messages\n\n    grep 'hard tab' commit-log.txt > hard-tabs.txt\n    \n  \n  \n    Convert remote path to local path\n\n    vi hard-tabs.txt\n:1,$ s/remote path/local path/g\n:wq\n    \n\n    ex:\n\n    vi hard-tabs.txt\n:1,$ s/data2/CVShome/home/Scott/Clients/SAIC/dev2/g\n:wq\n    \n\n    (note: use backslash to escape the slashes in the path above)\n\n    \n      Convert the hard tabs in the files with the expand command (using bash)\n    \n\n    for file in $(cat hard-tab-files-sorted.txt); do expand -t 4 $file > /tmp/expanded.txt; mv /tmp/expanded.txt $file; done\n    \n\n    \n      Retry the commit\n    \n\n    cvs commit -m 'some message'\n    \n\n    There’s a quicker way to do the search and replace if you also have Perl.\n\n    I’m always interested in these sorts of shortcuts, so if you know of some please include them in comments below!"
    ],
    "summary_t": ""
  },
  {
    "id": "5a6b98188b4fc03419b3797158b40039",
    "url_s": "https://opensourceconnections.com/blog/2008/02/25/the-quickest-way-to-install-suns-jdk-on-a-red-hat-machine/",
    "title": "The quickest way to install Suns JDK on a Red Hat machine",
    "content": [
      "For various reasons I inevitably need to install Suns JDK on a Red Hat-based machine. If youve done this, you know that Javas related files are managed by the **alternatives **system. I used to simply ignore that fact and rewrite the links manually; a PITA to say the least, but it saved me from Reading The Fine Manpage on **alternatives **and numerous attempts at getting the commands right.",
      "After a bit of Googling I ran across this great reference which describes what should be the definitive guide. For the impatient, Ill summarize the exact steps here:",
      "rpm --import http://jpackage.org/jpackage.asc",
      "cd /etc/yum.repos.d/",
      "wget http://www.jpackage.org/jpackage17.repo",
      "yum --enablerepo=jpackage-generic-nonfree install java-1.6.0-sun-compat.i586",
      "[addendum on 03/05/08]",
      "Actually, that last line will only work on a Fedora box. If youre using something like RHEL4 youll need to install Java from Suns site, then yum, then you can yum the compat package:",
      "rpm -ivh ftp://rpmfind.net/linux/dag/redhat/el4/en/i386/dag/RPMS/sqlite-2.8.17-1.el4.rf.i386.rpm",
      "rpm -ivh ftp://rpmfind.net/linux/dag/redhat/el4/en/i386/dag/RPMS/python-elementtree-1.2.6-7.el4.rf.i386.rpm",
      "rpm -ivh ftp://rpmfind.net/linux/dag/redhat/el4/en/i386/dag/RPMS/python-sqlite-1.0.1-1.2.el4.rf.i386.rpm",
      "rpm -ivh ftp://rpmfind.net/linux/dag/redhat/el4/en/i386/dag/RPMS/python-urlgrabber-2.9.7-1.2.el4.rf.noarch.rpm",
      "rpm -ivh http://dag.wieers.com/rpm/packages/yum/yum-2.4.2-0.4.el4.rf.noarch.rpm"
    ],
    "summary_t": ""
  },
  {
    "id": "1e792fa4d09e9bdfbfa3ee83e1f3bc46",
    "url_s": "https://opensourceconnections.com/blog/2008/02/27/14-days-with-resharper-31/",
    "title": "14 Days with Resharper 3.1",
    "content": [
      "First off, Resharper 3.1 (R#) is not a stand alone application, instead its integrated into Visual Studio (VS) 2005. I recently installed VS2005 on a new laptop, soon I realized that the features I love the most are not part of VS, but were, in fact R#, features. Little did I know I was a full R# addict. Quickly I downloaded and installed the 30-day trial version. The first hit is always free, no? My plan is to convince my employer to purchase it before the trial version expires.",
      "For a full description of Resharper 3.1 visit their website, but Ill summarize the features I use and enjoy the most.",
      "Enhanced Intellisense",
      "With R# lots more code is `intellisensed. Based on where in the code file you are typing R# presents context relevant choices. For example, at the beginning of a new code file R# presents choices for type modifiers – public, private, internal, etc. Then the user can select the type and modifiers – class, interface,struct, abstract, virtual etc.",
      "R# will show you all the overloads of a methods along with the parameters. And if you type a method name that doesnt exists R# flags the error, but also gives you the choice of automagically creating the method.",
      "If you use an object that is not referenced (using statement) R# will show a compile error. In this case a Cannot resolve symbol error, but click the R# hover box and the using statement is inserted at the top of the code file. This is helpful because I often cant remember what namespaces/libraries contain the object I need. For example, with one click R# will add using System.Text; (if its not there) to the code file when I type `StringBuilder sb = new StringBuilder();",
      "Continuous Compile",
      "Because R# is continuously compiling, errors and warnings show up immediately. Overall its a great feature, but at times its annoying. I dont need to be reminded that half written code wont compile. But R# does make intelligent guesses about how to fix or complete the code. When working on a code file R# gives a visual indication of suggestions, warnings and errors. Click on the warning bar and R# takes you right to the problem. Hover over the warning bar and the error text pops up. When writing code I find myself frequently checking the R# status light to make sure its green – no warnings or errors. As expected the user can configure R# to ignore various warnings.",
      "Code Generation",
      "Suppose you write some code to find the first day of the week. In this case, Monday:",
      "int i = ((int) DateTime.Now.DayOfWeek) – 1;\nif (i < 0) i = 6;\ndateFrom.DateValue = DateTime.Today.AddDays(i*-1);",
      "Later you need find yourself needing this code in another place. No problem. Highlight your code, right click and select Refactor –> Extract Method. Then define the new methods signature and boom its a new method. Even the calling is written for you.",
      "Now suppose you need some code in a try{} catch{} block. Highlight the code, right click, select Surround with &#8211;> try. And theres your code in a neat tidy try{} catch{} block.",
      "If your classes often implement interfaces, youll love this next feature. With one click R# will stub-out the interface contract for you, either explicitly or implicitly.",
      "Renaming identifiers (class names, interface names, variable names, etc.) is so handy. Over time an identifier might no longer reflect its use. But with one click R# will update the name wherever its used in the solution, including comments.",
      "The R# context menu has an option to quickly reformat code, which gives you several choices. \"Optimize using statements\" will remove using statements that are not used. \"Shorten references\" will remove unnecessary qualifiers. \"Remove redundant `this qualifier\" is self explanatory. Type members can also be reordered. Whats really powerful is that this quick reformatting can be done at the file, project or solution level.",
      "No Free Lunch",
      "Ah yes the dark, dirty side of R#. OK really its not that dirty or dark. But there are a couple of items worth mentioning. When loading or reloading a project, R# creates in-memory indexes of your code. My current solution has 17 projects and roughly 400 files. With R# enabled this solution takes an extra 5-10 seconds to load, about the time it takes me sip my coffee. Expect VS2005 to use more memory as well, roughly 10% more. My development machine easily absorbs this, but it has 3Gig of RAM. The most annoying thing R# does is remap the shortcut keys. Jeez, I hate that. After installing R# I reset VS2005 back to the default shortcut mappings, takes like three clicks. Again no biggie. Then I create some shortcut keys for the R# features I use the most.",
      "I dont intend to give a complete description of R# (check their website), just highlight the features I like the most."
    ],
    "summary_t": ""
  },
  {
    "id": "6bcb29b3dd22583061dec667518d7e6a",
    "url_s": "https://opensourceconnections.com/blog/2008/02/28/google-sites-launches-bringing-the-ultimate-mashup-maker-one-step-closer/",
    "title": "Google Sites Launches, Bringing the Ultimate Mashup Maker One Step Closer",
    "content": [
      "Today, Google announced the launch of its latest application, Google Sites. Google Sites is an application designed to make it easier for those who are not technically savvy to create a web page in just a few steps. While this may create a cottage industry of people who can throw together a personal home page in just a few minutes (a la what Geocities was before Yahoo bought it) for free, what it does not do is make it easier to aggregate data from other sources. Screen scraping, feed aggregation, and mashing up sites still require the same level of knowledge and skill as before.",
      "Google is taking a swipe at Microsofts SharePoint with this application, while Microsoft is taking a swipe at Google by trying to purchase Yahoo! Wouldnt the world be a better place if you could make a website as easily as you can with Sites and have the website aggregation power of Yahoo! Pipes? After all, Pipes mashes up other Google apps, such as Google Maps…or will it require the technical savvy (although this is quite powerful) that something like Bungees PaaS requires?"
    ],
    "summary_t": ""
  },
  {
    "id": "c58a32481db69cd18d8796b8afcef452",
    "url_s": "https://opensourceconnections.com/blog/2008/03/03/open-source-vs-proprietary-software-in-the-federal-government-week-3/",
    "title": "Open Source vs. Proprietary Software in the Federal Government- Week 3",
    "content": [
      "The numbers speak for themselves this week. There is no open source love in Federal Government procurement this Valentines Day!",
      "[TABLE=3]",
      ""
    ],
    "summary_t": "The numbers speak for themselves this week. There is no open source love in Federal Government procurement this Valentines Day! [TABLE=3]"
  },
  {
    "id": "a44400ff19ec42577a954faeca60a9b0",
    "url_s": "https://opensourceconnections.com/blog/2008/03/04/open-source-vs-proprietary-software-in-the-federal-government-week-4/",
    "title": "Open Source vs. Proprietary Software in the Federal Government- Week 4",
    "content": [
      "More solicitations appeared on FedBizOpps this week, but none mentioning my open source terms.",
      "[TABLE=4]",
      ""
    ],
    "summary_t": "More solicitations appeared on FedBizOpps this week, but none mentioning my open source terms. [TABLE=4]"
  },
  {
    "id": "a286daa9011d6e578415f498ff088aef",
    "url_s": "https://opensourceconnections.com/blog/2008/03/05/arriving-at-mix-2008-las-vegas/",
    "title": "Arriving At Mix 2008 (Las Vegas)",
    "content": [
      "Technically its 4am EST, but here its 1am (and I pretty much live on a west coast clock, even though I was born and raised in Va), which is prime nocturnal developer time to get something productive done without interruption. So Im sitting at the Starbucks at the Egyptian style Luxor hotel here at the 24 hour Starbucks (take notes Charlottesville, we need more 24 hour places for geeky people like me. Even Richmond had a 24 hour gym that was like almost a quarter of the price of the local gyms that close at 9 PM in Charlottesville), hearing the rambling of someone to spout something that resembles a cheesy pick up line and echoing of random strangers walking by.",
      "",
      "Mix 2008",
      "But lets face it, the real reason Im here is to find out all about the new and upcoming technologies that will hopefully make life easier not just for the developer, but you; the reader, the end user, the client, the father, the single mother, the person in the daily trenches that keeps things running smooth.",
      "Unfortunately, new technology does not always lead to make your life easier, in fact it often makes you dependent or duplicate processes that should other wise should be done once. Or maybe you are in that organization that hacks together things so youre ending up duplicating data in multiples places, chained to your cell phone that was supposed to bring you freedom, and using the latest and greatest software that is seriously lacking in features.",
      "However, the developers at osc (opensource connections)  strive to use technology for the benefit of our clients and end users, and find ways of using new technologies to fill in the gaps that old technologies or bad design often leaves behind.",
      "Tomorrow Microsoft is supposed to unveil the uber secret workings of IE 8 (Internet Explorer 8), have a keynote that includes Ray Ozzie (the guy replacing bill gates at Microsoft), Scott Gutherie, and Dean Hachmovicth.</p>",
      "The other highlight of tomorrow will be seeing how MySpace used .Net 3.5 and WCF (windows communication foundation) to create a massive RESTful API that obviously supports millions of hits per day.",
      "Myspace Note: For those of you that do not know, despite the fact that many of the urls on myspace still say .cfm, that site is in fact run on IIS and asp.net and the changes of late have reflected the new technologies like ASP.NET Ajax being used throughout the site.",
      "For those of you interested in how .Net technologies and opensource technologies blend better than the typical person might expect, you can always e-mail me at [email protected] to see what is really out there and happening.  For those of you that are totally in the dark in the last 2 years or so Microsoft has really been making an effort to work with the opensource community and has seen the power in it. Hence web sites like Port 25  and codeplex, not to mention the invention of the Microsoft permissive license.",
      "Tags: Mix+2008, Mix, Wcf, las+vegas, .Net, opensource, windows+communication+foundation, MySpace"
    ],
    "summary_t": ""
  },
  {
    "id": "586ae6c279c6cda82e528faceba1851f",
    "url_s": "https://opensourceconnections.com/blog/2008/03/05/edemocracy-barcamp-podcasting-session-summary/",
    "title": "eDemocracy barCamp Podcasting session summary",
    "content": [
      "Last weekend I attended the inaugural eDemocracy barCamp in Washington DC. It was a very good experience and I definitely want to thank all those who organized it. I attended sessions on Interactive messaging, eGovernment in the UK, voting methods, and integration of large scale conversations, all of which were very interesting.",
      "I also led a discussion on podcasting which focused on the uses of podcasting in eGovernment and political campaigns, as well as some discussion of where podcasting may be headed in the future.",
      "Thanks to everybody who participated in that discussion. We covered topics such as how to determine your audience, and what is the effectiveness of podcasting. The answer we agreed on to these questions was that you will always be podcasting to a niche audience, and you need to keep that in mind when setting your metrics and your goals for the podcast. Given its relative ease to do and low cost for the tools needed, it can be an effective way to communicate with people.",
      "The biggest challenge is how to distribute it to people and develop an audience, and there is no short answer on how to do that. It entirely depends on the target audience you are trying to reach.",
      "We also discussed the tools available, including how to setup an xml file to work with iTunes distribution, using Audacity as a free audio editing tool, using TalkShoe for group discussions and an easy way to record a podcast, and using PodSafeMusic or Opuzz for inexpensive ways to get royalty free music and sound clips.",
      "We also talked about how \"polished\" your podcast should be. Again, this depends on your audience, but one of the things I feel is important is authenticity. Making your podcast too polished can work against your group if you are trying to use it as a personal way to communicate with your listeners, and make it sound too much like a radio broadcast. Justin, who was visiting from the UK, talked about how some videos produced by government ministers on their trips to other countries were effective precisely because its a government minister being filmed by a handheld camera as he drives from one event to another. The same principle can apply to any podcast.",
      "During the discussion, I also mentioned some of my ideas for creating a website that would allow invited users to submit online their own segments towards a podcast, record them online, and then at some regular time and date all submissions to that podcast get automatically stitched together into an mp3 file and sent out to a feed. One of the hurdles to podcasting is the time you can spend editing it and coordinating multiple segments and interviews, and I received good feedback that this may be a useful tool for some type of podcasts. Hopefully more on that idea later as I flush it out for my Bhag project, but I did enough coding on it over the weekend to know its possible.",
      "Thanks again to everyone I met there for a great event!"
    ],
    "summary_t": ""
  },
  {
    "id": "d974da2d9675b00d842367e34e04397f",
    "url_s": "https://opensourceconnections.com/blog/2006/08/16/social-networking-20-a-blessing-and-a-curse/",
    "title": "Social Networking 2.0, A Blessing and a Curse",
    "content": [
      "When I was in the Army, a useful skill that I had was knowing who could do what and being a favor broker. Managing a much larger network is vastly different.",
      "Gartner recently identified emerging technologies, including the Web 2.0s social networking capabilities. The network is there. The ability to readily both data mine and qualify the network is not.",
      "If you go to myspace or flickr or blogger, you can find a maze of people who have posted random information. As one of the Technorati people says, \"50 million blogs…some of them have to be good.\"",
      "Yes, some of them have to be good. But how do you know?",
      "First, it helps to know what the tools of Web 2.0 are. Tim OReilly gives a great overview here.",
      "The crux of the value of Web 2.0 is in information aggregation. Most any piece of information that someone could seek is available on the web, but determining where that information is, and, once found, the value of the information remains a challenge. The value creation of the web beyond mere presentation of information (funny how mere presentation of information is accepted; yet, 10 years ago, would have been a taboo thought) is in aggregation and filtering. Here is where folksonomy (defined here:) helps out.",
      "The basic premise behind folksonomy is the same behind market theory. Over time, collective intelligence is better than singular intelligence. So, by using tagging and then community acceptance, information is found, organized, and voted on. It is the Survivor voting off ceremony of the Internet. Good information is tagged, approved, and disseminated. Bad information gets relegated to some dark corner of the Internet or gets sent to your inbox as spam.",
      "However, the data mining capabilities are still in infancy. A business example that we face illustrates this point. When we are ready to take on more developers into our team, we will use a variety of methods to find those people. Craigslist, Linked In, and Sourceforge will be three of our venues, along with others. However, only directly will we be able to use a community vote to determine who the \"winner\" is. Resumes do not have bookmark counts like they do in del.icio.us. Code does not have a beauty contest.",
      "In the end, we are responsible for the decisions we make. We will still have to review code and conduct interviews ourselves. After all, even if there were a social network for resumes and candidates, we are the ones spending the money to hire, and we can only look in the mirror when we spend wisely or poorly."
    ],
    "summary_t": ""
  },
  {
    "id": "f014c5bbe57e94c5f687d45a56639731",
    "url_s": "https://opensourceconnections.com/blog/2008/03/05/from-computerworlduk-open-source-in-schools-could-save-the-taxpayer-billions/",
    "title": "From ComputerworldUK: Open source in schools could save the taxpayer billions",
    "content": [
      "An article in ComputerworldUK, Open source in schools could save the taxpayer billions, is gaining some traction on digg.com.",
      "\"So much has changed so quickly that a model of Open Source school computing is emerging which could save the UK taxpayer billions of pounds and provide enormous opportunities for the home-grown technology sector based around Open Source software.\"",
      "I wonder how political this idea would be if considered in the US?"
    ],
    "summary_t": ""
  },
  {
    "id": "db8918b33ac81c0707bfda728d27cf52",
    "url_s": "https://opensourceconnections.com/blog/2008/03/05/ie-8-beta-1-released-at-mix-keynote/",
    "title": "IE 8 Beta 1 Released at Mix Keynote",
    "content": [
      "IE 8 Beta 1 will be released after the keynote, which is still currently going on as I write this. IE has been play catching up of late, but it seems IE 8 has not only caught up, but is now pushing the envelope with user interaction with the browser, that just might make the browser loveable again.",
      "",
      "IE 8 has caught up with Firefox with its now complete CSS 2.1 support, supporting standards mode by default, including a firebug like clone for developer tools, and forcing developers to now play ball by developing web compliant web sites.</p>",
      "However IE 8 is now pushing the web with starting to support HTML 5.0 specification, so that the back button does not break Ajax web sites, saving copies of web pages on disk while the browser is disconnected from the Internet so that if youre editing a blog page and your disconnected, the data will still save once connection resumes and their are even connect and disconnect events from the new HTML 5.0 spec.",
      "Also new are web-slices and web actions. Web-slices allows the user to save parts of page as a bookmark and it shows a thumbnail of that part of a page so you could easily view your favorite eBay auction or status notices on facebook, without having to browse to the actual page.",
      "Web actions allow the use to select any text into the page and send it a web services that uses that text as a query without having to open up a new page, and to the user it seems like a nice dialog popup. So you could highlight and address and get the Google maps location as a popup right in the page.",
      "Tags: IE+8, Beta, Browser, Web, Web+Slices, Web+Actions, Html+5.0, Css+2.1"
    ],
    "summary_t": ""
  },
  {
    "id": "9ed2b26b9228b0cf09fa662451a470b8",
    "url_s": "https://opensourceconnections.com/blog/2008/03/05/open-source-vs-proprietary-software-in-the-federal-government-week-5/",
    "title": "Open Source vs. Proprietary Software in the Federal Government- Week 5",
    "content": [
      "We are definitely trending the wrong way.",
      "[TABLE=5]",
      ""
    ],
    "summary_t": "We are definitely trending the wrong way. [TABLE=5]"
  },
  {
    "id": "3e4080aed81247a03a8a4c49e39590d6",
    "url_s": "https://opensourceconnections.com/blog/2008/03/05/technology-positions-in-the-2008-election/",
    "title": "Technology Positions in the 2008 Election?",
    "content": [
      "Its an election year, duh?? So what issues are getting traction – the war, immigration, the economy (of course), health care (again), taxes, the sub-prime crisis. But despite the importance of technology in general and the internet in particular, there isnt much technology discussion by the candidates. Admittedly they all use the internet extensively – fund raising, information dissemination, organizing supporters, etc. Manufacturing, for example, gets more political attention, despite being a smaller part of our economy. And If any of these candidates are technically savvy, they hide it well. Perhaps intentionally.",
      "Theres no shortage of technology issues that, in my opinion, deserve more attention – Net Neutrality, Online privacy standards, Online gambling, Broadband access, Technology education, Communications infrastructure, etc.",
      "Technology Position Statements",
      "Barack Obama – be sure to read the section on open government",
      "Hillary Clinton – I could use a link here",
      "John McCain – Again, please send me link to his technology position."
    ],
    "summary_t": ""
  },
  {
    "id": "cced140f351aebea1dc3928da0d813e4",
    "url_s": "https://opensourceconnections.com/blog/2008/03/07/report-from-mix08/",
    "title": "Report from MIX08",
    "content": [
      "",
      "Now that the conference is so well underway that its almost over, I thought is was high time to share some general thoughts and observations.",
      "I tried really hard to get lost in the mix. Maybe I didnt understand what that really means. I could not find the attendees in the picture. Perhaps they were off with the stock photo ladies from small company web sites. There were 4 or 5 women with conference badges but they were speaking German or French.",
      "As you would expect from a Microsoft event in a Las Vegas hotel, the Keynote was a grand production with live entertainment (albeit, no animals), huge video screens, booming sound, fancy Powerpoint presentations, subliminal messages, and lots of paparazzi. I was in the front row and still have a little blind spot from the guy taking pictures of the stage but curiously aiming his flash at me. Alas, sometimes all that glitters is not gold. In this case though, there were definitely some nuggets.",
      "For me, there were two highlights in the keynote. First was the demo of Internet Explorer 8 represented here by the red circle. The second was a demo from NBC Sports showing how video from this summers Olympics available online using Silverlight.",
      "One of the things that I think Microsoft has really started to get right is figuring out that they need to play nice with others. The main thing I really like about IE8 is its embracing of CSS standards. Having spent umpteen hours making code \"cross-browser\" compatible (meaning IE crack-patible) , this is a godsend. Eventually anyway. When everyone is using it instead of IE6 and IE7. There are also some cool innovations, namely Web Slices and Actions. Check out the IE8 section of the keynote video starting at about 0:27.",
      "I cant wait to watch the Olympics online this summer without having to see a single gymnastics event. Note to my bosses: I will be needing the whole Olympics off. One of the questions that I had before coming to MIX08 was – what is Silverlight good for, really? Now I know. The new NBC site blows away what was available last time. Users will be able to choose from a crazy number of live simultaneous events. Watch 2 events at a time with picture-in-a-picture. Chose from multiple shots of the same event, with alternate cameras live on the side. See events that happened while you were sleeping. Stop, start, and rewind the action. Damn. Its kind of like how I bought a Tivo for the 2002 World Cup and watched all the games every morning from the night before fast forwarding until the score changed – only way way better. Check it out in the keynote video starting at about 1:17.",
      "Finally, suggestion for next year. Skip the Ray Ozzie and similar \"we are excited about the cloud\" speeches. Skip the charts breaking everything into distinct sets of three colored circles and just hire Vegas home-town heros Panic at the Disco to play for an hour. Then show the good stuff. That would be better than a whole pitcher of Kool-Aid.",
      "Tags: MIX08, IE+8, Silverlight, Olympics, Panic+at+the+Disco"
    ],
    "summary_t": "Now that the conference is so well underway that its almost over, I thought is was high time to share some general thoughts and observations. I tried really ..."
  },
  {
    "id": "f2bd6b8f7eebbf97dc0ffa056f67b309",
    "url_s": "https://opensourceconnections.com/blog/2008/03/13/4-things-google-is-doing-internally-to-foster-collaboration-and-innovation/",
    "title": "4 Things Google is doing internally to foster collaboration and innovation",
    "content": [
      "There have been a couple interesting blog posts this week that highlight a webinar presentation given by a Google engineer this week about innovation and collaboration, and how they manage a company with 16,000 employees. The first post I found was Blogoscoped noting the presentation, and referring to the beu blog which provided a more in depth set of notes about the presentation. Better yet, the beu blog provided a pdf of the actual presentation which has screen shots of the applications I mention below. You can also see a replay of the webinar for a limited time.",
      "Some parts of the presentation are just showing off Google Docs and how that can be used for collaboration. Cool stuff, but Im not sure there was anything new in there. But heres four things they use internally which are pretty neat, and also remind me of various projects weve either done or speculated on at Open Source Connections:",
      "1) Weekly Snippets",
      "Every week developers are reminded to send an email to a specific address at Google with a status update of everything theyve worked on that week, and what they plan on working on next. Its a virtual stand up meeting. Then this is all archived and you can see what anybody in the company is working on. This is a type of application that has crossed my mind before, and Michael has considered building a project management framework for us internally that could include a similar feature. As consultants, we stay highly connected with our clients, but not always as well connected with what other consultants in our company are doing. We have some ways of maintaining that connection with our fellow consultants, but Google has formalized that within their company and made it automatic to some degree. Something like this would be a great way for us to keep in touch with each other, although it would not replace the value in getting together regularly for lunch, coffee, or beer!",
      "2) Google Ideas forum with voting",
      "Internal to Google they have a forum that allows employees to suggest new product ideas, and for others to vote on and comment on those ideas. A great example of encouraging a flat management hierarchy and inspiring innovation.",
      "3) Expert Directory",
      "Got a question and need to find a domain expert who can answer it? Google allows employees to search what sounds like a skills directory and see other employees who may be able to help them with that topic. This sounds very similar to a project Open Source worked on with one of our clients who is also a large company with lots of technical experts in remote locations, and needed a way to help those employees stay more connected.",
      "4) Google Sites",
      "We currently use a Trac based wiki site for managing our various projects and centralizing a lot of the information about them. Google Sites may be an interesting way to collaborate with clients as well, especially since it would allow for a central repository where our clients can upload their documents to us without any wiki knowledge or special permissions. The big advantage of keeping with Trac is being able to enter bugs/enhancements and giving the client a real time insight into the \"roadmap\" and progress of those issues."
    ],
    "summary_t": ""
  },
  {
    "id": "e1f37605ba8c4880881735baa72b75ec",
    "url_s": "https://opensourceconnections.com/blog/2008/03/14/semantic-web-isnt-meant-to-help-linkedin-and-facebook-and-myspace/",
    "title": "Semantic Web isn’t meant to help LinkedIn (and Facebook, and MySpace)",
    "content": [
      "There was a great post on TechCrunch about Yahoo embracing the semantic web. Basically, the semantic web is the ability to extract meaning from unstructured content. So if  LinkedIn marked up their content with microformats, then others could quickly and easily leverage that data.",
      "Michael Kimsal posted a response that caught my eye about why would LinkedIn want to make it easier for others to use their data, that it would allow freeloaders to take advantage of all the work LinkedIn has put into building up their network of data.",
      "But, I think the key point is that the Semantic Web IS NOT meant to help closed networks like LinkedIn. The same way that HTML and the browser led to the downfall of AOLs walled garden of content, the semantic web means to break down the walled garden of data built up by companies like LinkedIn and the other social networking sites.",
      "Im sure the prospect of people marking up their home pages with hResume microformatting tags, and listing their friends and acquaintances using XFN has the executives at LinkedIn shaking in their boots. Because when Yahoo indexes the web, and matches my XFN and hResume marked up data with similar information from all my friends and acquaintances (maybe via the homepage that I use for my OpenID profile???) then we no longer will need LinkedIn.",
      "The Internet is the Social Network, and the Semantic Web provides the bones for that."
    ],
    "summary_t": ""
  },
  {
    "id": "2caa3759739a620f32ad4b94f5e349e3",
    "url_s": "https://opensourceconnections.com/blog/2008/03/17/doing-business-in-second-life/",
    "title": "Doing Business in Second Life",
    "content": [
      "For a March 18, 2008 presentation on doing business in Second Life for the Charlottesville Business Innovation Council, I developed a white paper covering the background, economics, dos and donts, and notions to consider when starting a business in Second Life. The white paper is published under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 United States license in a PDF version here and in a HTML version here.",
      "For those who do not have the time to read the entire paper, I have copied the executive summary into this post. Appropriate citations are made in the full versions. I also added a citation list at the end of the post.",
      "Executive Summary",
      "\"Every human being is interested in two kinds of worlds: the Primary, everyday world which he knows through his senses, and a Secondary world or worlds which he not only can create in his imagination, but which he cannot stop himself creating.\"\n\n  €\"W.H. Auden",
      "Second Life is a 3D €œvirtual world€ that allows people to interact with others in the same environment through the use of avatars. An avatar is €œan Internet users representation of himself or herself, whether in the form of a three-dimensional model used in computer games, a two-dimensional icon used on Internet forums and other communities.€ Since its inception in 2003, membership in Second Life has grown exponentially, now boasting nearly 13 million residents, with nearly 500,000 residents visiting Second Life in rolling 7 days periods.",
      "Due to this growth and participation in the community, businesses have sprouted up all over Second Life, with nearly 55,000 businesses appearing in Second Life as of the end of February, 2008. Furthermore, over 328,000 of the residents in Second Life spend money.",
      "With this growth story, the allure of Second Life seems to be irresistible, and the promise of quick profits draws in companies both large and small to establish a presence in Second Life. Many companies underestimate the cost of establishing a vibrant presence in Second Life, and those who understand the costs do not get the short-term ROI that investments in the tech bubble of the early 2000 time period did.",
      "Still, the trend in moving towards virtual worlds is clear and nearly inevitable. As personal computers gain processing power and the ability to rapidly generate the complex graphics inherent in virtual worlds such as Second Life, more users, particularly those who regularly utilize the Internet, will begin to inhabit those worlds. As Gartner reported, by the end of 2011, €œ80% of active Internet users (and Fortune 500 enterprises) will have a €˜second life€™, [although] not necessarily in Second Life.€",
      "Therefore, a company that wishes to either establish or grow its online presence in selling goods and services to consumers will be wise to consider the implications that Second Life and other virtual worlds have on the future of consumer interaction with the Internet. Furthermore, doing business in Second Life is not as simple as setting up a website, nor does it have all of the same parallels of opening up a brick and mortar retail store. The paper discusses the implications of doing business in Second Life and what businesses not currently in Second Life should consider when deciding if and when to establish a presence.",
      "Sources Cited",
      "Wikipedia definition of avatar, found at http://en.wikipedia.org/wiki/Avatar_%28icon%29 on March 10, 2008.",
      "Second Life | Economic Statistics, found at http://secondlife.com/whatis/economy_stats.php on March 10, 2008.",
      "Riley, Duncan. €œWill the Last Corporation Leaving Second Life Please Turn Off the Light.€ TechCrunch, available at http://www.techcrunch.com/2007/07/14/will-the-last-corporation-leaving-second-life-please-turn-off-the-light/ on March 10, 2008.",
      "€œGartner Says 80 Percent of Active Internet Users Will Have A \"Second Life\" in the Virtual World by the End of 2011.€ http://www.gartner.com/it/page.jsp?id=503861 on March 10, 2008.",
      "€œSecond Life | What Is Second Life?€http://secondlife.com/whatis/ on March 10, 2008.",
      "Taken from Second Life website, available at http://s3.amazonaws.com/static-secondlife-com/screenshots/web/exp_driving.jpg on March 10, 2008.",
      "€œSecond Life | Economic Statistics.€ http://secondlife.com/whatis/economy_stats.php on March 10, 2008.",
      "€œSecond Life Virtual Economy Key Metrics (BETA) Through January 2008€ http://s3.amazonaws.com/static-secondlife-com/economy/stats_200801.xls on March 3, 2008.",
      "€œIs Second Life Empty?€ found at http://secondliferesearch.blogspot.com/2007/08/is-second-life-empty.html, quoting Stan Travena, €œ[SLED] Second Life Is Not Empty€\"Long response,€ available at https://lists.secondlife.com/pipermail/educators/2007-July/012619.html.",
      "Scott Berg. €œThe Changing Face of Media.€ http://h20325.www2.hp.com/blogs/thechangingfaceofmedia/archive/2007/01/16/2201.html.",
      "€œWired 14.10: Second Life: Facts For the Visitor.€ http://www.wired.com/wired/archive/14.10/slfacts.html.",
      "€œList of Countries By Population Density€\"Wikipedia.€ http://en.wikipedia.org/wiki/List_of_countries_by_population_density on March 10, 2008.",
      "€œGartner Says 80 Percent of Active Internet Users Will Have A \"Second Life\" in the Virtual World by the End of 2011.€ http://www.gartner.com/it/page.jsp?id=503861 on March 10, 2008. Wagner, Mitch. €œ€™Die Hard€™ 4 Opening Teaches An Important Lesson In Second Life Marketing. Yippee-Ki-Yay.€ the Information Week Blog. June 4, 2007. http://www.informationweek.com/blog/main/archives/2007/06/die_hard_4_open.html;jsessionid=B2IGY50DIZOSQQSNDLPCKH0CJUNN2JVN.",
      "Kaminski, Andrea. €œExchanging Real Money in Virtual Worlds.€ LinuxInsider. http://www.linuxinsider.com/rsstory/61893.html?welcome=1205036935.",
      "€œSecond Life | LindeX: Market Data.€ https://secure-web14.secondlife.com/currency/market.php (log in may be required).",
      "€œSecond Life Virtual Economy Key Metrics (BETA) Through January 2008€ http://s3.amazonaws.com/static-secondlife-com/economy/stats_200801.xls on March 3, 2008.",
      "Sidel, Robin. €œCheer Up Ben: Your Economy Isn€™t As Bad As This One.€ Wall Street Journal. http://online.wsj.com/article/SB120104351064608025.html?mod=hpp_us_inside_today",
      "€œMain Research Findings: Purchase Habits in Second Life.€ http://www.reperes-secondlife.com/image/Reperes_Main_research_findings_purchase_habits_in_SL.pdf.",
      "€œSecond Life | Economic Statistics.€ http://secondlife.com/whatis/economy_stats.php on March 3, 2008.",
      "€œGartner Says 80 Percent of Active Internet Users Will Have A \"Second Life\" in the Virtual World by the End of 2011.€ http://www.gartner.com/it/page.jsp?id=503861 on March 10, 2008.",
      "€œSecond Life Virtual Economy Key Metrics (BETA) Through January 2008€ http://s3.amazonaws.com/static-secondlife-com/economy/stats_200801.xls on March 3, 2008.",
      "€œSecond Life | Economic Statistics.€ http://secondlife.com/whatis/economy_stats.php on March 10, 2008.",
      "€œSecond Life | Economic Statistics.€ http://secondlife.com/whatis/economy_stats.php on March 10, 2008.",
      "Second Life provides the source code as an open source platform, available at http://secondlifegrid.net/programs/open_source.",
      "Au, Wagner James. €œ€\"And He Rezzed a Crooked House.€ New World Notes. http://nwn.blogs.com/nwn/2006/06/_and_he_rezzed_.html.",
      "Dickler, Jessica. €œReal $3.1M Mansion Causes a Virtual Stir.€ CNNMoney.com. http://money.cnn.com/2007/08/08/lifestyle/secondlife_house/?postversion=2007080911.",
      "Geller, Tom. €œReal Estate in an Unreal World.€ 3DWalkthroughs. http://3dwalkthroughs.wordpress.com/2007/09/03/real-estate-in-a-3d-world/.",
      "Dickler, Jessica. €œReal $3.1M Mansion Causes a Virtual Stir.€ CNNMoney.com. http://money.cnn.com/2007/08/08/lifestyle/secondlife_house/?postversion=2007080911.",
      "Geller, Tom. €œReal Estate in an Unreal World.€ 3DWalkthroughs. http://3dwalkthroughs.wordpress.com/2007/09/03/real-estate-in-a-3d-world/.",
      "Dickler, Jessica. €œReal $3.1M Mansion Causes a Virtual Stir.€ CNNMoney.com. http://money.cnn.com/galleries/2007/real_estate/0708/gallery.luxury_secondlife//index.html.",
      "http://www.fabjectory.com/index.php/gallery/secondlife-gallery/.",
      "Jana, Reena. €œStarwood Hotels Explore Second Life First.€ BusinessWeek. August 23, 2006. http://www.businessweek.com/innovate/content/aug2006/id20060823_925270.htm?chan=innovation_innovation+%2B+design_innovation+and+design+lead.",
      "€œBathroom Doors.€ aloft in Second Life. September 27, 2006. http://www.virtualaloft.com/2006/09/bathroom_doors.php.",
      "€œLast Night€™s Evening With the aloft Team.€ aloft in Second Life. May 9, 2007. http://www.virtualaloft.com/2007/05/last_nights_event_with_the_alo.php.",
      "Wagner, Mitch. €œ€™Die Hard€™ 4 Opening Teaches An Important Lesson In Second Life Marketing. Yippee-Ki-Yay.€ the Information Week Blog. June 4, 2007. http://www.informationweek.com/blog/main/archives/2007/06/die_hard_4_open.html;jsessionid=B2IGY50DIZOSQQSNDLPCKH0CJUNN2JVN",
      "Kintz, Eric. €œTop 10 Reasons As To Why I Still Need To Be Convinced About Marketing in Second Life.€ The Digital Mindset Blog, Hewlett Packard. http://h20325.www2.hp.com/blogs/kintz/archive/2007/04/02/2964.html",
      "€œInteresting Results: Virtual Product Experiences.€ http://www.markettruths.com/information/results/20070219.asp, citing Schlosser, Ann E. (2006), €œLearning Through Virtual Product Experience: The Role of Imagery on True versus False Memories.€ Journal of Consumer Research, 33(3), 377-383.",
      "Wagner, Mitch. €œWhat Can Real World Businesses Do To Succeed In Second Life?€ the Information Week Blog. June 20, 2007. http://www.informationweek.com/blog/main/archives/2007/06/what_can_realwo.html;jsessionid=B2IGY50DIZOSQQSNDLPCKH0CJUNN2JVN. See comment by €œDirjha.€",
      "€œReuters, Adam.€ €œIBM Accelerates Push Into 3D Worlds.€ Reuters: Second Life News Center. November 9, 2006. http://secondlife.reuters.com/stories/2006/11/09/ibm-accelerates-push-into-3d-virtual-worlds/.",
      "€œLinden, Stephany.€ €œThe Second Life Voice Viewer Is Live!€ Official Second Life Blog. August 2, 2007. http://blog.secondlife.com/2007/08/02/the-second-life-voice-viewer-is-live/.",
      "Wagner, Mitch, €œUsing Second Life As a Business-To-Business Tool,€ available at http://www.informationweek.com/blog/main/archives/2007/04/using_second_li_2.html for more information.",
      "Phillips, Ashley. €œAsperger€™s Therapy Hits Second Life.€ ABC News. http://abcnews.go.com/Technology/OnCall/Story?id=4133184.",
      "http://www.jbouteiller.net/photo/774347-947690.jpg.",
      "http://blogs.law.harvard.edu/cyberone/course-materials/second-life.",
      "A full list of participating schools who have registered with Linden is available at http://simteach.com/wiki/index.php?title=Institutions_and_Organizations_in_SL.",
      "http://www.secondlifeinsider.com/2006/09/12/harvard-law-in-sl/",
      "See http://slshakespeare.com/ for details.",
      "Au, Wagner James. €œHBO Buys U.S. TV Rights To Second Life Machinima Series, Promotes It As Oscar Nominee Contender.€ New World Notes. September 4, 2007. http://nwn.blogs.com/nwn/2007/09/second-life-mac.html.",
      "http://nwn.blogs.com/photos/uncategorized/2007/09/04/with_orhalla.jpg.",
      "€œEmergency Training in Second Life.€ http://metaversed.com/23-jul-2007/emergency-training-second-life.",
      "€œSecond Life Tip Sheet.€ BusinessWeek. November 27, 2006. http://www.businessweek.com/magazine/content/06_48/b4011417.htm.",
      "Riley, Duncan. €œWill the Last Corporation Leaving Second Life Please Turn Off the Light.€ TechCrunch. July 14, 2007. http://www.techcrunch.com/2007/07/14/will-the-last-corporation-leaving-second-life-please-turn-off-the-light/.",
      "€œSecond Life Tip Sheet.€ BusinessWeek. November 27, 2006. http://www.businessweek.com/magazine/content/06_48/b4011417.htm.",
      "€œGartner Says 80 Percent of Active Internet Users Will Have A €˜Second Life€™ in the Virtual World by the End of 2011.€ April 24, 2007. http://www.gartner.com/it/page.jsp?id=503861.",
      "See http://en.wikipedia.org/wiki/Bling.",
      "€œDo€™s and Don€™t€™s For Big Business.€ Second Thoughts. October 26, 2006. http://secondthoughts.typepad.com/second_thoughts/2006/10/dos_and_donts_f.html.",
      "http://secondthoughts.typepad.com/photos/uncategorized/adbrands.jpg.",
      "Au, Wagner James. €œTateru€™s Mixed Reality Directory.€ August 1, 2007. http://nwn.blogs.com/nwn/2007/08/taterus-mixed-r.html.",
      "€œSecond Life Tip Sheet.€ BusinessWeek. November 27, 2006. http://www.businessweek.com/magazine/content/06_48/b4011417.htm.",
      "Winters, Catherine. €œSeven Reasons Your Organization Should Consider Second Life in 2007.€ January 2, 2007. http://www.socialsignal.com/blog/catherine/seven-reasons-your-organization-should-consider-second-life-in-2007.",
      "€œDrama in Geekland.€ Valleywag. http://valleywag.com/tech/second-life/trouble-in-geekland-223304.php.",
      "€œSecond Life Tip Sheet.€ BusinessWeek. November 27, 2006. http://www.businessweek.com/magazine/content/06_48/b4011417.htm.",
      "Wallace, Mark. €œAnshe Chung Draws Venture Capital Investment.€ September 12, 2007. http://www.3pointd.com/20070912/anshe-chung-draws-venture-capital-investment/."
    ],
    "summary_t": ""
  },
  {
    "id": "58f9bb0cec398d40a78f6ec3abe3ccb1",
    "url_s": "https://opensourceconnections.com/blog/2008/03/17/jason-hull-will-be-presenting-doing-business-in-second-life-for-the-charlottesville-business-innovation-council/",
    "title": "Jason Hull Will Be Presenting \"Doing Business in Second Life\" For the Charlottesville Business Innovation Council",
    "content": [
      "On March 18, 2008 at 7:30 AM, Jason Hull will be presenting \"Doing Business in Second Life\" for the Charlottesville Business Innovation Council. The event has sold out; however, if you wish to attend, please call the CBIC office at (434)817-6300 to get on the standby list."
    ],
    "summary_t": ""
  },
  {
    "id": "77bf607704230f3e080089076191d810",
    "url_s": "https://opensourceconnections.com/blog/2008/03/19/catching-up-net-for-bdd-and-unit-testing-with-gallio-nbehave-and-moq/",
    "title": "Ramping Up .Net For BDD and Unit Testing With Gallio, NBehave, and Moq",
    "content": [
      "Unit Testing is a word that has inspired some or has become a bane for others. TDD, test driven development, has changed the way many people write their code, and others… not so much. Then a new catch phrase appeared on the test scene, BDD, behavior driven development. So instead of tests, youre writing specifications of expected behaviors and observations in code, that will also hopefully create your specifications documentation for you. Also there is this notion of doing story boards as well.",
      "Ruby has Rspec built on top of a well oiled web application framework known as Rails. So where does leave .Net developers, especially with the Asp.Net Mvc Option/framework on the horizon? Ive been keeping an eye on .net tools that would allow or be the Rpec for .net. Things like NSpec and NBehave popped up on Google, but there was really no documentation, read mes or even blog postings to demonstrate how to use these libraries. However a cool mock library named Rhino mock became the rage to heal the pain of NMock (but geeze man, all that \"Replay\" stuff is confusing, totally ignoring the KISS principle). And MbUnit, a unit testing on crack, which built on top of other Xunit frameworks, seemed to be stagnated.",
      "Enter 2008, the year of open source and openness for developers of .Net. MbUnit has been hard at working on Gallio, the neutral test platform, NBehave has merged with Nspec and Behave# and have an April 4th release date, but you can still play with the bits. Even Microsoft is doing the release often and having open input on the Asp.Net Mvc framework. A new cooler mock framework has come out known as Moq that uses the c# 3.0 extensions and lambdas, which Scott Hanselman gave a great overview about.",
      "Gallio, the good looking, time saving, neutral testing platform, saved me from time crunching with beating CC.NET into generating what would have been another ugly Unit Test report. Gallio took around 5 minutes after reading the walk through. It also allowed for me create the Unit Testing and NCover Report at the same time running it from an MsBuild script. Not to mention the UI for both the console and GUI are much prettier these days. You can grab the bits for Gallio here on google code.",
      "Icarus",
      "Echo",
      "NBehave, has been working hard along side of the guys at MbUnit to work together in creating a better test environment, however since they still have plenty to do, they put up a blog post, using the current bits of NBehave , in order to create specifications using NUnit, which can also be applied to other Xunit frameworks. Much of BDD has to do with DSL (domain specific language) and the wording structure and thought process, more than the inner workings of code. This can be demonstrated in that blog post, changing the attributes, which will decorate the same way you would in a unit test, but give the developer a new meaning.",
      "However, words like \"context\" and \"specification\" used in the example from the nbehave update can be blah and since people that are most familiar with BDD are probably rails developers using Rspec. So lets take this a step further, using NBehave, Gallio and MbUnit and product a spec. (Plus they tied that MbUnitSpecBase to Rhino Mock and used underscores in their method names.. grrrrr, dont tightly couple frameworks). C# isnt ruby.",
      "//-----------------------------------------------------------------------\n// &lt;copyright file=\"Copyright.cs\" author=\"Michael Herndon\"&gt;\n//     Copyright (c) Michael Herndon.  All rights reserved.\n// &lt;/copyright&gt;\n//-----------------------------------------------------------------------\n\nnamespace Amplify\n{\n    #region Using Statements\n    using System;\n    using System.Collections.Generic;\n    using System.Linq;\n    using System.Text;\n\n    using Gallio.Framework;\n    using MbUnit.Framework;\n    using NBehave.Spec.MbUnit;\n    using NBehave.Specs;\n    using NBehave;\n\n    using It = MbUnit.Framework.TestAttribute;\n    using Describe = MbUnit.Framework.CategoryAttribute;\n    using InContext = MbUnit.Framework.DescriptionAttribute;\n    using Should = MbUnit.Framework.DescriptionAttribute;\n    #endregion\n\n    [\n        Author(\"Michael Herndon\", \"[email protected]\", \"opensourceconnections.com\"),\n        Describe(\"ApplicationContext Specification\"),\n        InContext(\"using the context for managing application wide values and configuration\")\n    ]\n    public class ApplicationContextObject : Spec\n    }\n\n        [It, Should(\"lazy load or create the AmplifyConfiguration.\")]\n        public void GetAndValidationApplicationSection()\n        {\n            ApplicationContext.AmplifyConfiguration.ShouldNotBeNull();\n            ApplicationContext.AmplifyConfiguration.ApplicationName.ShouldBe(\"Amplify.Net Application\");\n            ApplicationContext.AmplifyConfiguration.ConnectionStringName.ShouldBe(\"development\");\n\n            ApplicationContext.ApplicationName.ShouldBe(\"Amplify.Net Application\");\n            ApplicationContext.ConnectionStringName.ShouldBe(\"development\");\n        }\n    }\n}",
      "ahhh yes, this looks more like familiar territory without trying to make C# into ruby using underscores in method names, while staying true to something like rspec. Of course you could easily just do this with MbUnit and creating your own extension methods and not use NBehave. Now you can even make multiple classes for different contexts (uses of something) for the same specification by using the Describe(CategoryAttribute), similar to how rspec does. So building off of NBehaves very cool idea of using what we already have, we can now use specifications in a cool neutral testing platform of Gallio, and tying that into our continuous integration setup, leaving the door open later to come back and generate nice reports from the above code.",
      "So of course now, im now using this type of testing setup for amplify and its made unit testing enjoyable."
    ],
    "summary_t": ""
  },
  {
    "id": "c5b20ec2f471302e35b14617ab8a98a6",
    "url_s": "https://opensourceconnections.com/blog/2006/08/22/selenium-boot-camp/",
    "title": "Selenium Boot Camp",
    "content": [
      "In true something or other style, at last nights Selenium Boot Camp the teacher was the student. We decided to hold an after hours \"boot camp\" so we could beef up our skills at automated testing. And while technically I was the teacher, I ended up learning just as much about doing Selenium testing as the students! Darn those hard to answer questions from professional QA people (thanks Cristen!).",
      "So, the biggest thing we learned about was using the Web Developer plugin for FireFox. That plugin really helps simplify testing with Selenium. So, a couple tips:",
      "Trying to figure out the name or ID of some content that is returned by an AJAX call? Typically doing \"View Source\" wont return that content! However, with the Web Developer, henceforth called WD, you can do \"View Source -> View Generated Source\" and see the source for your AJAX generated HTML.\n  Trying to figure out the field names and ids? Use \"Forms -> Display Form Details\" to see all the form element data. Much faster then digging through source.\n  However, if you are just building up a bunch of type or select lines to populate a form, then use \"Forms -> Populate Form Fields\" puts the names of the form elements into the fields in large text.\n  Lastly, if you want a quick summary of the form, then use \"Forms -> View Form Information\". You can get information like label or max length or size and quickly write up all your form data entry actions.",
      "We also learned how important sensible id= tags are in HTML pages. Without them you have to resort to all sorts of odd hacks and xpath queries to find what you want to click on. Unfortunantly AjaxScaffold doesnt support id= everywhere, but it was easy to add."
    ],
    "summary_t": ""
  },
  {
    "id": "90b105ff5771443ed5b3a7d1621a67c4",
    "url_s": "https://opensourceconnections.com/blog/2008/03/19/pavatars-gravatars-and-power-to-the-edge/",
    "title": "Pavatars, Gravatars, and Power to the Edge",
    "content": [
      "Power to the Edge is the concept of giving people at the periphery of the network the information and ability to interact with other members of other organizations, versus everything coming from the center of an organization. Its a great way of empowering people to make connection across organization silos. The white paper makes a great read!",
      "Recently, for HighTechCville I started aggregating icons for people and organizations. The organizational icons that I am looking for are Favicons, likes this: . However, for individual icons I was importing Gravatars (Global Avatar), like mine: . The problem with Gravatars is that they are all served up from a single Gravatar.com server, which has to be up and running in order for HighTechCville to show off the icons. I realized that this is another example of a centralized data structure. And in todays internet, we should push the ability to supply avatars to the edge! This led me to stumble upon Pavatars (Personal Avatar), which work similar to how favicons work, with a pavatar.png image at the root of your website that meets the Pavatar Specification is used by Pavatar supporting websites.",
      "Unfortunately, when I comment on someones blog article and am requested to enter a website, I use /, since that is where I work. And if we had a pavatar.png at the root, wed all be sharing the same icon. What we really need is our own individualized url. Which brings me back to OpenID. I already have a unique URL for OpenID: http://epugh.myopenid.com/, and this would be a great place to host my unique Pavatar at http://epugh.myopenid.com/pavatar.png! Hopefully this is something some of the OpenID providers will step up to supporting, and then maybe Pavatars hosted from the edge will become commonly looked for and supported!"
    ],
    "summary_t": ""
  },
  {
    "id": "d3be5a58dacf155ab19c37d3a58cf1f2",
    "url_s": "https://opensourceconnections.com/blog/2008/03/20/my-experience-upgrading-windows-vista-to-service-pack-1/",
    "title": "My Experience Upgrading Windows Vista to Service Pack 1",
    "content": [
      "If you are not already aware the long over due awaited Windows Vista SP1 is available for download at: Download Windows Vista SP1. As a long time Windows user I vowed not to upgrade my home desktop computer to Vista before Service Pack 1 became available (From what I have heard the upgrade to Leopard is no picnic either). However, upon employment with Opensource Connections I jumped at the chance to try out Vista with the purchase of my new work laptop. Thus far I have been pleased with Windows Vista and have only experienced a few minor software crashes. I have to mention that my favorite new feature is the \"Instant Search\" functionality. This feature allows you to press the Start key on your keyboard and begin typing the name of the program or file you wish to open and it magically appears. I never realized how inefficient the start menu was until I started using this feature.",
      "As every geek should be, I was nervous about upgrading my laptop to SP 1. Microsoft promises upgrading to SP1 will improve the reliability, performance, and security of Windows Vista, however, I was skeptical. If you are interested in all the gory details about the Service Pack browse to Gory Details. After downloading SP1 I hesitated to double click on the installation file in anticipation that if I do the computer will instantly display the \"Blue Screen of Death\". Finally I muster up the confidence to start the installation and receive a message stating the computer will require several reboots and about an hour to complete. Naively thinking my computer is fast, this should not take an hour, I continue with the installation. After several reboots, taking the dog for a walk, doing the dishes and watching the extended versions of all three Lord of the Rings movies the installation finally finished its last reboot.",
      "With the installation complete I am excited to see what new features are available only to be disappointed at the fact that everything looks exactly the same. It seems as though my computer is running smoother and faster, but I attribute it to the placebo effect. I guess I should just be content with the fact that the installation completed without incident, however, I make no guarantees that you will have the same experience.",
      "Every Windows Vista user that has upgraded to SP1 must do their civic duty and comment with their upgrade experience. The good, the bad and the ugly."
    ],
    "summary_t": ""
  },
  {
    "id": "2b3c070a96082e5815b958301847b794",
    "url_s": "https://opensourceconnections.com/blog/2008/03/28/apple-itouch-the-new-pda/",
    "title": "Apple iTouch, the New PDA",
    "content": [
      "Lets get crazy for a moment and declare the iPod touch the New Breed of PDA, a personal digital assistant, not to be confused with public display of affection, even though there are few zealous Apple proponents who might blend the two for their obsession with Apple devices. The iPod touch or iTouch is essentially the iPhone, minus the phone and things like Edge, but keeping the cool touch screen and allowing for connectivity not just through iTunes, but Wi-Fi as well. With the iTouch having the strength of the new iPhone SDK, the tech savvy jailbreak applications ranging from NES (Nintendo Entertainment System) emulators to Apollo Im, and even possible enterprise support similar to its cousin the iPhone, like syncing with a Microsoft Exchange Server; what is to prevent this extendable portable music player from becoming a full blown PDA device?",
      "The answer: nothing. Even Google has made their calendar accessible from the web to devices like the iPhone and iTouch. You also currently have the ability to sync your contacts and calendar through outlook or iCal, your todo lists from websites like rememberthemilk.com, and your e-mail directly from your account. One cant ignore the power of having your data, music, you-tube videos, movies, pod-casts, contacts, and more, locked into sleek colorful device that can go with you any where, even when disconnected from the web.",
      "You have to admit, this could even be seen as Apples play at creating a PDA, releasing both the iPhone SDK and creating the iTouch targeted at people who didnt want it as a phone. Or this could have even possibly been the target all along for the iPod, since it always (to my limited knowledge) had the ability to store contacts and calendar information from initial production.",
      "Granted the current, biggest drawback is that iTunes does not sync calendars with the Windows Calendar (on Vista) or any calendar application you use outside of Outlook on Windows, or the iCal on Mac OS X. But most people who have need for a PDA, are most likely going to have either the iCal or some form of Outlook. Combine that with things like Plaxo or even the Google Calendar syncing tool with Outlook, you can now have your mail, music, videos, games, contacts, calendar, grocery lists, and much more in the palm of your hand or for the avid jogger, in an band around your arm.",
      "If anyone has any cool applications for the iPhone/iTouch, that would enhance the quality life or help with organization, help the community and leave it in the comments below."
    ],
    "summary_t": ""
  },
  {
    "id": "1cbe4a3244bd6a41d763b884392c07c7",
    "url_s": "https://opensourceconnections.com/blog/2008/03/28/osc-team-members-speaking-at-university-of-virginias-lsp-conference/",
    "title": "OSC Team Members speaking at University of Virginias LSP Conference",
    "content": [
      "On April 15th, Jim Nist, Arin Sime, Caleb Doise, and Eric Pugh will be leading sessions on software development at the Spring 2008 conference for the UVA Local Support Professionals Conference. The LSP program is designed to support the people who directly help local departments at UVA. In the past it has been focused more on help desk and systems issues, but as the web has become pervasive, the conference was looking for speakers who do software development. Enter OpenSource Connections crack team!",
      "Jim will be speaking on \"Design Patterns – What are they and why should I care about them?\", Arin Sime will tackle \"Data Normalization/ Database Design\", Caleb Doise, C# Guru, will lead \"C# Rich Client Applications\", and Eric Pugh will team with Doug Chestnut for a session on \"Version Control with Subversion\".",
      "Were looking forward to participating in this conference, and getting to meet some new folks."
    ],
    "summary_t": ""
  },
  {
    "id": "46a7f33138c06fe48ca0ff4e70e080b5",
    "url_s": "https://opensourceconnections.com/blog/2008/03/29/if-free-is-the-future-of-business-why-does-the-government-pay-so-much-for-proprietary-software/",
    "title": "If Free Is the Future of Business, Why Does the Government Pay So Much For Proprietary Software?",
    "content": [
      "In the March, 2008 issue of Wired Magazine, Chris Anderson wrote an article positing that the marginal cost of technologies which drive the Internet (and, by inference, software) will approach zero over time. That timeline is shorter than many think. Because the incremental cost of the next bit of hard drive space is too small to dicker with and the number of people over whom that cost is amortized is so large, Yahoo! can offer unlimited e-mail storage and YouTube can offer unlimited file uploading.",
      "There are certain points at which this theory does not hold. A crucial concept of the Anderson article is that there must be enough consumers to amortize the costs over to bring the marginal cost to be effectively zero. Building a specific application for one person or a small number of people will incur significant cost because it cannot be used over and over again. Furthermore, laws such as Moores Law can not extend indefinitely, as they will eventually hit the limitations of physics.",
      "However, effectively, what Anderson posits means that software should be a depreciating asset, similar to a car. Cars lose value over time because they dont run as well as they did while they were new, and newer cars run more efficiently and effectively. The same can be said for software. A good example is spreadsheet software. Lotus 1-2-3, introduced in 1984, cost $499 for the PCjr. In 1985, Excel was released for $195; today, it costs $229 (cheaper in inflation-adjusted terms than the 1985 version), and has exponentially more power. Today, OpenOffice and Google Docs have free spreadsheets.\nYet, despite the declining value of purchasing proprietary software, the U.S. government almost always pays license fees and doesnt own code of software that its vendors produce for it. Rather than owning code and being able to get incrementally cheaper upgrades to take advantage of the increasing power and decreasing unit cost of technological improvements, the government is stuck with rapidly devaluing and depreciating software. We certainly understand the risk-aversion and lack of desire to be on the bleeding edge that many government agencies have, but to buy seven and ten year licenses seems (and often not even have rights to the code after that time), in light of the Anderson article, to be counterintuitive.",
      "Here are 3 steps government contracting officers could take to improve the situation:",
      "Require ownership of the code in the RFPs/RFQs: Many companies bank future profits on licensing fees. Make them earn profits on quality software instead.\n  \n  \n    Limit length of contracts for maintenance: If computing power doubles every 18 months and storage capacity doubles every two years, why lock into a timeframe that gets overcome by events?\n  \n  \n    Write requirements that adapt to the future, not reflect a near past: Demand that software be capable of hitting standards on the next generation of systems, not just the existing one. It will force scalability, modularity, and clean development."
    ],
    "summary_t": ""
  },
  {
    "id": "a34ab5befc4a1c6beee8ff21ea7d917d",
    "url_s": "https://opensourceconnections.com/blog/2008/04/01/observations-from-fose-day-1/",
    "title": "Observations From FOSE, Day 1",
    "content": [
      "We just completed Day 1 at FOSE. While I was not able to attend any of the presentations, I did walk around the exhibit floor and had a ton of conversations with attendees. Here are a few themes that I observed:",
      "There is still a lot of ignorance about open source. Despite the efforts of AFEI with conferences such as DoD Open, the message is not getting through at all. The comment I heard that most epitomized the ignorance was from a government contracting officer: \"First its closed, then its open, then its closed again.\"\n  Not enough open source is SEM certified. The desire for DISA and other agencies responsible for SCIFs to want to know whats going behind the cleared wall is understandable. However, open source products will have MORE auditability, not less. It seems like there are not enough advocates with enough experience in and understanding of open source products talking to information security officers about the benefits and overcoming objections. The other unfortunate side note is that its awfully hard to get SEM paperwork entered in as a bug into an open source project.\n  The hardware vendor market has to be close to saturated. I was surprised at the preponderance of hardware vendors exhibiting at FOSE.\n  The number of prepackaged software vendors is surprisingly high. Most software has a short shelf life at best. Why government purchasers are paying for long-term licenses and not insisting on code ownership is unknown to me. I think it relates back to the first point about the ignorance of open source in the government purchasing community. If it took a more open source friendly approach to procurement, I posit that costs would go down and quality would increase due to the benefits of meritocracy that the open source community experiences. Its not a panacea, but its a right step.\n  The number of companies at FOSE who do what we do is suprisingly low. Not everything that the government wants comes out of a box. Im surprised that more companies arent there making the pitch that \"when what you want software to do doesnt come prepackaged, talk to us.\" Software packages rarely do what you want straight out of the box.",
      "Hopefully the talk that Scott McNealy from Sun gave this morning will resonate with program managers, procurement officers, and developers in the government. Eric Pugh will be blogging about that presentation shortly; the Sun CEO spoke about many of the topics we have been blogging about in recent months."
    ],
    "summary_t": ""
  },
  {
    "id": "afee4d71ccb3e19d36ee2cfddd6da618",
    "url_s": "https://opensourceconnections.com/blog/2008/04/03/fose-is-the-federal-open-source-expo/",
    "title": "FOSE is the Federal Open Source Expo?",
    "content": [
      "FOSE means:\n\"Free/Open Source Expo\" – Scott McNealy, Chairman of Sun Microsystems\n\"Federal Open Source Expo\" – Eric Pugh, Principal of OpenSource Connections",
      "When the chairman of a company that spends $2 Billion dollars yearly on R&D makes more or less the same joke that you do, it makes you see the kernal of truth in that statement.",
      "I attended Scott McNealys keynote on the first day of FOSE expecting something about cloud computing, or network security. Some sort of very \"Federal IT\" type of topic. Instead he spent an hour making the case for why the Federal Goverment needs to embrace Open Source Software. And he made that case by walking through a number of reasons:",
      "",
      "No Barrier to Entry: Getting started with open source software doesnt require an RFP process to be undertaken. You can just download the software, and hed like you to download that stack that Sun has open sourced: OpenSolaris, Java, OpenOffice, MySQL etc. He made the case that with open source, you can prototype your application without spending capital up front on licenses before you know what youll need for a production system.",
      "More Interoperablity: The Federal Government builds systems that last a VERY long time. So there are lots and lots of different vendors, different platforms, different interfaces.. Scott cracked the joke that \"… you have two of everything, no, actually you have 200 of everything…\" which means that most of the Federal IT budgets are spent on maintence and support of existing legacy systems that are old, inefficient, and cumbersome, and only a very small slice is spent on new systems. Open Source systems drive interoperablity because the source is available. Anyone can look at the interfaces of a system built using open source and figure out how to provide interoperatiblity. Interoperabilty means the cost of integrating legacy and new systems drops drastically.",
      "Sharing: Sharing means more R&D per dollar. Sun is the 42 largest spender on R&D in the world. They spend over $2 billion dollars a year, but because they have open sourced most of their portfolio, they can leverage the efforts of others. For the $2 billion they spend they estimate they get over $5 billion a year in addtional value on their open source portfolio of software from around the world. Sharing begets sharing.",
      "Communites drive adoption: By opensourcing the Sparc microchip as \"OpenSparc, the Chinese technical universities have standarized on that platform as what their students will be learning on. Guess who is going to be advocating Suns Sparc chips when they graduate school? Guess where Sun will be finding innovative ideas for the Sparc chip design?",
      "Safe, Secure: Open Source is more secure then closed source. Software gets hacked when it contains sccrets that people find. A big secret leads to a big security hole. Open Source, by its nature, doesnt have secrets, therefore you can trust it to be secure. Open Source code is heavily vetted by peer communities.",
      "And biggest of all: No Barrier to Exit. Scott spent the most time on this point, suggesting that the barrier to exit is the biggest cost to any IT system. He said there are three costs: A) the initial acquisition cost… B) the lifetime opperating cost. And both of those are typically addressed in an RFP issued by the Federal Government. But cost C), moving to a new system is never factored into an RFP. So once a vendor has sold the government a system, they keep the renewel cost \"… just 5$ less then cost of moving to another system…\".",
      "So there you have it, from the chairman of Sun why open source is the way forward for Federal IT projects, direct from FOSE: the Federal Open Source Expo!"
    ],
    "summary_t": "FOSE means: \"Free/Open Source Expo\" – Scott McNealy, Chairman of Sun Microsystems \"Federal Open Source Expo\" – Eric Pugh, Principal of OpenSource Connections..."
  },
  {
    "id": "b86588c1d323908a411774cc2319d0c8",
    "url_s": "https://opensourceconnections.com/blog/2008/04/03/observations-from-fose-day-2/",
    "title": "Observations from FOSE Day 2",
    "content": [
      "The people who came by our booth yesterday seemed to represent a slightly different mindset than what we saw in Day 1. I can still categorize the attendees:",
      "People on a mission: They know what they want, and theyre very specific about what they are looking for. Questions like \"Can you do J2EE\" and \"Are you familiar with Drupal [Alfresco, Joomla, Expression Engine]?\" were much more common in day 2 than they were in day 1.\n  Browsers who either havent heard of open source or misunderstand its meaning: These are fine people to talk to, as there is an opportunity for us to tell them what it is and what it isnt. Firstly, let me say that open source is not the answer to everything (saying something like that at OSCON would get me lynched. As an aside, Id wager that the number of people who attend both FOSE and OSCON represents less than 1% of either conferences attendee population. That number disappoints me greatly, as both populations need to learn about each other over the next few years if our government is to remain competitive without spending greater and greater amounts of a percentage of GDP.\n  Schwag runners: The group of people almost sprinting through the exhibit hall sort of reminds me of the Filenes basement sale. It reinforces why we dont give out schwag. Perhaps the companies who give out schwag wind up having more conversations with people that they would not have otherwise had, but watching the surreptitious glances of candy grabbers hoping that nobody from the booth actually talks to them makes me think otherwise.",
      "The one group of people whom I expected to see some of that I have yet to see is contracting officers. I know that many agencies are not meeting their set-aside goals, so I would have expected the KOs to be swarming the small business pavilion to find the vendors who can help them meet their goals. We have seen a few primes come by and talk to us and when they discovered that we are a service-disabled veteran-owned small business (SDVOSB), then the timbre of the discussion changed (though wed rather have people be interested in working with us because of the quality of work that we do) . Maybe, as someone put it, the KOs have been burned too badly by low quality set-aside qualifying firms who cant actually do the job in the past that they are now looking at the major primes to serve as the vetting mechanism. Its not a bad idea.",
      "Im also surprised that nobody is blogging about FOSE. The number of blog posts found on Technorati covering FOSE takes up less than a page. Comparatively, OSCON is usually a top (if not the number one) search term on Technorati during OSCON. There are about 45 news articles on Google covering FOSE, but I would still expect people to be writing (and blogging) more about this convention. Its huge. There must be at least 4 times as many people here than at OSCON. At OSCON, crowds come in waves as people come out of sessions. At FOSE, there is a constant stream.",
      "Still, being at FOSE is worth it, because its important that government users of software become more acquainted and comfortable with open source products and dont have the not invented here (or by a small group of proprietary software providers) syndrome. There are pockets of users of open source products, and some who are interested in learning more, but the road ahead is a long one. We intend to be there; as citizens, we have a vested interest in how our taxpayer dollars are spent."
    ],
    "summary_t": ""
  },
  {
    "id": "c3bba9e1fd44e46d5b8fa74f9348e976",
    "url_s": "https://opensourceconnections.com/blog/2008/04/05/final-thoughts-from-fose-2008/",
    "title": "Final Thoughts From FOSE 2008",
    "content": [
      "FOSE 2008 is over, and were all back home. Here are my observations from the final day of FOSE 2008:",
      "It would have been nice to have our people attending more sessions. Arin Sime got to attend a couple of sessions, and Eric Pugh saw the opening keynote speech, but it would have been nice to attend more sessions to get the government point of view of information technology.\n  The attendees have a general distrust of custom solutions. Its not a wonder. They probably suffer from vendor lock, unresponsive clients, and have generally been burned by bad process.\n  There are pockets of Agile development, but not many. This probably leads to the problem cited above, but it also reflects a greater problem…\n  Contracting officers dont know how to price Agile development. The most common pricing scheme for development projects is fixed price. Agile can only deliver a fixed price for labor. Maybe it will be easier under a blanket purchase agreement or with sprint-sized task orders.",
      "Its too early to say whether this was a worthwhile trade show for us. Each day, we definitely had conversations where people had specific problems that they were trying to solve which our team and process could help them solve. Whether or not we can get from problem identification and solution provider acknowledgment to actually providing the solution remains to be seen."
    ],
    "summary_t": ""
  },
  {
    "id": "250da3396c8f195f996f26e52c62945a",
    "url_s": "https://opensourceconnections.com/blog/2008/04/08/a-skeptics-perspective-on-the-openspace-conference-format/",
    "title": "A skeptics perspective on the OpenSpace conference format",
    "content": [
      "I attended the Continuous Integration and Testing Conference (CITCON) in Denver Colorado this past weekend and experienced first hand the OpenSpace conference format. I was skeptical about the concept of the OpenSpace conference format because it just did not seem structured enough. It turns out I was pleasantly surprised by the amount of real world practical knowledge I received at the conference. For those of you who are not familiar with the OpenSpace conference format I will provide a quick description.",
      "The conference begins with introductions and an overview of the conference. Next all parties attending the conference are invited to propose a topic that they are interested in learning more about or information they would like to share. The attendees then vote on the proposed topics and the topics with the most votes are assigned to a session. Each session is a discussion by all members of the session participants of the topic at hand. The OpenSpace conference format abides by the \"law of two feet\", meaning if you are not getting what you like out of the session you are obligated to use your two feet and move onto another session.",
      "This format is different in that anyone can present, propose and discuss information at each session. The sessions become a discussion of the topic by peers rather than a power point presentation from an \"expert\". You get a real world perspective on what is and is not working for your peers. Often the sessions turns into a debate on the benefits and drawbacks of a specific framework, technology or software package. I learned quite a bit at the CITCON conference and I feel it is largely due to the OpenSpace conference format."
    ],
    "summary_t": ""
  },
  {
    "id": "f3465e9521408ce292b62a03255fbd2f",
    "url_s": "https://opensourceconnections.com/blog/2006/09/06/developers-need-project-managers-too/",
    "title": "Developers need project managers too!",
    "content": [
      "Oftentimes, lead developers wind up playing the role of project manager. For a small project, its OK because the level of administrative overhead is small. For large projects, its the wrong call.",
      "In resource constrained environments, lead developers are often asked to wear many hats. In small companies, this is both understandable and acceptable. Spending the extra money on a dedicated project manager when the lead developer can play that rol–albeit not as effectively–for only the opportunity cost of time may not make financial sense.",
      "However, once a team is working on a project with critical mass, utilizing a project manager becomes essential.",
      "Reason 1: Developers develop better than they manage. Though this is a generalization, and there are developers who manage quite well, the reason that they are on projects as developers is because they can deliver solid code. While they may be good at customer management, administration, milestone setting, and all of the other things that a project manager excels at, theyre probably better at coding. In economics, its the idea of comparative advantage.",
      "Reason 2: Every minute a developer spends on project management is a minute not spent on coding. Most projects have timelines for a reason, and anything that could divert a developer from delivering on that timeline is something that brings on additional risk to the project.",
      "Project managers handle the administrative tasks that set up the developers to be able to get into a zone and crank out great code. They knock down obstacles that come up. They are the face of the project and interact with the customer, keeping the developer from having to answer the customer questions that come up. The project manager can also help manage expectations and keep the project from veering like a driverless car.",
      "Project managers are not a panacea to a project. Putting a project manager on a project will not guarantee delivery. Its still up to the developers on the project to actually produce the code that satisfies the customer needs. When time and resources are constrained, then a willingness to invest in project management skills can increase the chances of successful delivery."
    ],
    "summary_t": ""
  },
  {
    "id": "dd77100e38960ef5316e20a247560eb2",
    "url_s": "https://opensourceconnections.com/blog/2008/04/09/new-fedbizopps-a-step-in-the-right-direction/",
    "title": "New FedBizOpps A Step In the Right Direction",
    "content": [
      "During FOSE, I was surprised to find that FedBizOpps had completely changed its format, look, feel, and usability.  I found out about this because I had bookmarked several pages to look at responding to potential RFPs, and none of them worked.  So, instantly, I had a predisposition against the new FedBizOpps, even though I consistently complained to colleagues about its lack of functionality.",
      "However, once I got over the initial dissatisfaction of losing ten or so bookmarks to a poor conversion, I started looking into the new website.  Now that Ive been using it for a couple of days, here are some observations:",
      "The search agents solve a key missing element.  Instead of having to type in the same searches over and over every day, I can save a search with the criteria that I want and have it scheduled to run on a regular basis, sending me an e-mail with the newest postings.\n  Searching by expiration date is a good capability.  I want to see whats yet to expire, expiring soon, and the like.  However, one element missing in a saved search agent is a dynamic date.  In other words, I cant say that I want postings which expire tomorrow through eternity and have that date move forward daily.\n  This site was not fully tested.  My two biggest pet peeves should have easily been caught in testing: 1) if I open a link (for example, to a posting) in a new tab, it messes up the original tab, and 2) the back button doesnt seem to work.  I like to return to previous activity by using the back button.  Even \"Return to Results\" does not work, and there is no obvious internally linked navigation.\n  Site visitation does not have permanence.  If I keep a tab open on a posting, its probably because I want to come back to it.  If I put my computer to sleep and come back, I cant navigate from that page anymore.  Its quite irritating.\n  Theres no obvious way for me to provide feedback.  If the government wants to improve and wants to serve its customers, then it should make interaction easier.\n  I didnt see it coming.  If I would have known that this was coming and the effects it would have on my searching efforts, I could have prepared adequately.\n  Theres no obvious FAQ or users guide.  Its truly a libertarian site–you figure things out on your own merits, apparently.  No paternalism here!",
      "All in all, as the title implies, I think its a step in the right direction.  The transition was poorly not handled, and its clear that testing did not incorporate the full cycle of usability testing.  I would be surprised if there was a detailed test plan, and if there was, it wasnt detailed enough.  However, the concepts of usability are improved; I just hope its not another several years before the government decides to take the next step."
    ],
    "summary_t": ""
  },
  {
    "id": "0a9b60cfbf1a72e48fedbaf75885ca10",
    "url_s": "https://opensourceconnections.com/blog/2008/04/14/continuous-integration-and-testing-conference-citcon-in-denver-colorado/",
    "title": "Continuous Integration and Testing Conference (CITCON) in Denver Colorado",
    "content": [
      "I attended the Continuous Integration and Testing Conference (CITCON) in Denver Colorado last weekend and I have to say that I was impressed by the effectiveness of the OpenSpace conference format. The following are highlights from the sessions I attended:",
      "What is Continuous Integration? What should be including in Continuous Integration 2.0?",
      "This session was focused on an introduction of continuous integration and a discussion on what should be included in the next version. The following contains a few bullet points of what was discussed at the session:",
      "Modularize the code to create smaller faster tests and quicker builds \n  \n  \n    Source control packages should provide an option to enable automatic updates on commit\n  \n  \n    Source control packages should verify build is green before allowing a commit to the code \n  \n  \n    Commit frequently to shorten the feedback cycle to discover errors or problems quickly\n  \n  \n    A good roll back process can eliminate downtime when a build is broken",
      "How to make builds faster?",
      "A person responsible for the builds at a company has run into the problem with large build times on their project. They were looking for ways in which they could reduce the time for the build. The ideas that were suggested for reducing the time of the build were as follows:",
      "A common practice is to restart the application server after a successful build to setup for the next build. \n  \n  \n    Modularize the build so that only the modified code may be built and not the whole project. Maven has a built in method for modularizing the code.\n  \n  \n    Add or upgrade the build server hardware. The build server should have as good of hardware as the developers hardware. \n  \n  \n    Implement a distributed computing model to offload the build onto multiple machines\n  \n  \n    Analyze the build to determine which part of it is inefficient and determine if the increase in build time is justified, specifically related to tests\n  \n  \n    Tests are a common culprit in extending build times\n  \n  \n    Break tests up into unit, function, integration and system tests. Consider only running each type of tests when it is appropriate \n  \n  \n    Assign a member of the team to monitor the build time using metrics to determine if the build has exceeded the set metrics and analyze the change made to determine if the build time increase is acceptable.",
      "Test data setup techniques",
      "A participant of the conference was interested in discovering the different ways people setup data when they run their tests. The following items were discussed at the session:",
      "Add code to the tests that will add data and later remove the data when the tests have completed. This solution can slow down the build and does not allow the tests to be run independently. \n  \n  \n    Write a script to duplicate the production database with minimal or no data and run update scripts on the newly created database. This gives you a clean database to start testing the application.\n  \n  \n    Maintain scripts to create the database with minimal or no data and run update scripts on the newly created database."
    ],
    "summary_t": ""
  },
  {
    "id": "a0623fbd6403214bc1c5bd9c591456ac",
    "url_s": "https://opensourceconnections.com/blog/2008/04/16/becamp-2008-is-may-2nd-3rd/",
    "title": "beCamp 2008 is May 2nd & 3rd",
    "content": [
      "beCamp 2008 is almost here! May 2nd and 3rd is just two weeks away!",
      "If youre a geek in or around the Charlottesville metroplex or even if youre merely tech-curious, this is the event you dont want to miss. beCamp is Charlottesvilles version of the BarCamp unconference phenomenonâ€\"organized on the fly by attendees, for attendees. Realizing that the most energizing parts of any tech conference are the ad hoc conversations that take place in the hallways between the sessions, beCamp facilitates these types of interactions for an entire event.",
      "As of this writing, we are at 66 campers! To participate, just add your name to the wiki page!",
      "A big thank you to all our sponsors, including Booz Allen Hamilton, University of Virginia ITC, Mustache Inc, The Rimm-Kaufman Group, SNL, and the UVA Library. And a special thank you to CBIC for providing us a great venue!"
    ],
    "summary_t": ""
  },
  {
    "id": "d02e253304c8771806abe8bc040ec258",
    "url_s": "https://opensourceconnections.com/blog/2008/04/16/osc-team-members-at-uva-lsp-conference/",
    "title": "OSC Team Members at UVA LSP Conference",
    "content": [
      "Yesterday four of us from OSC got to spend the day speaking to the UVA Local Support Professionals community on a series of development topics, and learning about the challenges that the UVA LSP folks face in supporting their local departments.",
      "A big thank you to Jim, Arin, and Caleb for joining me at the LSP Conference!",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "2e7fedbaa7f55543783f8e97690684d1",
    "url_s": "https://opensourceconnections.com/blog/2008/04/17/facebook-applications-and-privacy-concerns/",
    "title": "Facebook Applications and Privacy Concerns",
    "content": [
      "This week I spoke at the LSP Conference at UVa, and while I was there, I got to attend several other speeches that were very interesting.Â  One that I particularly enjoyed was on Building Facebook applications, and the potential privacy issues surrounding them.Â  Adrienne Felt is a graduating fourth-year student at the University of Virginiaâ€™s Computer Science department in the School of Engineering and Applied Science (also my alma mater).",
      "She first talked about how to go about building a Facebook app, which was interesting because I have been curious about it, but I hadnâ€™t had the chance to look into it yet.Â  But the second half of her talk was more thought-provoking, because she discussed her research into the privacy issues of Facebook.\nThose privacy issues are particularly relevant because of this article today in the Silicon Valley Insider, titled â€œFacebook Borks Blockbuster: Beacon Turns Into A Lawsuitâ€",
      "The short description of what happened apparently is Facebook and Blockbuster video had a deal where you could put an application on your Facebook profile, and this application in turn was broadcasting to your friends what movies you are renting.Â  When a lady by the name of Cathryn Elaine Harris rented a pornographic movie, she was apparently pretty embarrassed to see it broadcast on Facebook and now she is suing.",
      "Now go back to the Alley Insider article and read the comments if you havenâ€™t already.Â  Once you get past the crude jokes, youâ€™ll see a reply by a poster named Roy stating that:",
      "â€œYou did not need to do any type of opting-in to get this behavior. Simply being logged into Facebook was enough for Beacon to push my Blockbuster rentals to my Facebook news feed. There was no \"do you want to opt-in\" email, there was no \"do you agree to send information from one site to another\" option … it just happened one day.â€",
      "That quote really rings true with what I learned from Adrienne at the LSP Conference.Â  Check out her site on Facebook Platform Privacy.",
      "She is mainly talking about how when you build an application for Facebook, you can force people who install your app to let you get access to all their Facebook data or they wonâ€™t be able to install that app.Â  Most applications require you to let them have access to all your data, even though according to Adrienneâ€™s research only about 6% of them use it.",
      "Now this is a little different than the court case mentioned in Alley Insider, because that was a case of a company providing presumably private customer information to a public data feed without that customerâ€™s consent (or at least that is what her pending suit will allege).",
      "But nonetheless, the article reminded me of Adrienneâ€™s presentation and her work at UVa on privacy issues because it highlighted how willing we are to give up control of our private information to anyone on Facebook who asks for it, just so we can install a Facebook app like Zombie Killer and play an online game with a friend.",
      "While Zombie Killer is considered â€œsafeâ€ and is probably not doing anything bad with your Facebook info, the fact of the matter is that Facebook allows me as a developer to write an application, encourage you to install it, and then I am allowed to pull any information I want (except your email address) from the profiles of Facebook users of my application.Â  I can then store that data on my own server indefinitely and use it for anything I want.Â  Most uses of this will probably be for more direct marketing of products to you as a Facebook user, but frankly is still creepy to me."
    ],
    "summary_t": ""
  },
  {
    "id": "dd449648444480f195fc1f9b841c55e3",
    "url_s": "https://opensourceconnections.com/blog/2008/04/24/adding-timeout-to-nethttp-get_response/",
    "title": "Adding timeout to Net::HTTP.get_response",
    "content": [
      "One of the parts of Ruby that shows the languages lack of maturity is the Net::HTTP module. There are a lot of magic techniques that people have learned to make it work, and some methods work one way, and others differently. For example, if you want to put a timeout on a GET request (which you SHOULD have!), then you have to use two different techniques for Net::HTTP.get and Net::HTTP.get_response. The difference between these two methods is that get_response is a class method that returns an Net::HTTPResponse object while get is an instance method that returns a string. And get_response doesn’t support passing in timeouts of any kind!",
      "ARGH. Apparently no one has really tried to put a timeout on get_response because the only post I could find was this one. And the code just looked very complex and more hackish then I liked. In the solution I came up with, I am looking for favicon files at various websites. And if a website doesn’t actually exist (due to bad data!) then it can take ~160 seconds to timeout!",
      "require 'net/http'\nrequire 'uri'\n\nurl = URI.parse(\"http://SOMESITE/favicon.ico\")\n\n# url = URI.parse('http://www.medicalrobotics.com')\n\nhttp = Net::HTTP.new(url.host, url.port)\n\nhttp.read_timeout = 5\nhttp.open_timeout = 5\nresp = http.start() {|http|\nhttp.get(url.path)\n}\nputs resp.kind_of? Net::HTTPResponse\nputs resp.code\nputs resp.body",
      "By wrapping my call to http.get in an http.start{} block, I get the timeout ability of http.get with the return of an HTTPResponse that Net::HTTP.get_response normally provides.",
      "Update 1/22/2015:",
      "I’ve now received two emails from different folks thanking me for this blog post in the past 12 months. That makes me a very happy blogger!",
      "Update 7/22/2018:",
      "Thanks Cameron for your nice email.  Re-reading this blog post reminded me about a fix to an AngularJS app that had very long timeouts making the app slow!"
    ],
    "summary_t": ""
  },
  {
    "id": "7961dd0e297c866ef8c110e96ff787af",
    "url_s": "https://opensourceconnections.com/blog/2008/04/30/trip-report-shenandoah-ruby-user-group/",
    "title": "Trip Report: Shenandoah Ruby User Group",
    "content": [
      "Last night Joe Meade and I road tripped up to the Shenandoah Ruby User Group (ShRUG) meeting in Harrisonburg, Virginia, hosted at the RosettaStone offices.\n\nWe went there to find out why there is a hotbed of Rubyists in rural Virginia! There were 20+ people at the meeting which was hosted in a conference room at RosettaStone. RosettaStone sponsors the group, kindly providing great sandwiches and soft drinks.",
      "This weeks meeting was based around a Lightening Talk approch, so I of course spoke up and said Id be happy to do one!",
      "Lightening Talk 1: Learning Rails\nLynda.com is a good resource for beginners to Ruby and rails.. Most books deep dive into Ruby very quickly, and this site provies some simpler materials. Also some vidoes, including free ones…",
      "Heroku.com is an online IDE plus platform for Ruby… So dont fight with the installation, just go through web interface to get that first hit of coding Ruby on Rails!",
      "He had setup an account for us, the username is [email protected], and I have the password, however not sure if he wanted to publish it.. Email me and I can share it. You can see the deployed application at http://shrug.heroku.com/, right now its just an empty shell.",
      "The idea of using Heroku as a learning tool really worked for me. I know anytime I teach someone Ruby on Rails we have the initial battle in getting SQLLite to work, or the right gems in place. And I always have to say: trust me, the rest is much easier. Especially when showing some on Windows Ruby for the first time… With Heroku they can get something up quickly, and get over the \"I Suck\" stage quicker.",
      "I didnt catch the name of the presenter…",
      "Lightening Talk 2: Open ID\nI talked about OpenID, starting with Code Monkey, because I like the line about \"Maybe manager wanna write goddamn page himself\" which is at second 28 of the clip. I did a demo of how HighTechCville uses OpenID, and showed a bit of the Ruby on Rails OpenID plugin, and how easy it is to integrate.",
      "Lightening Talk 3: Prototype\n\"Snuggs\" did a presentation about how simple Prototype is, and showed us how he used it to quickly create a gallery of pictures that allows paging via Ajax and Javascript that is on his media site MonstarOnline The photowidget javascript all based on Prototype is at http://www.monstaronline.com/global/ecma/photowidget.js.",
      "Lightening Talk 4: IRB\nJeffMo talked about what IRB is and what we can use it for… He did a great session showing IRB and demonstrating how you can do things like spelunk what methods are available on your objects:",
      "s = String.new\n    s.methods",
      "or search for a specific method",
      "s.methods.grep /reverse/`",
      "We also learned a bunch about how to use method_missing to catch methods that arent defined, and make intelligent decisions on what to do with them, as well as defining methods on the fly.",
      "Lastly we saw a Domain Specific Language for playing Tic-Tac-Toe, all using dynamic methods and method missing! You can download the files and try it yourself from JeffMos site: http://www.jeffmo.us/shrug/\n.",
      "So, to sum up, while the Charlottesville RubyCodeJam has beer and cool shirts, and ShRUG has softdrinks, clearly were doing something wrong as they have double the turnout we usually do! It was great to meet some new people, and I hope some of them make it down to beCamp, which is THIS weekend!</p>"
    ],
    "summary_t": ""
  },
  {
    "id": "6a3327ef318db40ba10c67f4fee6363c",
    "url_s": "https://opensourceconnections.com/blog/2008/05/01/agile-friendly-test-automation-toolsframeworks/",
    "title": "Agile-Friendly Test Automation Tools/Frameworks",
    "content": [
      "I dont normally post about another blog article, but Agile-Friendly Test Automation Tools/Frameworks by Elisabeth Hendrickson is spot on about the challenges of Test Automation in an Agile world and well worth reading.",
      "",
      "A couple years ago I worked on a LIMS system, and we had extensive regression tests written using Selenium. We used the Selenium HTML script, which was great, but maintaining them was a real challenge as over time we renamed buttons, reorganized menus, and were constantly fixing broken regression tests. One thing that I would disagree with Elisabeth on is that the maintenance burden for regression tests goes down if you are running them constantly. If you run them constantly, and the developers are responsible for them passing they same as unit tests, then that keeps them up to date, and adding more value then they might otherwise. But I agree, they are a burden, and sometimes really only worth it for certain high risk/high error portions of your application. Unfortunately putting something like Mercury Interactive based tests under CI is hard, and Selenium isnt that much easier…",
      "A lot of my interest about writing \"Stories\" using the RSpec framework comes directly from trying to make the tests easier for people to read, and separate out the logic of clicking on a button from the desire to perform an action that would require clicking on that button!",
      "I met Elisabeth at CITcon 2007, and I still wear the Test Obsessed wrist band she gave me!"
    ],
    "summary_t": ""
  },
  {
    "id": "6878b2f6965b0c8a9a8f023558422472",
    "url_s": "https://opensourceconnections.com/blog/2008/05/03/becamp-session-notes-how-to-choose-your-development-language/",
    "title": "beCamp session notes:  How to choose your development language",
    "content": [
      "",
      "Today Iâ€™m attending beCamp at the offices of CBIC on the downtown mall in Charlottesville.Â  There have been some very interesting sessions already and attendance is great.Â  I suggested one session,Â which we just had this morning.Â  I titled the session â€œHow to choose languages – .NET vs Ruby vs Java vs Php vs ?â€.",
      "At OpenSource Connections, we have a wide range of skillsets covering all the major languages, and we have the pleasure of working with a wide variety of clients where we get to use all those different skillsets.Â  Itâ€™s a lot of fun and something I really like about OpenSource.Â  Often times for our projects, the clientâ€™s needs specifically dictate a particular language, but other times they have no preference or are building a system from the ground up and we get to helpÂ choose the language.Â  I have my ideas of the various benefits and disadvantages of the languages, but I proposed this session to hear other peopleâ€™s feedback as well.",
      "We had a great group of about a dozen people in the session, with skill sets that covered c, c++, perl, .net, php, java, ruby on rails, cold fusion, and even Delphi.Â  This wide range of experience brought some interesting perspectives.",
      "I started the conversation by stating that all development languages are just tools, and the important thing is what we build at the end.Â  The important thing is not necessarily did we build it in .NET or Ruby, but did we meet the customerâ€™s needs?Â  I prefer to be technology agnostic, and most everyone in the session seemed to agree with that sentiment.",
      "One of the first things said in the session was probably the most observant:Â  the choice of language is often presented to us as developers, and that decision is actually made by the business, not development.Â  I think David from the Darden Business school at UVa was the one who pointed that out first, and itâ€™s certainly very true.Â  The group seemed to agree with that, but with some reservation that we wish we could have more control over the language used.",
      "Mike from Fabjectory pointed out that in reality, the framework may be more important than the language chosen.Â  Ruby on Rails may be the best example of this, because itâ€™s an easy to use framework that encourages you to do good development practices.Â  Microsoft has release the MVC framework to accomplish a similar goal.Â",
      "There was some discussion at the beginning when Caleb expressed a hope that there would be fewer different development languages in the future so that developers donâ€™t have to try and keep up with so much, but others pointed out that competition and having many diverse languages is part of what encourages innovation and new features.Â  For example, Jim pointed out that Ruby on Rails is great for web development in part because itâ€™s geared specifically towards that process, whereas ASP.NET has procedural carryovers in the language from the old classic ASP and VB days.Â  Likewise, Jim expressed the opinion that C# has become a better language than Java because it is more fresh and doesnâ€™t have as many legacy features as Java.",
      "Your final deployment server will also drive the choice of language naturally.Â  If you company will only run Windows servers, then you probably donâ€™t want to try and run Ruby on Rails on top of SQL Server on a Windows server.Â  You should probably stick with .NET.Â  Likewise, if you are going with an Apache server, you should probably consider Ruby on Rails or PHP.",
      "Who will maintain your application after it has been deployed?Â  If you are a consultant, than the skills of your client need to be considered.Â  How much do they want to modify it themselves, and what skill sets do they already have that will make this easier.Â  I suggested that because of the development tools and the fact that itâ€™s a compiled language, I feel .NET is a much more scaleable and better language than PHP.Â  However, if a client wants to tweek small parts of the code themselves, then Php may be a better choice because itâ€™s easier to modify and perhaps easier to understand to a non-developer but tech savvy person.Â  But with that ability to make changes easily also comes risk:Â  If the people making the changes donâ€™t know the whole system as a whole or good development practices, then the easily â€œhackableâ€ nature of PHP may actually get them in trouble.Â  A more object oriented structure in .NET that needs to be compiled may be more stable and less susceptible to mistakes.",
      "Considering how to make modifications in the future is also important.Â  David from Darden expressed his opinion that when using Java web development frameworks, Struts is easier to make modifications to than Tapestry.",
      "Towards the end of the session, we also briefly touched on a few other topics which I will quickly summarize peopleâ€™s opinions here.Â  So here are more random thoughts from the session:",
      "Â  .NET is the best option for building windows based desktop applications, but you should consider QT if you want a more cross platform compatible application.Â  Java Swing according to one anonymous attendee â€œstill sucks 15 years later.â€\n  \n  \n    One of the great things about Ruby on Rails is the built in testing and framework scaffolding.\n  \n  \n    Mono can be used to make .NET applications run on other OSâ€™s. Â Though there wasnâ€™t much experience with actually doing that in the room, that is what some major companies are doing like MySpace.\n  \n  \n    Building webservices is very easy in .nET, REST based applications are also easy in Ruby.",
      "We ended the session on a funny note, when Jim made the statement that a lot of very large companies still use C for development.Â  Though the people he is thinking of are older developers and so they may just be tied to older languages, they are still some of the brightest minds heâ€™s known.Â  I believe it was pointed out that Amazonâ€™s services are built with C.Â  So weve come full circle through C to .NET, Java, and Ruby, and perhaps ultimately back to C!Â  (just to be clear, I state that with sarcasm.Â  Although you can undoubtedly do all kinds of things with C, the value of all these newer languages and frameworks is how much easier they make it).",
      "Thanks to all those who attended for a great discussion (completely devoid of any fist fights about which language is best), and Iâ€™m looking forward to the rest of beCamp this afternoon!",
      "You can see a slideshow of my pics from beCamp 2008 here"
    ],
    "summary_t": "Notes from the 2008 BeCamp"
  },
  {
    "id": "a1ee837706ed9cd489d2788021b68673",
    "url_s": "https://opensourceconnections.com/blog/2008/05/12/the-simple-knowledge-organization-system-skos-in-the-context-of-semantic-web-deployment/",
    "title": "The Simple Knowledge Organization System (SKOS), in the Context of Semantic Web Deployment",
    "content": [
      "This past Thursday, May 8th I had the privilege to attend a presentation at the Library of Congress by Alistair Miles, key developer of SKOS, and semantic web practitioner at the University of Oxford. The presentation was held at the Library of Congress because they are very interested in using this emerging technology for their catalog. SKOS is very close to becoming the W3C standard for publishing thesauri, classification schemes and subject headings as linked data in the Web. SKOS is built upon Resource Description Framework (RDF) and its main objective is to enable easy publication of controlled structured vocabularies for the Semantic Web. SKOS was developed to provide a more simplified alternative to OWL.",
      "Alistair started out the presentation by demonstrating the value of HTML links and how they have made web pages and the Internet more accessible. He then described SKOS as the standard means of linking data across the Internet. SKOS has concepts of a label, alternate labels, notes, broader and narrower where the label is the title, alternate labels are different spellings, notes are a description and broader and narrower attributes are used to link to related concepts.",
      "RDF was developed as a means to declare metadata of resources in a standard way. An RDF description of a resource contains a subject, predicate and object where the subject is the title of the resource, the predicate is the type of resource, and the object can be data or another resource. RDF is basically a standard XML schema for defining resources such as music, books, articles, etc. For an example take a look at the following from Dublin Core. An example of using RDF in an XHTML file is as follows:",
      "<p class=\"contactinfo\" about=\"http://example.org/staff/jo\">    <span property=\"contact:fn\">Jo Smith</span>.    <span property=\"contact:title\">Web hacker</span>    at    <a rel=\"contact:org\" href=\"http://example.org\">      Example.org    </a>.    You can contact me    <a rel=\"contact:email\" href=\"mailto:[email protected]\">      via email    </a>.  </p>",
      "This example contains several attributes with the word \"contact\" in it. Each attribute defines a piece of the contact information, for example, the attribute contact:fn defines the contacts first name.If you have a need for linking data in your web applications take a closer look at the technologies above for use in your application."
    ],
    "summary_t": ""
  },
  {
    "id": "fcbd90d7253c72e96ee6f4193cbbb8f9",
    "url_s": "https://opensourceconnections.com/blog/2006/09/07/brazilian-scrum-session/",
    "title": "Brazilian Scrum session",
    "content": [
      "Going to be in Brazil in October? Leandro Cruz from Web007 will be leading a session on Scrum during the Borland Conference on Oct. 6 through 8th. Hes done a lot of work with spreading Scrum among Brazilian agile developers and promises to have a great session."
    ],
    "summary_t": ""
  },
  {
    "id": "524d605030004e1421cce8c8a029a70b",
    "url_s": "https://opensourceconnections.com/blog/2008/05/13/becamp-2008-rocked/",
    "title": "beCamp 2008 Rocked!",
    "content": [
      "Well, the thank you notes to sponsors and volunteers have been written and mailed, the venue has been cleaned up, and the accounting for funds spent has been dealt with. beCamp 2008 is officially complete, and what a wonderful experience it has been!",
      "",
      "I want to thank the over 90 geeks that showed up to spend 36 hours sharing what they know, learning from each other, and pitching in to make beCamp flow smoothly. I know of no other approach to conferences that has such high signal to noise ratio with such a low level of administrative overhead. Id say it turned the Pareto Principle on its head, and 80% of the effort came from 80% of the participants!",
      "Thank you very much beCampers, and see you next year!"
    ],
    "summary_t": ""
  },
  {
    "id": "6ebc0dc81cfb26c591649965f61b8288",
    "url_s": "https://opensourceconnections.com/blog/2008/05/20/relaunch-of-charlottesville-dot-net-user-group/",
    "title": "Relaunch of Charlottesville Dot Net User Group",
    "content": [
      "Last week I decided to learn a bit more about .NET, and attended the relaunch of the Charlottesville .NET User Group who was featuring Geoff Snowman talking about Windows Workflow Foundation.",
      "I learned a couple of things that I wanted to share:",
      "A great starting point for learning about WF is a demo site at http://dinnernow.net/\n  meant for developers, not an enduser/business analyst product. \"Its just a DLL\" – Geoff Snowman\n  SharePoint heavily leverages the WF tool\n  Now being used by 3rd party folks.. K2 builds an enduser focused workflow tool. They have ditched their engine in favor of WF.\n  Workflow is a higher level of abstraction.. You can do it in C#, but why would you want to be in the weeds?",
      "A big thank you to Matt Sposato for getting the relaunch of ChoDotNet started! He did a great job getting the meeting going, and with ~40 folks, I look forward to seeing the momentum maintained!",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "a4fd4d54284b75b9e1d838a967d1e173",
    "url_s": "https://opensourceconnections.com/blog/2008/05/20/tornado-provides-gocho-com-with-its-first-test/",
    "title": "Tornado Provides GoCHO.com With Its First Test",
    "content": [
      "Last Thursday, as thunder pealed in the background, I turned the television on to the local NBC affiliate to see what was happening with the weather. As it turned out, our county was under a tornado warning, with high winds and heavy storms. As the anchors talked, they said that they had checked the Charlottesville Albemarle Airports website, www.gocho.com, to check on the flight status of inbound flights and that flights had not been significantly delayed because of the storms (there were no tornadoes, fortunately). The NBC 29 anchors were not the only ones checking www.gocho.com that night. Since we launched live on April 21, the website had not seen as much traffic as it did on the night of the tornado, and into the early morning hours. Note the May 8 and May 9 traffic. The tornado warning lasted from about 10 – 11 PM on May 8, and hits to the website carried over into the next day. The site that enabled the anchors at NBC 29 to see how the weather was affecting flights was an endeavor four months in the making. The old Charlottesville Albemarle Airport Authority website had a Flash introduction and a difficult to navigate website. The Director, Barbara Hutchinson, sought to embark on a strategy that made the Charlottesville airport Central Virginias airport of choice, and she wanted a website which contained all of the information that a traveler would want from an airport, with as much information as possible without clutter. Thus, OpenSource Connections, with Birch Studio Graphics offering design and Flash support, came up with an airport-focused content management system that allowed the GoCHO team to communicate most effectively with their clients. The biggest issue that they sought to overcome was handling phone calls from the public to answer questions that they could also find on a well-designed website. By reducing the volume of phone traffic and channeling those questions to the website, the Airport Authority could focus on operating the airport and improving the experience of travelers. Working with Ms. Hutchinson and her team, we were able to pinpoint the highest value information that travelers sought and to tailor and deliver a website that provided that information in an easy to find manner. Additionally, by implementing a content management system, we were able to facilitate the Authoritys communication of information to the public. The old site was in static HTML, and was difficult to update information, whereas the new website can be updated on the fly. JetBlast, the new GoCHO blog, is one example of ways in which the Airport Authority can now communicate much more rapidly with the general public, and receive feedback on what they have to say. We have also tied in FAA flight information and travel cost information from Kayak to a front page tabbed browsing system so that users can find flight deals from the CHO airport, creating a one-stop destination for people who want to find out more about commercial travel at the airport. The resulting site, we think, serves the public well.  What do you think?  Contact us and let us know!"
    ],
    "summary_t": ""
  },
  {
    "id": "5551d884d753dc0196622c2c95055259",
    "url_s": "https://opensourceconnections.com/blog/2008/05/23/semantic-web-technologies-rdf-and-owl/",
    "title": "Semantic Web Technologies RDF and OWL",
    "content": [
      "I had the pleasure of attending the Semantic Web Technologies RDF and OWL workshop with Bob DuCharme at the UVA New Horizons Conference. Bob is a well respected contributor in the semantic web community and has written several books related to the topic. After attending this workshop I feel like I finally grasped the concepts of the semantic web technologies. Bob did a great job of explaining the different technologies in a way that a technical person could easily understand. The workshop began with a discussion of Resource Description Framework (RDF), which is a means to store metadata about resources. A resource can be anything from an audio (mp3, wma, wav), video(wmv, mov, mpg), e-book, etc. The metadata can be stored within the actual file or in a separate linked location. The metadata is made up of a simple data structure containing a subject, predicate and object. Personally I am not fond of these terms as they are a bit confusing and I find the best way to describe them is with an example from Wikipedia.",
      "<rdf:RDF  xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"  xmlns:dc=\"http://purl.org/dc/elements/1.1/\">     <rdf:Description rdf:about=\"http://en.wikipedia.org/wiki/Tony_Benn\">        <dc:title>Tony Benn</dc:title>     </rdf:Description></rdf:RDF>",
      "The RDF code above can be parsed into the following triplet:",
      "Subject: http://en.wikipedia.org/wiki/Tony_Benn Predicate: http://purl.org/dc/elements/1.1/titleObject: \"Tony Benn\"",
      "Which in plain English translates into, \"The title of this resource, which is published by Wikipedia, is `Tony Benn’\". Now we could add several more tags to this example similar to the \"dc:title\" tag to describe many aspects of the object such as publisher, contributor, etc.RDF can be assigned to a resource in several different ways and using different syntax. For example, Notation 3 is a syntax for defining RDF in a readable format without the use of XML. RDF can be embedded in HTML as well to define resources within a web page. Several popular web sites currently embed RDF in their web pages including www.wikipedia.org and www.digg.com.",
      "Bob also discussed another web semantics technology referred to as Web Ontology Language (OWL). OWL is an extension of RDF Schema which is a common set of terms defined to describe a domain (A domain being something such as music, psychology or biology). Which basically means an agreed upon list of terms to be used to describe something. The Dublin Core is commonly used to describe video, sound, image and text with several metadata elements or \"terms\".Another technology discussed during the workshop was SPARQL Protocol and RDF Query Language (SPARQL) which is the query language used with RDF. If you think of RDF as a huge database available on web, SPARQL would be the SQL language used to query the database. It was with this thought during the workshop that I realized the remarkable potential these semantic web technologies possessed. Now imagine all these RDF \"databases\" are linked together and you can query all of them at once. DBpedia is a project designed to do just that by extracting information from Wikipedia, making the information available on the Web and linking to other data sets such as MusicBrainz.",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "316fe70d4893c87897fc4745bcb138c2",
    "url_s": "https://opensourceconnections.com/blog/2008/05/29/who-hires-veteran-owned-businesses/",
    "title": "Who Hires Veteran-Owned Businesses?",
    "content": [
      "A couple of days ago I did some competitor research. My goal was to see a five-year timeline of SDVOSBs that were operating in our NAICS (541519, 541512, 541511, 519130, and 518210). The metrics I was looking for were total income, employees, and federal contract award amounts. From this I hoped to draw some comparison to us.",
      "After navigating a metric shit-ton of reports, statistics databases, and spreadsheets, I was only able to come up with the table below (which is about a third of my goal.) Its based on a 2002 survey of small, minority, and women-owned businesses. In particular, the section Im including represents veteran-owned businesses. There is a section of the original table that does not account for veteran status, so a comparison could be made to show its affect on customer distribution. (By the way, they do this survey every 5 years and are doing one now. The new data wont be published for about 3 years.)",
      "</col> </col> </col> </col> </col> </col> </col> \n      \n      \n        2002 NAICS code\n      \n\n      \n        \n          Kind of business and types of customers\n        \n      \n\n      \n        \n          Employer respondent firms\n        \n      \n\n      \n      \n    \n\n    \n      \n        \n          Number\n        \n      \n\n      \n        \n          Percent \n        \n      \n\n      \n      \n    \n\n    \n      \n      \n    \n\n    \n      \n        \n          Total\n        \n      \n\n      \n        \n          RSE\n        \n      \n\n      \n        \n          Total\n        \n      \n\n      \n        \n          SE\n        \n      \n\n      \n      \n    \n\n    \n      \n        51\n      \n\n      \n        INFORMATION\n      \n\n      \n        \n           \n        \n      \n\n      \n        \n           \n        \n      \n\n      \n        \n           \n        \n      \n\n      \n        \n           \n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n         Types of customers, total\n      \n\n      \n        \n          4,457\n        \n      \n\n      \n        \n          4\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Federal government\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          3.7\n        \n      \n\n      \n        \n          0.4\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        State and local government\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          12.3\n        \n      \n\n      \n        \n          1.6\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Export sales\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          2.5\n        \n      \n\n      \n        \n          0.6\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Other businesses/organizations\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          73.3\n        \n      \n\n      \n        \n          3.8\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Household consumers/individuals\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          26.0\n        \n      \n\n      \n        \n          1.2\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        All others\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          12.6\n        \n      \n\n      \n        \n          1.1\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Item not reported\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          1.4\n        \n      \n\n      \n        \n          1.0\n        \n      \n\n      \n      \n    \n\n    \n      \n        54\n      \n\n      \n        PROFESSIONAL, SCIENTIFIC, & TECHNICAL SERVICES\n      \n\n      \n        \n           \n        \n      \n\n      \n        \n           \n        \n      \n\n      \n        \n           \n        \n      \n\n      \n        \n           \n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n         Types of customers, total\n      \n\n      \n        \n          79,207\n        \n      \n\n      \n        \n          -\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Federal government\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          3.8\n        \n      \n\n      \n        \n          0.2\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        State and local government\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          9.5\n        \n      \n\n      \n        \n          0.2\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Export sales\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          1.1\n        \n      \n\n      \n        \n          0.1\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Other businesses/organizations\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          59.0\n        \n      \n\n      \n        \n          0.7\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Household consumers/individuals\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          39.5\n        \n      \n\n      \n        \n          0.5\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        All others\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          21.2\n        \n      \n\n      \n        \n          0.4\n        \n      \n\n      \n      \n    \n\n    \n      \n         \n      \n\n      \n        Item not reported\n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          X\n        \n      \n\n      \n        \n          2.0\n        \n      \n\n      \n        \n          0.1",
      "Table 1: Customer Types for SDVOSBs",
      "If conclusions can be drawn from this, Id say that:",
      "These businesses were 3 to 4 times more successful at contracting at the state and local level\n  \"Other businesses\" were their mainstay…\n  Followed closely by individuals.",
      "Of course, there are some other considerations that are not factored into these numbers. Id really like to know if either mentor-protégé programs or teaming arrangements affect the distribution. Also, there is an increasing trend of federal procurement through large, multiple-award \"contract vehicles\" which could raise the barrier for small businesses."
    ],
    "summary_t": ""
  },
  {
    "id": "fe61a3f74d5b3ff537ef5c75723672ad",
    "url_s": "https://opensourceconnections.com/blog/2008/05/30/deploying-ruby-to-dreamhost/",
    "title": "Deploying Ruby to DreamHost",
    "content": [
      "Recently, I completed my first Ruby on Rails application, which you can see here. Its not a complex application, basically its a survey of political candidates with three data models: candidate, question, and answer. Pretty straightforward.",
      "I built the Ruby app using Scaffolding, which was very easy and I liked alot. Scaffolding gives you the nice pre-configured classes and pages for adding, editing, and deleting records, but my survey doesnt line up exactly to that model (ie, I dont want anyone to go edit or create the questions on the survey, and I wanted to create a flow from creating your candidate record to then answering each question in the proper order). With some help from Eric, I soon got into the REST mindset and got the application working properly. That was cool.",
      "I didnt consider deployment too much until I had the app completed and working on my local instance. My plan was to just deploy it to my DreamHost account, which I assumed would be easy. Im used to deploying .NET applications, which is extraordinarily simple, and it didnt occur to me at first that Ruby might not be as easy. After copying my files out to DreamHost, I found this wiki page of theirs. I was a little bit annoyed at first after reading that, because I realized I would have to run a bunch of commands to get rails running and the permissions correct.",
      "Fortunately, as Eric pointed out to me, DreamHost already has a solution. Now they use mod_rails, and all you need to do is copy out your code, check the box for mod_rails, and then use their configuration panel for that domain to point to the public directory used by ruby on rails. After doing that, everything worked great and I have to say that I am now pretty sold on Ruby on Rails. One thing Ive always liked about .NET is that its pretty easy to deploy. Now that Ive seen Ruby on Rails can be easy to develop and also easy to deploy (at least with DreamHost), its a much more attractive language to me."
    ],
    "summary_t": ""
  },
  {
    "id": "d31f7e9c47be0521bf1cf4dffe629798",
    "url_s": "https://opensourceconnections.com/blog/2008/06/02/charlottesville-net-user-group-meeting-info/",
    "title": "Charlottesville .NET User Group – Meeting Info",
    "content": [
      "Thank you to everyone who helped with the May 15th meeting of the Charlottesville .NET User group. In no particular order this includes Al Tenhunfeld of Dominion Digital for providing Pizza and assisting with other meeting related tasks, Eric Pugh of Open Source Connections for bringing a fine selection of Pilsners and Ales and creating the introductory slideshow, Eric Meier of the McIntire School of Commerce for founding our group and spreading the word at UVA, Geoff Snowman of Microsoft for his presentation of Windows Workflow Foundation and Charlottesville Business and Innovation Council for use of their offices. So whats next? Our next meeting is scheduled for Thursday July 17th. The current working topic is Microsofts Silverlight. But please suggest topics or speakers for future meetings."
    ],
    "summary_t": ""
  },
  {
    "id": "7bb983794a0e4ca226dd10719c6406c6",
    "url_s": "https://opensourceconnections.com/blog/2008/06/04/opensource-connections-announces-launch-of-aerowebonline-com/",
    "title": "OpenSource Connections Announces Launch of AeroWebOnline.com",
    "content": [
      "Today, OpenSource Connections announced the launch of AeroWeb, the airport content management system, available at http://www.aerowebonline.com. Based on the success of our initial AeroWeb platform client, the Charlottesville Albemarle Airport Authority, http://www.gocho.com, we are commercializing the platform by which airports can manage their public facing websites. AeroWeb delivers powerful features like up-to-date flight tracking, inline web editing of content, airfare deals, and flight booking€\"all in an easy-to-maintain and flexible architecture. It includes features such as inline page editing, flight tracking, booking and reservation engines, and airport blogs and news aggregators. Most airport websites are staid and not interactive; AeroWeb changes that and puts the airport customer at the front of the line. We will be demonstrating AeroWeb at the American Association of Airport Executives annual conference in New Orleans from June 8 – 11. Come check us out there, or go to the website, http://www.aerowebonline.com, for more details."
    ],
    "summary_t": ""
  },
  {
    "id": "4e240ccc7cf7232ef810dbba8f53643d",
    "url_s": "https://opensourceconnections.com/blog/2008/06/05/php-is-the-new-perl-22-reasons-php-is-hard-to-work-with/",
    "title": "PHP is the new PERL, 22 reasons PHP is hard to work with",
    "content": [
      "PHP was one of the first languages that I learned when web design was my primary focus as a career. It seemed to be simple with plenty of examples on how to use it as well as plenty of code to grab to use on the fly in order to get the job done so that I could concentrate on what I loved the most, doing design. However along the way, I somehow got sucked into the programmer paradigm and ended up being a professional code monkey.",
      "As such, my exposure to quite a few other languages, features, and coding paradigms have drastically increased as Im a sucker for new technology and things of the geekified nature minus star trek, dungeons & dragons, and obsessions with super models. Now PHP seems to be more of a thorn in my side as a programmer than anything. Since I do have working knowledge of the language, especially its Object Oriented Features, magic methods, its various editors, extensions, and its limitations and quirks, I tend to get drawn into PHP projects. Working on a PHP project makes me long for a good rails or asp.net project because PHP just makes me feel dirty as a programmer.",
      "PHP has gone the way of PERL: somewhat usable, a few good features and scripts, but stagnating with its ability to push the language itself to compete with other modern languages.",
      "So what makes PHP so bad to work with?",
      "1) Its object syntax is not intuitive. With symbols like \"->\" or \"::\" for accessing methods, properties, and fields on its objects is a pain to type, especially when most modern languages use a dot \".\". At least when you use C, you get a lower level performance boost, but a web languages that is interpreted, why make it a pain to use the OOP? So even with a great tool like the eclipse PDT for PHP, it still sucks cause you have to -> which is not as easy as just typing a . (How much is your time really worth?)",
      "2) Namespaces will not be supported till PHP 5.3.0 and its very limited. Not to mention, that if anyone uses namespaces it will not be backwards compatible with the rest of the PHP 5 versions out there which can become confusing.",
      "3) Core commiters on the PHP internals are not exactly developer friendly. I used to be a heavy user on Site Point Forums where there is a huge PHP community. Much of which have tried to give feedback, patches, and fixes for PHP, to no avail. (The creator of simple test, Chris Shiflett and some other big names in the PHP community have hung out on those boards). If anything you usually get a sarcastic reply if not completely ignored altogether.",
      "4) No support of mixins. Ruby, Javascript, even C# has some kind of support for mixins (bascially a friendly type of multiple inheritance) these days.",
      "5) Cryptic and chaotic naming of functions like strstr. stristr, strpos, empty, usort, etc. This makes its increasingly hard on anyone who has to know multiple languages to go back and forth because of the extra memorization required to be proficient to know what functions are which.",
      "6) Poor object oriented support and no php objects for value types. Array, String, etc should have be objects so that you can do something like $arr->count or $arr->sort();, or $str->toLower(); rather than trying to remember the cryptic names for things to manipulate strings and arrays.",
      "7) Forced heavy use of things like the function \"isset\" instead of just checking to see if a variable is null or not.",
      "8 ) Hidden language features like the DateTime object that comes without any real documentation or the soap extension and where to put http authentication.",
      "9) Heavy use of building templating languages/tools like smarty template engine, expression engine on top of PHP which is a HTML template language. To make matters worse there are no development tools built for the new templating language/tool like visual studio does for asp.net tags, so the developer has to guess what is available or shuffle through undocumented code to figure out the hidden things, that is not covered in the lack of documentation provided.",
      "10) Unicode support will not be available till PHP version 6 which has been in development for over 2 years or more.",
      "11) There is not a standard framework to build on like rails is for ruby or .net is for c#, therefore everyone does their own thing and there is a lack of convention over configuration since there is a lack of uniform coding conventions.",
      "12) PHP does not come with its own web server like rails has with webrick and asp.net has with cassini, so setup just for local development tends to be a pain.",
      "13) Very few PHP applications take relative pathing into account which makes development on various boxes, servers, etc a pain to accomplish (like one developer on a mac and another using a windows box with IIS).",
      "14) PHP just does not evolve fast enough as an opensource project should, especially now that its playing catch up to both c# and ruby. (Its now just getting namespace support and the zend framework is chasing after rails but failing to come close).",
      "15) PHP tries to imitate Java way too much and has paid the price of doing so. Java is generally not the tool for doing web development, to which PHP is targeted for, yet as PHP tries to work with java, ruby is already light years beyond with things like JRuby and Grails and does so without trying imitate java in any fashion.",
      "16) PHP does not get enough push to do things besides websites. PHP Gtk, PHP Phalanger (.net compiler for PHP, to which PHP runs faster when compiled as .net code) are both failures. However Iron Python and Iron Ruby are constantly being worked on and have a promising future.",
      "17) Lack of consistency with function signatures $needle, $haystack in many php functions are always switched up.",
      "18) Pear is simply a repository of poorly written code and holds nothing on ruby gems.",
      "19) Most PHP software works, but is a pain to customize because its mostly spaghetti code thrown together with very little actual documentation, use of design patterns, convention/structure, and almost always has a sub par module/plugin system that is cryptic at best and lacks any kind of real API that is easy to program against.",
      "20) PHP is like IE6 or Java, too slow, too lazy, and now too late to really make a come any real comeback to catch up with ruby or c# or python.",
      "21) If a static method is called from a derived class, there is no way to get the derived class type without overriding the static method in the derived class or passing the type in as a parameter. Which makes creating a activerecord class with factory methods a pain as you would need to copy methods to each derived class. So it would look something like",
      "Person::find(\"Person\", array(\"conditions\" => array(\"name = ?\", $name)));",
      "vs",
      "Person.find(:conditions => [\"name = ?\", name]) in ruby.",
      "22) PHP 6 is the new PERL 6. It is still coming, years later….",
      "Granted there are some nifty frameworks like prado or code igniter, but their limited due to the language limitations…\nSo shouldnt zend, the php company in Israel just scrap the language and start fresh, fixing all the language quirks? Maybe even use something like antler to speed up the process of creating a new web 3.0 language? Or maybe push PHP the community to get its act together and put something together thats not so behind the times before they go the way of the dinosaur? Or should php just march into oblivion?"
    ],
    "summary_t": ""
  },
  {
    "id": "c7aba4fb16654e748fa59544e202298b",
    "url_s": "https://opensourceconnections.com/blog/2008/06/10/photos-from-aaae-conference/",
    "title": "Photos from AAAE Conference",
    "content": [
      "Ive uploaded some photos to Flickr of the debut of AeroWeb at the AAAE Conference in New Orleans.Â  You can see photos of our booth, as well as some of the fun we had at the Mardi Gras World event on Monday night.Â  Its been a great conference, and I want to thank everyone who stopped by our booth to discuss AeroWeb!",
      "You can view the photos here.",
      ""
    ],
    "summary_t": "The debut of AeroWeb at the AAAE Conference in New Orleans."
  },
  {
    "id": "ab3f6d9ba09635644445ac050106a6ff",
    "url_s": "https://opensourceconnections.com/blog/2006/10/01/the-long-tail-of-the-it-supply-chain/",
    "title": "The Long Tail of the IT supply chain",
    "content": [
      "In the past two years, Chris Andersons Long Tail has become relatively common in the business lexicon. Does the long tail apply to coders?",
      "Common business knowledge says that companies should focus on their core competencies. Having fat staff that doesnt contribute to the focus of the company reduces overall efficiency of the companys resources. Return on assets and revenue per employee drops. Additionally, internal teams tend to lose their edge and do not keep up with the latest trends and knowledge.",
      "The answer to this problem is outsourcing tasks that require specialized knowledge. A common example is utilization of external law firms for major court cases. While companies maintain legal counsel for ordinary, non-specialized tasks such as employment contracts and basic SEC filing, few companies will look to their internal corporate legal staffs to lead the defense of a lawsuit against the company.",
      "This process also applies to IT staffs. Internal corporate IT staffs are generally well-skilled, but generalists. They can keep the engine running, but are not staffed appropriately to handle large projects. Hence, companies look to external agencies for staff augmentation. Not only can external agencies help smooth out staffing requirements, but these contracted developers can bring in new knowledge to the existing IT teams.",
      "In applying Long Tail theory, corporate IT staffs are the head of the curve, and the contracted developers are the tail.",
      "Typical recruiting processes focus on the head rather than on the tail. This makes economic sense for corporate recruiters and for recruiting firms. They seek to place developers in full-time jobs, and the economic incentives of working to develop the capabilities to deliver short-term contract developers with specific skills do not provide the profitability and value to move down the tail. Recruiting agencies utilize push processes; coders who want full-time jobs go to recruiters, and the recruiters push those coders to companies that are looking to hire coders.",
      "When looking for project staffing or short-term (under 1 year in duration) staff augmentation, look down the long tail for solutions. If you want to go down the long tail for your project or staff needs, contact us.",
      "Also, if you are one of those coders who lives in the Long Tail and doesnt want a full-time job, but also doesnt want to have to consistently self-market, send us a resume, code sample, and rate sheet. We are constantly looking to build up our contact list of exceptional coders who are interested in doing onsite short-term contracts."
    ],
    "summary_t": ""
  },
  {
    "id": "c30ed291a632baa64e306699c0fe4492",
    "url_s": "https://opensourceconnections.com/blog/2008/06/10/update-from-aerowebs-debut-at-aaae/",
    "title": "Update from AeroWebs debut at AAAE",
    "content": [
      "Its Tuesday morning, which if the final day for exhibitors at the AAAE conference for airport executives in New Orleans. Eric, Riley, and myself, along with my wife Lauren, have been here since Saturday talking to regional airports around the country about OpenSource Connections new product AeroWeb.",
      "The reception has been really great so far. While I think exhibitors always wish that there was more traffic by the booths, the quality of many of our leads has been very good. We are demoing the websites on our laptops, and have a screencast of AeroWeb running on a large monitor at the front of the booth. The screencast has certainly helped bring people into the booth, and our demos have been running very well so far and everyone who sees them seems to be impressed.",
      "Weve had conversations with a wide range of airports, with airport consultants who could conceivably recommend AeroWeb to their clients, with other vendors we could potentially partner with, and with industry journalists. Weve also made a few contacts with companies who may be interested in the software development skills of OpenSource Connections, outside of our airport product.",
      "The features we are showing airports for the most part seem to be right on. A couple of ideas that attendees have brought up have been interesting. One attendee noted that the main reason he felt people go to his airport website currently is not passengers, but vendors who are looking for information about RFPs. So he wanted a content section with a lot of information about RFPs. After looking at the websites of some other airports weve talked to, they also have at least a page with current RFPs listed on it, and PDF downloads of the RFPs right there. Of course, this is easy to do in AeroWeb, but we hadnt put a page like that in our demos.",
      "Another interesting observation made by a trainer from a small airport was \"Can you schedule content to be published at a certain date?\" The answer to this is also yes when you are editing content and blog posts through our back end Expression Engine administration tool. However that functionality and content versioning is not currently exposed through the inline editing tool we are primarily demoing to clients.",
      "Ive got some photos to post, so Ill post those and more comments later today after the show ends. For now, Ive got to run back to the booth for exhibiting hours!"
    ],
    "summary_t": "It’s Tuesday morning, which if the final day for exhibitors at the AAAE conference for airport executives in New Orleans. Eric, Riley, and myself, along with..."
  },
  {
    "id": "f858366b7bfaf619cb66ea98aa0e41bd",
    "url_s": "https://opensourceconnections.com/blog/2008/06/13/newport-news-virginia-goes-open-source/",
    "title": "Newport News, Virginia Goes Open Source",
    "content": [
      "Recently, the Newport News, Virginia Open eGov system was released. Using the Plone content management system, the system is designed for governments to install, out of the box, a website which also includes specialized departmental infrastructure. Newport News has also made the system available under the GPL; it can be found here.",
      "I found this quotation from their Lessons Learned document interesting: \"The adoption of new technology is an iterative process of innovation and learning…\" While they did not use the actual term, it seems that the team which developed Open eGov utilized an agile approach to software development. Agile development does not mean a harem scarem approach to development; the team said that they spent a significant amount of time conducting research on best practices and content management systems before undergoing the customization necessary to launch Open eGov.",
      "We are interested to see if this product gains traction. It is part of the PloneGov project, which, while claiming members in 20 countries, does not have an apparent member list, or much reach. I think that if the Newport News staff wants to extend their reach for the product, theyll need to answer some questions:",
      "How can they increase citizen participation in these sites? I see surveys, but there is no way to comment\n  How to spread the word about the availability of this product? The product is, from appearances, targeted at local governments.\n  Is the Plone/Python/Zope package the best one to facilitate widespread adoption? The community of developers is much smaller than of other developers. Naturally, Scott Stults, our resident Plone and Python expert, believes its the right answer and could be seen dancing with joy when shown the Newport News announcement.\n  Why not put the project into a system that allows user contribution to it? How a developer contributes to the Open eGov project is not particularly clear. Perhaps this is an intentional result of the lack of desire to be the gatekeepers of others contributed code.",
      "Seeing Open eGov is certainly, to us, a positive development. Now, well see what happens next. Building a great open source platform is just one part of the puzzle; developing a vibrant open source community is another kettle of fish.",
      "Thanks to our friend James Walker at EzGov Europe for pointing this article out to me!"
    ],
    "summary_t": ""
  },
  {
    "id": "860f4dec3a8e5df18f464c2232485e85",
    "url_s": "https://opensourceconnections.com/blog/2008/06/13/why-your-airport-needs-its-own-website/",
    "title": "7 reasons why your airport needs its own website",
    "content": [
      "This week I was at the AAAE Conference in New Orleans, showcasing the AeroWeb product that we have just released at OpenSource Connections. Everyone who saw our demo seemed to be very impressed. I think that many of the airport executives I met realized they needed the better website that AeroWeb offers, with features like Flight Tracking, Online Booking and Airfare Deals (with referral bonuses for the airport), blogs, inline content editing right from your browser, custom directions, flash maps, and more. All wrapped in great designs customized to each airport.",
      "But one reaction I got several times was \"this looks great, but our county/city controls our airport and our website, and they want us to use the county IT staff and web page.\" Basically they liked what we offered, but felt like they are locked into a single page static web page occasionally edited by County staff.",
      "As an example, compare the Charlottesville Albemarle Airport website GoCho.com with this static web page of a regional airport in Hawaii, which is part of a larger state web site. The differences should be obvious. GoCho is a dynamic site providing real time information of immediate interest to passengers, and gives them incentives to fly through Charlottesville. The Hawaii site provides a single paragraph of generic information about the airport. As a passenger, it doesnt really tell me anything of value about whether or not this is an airport I would want to consider using during a trip to Hawaii.",
      "If youre an airport executive, you inherently understand this. But how do you convince your County or governing authority to give you the freedom to create and manage your own site? Heres a few ideas for discussion points you can bring up with them. When youve convinced your board to build a new site, please make sure to contact us at AeroWeb so we can submit a bid!",
      "Show passengers you have great deals too! If you are a regional airport competing for passengers with nearby larger airports, then you know it can be a struggle to convince passengers that they can get good prices through you. Perhaps not all your airfares are cheaper, but with our Flight Deals tab, great deals targeted to your airport will be displayed from Kayak.com. Especially with rising gas prices, if you can show passengers that they are going to get a comparable airfare through your airport, they will skip the longer drive to larger airports.\n  Give passengers the information they need. When passengers enter your airport, the first thing they probably do is check the arrivals and departures board. With our online flight tracking, you can give them the same experience on your website. We can integrate our tracking widget with your internal FIDS or with one of our online data partners.\n  Easy content management. With our inline editing tool, you will be able to manage all the content on your website, without involving AeroWeb staff or your County IT staff, and without any technical knowledge! One reason your Countys IT department may be reluctant to let you have your own website is out of a fear that they will have to maintain all the content for you. With AeroWeb, you can manage the website yourself, so there is no burden on other IT staff!\n  Managed hosting. We will host your AeroWeb site for you, which removes another area of concern your IT staff may have. They dont need to worry about the database or server uptime, or providing you with traffic reports and other analytics about your website. We do all of that for you – so there is no extra burden on your County IT staff!\n  Provide updates to media and passengers. Since editing content on your site and blog is so easy, it can become a communications channel for you directly to your passengers and even local media. By providing the latest news about your airport online in a timely fashion, you may reduce your customer service calls and keep passengers happy!\n  Your airport website is not the same as the county landfill site. No disrespect meant to your local garbage collectors, but the fact of the matter is that your airport has more specific website needs than the local landfill. So why are they both using the same boring static pages on the County web site? The landfill is not competing for customers like you are, and they dont need to provide timely and updated information to passengers who expect you to have a great website.\n  Providing the latest functionality on your website. AeroWeb is tuned in with the features that airports need. In addition to the features described above, we offer other widgets for weather, customized driving directions, advertising tools, and more! We can do custom development to meet your airports unique needs, and we are working on adding additional features too!"
    ],
    "summary_t": ""
  },
  {
    "id": "27909891fa478eb600d86375a1819c58",
    "url_s": "https://opensourceconnections.com/blog/2008/06/20/scrum-war-stories-part-deux/",
    "title": "Scrum War Stories Part Deux",
    "content": [
      "Come join Eric Pugh and other local and regional Scrum practitioners for drinks and dinner where\nwell hash out real world issues that face Scrum Teams. Well be talking about what made us start using Scrum, the cultural challenges weve had, and how we measure our results. Well share some\nstories about wins and losses since we last met in May 2007!",
      "Please RSVP to [email protected] so I can make sure we have an appropriate sized room at West Main.",
      "When: July 24th from 5:30 to 8 PM\nWhere: West Main Restaurant http://tinyurl.com/5kdphs\nWho: Anyone who is using Scrum, thinking about Scrum, or quit using Scrum!\nHelp: Eric @ 434-466-1467"
    ],
    "summary_t": "Come join Eric Pugh and other local and regional Scrum practitioners for drinks and dinner where well hash out real world issues that face Scrum Teams. Well ..."
  },
  {
    "id": "620abbc342f450842d3da7f8c71788e3",
    "url_s": "https://opensourceconnections.com/blog/2008/07/07/xul-getting-started-guide/",
    "title": "XUL Getting Started Guide",
    "content": [
      "I recently was tasked with creating a Firefox add-on or extension for a client and here are some of the things I wish I had known before I began. XML User Interface Language (XUL) pronounced \"zool\" is the programming language created by Mozilla used to create Firefox extensions and cross platform applications. It is a language similar to DHTML allowing someone with DHTML experience to quickly learn XUL. So far my experience with XUL has been a pleasant one. Please support the XUL community and leave comments with your experience with XUL.",
      "Tips for development",
      "XUL files can be opened in Firefox making it much easier to quickly view the changes you make to your code. Use the Error Console in Firefox to view error messages your XUL code may produce which can be found under the Firefox Tools menu. The Extension Developer has a nice \"reload all chrome\" option, but it closes all the web sites you currently have open and crashed Firefox on my computer if I had Gmail open.",
      "XUL tools",
      "Spket IDE is the best IDE for editing XUL I was able to find and is based on the Eclipse IDE.Extension Developer is a Firefox add-on for building extensions. Firebug is an awesome Firefox add-on for debugging web pages.XUL Explorer is a nice little application to help you get started with XUL development",
      "Resources",
      "Mozilla Developer Center is the official web site for XUL with documentation.",
      "Born Geek contains several great tutorials.",
      "XUL Periodic Table has a nice web page containing most of the user interface components with source code.",
      "XUL Planet is the unofficial web site for XUL with documentation."
    ],
    "summary_t": ""
  },
  {
    "id": "f0b7ad4f078d51da8851f45dec26a874",
    "url_s": "https://opensourceconnections.com/blog/2008/07/08/vetbiz-conference-day-1/",
    "title": "Vetbiz Conference Day 1",
    "content": [
      "We are here at the National Veteran Small Business Conference for Day 1 of the three day conference. The agenda today focused mainly on legislative activities aimed at helping service disabled veteran owned businesses get more access and opportunity with the federal government. The main conference speakers focused on the accountability aspects of existing directives and new legislation that was working its way through Congress. The piece of legislation which got the most commentary (and vituperation for its current unpassed status) was H.R. 3867, the Small Business Contracting Program Improvements Act. This act would create parity in sole source authority with 8(a) firms. Currently, contracting authorities can sole source work up to certain thresholds ($3.5MM for services) to any 8(a) firm, whereas they can only sole source to service disabled veteran owned small businesses (SDVOSB) if and only if there is one responsive SDVOSB; otherwise, the work must be set-aside and competed.",
      "While I appreciate the work that Congress is performing to take care of veterans, particularly given a rapidly growing population of new, freshly minted veterans from Iraq and Afghanistan, I am as perplexed today as I was last year about the entitlement mentality that these companies have. Yes, they are veterans who served their country with dignity and honor and made sacrifices that others generally dont make. Yes, the country owes its veterans a debt of honor for what they have done. However, creating an entitlement culture and a handout mentality serves neither the veterans nor the government. There is not a government teat of endless money for anyone to suckle on, and the government has a duty to taxpayers to get maximum value for the money. Helping veterans is one thing. However, neither the government nor the veterans should confuse help for handouts.",
      "This leads me back to what sticks in my throat about the displeasure at the pace of movement of H.R. 3867. Its almost as if veterans are afraid to compete for business. Granted, for contracting officers, it is easier to find an 8(a) and sole source the work, but if we veterans truly want parity and want to do what is in the best interests of the government, we should be working to repeal the sole source authority for 8(a) companies, not fight for it ourselves. I understand that the government has a long-term interest in fostering smaller business to help them grow and to have them be employers; however, it also has a long-term interest in increasing viable competition so that it has a higher probability of getting what it needs from its contractors.",
      "In commercial business, set-asides do not exist. Businesses fight for work on merit, which is an ever-changing definition depending on the buyer. I am stunned at how many businesses who attend the Vetbiz conference fail to adhere to the basic tenets of successful business.",
      "I was also disappointingly surprised to hear government contracting officers say that they valued commercial experience as lower than government experience. I would have expected a predilection (or at least a neutrality towards) for bringing in industry best practices into government work. I also understand the risk mitigation (aversion?) that exists in the government, but I was disappointed to hear that explicitly stated by contracting officers.",
      "Still, by and large, I am impressed by my comrades who I have met. There are many companies doing many great things and there are many, many government agencies who want to do the right thing and who are very dedicated to helping the veteran community. I was honored to see the genuine emotion about this industry shared by many, many people, and I was even more honored to be graced by the presence of many, many true American heroes and heroines whose service and sacrifice is absolutely astounding."
    ],
    "summary_t": ""
  },
  {
    "id": "542161c98d4f8055e282f1b280a949d8",
    "url_s": "https://opensourceconnections.com/blog/2008/07/11/rails-deployment-tips-with-rimuhosting-vps/",
    "title": "Rails Deployment Tips with RimuHosting VPS",
    "content": [
      "Weve been using RimuHosting.com`s VPS servers for a couple of small prototype style RoR applications that dont warrent the \"full\" treatment of their own dedicated servers in a clustered, redundent, scalable, etc environment. An example is my side project learning about the Semantic Web and Microformats: HighTechCville.",
      "One of the parts that I really like about RimuHosting is there very complete use of the Webmin administrative console. The more I work with it, the more I like it. While the GUI isnt anything incredible, it is very serviceable. And you can pretty much do most SysAd tasks from it, from installing packages, which really surprised me, to setting up log file rotation.\n\nSo here, in no specific order are some tips for folks doing a fairly plain jane RoR deployment on a RimuHosting VPS:",
      "Dont forget Log Rotation! We did some load testing for a day on one app and generated a 1.2 GB production.log! Webmin provides this under System -> Log File Rotation. You can easily rotate all your mongrel logs by doing /opt/apps/hightechcville/shared/log/mongrel.800*.log. By gzipping them you get much smaller files, and they will be pruned after 4 rotations. I did have an issue with BackgroundRB not being happy with the log file being moved under it, however I get so little output from that that when BackgroundRB restarts it resets the file.\n  Process Monitoring! Under *Others -> System and Server Status *is the ability to monitor how your server is working. You can do both local monitoring, for database, processes, memory usage, but also you can check if you RoR app is up and running by adding a Remote HTTP Check, and just supply the url of your application. Whats nice is you can add a text pattern to look for, or to ensure isnt there, so you can check and verify you are NOT getting the usual Service Down or An Error has happened page. Of course, if your box goes down in a bad way, then you may not be notified, but its at least a quick and easy first step.\n  Cron related tasks. Often we need timed processes to do a bunch of work. Before investing in the complexity of integrating something like BackgroundRB, can you do what you want via a Ruby script or write a Rake task? The invoke it from Cron. Alternatively, if you dont want something external to your Rails app, just add the periodic task code to some sort of RecurringJobController, and invoke it from cron via wget localhost:3000/recurring_job/run call! Sure, your mongrel will block, but if you have a cluster, who cares. You can also schedule this via a HTTP Remote Process check ;)\n  Tweak Postfix? We really like the ExceptionNotification plugin for RoR, getting immediate notice of errors on lower traffic sites is great. However, we found that we had to tweak the local_recipient_maps setting to get email to be sent out, otherwise you get a 550 server error. Just go to Servers -> Postfix and choose edit config files. You want to edit the main.cf and set local_recipient_maps= so it is blank.",
      "Got your own tips for easily setting up a reliable RoR app on a VPS, please let me know in the comments!"
    ],
    "summary_t": ""
  },
  {
    "id": "6e6750e86c205074a9eb21572e407840",
    "url_s": "https://opensourceconnections.com/blog/2008/07/14/sqlexpress-2005-install-surprises-issues/",
    "title": "SqlExpress 2005 Install Surprises (Issues)",
    "content": [
      "3 years later and it never ceases to amaze me with how many surprises SQL Express comes with when you try to install it on a windows 2003 server for whatever reason.  So today when I was playing the role of \"Server Admin\" and attempting to install SQL Express, I ran into a couple of issues.",
      "The first one was",
      "The SQL Server System Configuration Checker cannot be executed due to WMI configuration on the machine* * Error:2147749896 (0x80041008)",
      "So after the initial, \"why, God, why?????\", I found this nugget off of google. Its basically a command script that goes through and fixes possible errors for the WMI configuration (FIXWMI.CMD).  So after running this, I could finally get SQL Express installing, well, sort of.",
      "Evidently SQL Express sometimes has issues installing, when it installs itself in \"stealth\" mode which is how it installs when you run the \"SQLEXPRE32.EXE\" installer.",
      "SQLexpress fails with the error: An installation package for the product Microsoft SQL Server Native Client cannot be found. Try the installation again using a valid copy of the installation package â€˜sqlncli.msiâ€™",
      "So to get around this, you need to create a temp folder where ever your evil server admin heart desires. I chose to be spontaneous and created: \"c:\\temp\\sql\".   Then you need to open a command line and change the directory to where ever you have the file \"SQLEXPRES32.EXE\" kept and run the following command:",
      "$> SQLEXPRES32.EXE /x:c:\\temp\\sql",
      "This will unpack everything into that folder.  You should now have a \"c:\\temp\\sql\\Setup\" folder, inside of which is a file/msi called … (drum roll), \"sqlncli.msi\". Double click on that msi file and run \"repair\".  After that rerun the \"SQLEXPRES32.EXE\" and all should be golden.",
      "This blog was sponsored by the letter S and number 5 and does not imply warranties of any kind, use at your own risk, the same kind that you take when you open up regedit and begin mutilating various keys and values."
    ],
    "summary_t": ""
  },
  {
    "id": "b234a3bdaed4e4cbecf96c15a0f8433e",
    "url_s": "https://opensourceconnections.com/blog/2008/07/15/charlottesville-net-user-group-thursday-july-17th-630-830pm/",
    "title": "Charlottesville .NET User Group: Thursday July 17th 6:30-8:30PM",
    "content": [
      "This Thursday, July 17th 2008, the Charlottesville .NET User Group will meet from 6:30-8:30PM. Kevin Hazzard will present \"Accessing Web Services from Silverlight 2 Beta 2″. The meeting will be held at the SNL Galactic Headquarters: 1 SNL Plaza, Charlottesville, VA 22902 (map).",
      "Agenda:",
      "6:30 – 7:00: Sign In, meet & greet",
      "7:00 – 8:00: Presentation",
      "8:00 – 8:30: Informal discussion, socializing",
      "Description:",
      "Silverlight is a client-side technology. So its not really a part of your SOA strategy, right? You may want to think twice about that. SOAP and WSDL support are coming to the web desktop via Silverlight. And Silverlight has good client support for REST+ JSON/POX and RSS/ATOM-based web services, too. During this discussion, well dive into data serialization, security and cross-domain access policy capabilities inside Silverlight 2 Beta 2. We also talk about the nuances and pitfalls of provisioning your web services for an Internet audience. This presentation will be heavy on coding, demonstration and interactive discussion.",
      "There is no charge for this event and everyone is invited. Food and beverages will be provided."
    ],
    "summary_t": ""
  },
  {
    "id": "6ea46fb3a63c9c4e5b1ec2d631c37171",
    "url_s": "https://opensourceconnections.com/blog/2008/07/17/importance-of-virtualization-and-saas-for-the-future-of-it/",
    "title": "The Importance of Virtualization and SaaS for the Future of IT",
    "content": [
      "Two subjects have been the main focus of the IT business in the last couple of years, Green IT and Virtualization. The first one is ITs natural reaction to the worlds new favorite topic: Global Warming. All industrial countries are being pushed to reduce energy consumption, in fact in Europe it has become the law and the U.S. will soon follow. Whereas virtualization is the new technology that was waiting for its ticket into the industry and found it in Green IT because of its ability to save on energy consumption.",
      "I am OpenSource Connections newest hire. I am a recent UVA graduate and for part of the requirements for the fulfillment of the BS in Computer Engineering degree I wrote a thesis titled \"Designing and Building and Environmentally Friendly Data Center\", which was based on the research done for the white paper I authored entitled \"Greening the Data Center\".",
      "Researching the subject of my two papers has allowed me to gain expertise in a very important topic in the field. Now I am able to bring this experience with me to my new work place. OpenSource Connections has a group of people very knowledgeable in their areas of work. My research has allowed me to bring in an immediate added value to the company which will allow us to provide better services for our customers.",
      "The May 2008 issue of InformationWeek was entirely dedicated to a EMC World special where the topic of focus was virtualization and data consolidation, whereas the July 2008 issue of Baseline featured articles about Software as a Service (SaaS) and Google Apps. The trend is clear and OpenSource Connections is proud to be adopting this shift in the way businesses will be running their IT centers in the future. One of the projects that we have taken on is helping Madison County schools revamp their IT infrastructure by establishing a road map of the services they will provide their users and what technologies will be used, whether it is virtualization or SaaS and then helping them implement it. I am personally very excited about this project because it will allow to leave my leave my mark very early on and will give me the opportunity to work on something that will ameliorate the way students and teachers will be making use of computer technologies."
    ],
    "summary_t": "I wrote a thesis titled Designing and Building and Environmentally Friendly Data Center"
  },
  {
    "id": "cf2d3dfa27627d4ce892bba7dcad9cf8",
    "url_s": "https://opensourceconnections.com/blog/2006/07/12/trying-to-explain-what-open-source-is/",
    "title": "Trying to explain what \"Open Source\" is?",
    "content": [
      "Here are a couple articles related to both open source software, as well as the culture surrounding the open source world.",
      "Open Source in the Public Interest",
      "An Overview of Open Source License by American Bar Association",
      "Open source software: A short interpretive history",
      "The Cathedral and the Bazaar"
    ],
    "summary_t": ""
  },
  {
    "id": "bc925124560371d3690445c990562734",
    "url_s": "https://opensourceconnections.com/blog/2006/10/12/continous-integration-presentation-for-cojug/",
    "title": "Continous Integration Presentation for COJUG",
    "content": [
      "On Tuesday, October 10th I had the opportunity to speak to the Columbus Ohio Java User Group. They are a pretty large group, that actually has seperate lunch and dinner sessions to fit into peoples schedules!",
      "I spoke on Continous Integration, focusing on the case for CI, as well as the challenges in instituting CI on a long term basis. Because this was a Java user group, I focused on CruiseControl and Continuum. I have posted the slides for download.",
      "Thanks to Chris Judd for the invitiation and being a great host. I met some cool people, and had a really good time. My first visit to Ohio was a great visit."
    ],
    "summary_t": ""
  },
  {
    "id": "17e46be8385f22219f15d1d074f22bdd",
    "url_s": "https://opensourceconnections.com/blog/2008/07/22/what-do-your-business-tags-say-about-your-company/",
    "title": "What Do Your Business Tags Say About Your Company?",
    "content": [
      "Erics R&D project is HighTechCville, a website aimed at helping people identify technology communities of interest in the Charlottesville area. The engine behind the website pulls in information about people and companies from disparate sources such as the Neon Guild, the Virginia Biotechnology Association, LinkedIn, and personal and business web pages. It then performs cluster analysis on the people and organizations to determine communities of interest.",
      "Naturally, I was curious about what our website (and appurtenant profiles) said about us through the eyes of HighTechCville. Our tags are Ruby and Rails, which is not surprising given the provenance is from our entry on the Working With Rails website. I decided to dig a little deeper to see the story that our website told to the search engines. The ten most common search terms leading to our site for the past three months are as follows:",
      "apple itouch\nitouch calendar\nnew pda\ntextmate clone\ndedupe itunes\ntextmate windows\nitouch\nnew apple itouch\nopen source connections\nitunes dedupe",
      "Of the top 10 terms, only 3 really relate to what we do. As an aside, Im sure that simply posting these terms in this blog entry will only solidify the results. Naturally, this concerns me, as the people who need what we provide obviously arent coming to us via the website.",
      "How should we tag our business? I think that our work falls into the following 5 broad categories:",
      "Open Source for the Government: The government, as a steward of taxpayer money, has a responsibility to provide the highest value products and services for the lowest price possible. Paying license fees for commercial applications that can be performed equally well (and often better) by open source applications does not fall into the category of good stewardship. Furthermore, the government could and should own as much source code as possible to encourage broader and higher quality competition for subsequent work. This work takes two vectors–leveraging existing open source products and applications (and hopefully contributing back where possible) for government work, and helping government agencies learn the lessons of development in open source communities to improve their development methodologies. (Note: opensource.gov, if it was up to us, would encompass much more than just translations and analysis of open source media for government policy and implications) We feel that these lessons also segue well into our second business tag…",
      "Build Better Software: Our developers are fascinated by the craft of software development. While not pedantic to the point of being obsessed about process for the sake of process, we do believe that software development is an art that follows the rules of Pareto optimality. In terms that undersell the definition, 80% of the work in software development is plumbing, and 20% is \"secret sauce.\" We leverage and create tools which minimize the amount of human time necessary to get to the point where developer skill has the maximum impact. This covers the range of the software development life cycle, from ensuring that the right questions are being asked to determine a pain point we are to actually solve (often rather than the one that is first described to us) through to code coverage metrics and unit testing to automated test scripts before deployment. We also are avid proponents of Agile Development methodologies during development to ensure that our skills are applied to the right problems at the right times to maximize customer value. Agile Development involves rapid prototyping, which helps us help customers to move…",
      "From 0.1 to 1.0: We assist companies who have a great concept to get through prototyping phases and to develop a working application that is an embodiment of their ideas. Often, this occurs when companies successfully raise an initial round of venture capital financing and now need to bring a product or service to market to generate beginning sales or beta customers. They must demonstrate working features for customers to get orders for the full product, and often do not have the team put together to meet the aggressive deadlines of their investors. We quickly embed ourselves with the business and deliver on the highest priority features to facilitate rapid sales cycles. Of course, we dont just focus on version 1.0, as we specialize in…",
      "Web 3.0 for the government: The government owes it to citizens to be as transparent as possible, and it has started to achieve this in limited areas, such as the TSA blog and the Charlottesville Albemarle Airport Authority blog. However, this is only a small step in the right direction. Not only does the government need to reach out more to its citizens in a proactive manner and engage them (note: the TSA is experiencing the problem of engagement, discussed here. They could use a few hours with the Rimm-Kaufman Group.) Making the plethora of information available more user-friendly and accessible for self-help is a good start. The government also has another, more somber need for Web 3.0. No matter how one defines it, the United States is locked in a war on terror against unconventional enemies who are very smart, very adaptive, and very technology savvy. We need to maximize our usage of Web 3.0 technologies to get in front of enemy decision cycles and interdict their actions. Our intelligence agencies collect far more signal and human intelligence than our analysts can sift through, and throwing more analysts at the problem is not the answer; technology is the only way we can identify what analysts should take a deeper look at.",
      "Location aware applications: One specific area of focus of leveraging Web 3.0 capabilities is in location aware applications. These applications provide context based on geographic location and temporal activity to isolate information specific to a given area of geographical interest. Identifying not only where information is tied to, but also who is interested in the same location helps quickly automate creation of communities of interest based on the specifically identified locus. Through the use of geographical information systems, users can overlay and share appropriate information in a visually intuitive manner that helps others quickly grasp the salient points to be shared. This can also be used to plot information over time (time is, after all, another point on a geographic grid) for users to see quick patterns emerging.",
      "Clearly we have a long way to go to get our website to tell the story of our business. Maybe a pleading letter to Matt Cutts is in order.",
      "How do you perceive us? Are you surprised at the tags we have self-identified? What tags would you use to describe us? What tags would you use to describe your business?"
    ],
    "summary_t": ""
  },
  {
    "id": "55e2f4a4e9ea8627d6f2e33d32da2323",
    "url_s": "https://opensourceconnections.com/blog/2008/07/23/jason-hull-to-present-at-aaae-nextgen-airport-conference-and-expo/",
    "title": "Jason Hull to present at AAAE NextGen Airport Conference and Expo",
    "content": [
      "On August 6, 2008, Jason Hull will be presenting at the AAAE NextGen Airport Conference and Expo. He will cover how airports can use their websites as marketing tools to passengers, airlines, and general aviation. He will discuss the case study of the Charlottesville Albemarle Airport website and the AeroWeb product and cover best practices of websites throughout the industry.",
      "More information can be found here.",
      "Slides of my presentation are available here."
    ],
    "summary_t": ""
  },
  {
    "id": "1807a93c942e1ee052df4f6f0cc720eb",
    "url_s": "https://opensourceconnections.com/blog/2008/07/31/3-flaws-in-the-government-rfp-process/",
    "title": "3 Flaws in the Government RFP Process",
    "content": [
      "Ive recently discovered Blair Ennss Win Without Pitching website (thanks to David Robinson of Birch Studio Graphics for pointing me to the fantastic FunctionFox seminar). The main theme of Ennss site, as I have gathered, is that proposal writing is, for the most part, an ineffective way for buyers to purchase specialized services, and for sellers of specialized services to prove their value. It is ineffective for buyers because the request for proposal (RFP) process often doesnt allow the buyer to truly find out if theres a fit between the seller and the buyer, and it attempts to normalize factors between competitors to create an apples to apples analysis. As Enns points out, what if youre a pomegranate? You dont want to be compared to an apple because youre NOT an apple. This is why proposal writing is ineffective for providers of specialized services, because proposals which normalize factors often then reduce the reason that youre specialized in the first place. A filet mignon prepared by Bobby Flay (or any other Food Network denizen) is not the same as beef tips purchased from Ponderosa Steakhouse. Yet, without the opportunity to engage in a conversation, the filet mignon is often reduced to the level of the beef tips in comparison.",
      "So, how does this impact the largest buyer in the free world, the U.S. government? Less than it should. Check out the FedBizOpps website and see how many open proposals are available for a seller to peruse. As of the evening of July 30, there were 6,736 requests for proposals with expected response dates of July, 2008. If the process is flawed, then those flaws replicate themselves many times over on a regular basis.",
      "Here are three flaws that I see in the process:",
      "RFPs seek to mitigate the wrong risk. As a buyer, I want to know if I pay for something, then Im going to get my moneys worth out of it. The risk i want to mitigate is that the provider doesnt know what theyre doing or provides me a bad product and I cant take it back for a refund. However, many RFPs seek to mitigate cost risk first and foremost, looking for price reductions, best offers, and fixed price bids in situations where expected outcomes are unclear or not well-defined. The mantra \"nobody ever got fired for hiring IBM\" seems to come to mind (though, to be clear, there are many cases where IBM should get the work), and that low cost bidders will often win. The risk that buyers of specialized services should seek to mitigate is fit, and by normalizing on other factors (are you CMMI qualified, give me your three best references, etc.), the government is trying to indirectly answer the question of fit. However, process gets in the way of answering that question…\n  \n  \n    The strict adherence to the process makes meaningful conversations difficult. Once a RFP is published, then communication with the actual customer is not allowed. Questions must be asked via the contracting officer, who then relates them to the customer, who then answers the question for everyone to see. This process creates a game of \"gotcha\" because a potential provider of services doesnt want to ask the \"a-ha\" question that will reveal the true nature of the need and alert competitors to the same need. As a buyer, Id rather get the deep, insightful question from one seller that reflected higher skill and continue conversations with that seller to explore fit. It seems inimical to then take away the benefit of that thoughtfulness by sharing that information with all other potential sellers. Yet, thats what the government regularly does. I understand that this is a question of access, and with potential sellers often numbering in the hundreds, its a tough problem to solve. Still, encouraging more real conversations in the process would help to address the issue that the strict RFP process tries to mitigate through its normalization efforts.\n  \n  \n    The vendor qualification process is not rigorous enough. To become a registered government vendor, you need to follow a few steps: a) Register as a legal entity (corporation, LLC, partnership, DBA, etc.); b) Get an EIN; c) Get a DUNS number; d) Register in the Central Contractor Registry (CCR). As a result, there are, as of July 30, 2008, 465,684 active registrants in CCR. Increased viable competition for the government is great–it gets more value for the taxpayer dollar. However, increasing the noise-to-signal ratio does exactly the opposite. Its why contracting officers hate full and open competitions–they have to read volumes of non-competitive responses from people who think that throwing a proposal over the wall is the surefire way to success. If contracting officers knew that they would get responses only from companies who had a reasonable probability of being able to do the work, their job would be easier. Put another way, if contracting officers knew of the fit between client and provider, their job would be easier. Tightening up the registration requirements would be one way to help this.",
      "I believe that the government RFP process is flawed. It often puts out interpretations of needs and opens it up to the masses, putting the decision-makers in a tough situation of trying to discern differentiation from dozens, if not hundreds, of potential suitors. As a result, the RFP process tries to normalize as much as possible to reduce decisions down to a few factors, and, where possible, down to one–price. It does not address the issue of fit and localizes variables rather than finding global optimization. The process leads to over budget, out of scope, and missed timeline performance. By trying to be fair to EVERYONE who wants to participate in government contracting, the government is not fair to the most important stakeholder of all–the taxpayer."
    ],
    "summary_t": ""
  },
  {
    "id": "3116c09a8018309e5f5df9e0d84a6b57",
    "url_s": "https://opensourceconnections.com/blog/2008/08/04/google-more-services-more-problems/",
    "title": "Google: More Services, More Problems",
    "content": [
      "Google came out with Street View which allows people to view a real image of the street address they are getting directions for on Google Maps. This is not available everywhere yet but it is a cool feature to have. Many times I looked for directions to a place I had never been before and ended up circling around the area many times to actually find the building I was looking for. The directions you get off Google Maps or MapQuest or whichever service you prefer usually end with something like \"Arrive at `street number. \" but for the life of me I can never find the numbers on the buildings. What street View provides is a visual way to find your destination based on landmarks or even just the shape of the place you want to get to.",
      "An article on the BBC news website entitled <a href=\"http://news.bbc.co.uk/2/hi/technology/7536549.stm\" target=\"_blank\">Google accused on privacy views</a> reports that Google was accused of two things: violating peoples privacy and hypocrisy. The first one can be thought of as a criminal offense and a lawsuit resulted from it whereas the second one is merely an opinion but oddly enough it was the topic of the article. Both accusations tickled me and I am going to tell you why.",
      "Street View provides images of public places and streets but unfortunately they made the mistake of including private driveways and homes. One can argue that the view of a house from the outside is public enough that anyone driving by can see it which would not violate anyones privacy. In fact anyone can get the directions to a house and drive by and get even a better view of the place than what Google provides. If someone was looking for a picture of some house or private property, my guess would be that they would not be looking for it on Google Maps. They could use another one of Googles tools which is Google Earth, or they could try to look through real estate websites. The house probably was on the market at one point and a picture of it is available somewhere. I do not think that Street View violates anyones privacy simply because it provides, as the name suggests a view of the street from a passing car. I did say however that Google made a mistake. They included private homes in their service which begged for a lawsuit. Some family somewhere was bound to either feel violated or feel their mouth watering in sight of an opportunity to sue a big corporation for a lot of money. Yet another instance of the McDonald coffee lady (http://en.wikipedia.org/wiki/McDonalds_coffee_case).",
      "What captured my attention the most was the second accusation. What interests me about it is the fact that the US National Legal and Policy Center (NLPC) accused Google of hypocrisy about an issue that is not confined to Google only. This is a subject of many legislation and practices of the US government that go beyond posting a picture of someones house on the internet. At least in the case of Street View you are knowledgeable about its presence and can even control it by removing the picture if desired. I am not criticizing government policies, I leave that to others. I just find it odd that NLPC targeted Google when many other and bigger players in the privacy game exist. As a software developer I am concerned with what comes out of the big technology factories such as Google and Microsoft and how it affects society and peoples lives. I am also concerned with how the publics view is shaped and with the trend of labeling computer companies as they grow bigger and more successful with the tag of evil. We saw it happen with Microsoft (I reserve my opinion on that one) and I see it take place with Google who was the \"good\" company not long ago and for some it was their \"savior\" from everything Microsoft but now is getting more and more scrutinized for everything it does.",
      "Google needs to be careful about what they do especially when it comes to privacy issues because of the nature of the services they provide. The company has admirable stands on a lot of issues but the public is merciless and it would be a shame to see another company grow less popular as it grows bigger and more successful."
    ],
    "summary_t": ""
  },
  {
    "id": "5a738993085f125fc47808156ea0398b",
    "url_s": "https://opensourceconnections.com/blog/2008/08/04/rails-vs-django-a-beginners-point-of-view/",
    "title": "Rails vs. Django: a Beginners Point of View",
    "content": [
      "For an agile web developer two frameworks are available for quick and easy deployment of a new website:",
      "Ruby on Rails for the Ruby fans and Django for the Python fans. These two tools should provide the same functionality and as someone whos been working on two projects for the last month, each using one the frameworks, I have had the experience of starting from scratch on both of them and trying to learn both in parallel (or at least one right after the other without acquiring more experience in one and becoming biased for one over the other).",
      "So what has this experience taught me?",
      "Deployment",
      "It is very easy to start an application using both Rails or Django: rails myapp vs. django-admin.py startproject mysite even though one is wordier than the other it is pretty simple to start the project with all the directories and files needed automatically generated.",
      "Starting the Server",
      "This task is as simple as the previous one: ruby script/server vs. python manage.py runserver in both cases there is a call to the language in use, then passing a command. In Rails case the command is an executable found in the script directory whereas in Django’s case the command is an argument passed to the Python manager.",
      "Configuration",
      "Both frameworks provide a place to configure the application. Rails provide an entire directory to configure the database setup and the Rails environment whereas Django provides a settings.py file where all the configurations can be present but also provides the option of having a config directory for multiple personalized configurations.",
      "Updating the Database",
      "Yet another simple task: rake db:migrate vs. python manage.py syncdb In this case we notice that with Rails the call is actually to rake whereas for Django it is still consistent with the previous format. But well see that Rails is also consistent where rake is called for everything relating to the database.",
      "Creating Models",
      "Where some of the differences begin: With Rails you create one application that can have multiple controllers, models and models and link to each other using\nruby script/generate * (where the * differs depending on what needs to be generated). Whereas in Django you create multiple applications which each has its own models and views using python manage.py startapp myapp",
      "Documentation",
      "It seems that Rails is more widely used which results in more available resources online for documentation and help. None the less, the Django documentation site is very extended and provides sufficient help.",
      "The Dirt",
      "In the case of Rails I started the project from scratch by first following with the example in the book Agile Web Development with Rails, Third Edition and then applying what I learned to the project I was working on. Whereas in the case of the Django project, I was put on it late in the development process and tried to learn the language and at the same time understand what was already done on the project. My experience with both has been limited and what I have found is that the Rails magic seems to be the difference maker for most people. The majority are not comfortable with that \"unknown\" but I have seen some magic done with other languages like Java, although it is not the same type but in either case there is something in the background that is doing some sort of work for the programmer. And I have become comfortable with such a setting and because I have seen a bit of how programming languages interpreters work I understand and appreciate the Rails magic. Even though I was more productive using Rails than I was using Django, which may have influenced my opinion about them, I am still willing to give Django a chance. Both are great tools for developing web applications in an agile manner. My recommendation though, if I have to make one, will go for Ruby on Rails, just because I was more comfortable with it."
    ],
    "summary_t": ""
  },
  {
    "id": "b5ca9e52e93548ec7d359e0286d99f25",
    "url_s": "https://opensourceconnections.com/blog/2008/08/05/designing-and-building-an-environmentally-responsible-data-center/",
    "title": "Designing and Building an Environmentally Responsible Data Center",
    "content": [
      "A new buzz word, \"Green\", is on everyones tongue recently, which is the result of a growing trend all over the world. For years groups like Clean Air Watch and the Sierra Club have advocated a change in modern societys habits in favor of a cleaner and \"greener\" Earth. The industrial world is embracing this trend with the hybrid car. For example, even Capitol Hill is moving toward a change with U.S. House Speaker Nancy Pelosis plan to \"green\" the Capitol complex.",
      "While \"Green\" carries a specific connotation of minimizing energy consumption and/or carbon emissions, it also makes superb business sense in the IT community. IT systems, particularly for data centers are a significant consumer of electricity. The United States Environmental Protection Agency (EPA) has stated that data centers consumed 59 billion kilowatt hours (KWh) in 2006 ($4.1 billion) of which the federal government is responsible for 10% and it is estimated to increase up to 103 billion KWh by 2011. Many data center managers see the energy bill for operating the equipment and power consumption becomes a major concern for them. Servers and storage have developed into a very power-hungry element of data centers. Thus Data Center managers are beginning to embrace \"Green IT\" and \"Green\" data centers have become a significant factor in future data center design.",
      "Procuring power efficient hardware is not just a smart business move for companies that are trying to save money, but it will soon become a mandatory shift when laws are passed to force such a change. Europe has already seen signs of this shift with the European Commission publishing the Directive 2005/32/EC on the eco-design of Energy-using Products (EuP) and with the recycling regulations that are already in place.",
      "People around the world have grown socially aware, questioning the morality of political, social and business decisions taken. Thanks to the Internet, information has become easily accessible to everyone, which has allowed people to be more aware of their surrounding. Businesses and agencies are thus under a magnifying glass and every move they make is judged by society. Therefore, they do not want to conduct business in such a manner that would reflect a bad image. Since the hot topic of today is being environmentally friendly, it is crucial that companies adopt the \"Green\" attitude.",
      "In the United States of America, this movement has been fueled by a society that has been advocating this movement. But soon enough government regulations in the US will be another driving force for IT companies to go \"Green\". The European Union has already started on that track with the Restrictions on Hazardous Substances (RoHS) and Waste Electrical Equipment (WEEE) regulations. Although such regulations do not exist in the US, the Environmental Protection Agency (EPA) is working to include with the ENERGY STAR power consumption ratings hardware such as servers in its certification program.",
      "As system components become faster and more effective, they also dissipate more heat. Chip manufacturers have focused their design on functionality and performance but not so much on heat efficiency. This allows more heat dissipation, while at the same time lowering the maximum acceptable temperature of operation for these chips. Such characteristics of the systems in use lead to a higher need for cooling per chip to avoid overheating and damage to the processors. The heat problem translates itself into a power and efficiency problem. These chips consume more power than their predecessors, so data center managers these days can only stack ten servers in the same rack that used to hold up to thirty servers. At the same time, these devices generate more heat, which equates to higher cooling power. Cooling a data center requires sophisticated and elaborate equipment that consumes power, exacerbating the power dilemma.",
      "With the average cost of Kilowatt-Hours (KWH) in the U.S at around $0.092 in 2007 and $0.0892 in 2006, running a data center can be costly. Cooling accounts for a major portion of the energy bill, second only to the cost of running the equipment itself. The lack of focus toward designing an efficient data center has resulted in the need of between 0.5 and one watt to cool one watt of equipment when ideally managers would like to achieve a 0.3:1 cooling watt to equipment watt ratio. Making cooling efficiency the major concern in the design of a data center, and purchasing energy-efficient or \"green\" hardware becomes compelling.",
      "Data centers energy bills are rising fast, its becoming a budget issue for the entire company. Managers need to find a way to reduce the energy cost while maintaining their high efficiency productions that business customers require these days.",
      "Processor chips are designed to include the maximum computing power possible in the least space possible. This results in a need for more power and dissipating more heat per unit of equipment. It turns out that this strategy is less efficient because neither data centers nor the electric utility companies are able to provide enough power to the racks housing the hardware or to generate the necessary cooling that is required to compensate for the heat produced by the servers, switches and routers present in the data center.",
      "Data center managers found out through experience that the key to efficiency was not physical space but power consumption. Unfortunately the chip manufacturing industry focuses the design mainly toward speed and not power efficiency. Throughout the years, semiconductor design has favored higher speed allowing higher leakage currents. Leakage current is wasted energy flowing through the junctions when the transistor is in the \"zero state\". Estimations indicate that leakage current in high-end processors is between 18% and 20% of total power consumption.",
      "Measuring the efficiency of a data center lies in measuring the ratio of systems per rack. But an average data center is equipped to handle racks powered up to 5-6 KW worth of equipment and its equivalent cooling power. With the hardware specifications discussed earlier, stacking racks to their full capacity will require them to be powered with approximately 25 to 30 KW per racks, which becomes a design issue and a critical point for vendors and data center managers.",
      "The IT market is very competitive in terms of providing the best services, but what characterizes such a service is the speed and functionality of the equipment rather than the component efficiency. Power supplies are the main component that gets neglected in favor of such competitiveness. The consequences are a data center that consumes more power in power conversion and cooling than the computer systems actually need. This means that most of that power is wasted energy which we can work on saving by using energy efficient equipment. There is a classic cost tradeoff between a more efficient (expensive) power supply and the cost-savings over the life cycle.",
      "In this discussion, we will present the common techniques and technologies used in todays data centers, we will then show why they are inefficient in terms of energy consumption. Once we have determined the problem we will give solutions for deploying and operating an energy efficient data center… Read more."
    ],
    "summary_t": "There is a new goal in data center design: being Green"
  },
  {
    "id": "679ec11d7064e10e724c36536bf93a75",
    "url_s": "https://opensourceconnections.com/blog/2008/08/08/scrum-journal-introducing-scrum-with-a-new-client/",
    "title": "Scrum Journal:  Introducing Scrum with a new client",
    "content": [
      "One of the things we take pride in here at OpenSource Connections is what we like to call the OpenApproach.Â  The OpenApproach is our implementation of best practices for software development and project management, and is largely based on the concepts of Agile and Scrum.Â  In addition to providing development services to our clients, we often coach our clients on OpenApproach techniques so that they can improve their own development environment and learn from our success on other projects.",
      "Most recently, I participated in a Scrum team on our own internal software development project:Â  AeroWebOnline.com.Â  This was a very time sensitive project with various OSC developers going in and out of the project, geared towards a product launch at the AAAE conference in New Orleans.Â  That project was a definite success, although one Scrum lesson we re-learned was that 1 week sprints really are too short, and 2-4 week sprints are much more effective.Â  As both a developer and a project manager, I really find Scrum rewarding because it keeps me aware of whatâ€™s going on in the project as a whole, as well as very focused on my own development tasks.",
      "",
      "Â",
      "Currently Iâ€™m with a new client of OSC, and weâ€™re on another time sensitive project with up to 8 developers (though most also have other project obligations), an architect, a project manager, a business lead, and 2 QA/build staff.Â  Itâ€™s a big project with a tight deadline and for several reasons I felt Scrum was the perfect approach for this project:",
      "1)Â Â Â Â Â  The team needs to have improved communication and a real time sense of how the project is going and what everyone is doing on it, without introducing a heavy process.",
      "2)Â Â Â Â Â  The business needs strong insight into the project, because the deadline is very important to them.Â  If the timeline doesnâ€™t go well because developers are on too many other projects, they need to be able to see that effect directly.",
      "3)Â Â Â Â Â  Developers need to be encouraged to trade tasks and to cross train, because since many of them will be called on for other projects or production support, itâ€™s essential that the project not be too dependent on any one developer.",
      "So I pitched the idea of Scrum to the team leadership, and weâ€™re going forward with it.Â  Iâ€™ll record here some of our experiences so far, how Scrum is helping us, what we can improve on, and the teamâ€™s reaction to a different process.",
      "We have essentially a one month timeline for the project, so we decided to split it into two sprints of two weeks each.Â  Last Friday we had our planning meeting for Sprint 1.Â  Since this was the first introduction of Scrum to the whole team, I started by giving a talk about Scrum and showing examples of past burndown charts and task lists in other OSC projects, and talking about the responsibilities of each Scrum role (developer, scrummaster, and product owner).Â  I used this â€œScrum in 5 minutesâ€ document as a set of talking points for the presentation.Â  But no, it didnâ€™t take me 5 minutes.Â  It actually took about an hour of me talking and the team asking questions.",
      "",
      "",
      "But after that, we went right into planning for Sprint 1.Â  I setup the sprint planning spreadsheet in Google Docs as we do with most of our OSC projects, and I served as Scrummaster in the planning meeting.Â  However, my goal is not to be the Scrummaster myself, but to train the clientâ€™s project manager on how to be a Scrummaster so that they can continue this process on other projects.Â  So while I served as the Scrummaster for the first planning meeting, she is now leading the daily stand ups, and I expect she will lead the planning on the next sprint with my guidance.",
      "The architect for the project had already prepared his own set of goals for the sprint, and so we used those as a starting point with the team, which the rest of the development staff added to.Â  The team as a whole came up with their own list of sprint tasks for those goals, which took us another hour and a half probably.Â  There was of course much discussion about the tasks, but it was my job to make sure that discussion did not go into â€œimplementation detailsâ€ so that we did not get into too technical or indepth of a discussion.Â  That was a real challenge, but for the most part we did it.",
      "",
      "Â",
      "After all the tasks had been laid out, we moved into task estimation.Â  To do this, Eric Pugh strongly suggested that we try Scrum poker.Â  I was a little skeptical how well that part might work, but I was really pleased with the results.Â  We didnâ€™t have time to order the official Scrum poker cards from Sweden, so I cut a stack of index cards in half and everybody got cards with these values on them:",
      "?,2,3,5,8,13,16,20+,donut.",
      "Some of the developers also had fun adding their own cards to the mix.Â  Chris added a pizza card when it got close to lunchtime.",
      "",
      "Â",
      "So how does it work?Â  As the Scrummaster, I would read the next task off the list, and ask everyone to pick a card from their deck.Â  When everybody had set a card face-down on the table, I would say â€œflipâ€ and then I would read off the estimates given around the table.Â  The 20+ card was used to indicate you think the task will take longer than 2 days and it probably should be subdivided into other tasks.Â  The donut card was used to say â€œmy brain hurts, pass the donuts.â€Â  And the question mark card was used to indicate you didnâ€™t know enough about the task to make a reasonable guess.",
      "Those who gave the lowest and highest estimates would be asked to make a quick case as to why they made that estimate.Â  Sometimes those defenses would lead one or the other to realize they had made an incorrect assumption about the task, and the team would quickly be able to agree on a reasonable estimate.Â  Other times, it brought out important points about the task that we then used to make a better estimate with.",
      "Scrum poker addresses a real problem in task estimation, and I was really pleased with the results.Â  Normally, if you have a group of developers and ask them how long it will take to do a task, someone will speak up first and throw out a number.Â  No matter whether that estimate is really high or low, the fact is that most other developers are now going to give estimates somewhat similar.Â  Even if they would have guessed a much higher estimate, they will now downgrade theirs because they donâ€™t want to sound like a slacker compared to the initial low estimate.",
      "I think that would have definitely happened with this team, because I observed that the developer who was probably the most confident in his estimates was also the most optimistic in his estimates.Â  Without Scrum poker, I think we would have had a much lower set of estimates and we would be less likely to hit them.",
      "Estimating the tasks took about 2 and Â½ hours, and was the longest part of the process, though I think the end result was good.Â",
      "The final thing we did was to assign the tasks.Â  In Scrum, the best way to do this is to go â€œround robinâ€ around the room and each developer picks one task at a time, until no tasks are left.",
      "However, we had been meeting for a long time at this point and people were fading.Â  The project manager asked if we could just print out the task list and pass it around our desks to assign the tasks.Â  I acquiesced, but in retrospect I should not have given in.Â  At that point, I essentially lost control to enforce the process, and instead of people signing up for one task at a time as they were supposed to, developers signed up for multiple at once.",
      "The end result of this Scrum â€œfaux pasâ€ was that developers stayed in their comfort zones too much, and that there is not enough cross-training among members in my opinion.Â  The architect took all the tasks around a rules engine, and a senior developer took most of the tasks regarding how the workflow would be implemented.Â",
      "This was a definite downside of the process in my evaluation, and one that could have been avoided had I not allowed meeting fatigue to get the best of us.Â  However, I think this will fix itself to some degree because some of these tasks will have to be traded around over the course of the sprint in order for everything to be done.Â  Itâ€™s nearly a week into the sprint and that is exactly whatâ€™s happening.",
      "All in all though, introduction of Scrum into this team has gone very well so far.Â  The developers have responded well despite the fact that I introduced Scrum to them in something of a top down fashion (Ideally a movement to adopt Scrum would start with the developers, not with management, although I felt in this particular case it was critical that I get buy-in from the management of the team first).Â  The project manager seems to really like the process, and commented to me that â€œthis lets me manage projects the way I want to anyways.â€Â  And the business seems excited about it because they get to keep their finger on the pulse of the project without being too intrusive on the developers.",
      "Two days ago one of the senior managers in the company came up to me and was talking positively about Scrum and what we are doing on this team.Â  Although he had not been in our Scrum meetings, word about the process is getting around the company.",
      "",
      "Â",
      "Finally, since weâ€™re nearly halfway through the first week of sprint 1, Iâ€™ll show the burndown chart as it stands right now.Â  You can see that the first few days very little apparent progress was being made.Â  This did not surprise me, because at this point all the teamâ€™s developers except one are still being assigned small tasks or even large tasks on other projects, and so they keep having to divert attention away from this project.Â  Just in the last day that is starting to change and more work is getting done, so now we are starting to see a steeper drop in the burndown chart.Â  I think having that chart is helping to communicate the effects of side projects and reinforce the deadline of this sprint.",
      "Ultimate success of a process is determined primarily by the end result of a project, and so it will continue to be interesting to see how this sprint and subsequent sprints will go.Â  But so far, Scrum is working quite well!"
    ],
    "summary_t": ""
  },
  {
    "id": "b2703598a642bc0058e0252da90849d3",
    "url_s": "https://opensourceconnections.com/blog/2008/08/13/when-is-an-airport-not-an-airport/",
    "title": "When is an Airport Not an Airport?",
    "content": [
      "When its a mall, a kennel, a wireless hotspot, or a restaurant.",
      "Here at the 50th AAAE Northeast Chapter meeting the session I was most looking forward to was Non-Airline Revenue since our product, AeroWeb, has a lot to offer in that area. Airports are faced with declining flights, and therefore declining revenue from airlines and are looking for other ways to make up the lack.\n\nWhile this session briefly talked about 18 areas that could contribute more non airline revenue, from air cargo, to airline services, the focus was on maximizing revenue per passenger through better services. The biggest source for gaining significantly more revenue quickly was extending advertising opportunities, and increasing revenue per passenger through food and retail. Many of the airports on the Northeast have large atriums, and those high traffic areas are great places for additional advertising via banners. Additionally, by featuring premium restaurants with a local tie to the airport, you encourage folks to come in. I know I go to Legal Seafood in Logan versus the generic sports bar located in the same terminal. Also touched on were how airports are hosting kennels, and even hotels on the premises.",
      "What wasnt touched on was the opportunity to increase revenue by making an airports web site work harder. Airports who have free wireless available typically send the user to the standard public airport homepage. But what about instead sending those folks to a specific internal page that provides what they are looking for (since you know they are in the terminal!):",
      "Where are the restaurants and retail?\n  Advertising/special deals for venues in the airport?\n  How long will it take me to get to my gate.. More importantly, how long can I shop before I go to my gate?",
      "",
      "Weve been discussing what it would take to allow a passenger to subscribe to text messages providing detailed information about their flight, from delays and gate changes, to details on who can board when. Just look up a phone number on a FIDS display, punch it into your cell and now you can walk around the airport, secure that you are being notified regardless of where you are. This might even help those \"Gate huggers\" who will immediately bypass all shopping and retail services in a mad rush to get to their gate, just to wait around for an hour.",
      "Airport Managers should also be looking at what services you are paying for related to your website, and see if you can turn those around. For one client, the airport was paying a monthly fee for a ticket booking engine that didnt even work well. We integrated Kayak into our AeroWeb airport website product, and now that airport is making affiliate revenue for driving traffic to Kayak! The same situation often pops up for weather data, you can set yourself up as an affiliate to Weather.com, and even potentially earn revenue on people who click through to Weather.com.",
      "While I love the idea of sleep pods and kennels in airports, I think the real \"service\" that takes advantage of your being away from home is routine services for your car. Getting maintenance done on your car is always a hassle. However, if you can just drop your car off for an oil change or your 90,000 mile overhaul, and pick it up when you return, then it makes sense to book those activities (maybe via a web interface?) while you are traveling.",
      "Here in Charlottesville you can take your car to Airport Road Auto Center, and theyll run you the four blocks to the airport, and then on the day you return drop your car off in the short term parking lot, all tuned up and ready to go!"
    ],
    "summary_t": ""
  },
  {
    "id": "425a57b59f14c9111075ea03ace17f40",
    "url_s": "https://opensourceconnections.com/blog/2008/08/19/day-2-of-afcea-landwarnet/",
    "title": "Day 2 of AFCEA LandWarNet",
    "content": [
      "Eric Pugh and I are at Day 2 of AFCEA`s LandWarNet conference.  While we havent been in many sessions, we did get to sit in on LTG Sorensons lunch briefing today outlining the IT direction that the Army is taking through 2011.  One of the issues that he talked about was the difficulty that units and soldiers faced in portability of their applications from garrison to predeployment to warfighting.  Because connection to applications is not universally portable, maintaining continuity is problematic.",
      "We also attended the Army Knowledge Online Single Sign On session.  They use SharePoint for the SSO solution and then push the SSO to the approximately 450 applications which tie into AKO.  As an aside, wed prefer that they use JA-SIGs CAS single sign-on solution.  Instead of needing to be inside a military installation to access AKO, users can sign in from anywhere.",
      "It seems that once the infrastructure is in place, the Army could use a similar approach to allowing soldiers to link in to the applications they need wherever they are in the world.  Pushing applications down and out will grow more important as, as LTG Sorenson said in his opening speech, network centricity is pushed closer to the soldier.",
      "Some other observations:",
      "Soldiers who in the field are given a broad swath of responsibility and find themselves with that responsibility reduced when they return to garrison are discontented.  Thats no surprise.  If were going to be successful at war long term, garrison ops need to adjust to more closely mimic in-theater ops.\n  LandWarNet is in Second Life!  Im glad that the military is learning the value of Second Life.  The SLURL is here."
    ],
    "summary_t": ""
  },
  {
    "id": "3d2c2a17d887429c98f429930f3b41a5",
    "url_s": "https://opensourceconnections.com/blog/2008/08/21/landwarnet-08-day-3-collaboration-can-connect-a-disconnected-army/",
    "title": "LandWarNet 08 Day 3: Collaboration Can Connect a Disconnected Army",
    "content": [
      "Eric and I spent Day 3 at AFCEAs LandWarNet having a lot of conversations with vendors and with members of Army programs.  The main themes of our discussions were security, collaboration, and connectivity.  All three are interconnected.  Army members must be interconnected if they are going to be able to communicate with each other, particularly in a warfighting environment where events in Southwest Asia are tied into teams back in the United States in real time or near real time.  They must collaborate to share ideas and to stay ahead of a very smart and very motivated enemy.  Finally, they must act in a secure IT environment (much less a secure physical environment) to ensure that the same enemy doesnt discover our plans and get into our decision cycles.",
      "One of the means by which solders can and should be connecting with each other is through the Armys knowledge and content management system, Army Knowledge Online (AKO).  Its one of, if not the, largest knowledge management systems in the world.  It has over two million users, and hundreds of thousands of pages.  What it does not have is a good way for people to discover each other and common communities of interest.  A good example is when the Stryker Brigades deployed to Iraq.  In the beginning, there was no knowledge how to deploy, since nobody had done it before.  However, as more and more knowledge (tactics, techniques, and procedures) is developed, it should be shared for new Stryker leaders to leverage.  AKO could be a way to push information to people who may need that information.  By leveraging this portal to share information in a way similar to the Netflix recommendation engine, the Army could disseminate information more quickly and help innovate.  The enemy in the Global War On Terror is innovating; we need to make sure our soldiers have the tools to innovate as well."
    ],
    "summary_t": ""
  },
  {
    "id": "1b57b877590ff46bb3e4a3e11dfabe68",
    "url_s": "https://opensourceconnections.com/blog/2006/10/16/whos-watching-road-show-is-starting/",
    "title": "\"Who’s Watching\" road show is starting!",
    "content": [
      "",
      "Join me and others for the Cyber Security Roadshow being put on by the \"Whos Watching\" campaign to increase public awareness of the profile people leave using the Internet.",
      "Ill be talking tonight and Wednesday about the scams and \"phishing\" attacks that target Senior Citizens."
    ],
    "summary_t": "Join me and others for the Cyber Security Roadshow being put on by the \"Whos Watching\" campaign to increase public awareness of the profile people leave usin..."
  },
  {
    "id": "f2ee1162836b10ac57f3c31d1de67859",
    "url_s": "https://opensourceconnections.com/blog/2008/08/22/leveraging-multimedia-for-learning/",
    "title": "Leveraging Multimedia for Learning",
    "content": [
      "A couple months ago I served on the CBIC Innovation Awards research committee. I was able to sit in on an interview with Beth White from Western Albemarle High School who eventually won the Red Apple Award for technology in the classroom. My co-researcher, Sallie Hill, was evaluating, among other things, how a teacher leverages technology as a tool, but doesnt become captured by the glitz of the technology.",
      "I recently stumbled across this white paper on Leveraging Multimedia for Learning done by Ruth Clark of Vcom3D. The paper was done for Adobes Captivate tool, however the lessons learned in the paper are applicable to anyone doing rich presentations.",
      "What was interesting was that the lessons in the white paper mostly echoed the things that we saw Beth White do in the class room. Lots of small bite sized use of technology. A willingness to move on when technology became a roadblock. A focus not on the tools, but on the material being learned. Technology allowed the students to learn more effectively by engaging more of their senses and bringing out their creativity.",
      "The white paper also echos a lot of the messages that Presentation Zen, one of my favorite blogs says about doing great PowerPoint based presentations.",
      "Ive been working on a project the involves indexing large amounts of data and pulling out patterns. One of the lessons that I am learning is that while I can pull out lots of patterns, show off lots of details, and do analysis in many different ways, I am becoming captured by my cool techno tricks! The project has mostly been a R&D effort, but now I am looking to start crystallizing the value of my R&D.",
      "I am mentally sitting down and going through the various assumptions that I have made about my users, and evaluating each cool analysis trick through the lens of \"Does this actually meaningfully help my users\"? And anything that is in there for the \"cool\" factor is being ruthlessly pulled. This is going to lead to a lot less \"wow\" factor, but a much better tool."
    ],
    "summary_t": ""
  },
  {
    "id": "cca83ed0c8bbfeb900af6765620097a6",
    "url_s": "https://opensourceconnections.com/blog/2008/08/28/securing-cc-rb-from-the-world/",
    "title": "Securing CC.rb from the world",
    "content": [
      "I love CruiseControl.rb, but one of the things Ive found is that it exposes via the web interface a lot of what is potentially confidential data. From the cruise_config.rb containing accounts on our SVN server, to being able to browse source files, which would include database.yml, again, exposing usernames and passwords.",
      "And, since CC.rb runs on port 3333 by default, I expect some enterprising hacker has been scraping the internet looking for all the IP addresses that have a server running on port 3333! A lot of folks have been using Apache and mod_proxy and mod_auth to try and wrap CruiseControl.rb, but you still need to add firewall rules so people dont just go directly to the server on port 3333. There has been quite a bit of traffic on the CC.rb mailing list lately about this. However, I found that HTPasswd was a much simpler solution. HTPasswd is one of those little know gems of a plugin. It makes it dirt simple to add both Basic and Digest authentication to Rails apps. I just SSHed into the server running CC.rb, browsed to the CC webapp directory and ran ./script/plugin install http://wota.jp/svn/rails/plugins/branches/stable/htpasswd/. I added the line htpasswd :user=>\"MYUSER\", :pass=>\"MYPASS\" to application.rb, restarted CC.rb, and now have a secure CruiseControl.rb instance! Much simpler then hacking around with Apache configuration files. Please go to the HTPasswd page on AgileWebDevelopment and vote for this great plugin!"
    ],
    "summary_t": ""
  },
  {
    "id": "e7a76a4ceba22031b31800a76d33c519",
    "url_s": "https://opensourceconnections.com/blog/2008/09/02/charlottesville-net-user-group-meeting-sept-18th/",
    "title": "Charlottesville .NET User Group Meeting: Sept. 18th.",
    "content": [
      "The Charlottesville .NET User Group will meet on September 18th from 6:30-8:30PM at the SNL Headquarters building (One SNL Plaza, Charlottesville VA 22902) G. Andrew Duthie, Microsoft Corp. will present \"Controlling AJAX when youre out of control\" While implementing and troubleshooting AJAX functionality in an application is rarely easy, it gets even harder when you dont have complete control over the site where you have to implement it. Shared hosting environments, blog accounts on large blog sites, etc. are situations where you may have only limited control over where and when your javascript gets loaded and executed. In this talk, Microsoft developer evangelist G. Andrew Duthie will walk you through some examples of how you can overcome limitations on such sites, and how to troubleshoot some of the inevitable issues you may run into. Some basic understanding of javascript and AJAX is assumed. Examples will span from simple mouseover image manipulation to implementing controls such as Microsofts Virtual Earth and Vertigos Slideshow Silverlight photo gallery."
    ],
    "summary_t": ""
  },
  {
    "id": "ef81d55ea5389c5bdea00eb5f0b5e064",
    "url_s": "https://opensourceconnections.com/blog/2008/09/09/new-aggregated-feed-of-ci-related-blogs/",
    "title": "New aggregated feed of CI related blogs",
    "content": [
      "Eric Lefevre has just put together an aggregated feed of various blogs (including this one!) that discuss techniques and best practices related to the field of Continuous Integration and agile testing in general. You can subscribe to it using this Feedburner link: http://feeds.feedburner.com/CitconBlogs.",
      "While he still needs to do some tweaking on the criteria for the blog postings, it is a great first step. For example, if you have the word unit in your post, then it gets included. Which unfortunantly brings in some of our posts about the United States and forming Communities of Interest.",
      "Its a great use of Yahoo Pipes and Im looking forward to using it to help me focus on the specific blog postings that I am interested in, versus reading the full blog everyday."
    ],
    "summary_t": ""
  },
  {
    "id": "377efb8908474689c6242b6067c9c8f1",
    "url_s": "https://opensourceconnections.com/blog/2008/09/09/the-hottest-thing-in-internet-browsers-google-chrome/",
    "title": "The hottest thing in Internet browsers: Google Chrome",
    "content": [
      "Google recently released a beta version of its new web browser appropriately named Chrome. Similar to Google Search, Chrome contains a minimalist, clean and simple user interface. It appears as though Google took the best elements of both Firefox and Internet Explorer and incorporated them in Chrome. The download manager is similar to that of Firefox and it uses the status bar at the top of the browser for the password saving feature similar to Internet Explorer. I really like what they call the \"One box for everything\" feature. They have combined the search, history and address bar into one box which makes finding web sites nice and easy. I really like the new tab page that contains thumbnails of the web sites that have been viewed most often. I was not impressed by the pop up blocker implementation. If a pop up window is blocked a bar shows up at the bottom of the page that becomes a draggable window if you choose to allow the pop up window. I must say I was most impressed by the speed at which web pages load in Chrome, in particular the web site I browse most often: GMail. You can read about all the great features of Chrome at http://www.google.com/chrome/intl/en/features.html?hl=en. I encourage all Internet users to download Chrome and try it out for themselves."
    ],
    "summary_t": ""
  },
  {
    "id": "e63aec9215827b70e55c9c978664b2fc",
    "url_s": "https://opensourceconnections.com/blog/2008/09/10/hightechcvilleneon-guild-91508/",
    "title": "[email protected] Guild 9/15/08",
    "content": [
      "Ill be presenting HighTechCville to the Neon Guild next Monday, September 15.",
      "Ive been looking forward to this for months because most of the people information in HighTechCville comes from the Neon Guild public membership database. My initial success in finding Communities of Interest came about by looking at the over 200 people in the Neon Guild and finding 8 folks who were all technical writer folks! I would never have guess that there are enough people in the Neon Guild who do technical writing to do a group dinner together!",
      "See youall there!",
      "Here are directions from Debra Weiss:",
      "Location:\nInova Solutions\n110 Avon Street\nCharlottesville, VA 22902",
      "Directions from downtown Cville:\nTake Market Street E to Ninth/Avon St, turn right.\nGo over the bridge, get in the left lane.\nLook for Spudnuts on the left.\nTurn Left at Spudnuts, and then another immediate left.\nFollow around, youll see a large brick building. Thats Inova.\nGo around to the front of the building and park.\nTake the elevator to the second floor. Were in the cafÃ©."
    ],
    "summary_t": ""
  },
  {
    "id": "8525e9cff1f51df04f6afdb9d8f3820f",
    "url_s": "https://opensourceconnections.com/blog/2008/09/11/is-your-airport-giving-up-flights-without-a-fight/",
    "title": "Is Your Airport Giving Up Flights Without a Fight?",
    "content": [
      "ABC News posted a story today positing the imminent decline of the small airport.  While the article only specifically cites the reduction of flights at the San Luis Obispo airport, it does draw more general conclusions about the impact of reduced airline traffic on regional and municipal airports.  A similar report in June prematurely predicted the demise of many airports. prompting at least one airport director to respond directly to the allegations.  While reports of the death of the regional and municipal airport may be greatly exaggerated, there is no doubt that airports cannot simply sit back and rely on airlines to do their marketing for them.  The presence of planes does not ensure the presence of passengers, and, as my Army buddies are wont to say, its time for airport marketers and directors to lean forward in the foxhole and take charge of their own destinies.",
      "What can airports do?  Here are a few suggestions:",
      "Talk to your constituents.  This isnt just the flying public, but everyone whos affected.  Airports mean jobs and commerce, so there are more people than just airport/airline employees and fliers who are affected.  Tell them what is going on more often, and dont expect them to read the minutes of your board meetings.\n  Tell the good stories.  There are reasons that people like to fly out of municipal airports, and that message needs reinforcement.  As one passenger said, \"When you want to make connections…its really important to have a local community airport.\" \n  Tell the WHOLE story.  Its easy to go to an airfare aggregator such as Kayak and look at fares from regional and large airports.  Does your airport have shorter security lines?  Does it have more convenient parking?  Cool art exhibits?  Tell the world!",
      "If regional and municipal airports dont do their part to market to the flying public, then they will most certainly lose flights as the cost of oil and consolidation within the airline industry put the squeeze on the smaller players.",
      "Is your airports website telling your story?  Does your website, the most commonly seen branding statement of your airport, tell the story you want it to?  If not, check out our AeroWeb solution and see if we can help.  Contact us to see how we can help you fight back."
    ],
    "summary_t": ""
  },
  {
    "id": "8b3a2e2f9ed820d6c16c5593833f15d6",
    "url_s": "https://opensourceconnections.com/blog/2008/09/19/separating-the-true-web-from-the-tabloid-web/",
    "title": "Separating the True Web From the Tabloid Web",
    "content": [
      "The founder of the World Wide Web fears it has become a web of lies.",
      "At the initial test run of the Large Hadron Collider, Sir Tim Berners-Lee cited the spread fears of the world about the impact of the LHC on Earth via the Internet as an example of the Web gone wrong.   While his World Wide Web Foundation looked at labeling websites, similar to a simple IQ rating, they decided that multiple ways were needed by multiple users to brand the usefulness and viability of websites.",
      "This leads to the need for a reputation engine for the Internet.  Just as Stephen Colberts assertion on Wikipedia that rabbits are carnivorous, should have been flagged as questionable because of Colberts lack of previous contributions and lack of previous information acceptance.  All it takes is a few people to believe what they see, tell their friends, and the wildfire of an urban legend is born.  Its how Nigerian scamsters work.",
      "Andy Gregorowicz and Mark Kramer at MITRE have come up with an initial pass at how to validate the information presented by multiple contributors.  Their paper points out the difficulty of determining provenance of information and the many to many relationship of contributors and information.  Still, it is good start towards separating truth from fiction on the Internet."
    ],
    "summary_t": ""
  },
  {
    "id": "f1f62f5d074fafd863725222e52933b3",
    "url_s": "https://opensourceconnections.com/blog/2008/09/26/helping-todays-high-schoolers-become-future-leaders/",
    "title": "Helping Todays High Schoolers Become \"Future Leaders\"",
    "content": [
      "Stuarts Draft High School is launching their new event called: \"Lunch with Future Leaders\" and we have been invited to spend some time with the students to discuss with them our careers and the paths we took to get to where we currently are. This is a great opportunity for us to show our passion for what we do and give some insight to students who are trying to make important decisions about their future. As a community focused company we value events like this that allow us to interact with young people who will soon become the bright minds of the future.",
      "Eric and Youssef will be having lunch with the students at Stuarts Draft High School on Tuesday, October 14, 12pm – 1pm.",
      "Below is one of the news releases sent out by the school:",
      "Stuarts Draft High School is launching \"Lunch with Future Leaders\", an opportunity for young professionals to promote job skills and career pathways. Once a month over lunch in the high school cafeteria, the featured guest and a group of students will be able to discuss educational foundations, the joys and challenges of a particular career, and job-specific knowledge necessary for achieving the students plans for their future. The first guest will be Sam Rasoul, candidate for the United States 6th Congressional District on Spetember 18th. Anticipated future guests will include local individuals in their 20s and 30s from a variety of interesting fields and professions. The core idea behind \"Lunch with Future Leaders\" is to allow students the opportunity to learn first-hand about the varieties of careers available and how to reach their potential in a chosen field. The featured professionals will also benefit from hearing about the next generations ambitions and enjoy the chance to display how their own dreams have come to fruition. Questions about this program can be directed to SDHS Principal, Donna Abernathy, or program sponsors Jonathan Kern, Government teacher, and Jenny Gardner, Career Coach."
    ],
    "summary_t": ""
  },
  {
    "id": "09ad7f113e615ab87014630b5395b530",
    "url_s": "https://opensourceconnections.com/blog/2008/09/30/a-case-study-on-the-need-for-a-reputation-engine/",
    "title": "A Case Study On the Need For a Reputation Engine",
    "content": [
      "This morning, I received an e-mail asking if wed opened up an office in Zambia.  Given my fawning over my trip last year to Botswana and South Africa, it was only natural that someone on the team be curious if Id actually made the leap and left.  After all, he found someone on LinkedIn who claims to be a member of OpenSource Connections.  A new employee in the Zambia office?",
      "Not quite.  As it turns out, the person who makes the claim to be a \"co-ordinator\" at OSC may simply be a mugu who goes by the name of \"Simon Mugala.\"  The more likely probability is that Mr. \"Mugala\" is a member of a 419 scammer who is trying to prove to the poor people who respond to his spam e-mails that hes a legitimate member of a legitimate company that has millions, if not billions, in oil fortunes that need to be repatriated to the U.S. and only needs a bank account number to get that money over there.",
      "The unknowing respondent (and, really, arent most people who respond to a 419 scam unknowing?) will probably look at Mr. \"Mugalas\" profile on LinkedIn and assume that hes from OSC.  However, even the most cursory examination will reveal the flimsiness of this claim:",
      "He has 0 connections.  This should be the big a-ha.  Unless hes brand new to social networking and hasnt had a chance to connect, then he should have some friends.  Its possible that Zambia hasnt discovered LinkedIn, but…\n  He doesnt link to the home page.  There isnt a website link in the page for his (our) company.\n  A Google search of Simon Mugala + OpenSource Connections yields no results.  Youd think hed be in an employee directory somewhere.",
      "What LinkedIn lacks is the way for me to tell them that hes not a part of our organization.  Furthermore, what it lacks is the reputation engine to state that based on existing evidence, hes probably not a member of our organization.",
      "As for \"Simon,\" I hope he exists, and I hope hes a Zambian who loves open source and just accidentally filched our companys name.  Id love to see Zambia one day, as my friends Carla Rountree and Amanda Hilligas can testify to, and maybe I can meet \"Simon\" and we can talk about all things FOSS!"
    ],
    "summary_t": ""
  },
  {
    "id": "d7cc64b7aa87fb379d5dbebb812d233a",
    "url_s": "https://opensourceconnections.com/blog/2006/10/20/selenium-presentation-neon-guild/",
    "title": "Selenium presentation @ Neon Guild",
    "content": [
      "Hi Guilders:",
      "Heres your Friday-before reminder:",
      "Next Neon Guild Meeting: October 23, 2006 6-8pm\nTopic: Selenium\nPresenter: Eric Pugh, OpenSource Connections",
      "Selenium is a test tool for web applications. Selenium tests run directly in\na browser, just as real users do. And they run in Internet Explorer, Mozilla\nand Firefox on Windows, Linux, and Macintosh. No other test tool covers such\na wide array of platforms.",
      "If you do any sort of development work, this will be a very useful session\nto attend!",
      "If you dont do development work, come for the pizza and chitchat before the\npresentation."
    ],
    "summary_t": "Hi Guilders: Heres your Friday-before reminder: Next Neon Guild Meeting: October 23, 2006 6-8pm Topic: Selenium Presenter: Eric Pugh, OpenSource Connections ..."
  },
  {
    "id": "2a8b0f88013be79a1e2dcf48fa337c3f",
    "url_s": "https://opensourceconnections.com/blog/2008/10/02/attending-ruby-dcamp/",
    "title": "Attending Ruby DCamp",
    "content": [
      "Ruby DCamp is around the corner, and Youssef will be representing OSC at the event.",
      "The event details are:",
      "What: Ruby DCamp\nWhen: Saturday, October 11, 2008 at 09:00 am to Sunday, October 12, 2008 at 05:30 pm\nWhere: Holiday Inn, Arlington\n4610 Fairfax Dr\nArlington, VA 22203",
      "For more event details, visit the event page: http://rubydcamp.eventbrite.com.",
      "Hope to see you there!"
    ],
    "summary_t": "Ruby DCamp is around the corner, and Youssef will be representing OSC at the event. The event details are: What: Ruby DCamp When: Saturday, October 11, 2008 ..."
  },
  {
    "id": "6a1dd8ef743f93895388bf9654e9c3c2",
    "url_s": "https://opensourceconnections.com/blog/2008/10/02/up-and-coming-microsoft-technologies-in-open-source/",
    "title": "Up and Coming Microsoft Technologies In Open Source",
    "content": [
      "Typically speaking in the past, critics would have laughed or promptly smashed any one who supported the \"evil empire\" in any way, shape, or form.  However, times change and the pendulum swings.  Thanks to people like Scott GU,  there has been much change to how Microsoft is doing development and relating to developers who work on their platform.  Granted its not perfect or without incident, but there is change from within the evil empire.",
      "On the skirts of the empires domain, there has been for some time, open source projects that really have been the cornerstone and often taken for granted to daily development.  NUnit, NDoc, NAnt, Log4Net, NHibernate, Spring.Net, just to name a few projects that have really helped .net development community in general.",
      "However, in the last couple of years, the reach out to developers of .Net who use opensource projects, software, tools, etc, has increased, with the likes of web sites like CodePlex and Port 25, which were not only created, but constantly improved upon and continued to be improved upon. CodePlex now supports SVN as a code repository.  Not only that in the last year or so; Microsoft has actually been helping Mono (which is an open source project that helps bridge .net to work on other platforms like linux or mac) with its adoption of Sliverlight, called moonlight.  Microsoft has even released their own shared/open source licenses like Ms-PL to the community.</p>",
      "Now there are a ton of crazy things that are currently going on, and there are some hidden gems that are being developed that might not get as much attention as they deserve.",
      "Hidden Gems",
      "Unity** (**Ms-PL**) – **a dependency injection framework with the latest version of Object Builder.  The configuration seems much less evil than spring.net.\n  **White (Apache) **- Windows Forms, WPF, & SWT (java) automation.  Basically a library to help automate UI tests or batching for .net thick client UIs. People that use NUnitForms really should give this a go. Thanks ThoughtWorks. \n  WaitN** (**Apache**)** – Web site automation for the .net runtime, works with both IE and Firefox. \n  Subsonic** (**MPL 1.1**) – **a toolkit for web sites that tends to use rail like concepts, including generating the DAO, migrations, etc. \n  BlogEngine.Net** (**Ms-RL**)** – light weight blog engine\n  Facebook Toolkit** (**Ms-PL**)** – .Net Facebook API wrapper Library\n  Gallio** (**Apache**)** – Automation Platform for .net let you use various tools and testing frameworks side by side such as MB-Unit, NUnit, PowerShell, Pex, NBehave, etc.\n  MB-Unit (Apache) - Generative Unit Test Framework that can also be super useful in creating functional tests as well as unit test, (and to me, it has more to offer than the testing framework that comes with visual studio).\n  Flickr.Net Api Library** (**LGPL**)** – a .net wrapper around the flicker api.\n  Avalon Dock** (**New BSD**)** – WPF Docking Suite, much like its win forms forms counterpart.\n  Ajax Toolkit  (Ms-PL) – A library of AJAX controls for .net web sites.\n  Iron Python** (**Ms-PL**)** – implementation of the Python programming language running on .NET\n  Iron Ruby – still in the works, but this will be an awesome addition to .net when its finished.\n  BOO** (MIT/BSD)** – a wrist friendly language for .net, also still in the works.\n  F#** (pending)** – it supposedly will be released under the Ms-PL when it reaches the 1.0 state.",
      "New Software Options from Microsoft",
      "Asp.Net MVC** – **Microsoft is writing a rails like MVC option for its web development genre, that is being released, including source code and tests in iterations to the public for testing, feedback, developer interaction,actually depending on the community to help drive what is being built and how it functions. \n  JQuery – A well known Javascript library, is now being included on the MS tools platform, it will be released in Asp.Net MVC, included in the next version of visual studio and so on. \n  Prism** (**Ms-PL**)** – Composite Application Guidance for WPF, (similar to CAB in a way)  in order to help build WPF clients in general. \n  WPF Toolkit** (**Ms-PL**)** – basically the initial release of the WPF DataGrid as well as making room for MS to release new WPF controls without having to release a new version of the framework.",
      "Of course I have to make a small plug for Amplify, a BHAG project and an open source framework that Im working on in my spare time. Ive started working on a dashboard / GUI tool called FUSE for amplify that will help create database fixtures, help with migrations, and other code habits in general.  Ill be using Unity, Log4Net, Avalon Dock, Gallio, Mb-Unit, and White for this tool, its using the latest version of .Net 3.5 sp1 and will be using WPF instead of win forms for the first release.  A preliminary screen shot, below of FUSE pulling info from the databases schema.",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "So with all that being said, the next time you go to pick a platform and think open source doesnt apply to c# or .net in general, you might want to take another look.",
      "If you know of any other hidden gems or tools that I have missed, please, add them in the comments below.",
      "Technorati Tags: Microsoft,Opensource,Open,Source,.Net,BHAG,Projects,Amplify,Gallio,WaitN,Mb-Unit,White,Unity,Log4Net,Prism,JQuery,Asp.Net MVC"
    ],
    "summary_t": ""
  },
  {
    "id": "cf5f72557daaf8f5d1f59fa7930f62b6",
    "url_s": "https://opensourceconnections.com/blog/2008/10/03/three-ways-semantic-mediawiki-can-supercharge-your-wiki/",
    "title": "Three Ways Semantic MediaWiki Can Supercharge Your Wiki",
    "content": [
      "Semantic MediaWiki (SMW) is a a single extension to the ever-popular MediaWiki that instantly adds semantic web capabilities to your wiki pages. That means that you can annotate important data in the text, relate it with an ontology, format it so it looks pretty on the screen, and share it across pages or even applications. Thats a lot to digest from just one extension, so Ill highlight some common uses.",
      "Effortless Wiki Gardening",
      "Once youve annotated some text in your wiki, you can then slice it any way you wish. That means you can reuse bits of text rather than duplicating them, combine them in unique ways, and order them according to some logic. An ontology creates inherent relationships between things, and SMW lets you use that to create inherent relationships between your articles.",
      "Ad Hoc Templates",
      "Most of the wikis Ive dealt with require coding in the wikis native language to create custom macros or page templates. For me that pretty much kills any urge to templatize at all – thats just more code to keep track of, debug, and tweak. With Semantic MediaWiki you can create a custom data type that users \"instantiate\" and fill in with data. That way you can encourage a standard set of properties much like an LDAP schema. The presentation is separate from the data, so the users are shielded from having to master the ins and outs of wiki formatting or HTML. The cherry on top is that the template itself is created just like a normal wiki page!",
      "Queries Without the Database",
      "Once you have your wiki filled with annotated data you can include that data in any other part of the wiki. The closest comparison I can make is to how images are included within articles. Once youve uploaded an image you can then reference that image (or in some cases various sized thumbnails) in any number of other pages without re-uploading. An example borrowed from the SMW site is that of the population of California. If you used that bit of information in several pages, youd have to update each page whenever somebody was born or died. Thats a slightly contrived example, so heres something more practical: If you kept your companys personnel records as wiki pages you could create a report page that shows the complete list of employees, followed by who does or does not take part in the health plan.",
      "Why You Care",
      "Wikis are the dominant form of information sharing within technology-savvy groups. Thats due in large part to the ease with which information can be added to them. However, without structure and careful organization information can easily be lost within them. Search features help, but only if you know what youre looking for. Thats the point where flat text simplicity intersects the utility of structured data. Thats the brink where information becomes knowledge."
    ],
    "summary_t": ""
  },
  {
    "id": "ea215a7c51236601239b3142e2ba8fa9",
    "url_s": "https://opensourceconnections.com/blog/2008/10/06/trip-report-shenandoah-ruby-user-group-2/",
    "title": "Trip Report: Shenandoah Ruby User Group",
    "content": [
      "Last week four of us from RubyCodeJam took the trip over the mountain to Harrisonburg, VA to join ShRUG for the evening. Alex Herron had arranged for Rich Kilmer and Marcel Molina from InfoEther to come speak. I pretty much took notes on Marcels presentation:",
      "Marcel Molina gave us a talk that drew on his previous presentation on Beautiful Code to focus on some aspects of Ruby that prevent us from being more productive.\n\nHe started out talking about how some things like the ability to refactor code by being able to extract a method, while a small tooling change, has led to big productivity gains in how we write our code. But, as weve become better at writing compact expressive code, we still keep running into constraints.",
      "Specifically he talked about how we still need too much infrastructure to be productive. For example, why in Ruby is scheduling so hard? What cant I write:",
      "5.minutes.from.now do\nsome_big_method\nend",
      "and let the Ruby runtime handle that scheduling? We keep using tools to make custom daemons, or use BackgroundRB, or setup external job tools like cron.",
      "Another example is persistence of objects. ActiveRecord is amazing, but it is still a translation layer over RDBMS. I mentioned Madeleine as a potential solution, and asked why it wasnt more popular. The group feedback was that it wasnt sufficiently robust, and required you to adopt the Command Pattern for persisting your data.",
      "Lastly Marcel talked about how come doing threaded programming wasnt easier. In his mind, being able to parallelize your code is something that the compiler/runtime should be doing, and you shouldnt have to think about if some tasks can be done in parallel.",
      "Marcel talked about \"big\" topics, and that was great. It was very inspirational to hear thoughts on how we can become better developers, how we can enhance our environment to be better. The venue (a microbrewery!) was awesome, and it was great to meet other Ruby folks."
    ],
    "summary_t": ""
  },
  {
    "id": "beb6fd9d764d8624e30d623784fa4415",
    "url_s": "https://opensourceconnections.com/blog/2008/10/08/played-poker-today-scrum-poker-that-is/",
    "title": "Played Poker Today – Scrum Poker that is….",
    "content": [
      "Weve been using \"Scrum Poker\" to help estimate product backlogs. Today, for a new project, we had planned on using regular playing cards. We were ready to get started, and I realized I didnt have the playing cards with me, so I googled for some that we could print out and found a great set: http: //projetosetal.wordpress.com/2008/09/15/get-your-own-poker-planning-deck-of-cards</http:>. I wondered if there was a solution for remote teams, and sure enough, someone has built a tool: http: //www.planningpoker.com</http:>"
    ],
    "summary_t": ""
  },
  {
    "id": "ab1909d55193c0a33070d76cf7f81ca7",
    "url_s": "https://opensourceconnections.com/blog/2008/10/10/introduction-to-xul-development-at-betech/",
    "title": "Introduction to XUL Development at beTech",
    "content": [
      "Wednesday, October 12th, I will present on XUL development and discuss what worked while I worked on a XUL project for Opensource Connections. XUL (XML User Interface Language) is Mozillas XML-based language that lets you build feature-rich cross platform applications that can run connected or disconnected from the Internet. These applications are easily customized with alternative text, graphics and layout so they can be readily branded or localized for various markets. Web developers already familiar with Dynamic HTML (DHTML) will learn XUL quickly and can start building applications right away.",
      "When/Where:\n  \n\n  \n    * Wednesday, October 15th\n  \n\n  \n    * 10:00 – 11:30 am\n  \n\n  \n    * Clemons Library, Room 407 (to the left of the reference desk)\n  \n\n  \n    Please take a look at my previous blog post XUL Getting Started Guide\n  \n\n  \n    Slides from Presentation"
    ],
    "summary_t": ""
  },
  {
    "id": "afd88e280188cab0787d96254dccb590",
    "url_s": "https://opensourceconnections.com/blog/2008/10/12/ruby-dcamp/",
    "title": "Ruby DCamp",
    "content": [
      "Ruby DCamp is the first conference, of any type related to ruby or my recent work with Ruby on Rails, I have attended since graduating from UVa. I have been awaiting this opportunity for a while now because my colleagues at OSC have been raving about the kind of people that will be present. People like Rich Kilmer, Brian Marick, Jeremy McAnally, Lucas Cioffi, David James and many more. Being new to the Ruby community, it is very important to meet the people who are responsible for the development of the Ruby ecosystem.",
      "It was very convenient that the first topic that we discussed on the first day of the conference was the Ruby ecosystem. One of the key points that I got from that discussion was how people got introduced to Ruby through different ways and different circumstances but the majority got in through Rails. But a consensus between the \"expert Rubyists\" was summed up by one of them in the sentence \"Came in for Rails, stayed for Ruby\" which indicates that although Rails is the reason why people start learning Ruby, it is in itself not as appealing as Ruby is. This is why during one of the discussions we had, people were interested to find out more about the different technologies out there that leverage Ruby and give the developer the same functionality as Rails such as Merb. It was really cool to get the input of the people at the conference on their experiences with the language and the community itself.",
      "There were two interesting things I want to mention specifically from the first day. The first one is that I found out that I am not as much a beginner as I thought myself to be. I came to this conclusion from the Beginner Talk discussion I listened in on. During that talk, a few people who have been working with Ruby for a long time and one of them even teaching it were giving an outline of the language and mentioning things to look out for. Surprisingly enough, I already had experience with all that stuff unlike a few others who have a lot of experience with other programming languages but are new to Ruby. This was interesting to me because I came in thinking that I was the least knowledgeable person attending RubyDCamp in terms of Ruby and Ruby on Rails, but that was not true (much to my relief). The other thing would have to be the fact of how smart some of the Rubyest at the conference are. I heard all about it from my coworkers but I had to see it for myself to believe it and it was very apparent the moment I observed those people planning to patch ruby gems to add a functionality they figured to be important. This was not just it, as people were mentioning ideas of what needs to be done, one person in particular was already done implementing some things and the best part was how he had it all done before everyone even reached a consensus about it.",
      "Saturday was great in the sense where I got to meet a bunch of new people and get an idea of what kind of projects everyone is working on. Sunday on the other hand was great to get into more detail. The first session I attended was titled \"CouchDB hack session\" and as the title indicates, it was a session focused on CouchDB. I think it was the most popular session of the morning because there were a lot of talks about it during the first day but most of the people had no idea what it was. It was a good session with a lot of discussion about CouchDB and its uses.",
      "The next session I sat through was a session about SproutCore. I already had an idea of what SproutCore is because I had considered using it to develop one my apps, but ended up not using it. So it was interesting for me to get the perspective of someone who had already used it and also contributed to it. What distinguishes SproutCore is how it is modeled on Rails and allows the developer to use Ruby in development but then generate JavaScript and Json to be deployed once the application is done. This allows developers to keep using Ruby even in environments that limit them such as developing an application for OpenSocial. The main thing I got from this session was some sort of validation that SproutCore is a technology that can be trusted and used, which is a fear that I had because I did not want to develop an application using a new technology that will not be supported in the future, but I learned from the conference that Apple used SproutCore for their MobileMe application and also contributed to the project. This gives me some sort of relief that it will be around in the next few years.",
      "At lunch we started sharing our \"ruby experiences\" and our \"favorite tools\" and heres some of those tools: git-bisect, limelight, hpricot, mechanize, jruby… So if you havent heard of these tools before, make sure you do your research!",
      "Oh… and one more thing that I should mention: We at OSC need to do a bit more pair programing!",
      "All in all, it was a great conference and special thanks to Evan Light for organizing the event. Hopefully we will be able to build on this to grow and improve the Ruby community."
    ],
    "summary_t": ""
  },
  {
    "id": "914bb39a0435e9728e4605ac651acec6",
    "url_s": "https://opensourceconnections.com/blog/2008/10/17/let%e2%80%99s-get-ready-to-rumble-again/",
    "title": "Lets Get Ready to Rumble!!!!  Again!!!!!",
    "content": [
      "",
      "RailsRumble is back, and this year weve got a great team, made up of some returners, and new folks. Were feeling really optimistic that one of us will be walking around with a new belt.",
      "Returning from last years competition, are Eric Pugh and Ashish Tonse. Joining us are first timers Arin Sime and Youssef Chaker. Michael Herndon, who was on last years team is hanging out for the weekend as well, but working on other stuff. But he is here in spirit!",
      "Our entry this year really ties into our evolving \"Business Tags\". Its a location aware application that integrates disparate datasources, and in the true Web 2.0 fashion mashes up multiple data streams.",
      "Oh, and were using our OpenApproach, but instead of 3 week sprints we are doing 3 hours sprints. The first one, themed \"Git Started\" runs from 9:45 PM till 12:45 AM!"
    ],
    "summary_t": "RailsRumble is back, and this year weve got a great team, made up of some returners, and new folks. Were feeling really optimistic that one of us will be wal..."
  },
  {
    "id": "4e8882e29c7ca90f01a1f46f33cc5cf5",
    "url_s": "https://opensourceconnections.com/blog/2008/10/21/azimuth-treasure-hunt-app-release-the-hounds/",
    "title": "Azimuth Treasure Hunt App: Release the Hounds!",
    "content": [
      "As Eric already indicated in a previous post, this past weekend 4 members of the OpenSource Connections team competed in Rails Rumble 2008.Â  We built a Ruby on Rails app in less than 48 hours, which we called Azimuth and you can see it online here.",
      "Azimuth is a fun little app that allows you to create \"treasure hunts\" for your friends, where they get SMS text messages indicating whatÂ the next clue is.Â Â They have to go find the \"treasure\" related to that clue, and then text back to Azimuth that they found it.Â  To prove they found it, they have to either text back the latitude and longitude of the treasure, or text back a \"key\" word, which presumably the person who organized the hunt would have left at the site of the treasure.",
      "It was a lot of fun building the app, andÂ although it was an intense weekend, I think we all really enjoyed it.",
      "",
      "ButÂ to put Azimuth to the test, I decided on Monday night toÂ setup a treasure huntÂ for my sons and some neighborhood kids.Â  IÂ hid five post it notes around my house, each with a \"key\"Â written on it.Â  I opted to go for keys instead of lat/lng coordinates since I was doing everything in aÂ limited geographic area, and IÂ dont have a gps device.Â Â But for wider hunts, you could also useÂ lat/lng.",
      "",
      "Then I got my sons and a few of their friends together, gave them my cell phone, and told themÂ how it worked (in case youre wondering, yes, my son is wearing a box in the picture.Â  he does own clothes, hes just building his own halloween costume and likes to wear it!).Â  My sons havent texted much before, but two of the girls from the neighborhood who were over at our houseÂ already know all about texting.Â  So they had no trouble withÂ my phone.",
      "",
      "It was great watching the kids as the ran from oneÂ part of the house to another trying to figure out the current clue.Â  After they found it, they would text back to azimuth a message like \"azimuth1 piano\". This indicated that they had found the blue post-it note I hid on the piano, which used the \"treasure\" keyword piano.Â  The \"azimuth1″ prefix on the message is required for all messages. This allows the freeÂ SMS toolkit we used, Zeepmobile, toÂ know what software app to send the message to.Â  Our code then receives the text messageÂ on a landing page and processes it.Â  If the code determines that the key the kids typed was correct, then our website immediately sends them a follow up text with the next clue.Â  If they mistyped the key (which they didÂ at one point), then a text message gets sent back letting them know to try again.",
      "",
      "The kids absolutely loved the game, and Im probably going to get in trouble with their parents because they went home asking toÂ borrow Mom and Dads cell phone and play another game.",
      "My oldest son really likes the whole idea, andÂ so he immediately went about setting up his own treasure hunt.Â  He and I typed the clues into the azimuth website, and then sent the kids on another hunt around the house.",
      "",
      "Regardless ofÂ whether or not we rank well in the Rails Rumble contest, I hope we will be able to continue hosting this site somewhere and begin to more actively promote it, since I thinkÂ a lot of people would enjoy it.Â Â The codeÂ isnt bug free right now, but I think its actually pretty impressive how close we came to a bug free solution in only 48 hours.Â Â That says a lot about theÂ ease of Ruby onÂ Rails, as well as the quality of my teammates.",
      "Speaking of my teammates (who rock), heres a photo of the team having a toast at the end of the celebration.",
      "",
      "From left to right:Â  Arin, Michael, Youssef, Ashish, and Eric.Â  And just for the record, Michael wasnt actually part of the Rumble team, he was there working on another project for the weekend (Rumble teams are only allowed four members).Â  Dont we kind of look like the power rangers in our multicolored shirts?",
      "To wrap up, is Azimuth just for kids treasure hunts?Â  Heck no!Â  I can see all kinds of people having fun with Azimuth:Â college fraternities or sororities, high school kids, scout groups, co-workers, pretty much any group of people looking for a funÂ twist on scavenger hunts.Â  You can setup a hunt around your house like I did, or around your town, or even around the world!Â  Theres pretty much no limit to the fun you can have with this.Â  My wife is already planning a neighborhood treasure hunt for the next block party.",
      "Oh, and please vote for our app in the Rails Rumble contest!Â  Youssef really wants to win.Â  Seriously.Â  The guy woke us all up early Sunday morning saying \"I want to win!\"Â  He must want that kung fu lesson with Chuck Norris."
    ],
    "summary_t": ""
  },
  {
    "id": "18dba3391a0de2ef0651cedb59a14462",
    "url_s": "https://opensourceconnections.com/blog/2008/11/05/linkedin-needs-to-break-the-chain/",
    "title": "LinkedIn Needs To Break the Chain",
    "content": [
      "I received an e-mail from LinkedIn this morning saying that there were a whole slew of new applications waiting for me to use. Surprised, I went to their website to find that while the e-mail had promised me a slew of new ways to collaborate with my circle, the steak did not match the sizzle.",
      "The nine applications that appear do little to whet my creativity or appetite to leverage LinkedIn further. Two of them are even functional duplicates. Compare the LinkedIn list to the number of add-ons available for Firefox. Firefox has seventeen categories of applications.",
      "The main difference between the two applications is openness. LinkedIn has somewhere north of ten million users. Meanwhile, Firefox has somewhere north of 125 million users. Thats an order of magnitude greater number of users for an application that, normalized, had been in existence for 6 months more. Users can develop apps for Firefox, Facebook, MySpace, etc. They can also access and synthesize data from those applications, which they cannot do in LinkedIn. LinkedIn remains a \"walled garden\" and misses the opportunity to grow into something huge and ubiquitous as a result.",
      "LinkedIn is missing a much greater opportunity for revenue growth. While topping $100 million in revenues in 2007 (and, no, we cannot claim that number, so they are doing a few things right, I admit), the potential for growth is enormous and missed. A year ago, a comment on a blog post hoping that 2008 would be the year of LinkedIn captured the blind spots:",
      "\"LinkedIn really needs to do something with this social network it has gathered. Right now it just feels like a glorified Rolodex and resume holder. They have huge potential to become the de facto business portal and they€™re squandering it on features of marginal concern to their existing user base like internationalization. Build some depth! Give me some reason to come back to LinkedIn beyond occasionally accepting some new FOAF as a contact. There€™s a whole world of features that could be incorporated: blogging, groups, industry pages, integration with business news feeds and job listings from other websites.",
      "\"People maintain separate personal and professional networks in real life, I don€™t see why LinkedIn couldn€™t adapt more of the rich features of Facebook tuned to professionals.\"\n  \n\n  \n    It looks like round one of the feature growth missed the mark."
    ],
    "summary_t": ""
  },
  {
    "id": "a9a05a0134b6ae333dd7e1cded4e57b0",
    "url_s": "https://opensourceconnections.com/blog/2006/10/24/recap-of-seleniumneon-guild/",
    "title": "Recap of  [email protected] Guild",
    "content": [
      "\"Nothing is ever easy\" has been my motto for a while, and last night difficulties in getting a working projector going at the Neon Guild reinforced that motto!",
      "To make a long story short, I left my mini DVI -> VGA adapter in Columbus Ohio. I got a replacement, only to discover 90 minutes before my Selenium presentation that it was the wrong adapter!",
      "Eventually I ended up with my laptop VPNed into another site running the demo, the conference room desktop connected to the projector, and a free trial version of WebEx connecting the two!",
      "While things painted a bit slow on the projector, everything worked out.",
      "I wanted to thank Jason See and Jeff Cooper for their effort in making sure the network was setup for my presentation on Selenium.",
      "For those interested in the slides you can download them.",
      "If you have any other questions about Selenium, please drop me a note!"
    ],
    "summary_t": ""
  },
  {
    "id": "3820c2c0973acbda74ca018990732cae",
    "url_s": "https://opensourceconnections.com/blog/2008/11/10/ill-be-mcing-the-openspaces-portion-of-opensqlcamp/",
    "title": "I’ll be \"MC’ing\" the OpenSpaces portion of OpenSQLCamp",
    "content": [
      "This weekend is OpenSQL Camp, here in Charlottesville, Virginia at CitySpace!\n\nIve volunteered to help MC the meet and greet and OpenSpaces portions of Friday night. Weve got people coming in from all over the world, plus a good turnout of interesting Charlottesville folks, so Friday will be key to blend these folks into a cohesive conference!",
      "The OpenSpaces planning portion should be pretty easy as were really just nominating sessions for a handful of slots. Hopefully well still manage to keep the flexibility and real time applicability of a totally OpenSpaces style conference! Remember the Law of Two Feet."
    ],
    "summary_t": "This weekend is OpenSQL Camp, here in Charlottesville, Virginia at CitySpace! Ive volunteered to help MC the meet and greet and OpenSpaces portions of Friday..."
  },
  {
    "id": "16d0005770b03bfbf711032adee9bd5d",
    "url_s": "https://opensourceconnections.com/blog/2008/11/15/keynote-live-blog-of-dod-open-tech/",
    "title": "Keynote Live blog of DoD Open Tech",
    "content": [
      "Note: This post was delayed a couple weeks.",
      "Today Scott Stults and myself traveled up to DC to exhibit and attend the DoD Open Technologies Conference. We attended last year where Matt Jenks and myself spoke on Agile techniques for Green Field/Brown Field Enviroments. The conference has moved to the Ronald Reagan International Trade Center in downtown DC, and as a venue is a much nicer, much brighter space.",
      "",
      "The initial keynote was by Bill Vass, President and COO, Sun Microsystems Federal, and I expected something similar to Scott McNeelys keynote at FOSE. His presentation touched on the same topics of why Sun has open sourced its IP, but focused a bit more on the US Governments involvement. One of the key strengths that Bill talked about was the ability for the Government to influence open source projects, and he gave a couple examples. One of the ones that he gave was that the NSA wanted to add \"TE\", Type Enforcement, to Solaris. They asked Sun to do it, and Sun wasnt interested because Suns own internal security engineers didnt feel that TE was part of the vision for security. So what did the NSA do? They funded an open source project that extended Solaris to support TE, possible only because of Solaris being open source! And lo and behold, time passed and Sun ended up taking that enhancement and rolling it into their supported version of Solaris. There were a number of other examples ranging from changes to Solaris to changes to ODF made by various government agencies possible only because of open source. Without the source being open, even if the Government issued an RFP to a commercial vender to make these changes, without access to the source it wouldnt be possible.",
      "Bill also put out some statistics on Sun as the #1 contributor to open source, which, while true, is mostly because of them taking so much proprietary code and open sourcing it, versus starting projects from open source. Suns revenue from licenses has gone up drastically since open sourcing their stack, but I feel that is because people were not willing to pay, post the dot com era, for Solaris when Linux was available. But with Solaris being on the same pricing structure as Linux, then it is more compelling!",
      "He also made the same plug that Scott did that Java is on 6 billion devices. I hate that statistic, because when someone hears that, they mentally think of a rich environment where devlopers can write an app and have it run everywhere! But what that really means is that 5.5 billion of those devices are cheap cell phones running various versions of J2ME. Ive done some hacking with J2ME, and no two phones support it the same way, and there are too many levels of J2ME compliance. J2ME really is \"Write Once, Test Everywhere!\"",
      "He pointed out that open source has many very compelling solutions in the IT plumbing realm of storage, server virtualization, os, database. However, I think open source still has a way to go up the food chain from making a big data center more efficient. He acknowledged that issue, talking about how moving to open source solutions for J2EE and database SQL centric apps is much easier then more traditional, older style apps like an ERP. J2EE, for all the shortcomings of JSP, EJB, etc, does make it easier to move from a commercial app server to something like JBoss or Glassfish by providing very open standards.",
      "He also talked a bit about moving back office functions like calendering and email off of Exchange. \"Its hidden from the user, they still use Outlook\". While I like the message, I dont see it yet. Exchange and Outlook work very closely together, and peoples email and calendering are very hot button issues.",
      "Lastly he summed up Suns Lessons Learned after moving to open source in such a big way:",
      "More Secure\n  Cost less, but not free\n  Watch out for putting up your own dev shop, leave customizations to vendors\n  Indemnity provides the same protection as a license. This sounded like another plug for Suns version of the projects!!!\n  Stay with open standards, they make open source work. TCP/IP is critical to use open source networking tools for example. Standardizing on AppleTalk precludes the option of open source solutions!",
      "It was a good presentation, I liked the focus on government helped the message resonate with the audience. However I felt like the focus on desktops moving to open source was a bit of a reach. There is so many places on the server, network end of things that open source can help, that changing the desktop is still a long way away. Unless you are building out a pure kiosk type solution, then the advantages that Microsoft enjoys on the desktop are still too strong. However, moving more to the web will reduce that advantage over time!"
    ],
    "summary_t": ""
  },
  {
    "id": "42e92cec7face830be8e5762bf34fbdd",
    "url_s": "https://opensourceconnections.com/blog/2008/11/16/how-the-earth-shifted-weight-in-one-weekend/",
    "title": "How the Earth Shifted Weight in One Weekend",
    "content": [
      "If you have noticed the earth move or shift this past weekend, do not fear I have the answer. This was the result of the evil planning of Baron \"Xaprb\" Schwartz who had the genius idea of inviting every big name in the open source database community for the OpenSQL Camp. The list of attendees is so awesome I cant even dare name some of the people, you can find the full list at the event site. But this is what I can say, to have all these big egos and big brains in the same room in the small city of Charlottesville is dangerous, it can create a shift in the weight balance of the earth. It is just amazing to have all these big names in the open source community all gathered together in my town.\nOpenSQL Camp used the open spaces conference style which means that multiple sessions took place at the same time in different rooms and the topics were suggested by the people attending. Most of the sessions were too technical and went over my head, which is a testament of the caliber of the people at the conference. I probably was the only person there who has no experience managing large databases. My favorite moment of the weekend had to be Friday night. That was the session dedicated as a kick off session where everyone got to mingle and meet everyone else. It just happened that alcohol was served that night and it was hilarious for me to watch hackers and geeks get drunk and try to discuss database technologies. I think its amazing when someone tries to juggle both keeping his balance and make a technical argument and fail, but what he fails at is the balance part. A natural skill that humans learn at age 2 becomes difficult for people like the ones who attended the conference who have spent the majority of their life sitting in front of the computer hacking away.\nThe main thing I got from this weekend is the following: when you have people in charge of so many important projects like Postgres, MySQL and many others come from all over the world (like Finland and Australia) to Charlottesville for a conference, it says a lot about the open source community and makes me proud to be part of it."
    ],
    "summary_t": ""
  },
  {
    "id": "b513137be9e21a90281eff0c527454f4",
    "url_s": "https://opensourceconnections.com/blog/2008/11/24/in-madrid-over-thanksgiving/",
    "title": "In Madrid over Thanksgiving?",
    "content": [
      "Come see me speak at Expo:QA \"Jornadas Profesionales de Calidad y Testing de Software\" being held in Madrid, Spain. Ill be talking about Continous Integration, or Integracion Continua on Friday, November 28. Hopefully see you there."
    ],
    "summary_t": ""
  },
  {
    "id": "68d2fc614c386196ae6d369c7badf628",
    "url_s": "https://opensourceconnections.com/blog/2008/11/27/live-blog-of-keynote-at-expo-qa-2008/",
    "title": "Live blog of Keynote at Expo QA 2008",
    "content": [
      "You know Agile is becoming prevalent when the keynote of a conference about testing in the more traditional sense of testing major applications used by airlines, banks, and other financial institutions.\n\nFran OHara from Insight Test Services is speaking about Agile Test Strategies and Experiences. He asked at the beginning of his presentation how many companies have embraced Agile, about a quarter of the ~250 people at the conference raised their hands (including me!). He asked how many are evaluating/thinking about it, and about 50% of the folks raised their hands. And then how many are not looking at Agile, and an oustounding 25% raised their hands! Id have thought that Agile had been the \"next new thing\" for long enough that everybody would have at least looked at it, and either decided to adopt it, or discard it as not strict enough.",
      "He highlighted some of the types of tests that dont really fall into the purview of normal unit or iteration testing including:",
      "Performance testing\n  Combination/feature interaction testing\n  Business cycle testing such as End of Month processing",
      "Weve seen these types of tests not be written while heavy development of the system is happening. Developers and testers are all focused on just churning out the next feature/solving the requirements and verifying they work properly. However, once youve had a couple of interations under your belt, then you start thinking more about those complex scenarios. We worked with a client that had a device that processed large amounts of data. The first 5 or 6 iterations were all focused on making the device work. But iterations 7 through 11 were all about writing analyses tools and complex tests that exercised the device and flushed out issues in the algorithems or verified complex interactions worked as expected. In these later sprints the developers and testers worked hand in hand to understand where the problems are and what the expected behavior should be. Writing those tests earlier would have been very difficult if not impossible, because often what the expected behavior should be cant be defined until the device was up and working!",
      "Fran emphaized that by having the testers integrated directly into an Agile development team they can have a lot more input into the project. They can ensure that the customers needs are met, not just that the software is technically correct. And when you have a sepearate QA team, then that imples that Quality isnt something the developers need to be concerned with. Which is a recipe for disaster. By pushing the need for Quality down to every level, you end up with a better system. Think Honda and Toyota versus General Motors.",
      "He also pointed out that one of the key reasons to have Testers invovled directly is becasue often Agile is introduced by the developers, which can lead to making mistakes such as using unit tests for accepetance tests. Or thinking that automated tests are sufficient!"
    ],
    "summary_t": ""
  },
  {
    "id": "32f44e7f7a002005547b4cfdb0a4edad",
    "url_s": "https://opensourceconnections.com/blog/2008/11/27/storytelling-session-at-expo-qa-2008/",
    "title": "Storytelling Session at Expo QA 2008",
    "content": [
      "Karen N. Johnson gave the keynote after lunch on the topic of Story Telling. I love these kind of topics that at first dont seem very related to our field, but then end up being relevant. I like to say that software development is like writing poetry. We as software developers create something out of nothing. So the idea of software being storytelling resonates for me. However, since I am at a Testing conference, I am curious to see how she weaves the idea of story telling into testing software. Fortunately, being right after lunch, she was very interactive and kept us all alert!\n\nThe section about picking the words used in doing something as mundane in as giving a status report. Think about what words you might want to use. There are some words that will make a group defensive. There are other words that give a better flavor to what you are trying to communicate. You use different words when you are talking to testers in the trenches compared to developers compared to senior managers or stakeholders. Which makes a lot of sense, everyone filters their message depending on their audience.",
      "She talked a lot about how to paint a verbal picture. When we are proposing something, its always better if you can show a mockup. A line drawing of a website, a fake Firefox plugin, or a reference site. When you dont have those props, then you need to put more effort into painting a verbal picture. Talk about the error message being \"splashy red\" or the pages JavaScript \"grinding to render\".",
      "To become a better storyteller she mentioned some books, including Presenting to Win: The Art of Telling Your Story. She talks about the need to build your own voice, versus copying someone elses. I guess this is why when I present, wearing jeans and a black shirt, I dont come through the way Steve Jobs does! We all need to practice our presentations, experiment with different approaches, and eventually come out with our Story Telling voice. I know that I started out being very informal in my presentations, then I moved to being very PowerPoint driven. Later Ive experiemented with no slides, just demonstrating code on the screen. Now I am back to using some slides, but trying to do the live code as part of the presentation. With fewer bullet points per slide, and larger more vivid graphics. Still not there yet.. And my presentation tomorrow is completely scripted out since it is in Spanish, and I can do that free form!",
      "Ill make a plug for my favorite blog dealing with being a speaker: PresentationZen. It really has helped me think more critically about my presentations. Im looking forward to trying to weave in the ideas behind Story Telling that Karen shared."
    ],
    "summary_t": ""
  },
  {
    "id": "819ca7c8ec40cda167ab32da54ee85ff",
    "url_s": "https://opensourceconnections.com/blog/2008/11/28/iso-standards-talk-at-expo-qa/",
    "title": "ISO Standards Talk at Expo QA",
    "content": [
      "Stuart Reid of Testing Solutions Group gave a presentation on the new ISO standard for testing.",
      "While ISO standards are not normally my cup of tea, I was interested in learning more about how these standards are generated, especially since software testing is near and dear to my heart.",
      "Stuart started out with some interesting points about standards. For example, Standards are not \"Best Practices\". They are on a continuum between current practices and best practice. They are not best practice because by definition, only one entity at a time could claim \"Best Practice\". So the goal of standards is to set a goal that is achieveable, but beyond where we are. Hence standards being \"good standards\".",
      "I liked how he illustrated what standards are by pointing out that the pram built to the same level as a tank:\n\n\nMatches the same standard as your basic cheapo stroller:\n\n\nBoth meet a \"good\" level of safety.",
      "Stuart is leading the working group that is developing the ISO standard for Software Testing: ISO 29119. Astonishingly enough the concept of Software Testing doesnt have an ISO standard yet!",
      "There is an interesting slide that there are lots of new standards related to testing. There has been massive growth in them, and therefore in an environment of many standards, you have no standards! Hence the impetus to creating an international standard for software testing. The new ISO standard will build on existing standards, but will also replace existing standards, like the IEEE 829 standard for documentation.",
      "He made the call for volunteers to become more involved in this standard. Today is the time to become involved because this will have significant impact in the future for our industry. Here is some more background information written by Stuart about the standard and its history."
    ],
    "summary_t": ""
  },
  {
    "id": "ac09402cd47fb940f9410312c0a51ff7",
    "url_s": "https://opensourceconnections.com/blog/2008/12/01/expoqa-integracion-continua-wrap-up/",
    "title": "ExpoQA Integracion Continua Wrap Up",
    "content": [
      "Last Friday I had the huge honor of presenting at ExpoQA 2008 in Madrid on the topic of Continuous Integration, or maybe I should say Integracion Continua since I gave it in Spanish!",
      "Ive posted the slides along with the transcription of the talk in Spanish on SlideShare:",
      "Integracion Continua \n    View SlideShare document or Upload your own. (tags: expoqa2008 espanol)",
      "",
      "A huge round of thanks needs to go out to Raynald Korchia and the rest of the Sogeti crew for putting together this conference. It was wonderful to see such a large and diverse testing community in Spain, with what I guessed as around 250 people attending the conference, and a great set of vendors as well.",
      "I really enjoyed spending time with the attendees and speakers, I learned a lot from your individual experiences in the fields of testing. Im looking forward to getting more plugged into the world of software testing in Europe, and especially some of the upcoming conferences in Spain that people mentioned.",
      "And lastly, a huge amount of thanks needs to go to my wife Kate, with whom I spent hours and hours over bottles of Riojan wine transcribing my badly formed thoughts in Spanish into proper Castellano, and generally making sure my presentation was a success!"
    ],
    "summary_t": ""
  },
  {
    "id": "113b0a4b18905a1f962d8350eab389aa",
    "url_s": "https://opensourceconnections.com/blog/2008/12/02/continuous-integration-at-cvreg/",
    "title": "Continuous Integration at CVREG",
    "content": [
      "Missed my presentation on CI in Spain? Fortunately Ill be in our backyard, Richmond, next month to talk about CI with a Ruby flavor to CVREG: Central Virginia Ruby Enthusiasts Group.",
      "Come to the CVREG presentation at Strategy Cafe on January 13th. More information, and a map, is available on CVREGs upcoming events page."
    ],
    "summary_t": ""
  },
  {
    "id": "d5258e5fc3e3748739eb5e29263d3663",
    "url_s": "https://opensourceconnections.com/blog/2008/12/09/indexing-information-about-people-needs-a-time-axis/",
    "title": "Indexing Information about people needs a \"time axis\"",
    "content": [
      "I bet most people have done the vanity search on Google for themselves, I know I have. The problem with most indexing systems is that they go out and collect lots of information, but most of that information doesnt have have any sense of time. They are just random points of data. But, as the years pass, and we put more of ourselves on the Internet, we mostly want to build up a picture not of ALL the data about a person, but a picture of person based on data that is applicable RIGHT NOW. For example, in my vanity search, today my blog on JRoller comes up first. But I havent blogged there since March 2007, and the things I am interested in right now are better exhibited by links 2, 8, and 9. Being, respectively, my company OpenSource Connections, Open Source in the Federal Government, and Ruby on Rails. We need to be able to cluster and show data about people, but also be able to plot it on a timeline. So that older data doesnt overwhelm the newer information. On Amazon, I am still getting recommendations for Java books, even though I am a Rubyist now!",
      "Of course, adding \"time\" to data is hard. For some data you could base the time of a piece of data about something based on the context it is in: \"I graduated in 1994″. Alternatively you could try and infer date based on when content was created, like in an RSS article. Or, hopefully have some sort of meta tag specifing when stuff was created.",
      "For HighTechCville, my research project, I am struggling with the fact that a lot of companies address data in HTC is out of date because the data source, a survey taken by CBIC, is a couple years out of date, and based on older tax records. While I am preserving metadata about when information is added and changed, that doesnt really give me \"true\" sense of what \"date\" goes with each data source.",
      "A recent article on OReilly by Nick Bilton talks about the value of Twitter having a constant stream of information that CAN be dated, because its all real time \"what am I interested in Right Now\" and can provide that timeline of changing user data. But for a project like HTC that is trying to backwards infer that information, its a lot harder, and a lot fuzzier!",
      "Any great suggestions, please leave them in the comments!"
    ],
    "summary_t": ""
  },
  {
    "id": "a22fbd6fbd63c5ca5b05156e897b8b98",
    "url_s": "https://opensourceconnections.com/blog/2006/10/25/how-to-dedupe-your-itunes-collection/",
    "title": "How to dedupe your iTunes collection",
    "content": [
      "Update 5/25/20 Rinse appears to be gone, or just out of date and bad.  Not sure, with Apple Music etc if we dedupe these days!",
      "Update 4/24/2011. TidySongs is now Rinse! Same great product, but new vendor with a deeper music database that seems to match better. I’ve updated the links.",
      "Update 5/16/2010. When I am old and grey, I am going to tell my children about the most popular blog post I ever wrote, this one about iTunes deduping! I finally have to pass the baton of deduping on to",
      "TidySongs, a product from a company that coincidentally is in my home town of Charlottesville, Virginia!",
      "While iTunes provides a \"View Duplicates\" function, it intentionally doesnt provide anything to help you delete all the duplicates. Apples theory being that you should be very careful with all your music! And an automated system cant detect betweend duplicate songs, and just two versions of the same song.",
      "There are a couple AppleScripts out there that can help find all your duplicates as well. However, then you still have to manually go through the playlist and delete them.",
      "I unfortunantly have duplicated 780 songs, adding an extra 1.8GB of music files! As you can imagine, manually fixing this was not going to happen. Fortunantly I was able to programmatically solve this. I used the AppleScript Corral All Dupes to find all the duplicates, and made a minor tweak to it so that it would put into my \"Dupes\" playlist only the second version of the song. Just fire up Script Editor, and then open up the Corral All Dupes applescript application.",
      "Find the line that looks like this:",
      "â€\" copy the list of dupes to the \"Dupes\" playlist\nrepeat with y from 1 to length of dupesRef",
      "And change the line to look like this",
      "repeat with y from 1 to (length of dupesRef) - 1",
      "This will take all but the last last duplicate and put it on your Dupes playlist.",
      "Then, once it is done, just highlight them all, and hold down Option and Delete. The Option key makes iTunes delete the actual music files, versus just remove them from the playlist. This was a tidbit that took quite a bit of searching to find!",
      "PS, Corral All Dupes is painfully slow, but making this hack was much easier then in the PERL enabled version."
    ],
    "summary_t": "ong>Update 4/24/2011. TidySongs is now Rinse! Same great product, but new vendor with a deeper music database that seems to match better. I’ve updated the..."
  },
  {
    "id": "97a0dbf061896896e43c99024792fdbd",
    "url_s": "https://opensourceconnections.com/blog/2008/12/12/in-2009-venture-capital-opportunities-still-exist/",
    "title": "In 2009, Venture Capital Opportunities Still Exist",
    "content": [
      "On Wednesday, I had the opportunity to attend the Potomac Tech Wires 2009 Venture Capital Outlook.  First, I applaud the Potomac Tech Wire for putting together a fine panel.  All of the panelists were insightful and informed, which made the discussion interesting and flowing.",
      "While there were too many nuggets to write down, I tried to capture the most noteworthy ideas.  The bottom line of the discussion was that for entrepreneurs with good ideas, there will be money available in 2009.  In the .com bubble, many venture capital firms pulled back their investments, and they missed out on the next wave of great companies.  Its easy, relatively, to make money in a boom economy, but companies that make money in down economic times are the ones that tend to be the strongest long-term.  So, the large VC firms realize that they will want to take advantage of the opportunity to find the strong companies which emerge from a recession.  Its in hard times that companies look more to innovate, and the panelists expected innovation to continue to drive costs down geometrically, expanding Moores Law into areas like clean energy.  To quote Harry Weller, \"Capital is there for talented people.\"",
      "Naturally, my interest was more in software and web-based opportunities and where the panel saw the trends.  One of the most prevalent themes was the change in the advertising model for online communities.  In the past, the idea was that websites could simply be massive advertising platforms, but given the poor clickthrough performance of social networking sites like Facebook, the marketing model is rapidly changing.  Now, media buyers want to see advertising leading to incremental sales rather than just brand building.  There is still a recognized difference between brand building and sales-driven advertising, but the advertising agencies want to see the potential to drive sales rather than just create awareness.",
      "This doesnt mean that the web as an advertising platform is a dead idea.  The big advertising agencies realize that there is a disproportionality of advertising money spent on interactive media compared to the percentage of time that people spend on it.  The trend is shifting towards two areas of focus: video and direct response advertising.  According to Sean Greene, direct response advertising is important because it drives sales, but the entrepreneur that wants to get the attention of the venture capital community needs to have significant numbers on a unified advertising platform to get the interest of advertisers.  When asked to quantify that number, he said that for general advertising purposes, a website needed to drive 1 million unique visitors per month to garner interest, although with niche markets, the number would obviously be lower.",
      "Since measuring brand equity is difficult, adverstising agencies are looking for better metrics to provide them with a true way to measure performance-based online advertising.  In an ideal world, advertisers could link a direct line to a purchase from an advertisement; however, most purchases have some level of attenuation from the viewing of the advertisement.  Something along the lines of Andy Gregorowicz`s Wikipedia Concept Extractor could provide a baseline for extrapolating the ads performance against the context in which it was placed.  Whatever the idea is, the goal is to make the venture capitalist say \"Oh my God, I have never seen this before!  Its going to be fundamentally disruptive!\" (…and theres money to be made here)",
      "As Don Rainey put it, the next few years will see the move from the information age to the wisdom age.  People arent going to look to the Internet for information, as it is now saturated with information.  Instead, they are going to look to the Internet for wisdom, for the synthesis of the information into the pertinent chestnuts of wisdom that help them do their jobs, live their lives, and pursue their dreams more effectively.  The idea that the Internet would let the masses publish and that everyone could contribute has led to the realization that not everyone is a source of authority.  According to Forrester, 18% of people surveyed trust personal blogs (compare that to a 16% level of trust for company blogs…makes me wonder if theres any value to this whole \"blogging\" idea!) compared to 77% of people who trust e-mail from people they know.  What is missing is the concept of the reputation of the information provider.  E-mail from people you know has a higher threshhold of believability because of the implicit trust already established, whereas a random blog plucked from the blogosphere lacks that same level of implicit trust and credibility.  Thus is born the need for a reputation engine.",
      "Because of the boom-bust cycle of the 2000s, venture capitalists are moving towards a 5-15 year time horizon.  The dearth of IPO filings in 2008 means that quick exits through the IPO market are not the first choice option that they once were.  While other companies are still acquisitive, that market, too, has slowed.  As a result, capital efficiency is paramount.  At one time, entrepreneurs could get all of the money they asked for because the exit markets were so liquid.  However, now, entrepreneurs must learn to get by with less, to be more efficient, and to be more innovative with the capital they are given.  This means that the amount of money to be expected in early stage ventures will be lower, and venture capitalists will look for proven performance (breakevens, or, even……profits!) for later stage enterprises.",
      "Even with the economic slowdown, venture capital and private equity is not going away.  As Mark Heesen said, 10% of American employees are employed by venture capital-based employers, and those firms generate 18% of the U.S. GDP.  In a time when leverage seems to be such a curse word, thats a pretty good use of financial leverage."
    ],
    "summary_t": ""
  },
  {
    "id": "ae6f1cf4d39ab057fe8ce4732e20dd89",
    "url_s": "https://opensourceconnections.com/blog/2009/01/08/2009-it-is-a-new-year-be-productive-but-dump-your-resolutions/",
    "title": "2009: It is a new year, be productive but dump your resolutions.",
    "content": [
      "Every year people tend to spend the first month or two making promises they are going to break a few moments, maybe weeks later.  We tend to lack resolve and have seasonal habits.  We act nicer in December to our fellow man (woman), unless it comes to taking the last tickle-me elmo off the shelf, then its every person for themselves.  We pull out the big grill  that you probably wonâ€™t use for the rest of the summer for labor day weekend that you got for xmas to outshine your neighbors 2k stainless steal beast he got the year before, all the while making tim the tool man tailor grunts while cooking, .  And of course, its January, its time for a new diet fad (this year it seems to be acri berry diets that boasts you donâ€™t need to really diet or exercise, all thanks to Opera Winfrey having a show about the most nutritional foods with acri berry being #1).",
      "But I digress.  However I do have some suggestions for the new year because more than likely people are more open to ideas this time of year and hopefully maybe one or two will stick, thereby making your life that much more productive and a little easier, or fun.",
      "Optimize the way you work on a pc, Pimp your desktop.",
      "You donâ€™t need to be a power user like on linux or have to buy a mac to have an intuitive and shiny UI (User Interface, in this case, desktop) to be productive. Its taken me some time, but Iâ€™ve finally found the tools that really help clean up the desktop.",
      "",
      "First off clean up your desktop, its rare that you really need those icons, especially if you have windows vista with the new start menu. If you like the macâ€™s quicksilver application, but like me, tend to work on a windows os, you can try launchy, skylight, or slickrun. Right now I prefer skylight since its written using .NET WPF and its extendable.  With Alt + Spacebar, the window appears and you can quickly find whatever program you want.",
      "Take time to find a desktop wall paper you like, deviant art is good place to start.  People in general tend to enjoy working with things that are appealing to eye. Everyone has different tastes and different styles of work flow. Experiment not only with the wall paper, but also where the taskbar is placed and what toolbars are shown, etc.  Also play with the visual styles of windows as well. Right now I tend to like the vista glass with a lil black in it.  If you have xp, you can get vista glass.",
      "Add lifehacker.com to your RSS feed, they always have some pretty nifty desktop ideas, tools, and even lists of pimped/tricked out desktops, and just good productivity tips in general.",
      "If you need icons or if you like the Macâ€™s Dock, give rocket dock a try, its pretty customizable and you can replace the icons for whatever your trying to open.",
      "Gadgets.  If you have windows vista, you can try the gallery for finding some halfway useful gadgets. If youâ€™re using xp or want better gadgets on vista you can try yahoo widgets or google gadgets.  Though gadgets are a cool desktop concept, they have really yet to take off with usefulness or high end eye candy, but you might find one or two to meet your needs.",
      "Practical websites",
      "Live Mocha â€\" This is actually social networking language learning website that really has nothing to do with coffee other than the colors of the website. Its a decent learning tool and not to mention a good way of finding pen pals or other people interested in the same languages that you can practice with.",
      "Live Strong â€\" Calorie Counter that has tons of foods, and exercises taken into account? check. Must have information on the latest diet fads?  check. Must have it as an IPhone or ITouch application? check.  Must be a social network so you donâ€™t feel aloneâ€¦ loser, i mean check.",
      "Mint â€\" looking for a good online free way to manage/budget/invest your money with a ton of awesome tools and that you can take with you on your IPhone or Itouch, then mint.com is the place for you",
      "Life Hacker â€\" need ways to be productive, find new gadgets that are worth the money, or just like to learn new ways of doing thingsâ€¦",
      "Mozy â€\" online data/documents/files backup that is unlimited for just 5$ a month isnâ€™t bad, especially if youâ€™re the tech geek for your family. If they have under 2 gigs of stuff, its free.",
      "Remember the milk â€\" Into the GTD (getting things done) way of doing things? Then check out remember the milk for your task lists. They also provide google gears, iphone application, gadgets and many other ways of entering and keeping track of your tasks.",
      "Doing dot .net?",
      "Pimp your color themesâ€¦    Visual Studio Color Themes and Is your IDE hot or not?",
      "T4 (Text Template Transformation Toolkit) â€\" Visual Studio has code generation that has been there for while under the covers, not really used, but very useful.  Why are people spending money on code gen tools when you already spent a fortune on visual studio and comes with this gem?",
      "Gallio â€\" The one stop shop for running your unit tests and it comes packages with the killer mbunit 3.0.",
      "Moq â€\" need an mock library that doesnâ€™t suck, isnâ€™t confusing and uses lambda? then take a look moq (mock you).",
      "Test Driven .net â€\" Iâ€™ve been using this on my opensource project.  This is one of the very few add ons that I install for visual studio. It lets you run tests, code coverage reports and other nifty things right inside of Visual Studio. A time saver.",
      "Reflector â€\" Red gate now owns and updates reflector, but its still free, want to look at the source code for a dll, check this out.",
      "Linqpad â€\" need to write some linq queries and want to make sure they execute correctly, check out linqpad.",
      "",
      "General Coding.",
      "E-Texteditor â€\" The power of text mate on windows.  This is pretty my replacement of note pad.  If you have mac font envy, you can grab monaco for windows. Or you can just use Consolas.",
      "Stylizer â€\" css styling for ajax applications, even with tools like firebug, can be a pain, cause that doesnâ€™t exactly help you with IE and it doesnâ€™t always help with doing heavy ajax applications with pops up and such. Enter stylizer (also coming to macs soon), which lets you style with a real time preview of how it would show in the actual browser (IE and firefox).",
      "FTP",
      "winscp â€\" you can ftp down, edit in e-texteditor and ftp the file right back when you save changes to the file.  nifty.",
      "filezilla â€\" slick fast opensource ftp client and server.",
      "",
      "Instant Messenger?",
      "pidgin â€\" all your fav instant messengers and even ones you donâ€™t know wrapped into one program.",
      "skype â€\" voip, phone calls, sms texts, chats.  Definitely the professional grade instant messenger/ communications program. calls are even encrypted.",
      "meebo â€\" you online web page instant messaging client, in case your away and canâ€™t download a client.",
      "Challenge.",
      "Anything that will help you to be more productive always costs something. It could be money, but most of the time, its costs time. But invest some time now that might reap huge benefits later.  So I challenge you to take some time, get organized, find new efficient ways of doing things that fit your way of life and if you know of anything, programs or otherwise, feel free to add those in the comments below."
    ],
    "summary_t": ""
  },
  {
    "id": "0743ef36ffee4fb9b08469dd1a25a30e",
    "url_s": "https://opensourceconnections.com/blog/2009/01/14/recap-of-ci-presentation-at-cvreg/",
    "title": "Recap of CI presentation at CVREG",
    "content": [
      "Last night I had the privilege of talking about Continous Integration (CI) to the fine folks of CVREG, and had a wonderful time. CVREG had good turnout, and it was a great discussion of CI, and how CI can be part of a virtuous circle of becoming better software developers.",
      "I mentioned that I would publish my slides, and if anyone has questions, please let me know at [email protected]",
      "The slides are online at http://www.slideshare.net/o19s/ci-presentacion-presentation/.",
      "Also, a couple people pinged me about beCamp, and Ill be getting started on it! Any volunteers to setup the 2009 wiki page? ;)",
      "."
    ],
    "summary_t": ""
  },
  {
    "id": "54c6a3de241460153de110f54eb04d54",
    "url_s": "https://opensourceconnections.com/blog/2009/01/20/change-comes-to-the-white-house-blog/",
    "title": "Change Comes to the White House…Blog",
    "content": [
      "Less than 30 minutes after the completion of the swearing in and taking of the oath of office of the President, one of his mantras, \"Change is Coming\" came true on the White House blog.  Macon Phillips, the Director of New Media, promised winds of change to the White House blog. President Obama leaned on social media and the power of the Internet to carry a Presidential victory.  It appears that his official site is starting off on the right foot in perpetuating that meme.  If only he were Twittering while sitting up on the podium…"
    ],
    "summary_t": ""
  },
  {
    "id": "7c8199eb7d77bcd15b2abb23aa25ad95",
    "url_s": "https://opensourceconnections.com/blog/2009/01/29/ltg-sorensons-vision-for-army-collaboration-is-on-the-right-track/",
    "title": "LTG Sorensons Vision For Army Collaboration Is On the Right Track",
    "content": [
      "Jim Stogdill of OReilly relates a presentation by LTG Sorenson about the new Battle Command system.  I dare not try to recast his insight here.  However, he did bring up one topic which I want to elaborate slightly on.  We have noticed a trend to tack on new trends and buzzwords to systems rather than either building modular, loosely coupled systems or building new platforms to take advantage of new capabilities.  The result is that while systems last longer, they become outdated more quickly, and because each new capability is simply appended rather than integrated (or the code refactored), maintenance costs grow exponentially.  Look at FBO and see how many large scale efforts are underway to deal with outdated systems.  Stogdills post and the subsequent discussion in the comments illuminate what happens when concepts are misunderstood.",
      "You can read Stogdills excellent analysis here."
    ],
    "summary_t": ""
  },
  {
    "id": "c0e099b63435c200f7ca06cf184cf79a",
    "url_s": "https://opensourceconnections.com/blog/2009/02/06/jason-hull-to-moderate-social-media-panel-at-quadruplicity-conference/",
    "title": "Jason Hull To Moderate Social Media Panel At Quadruplicity Conference",
    "content": [
      "On February 12, Jason Hull will be moderating the social media panel at the Charlottesville Regional Chamber of Commerce Business Womens Roundtables Quadruplicity conference.",
      "Have you wondered how to use social networks to improve your personal brand?  How to use them to grow your small business?  How to use them to make yourself indispensable at work?  Take part in a discussion with Polly Black, Bryon Sabol, and Marijean Jaggers as they discuss how you can leverage social media to improve your professional life.",
      "Register here and friend us on Facebook.  See you next Thursday!"
    ],
    "summary_t": ""
  },
  {
    "id": "07726a5c60e113e3ecb2800b8c9c687a",
    "url_s": "https://opensourceconnections.com/blog/2009/02/06/soa-and-the-commonwealth/",
    "title": "SOA and the Commonwealth",
    "content": [
      "Last week I attended a breakfast hosted by the Greater Richmond Technology Council, where I had the opportunity to hear Peggy Feldman discuss the Commonwealth of Virginias plans for a more Service Oriented Architecture (SOA). Ms Feldman is the Commonwealths Chief Applications Officer.",
      "Ms Feldman pointed out a number of challenges facing the Commonwealths IT departments in coming years. 40% of the states workforce could retire in 5 years, budget projections look flat, financial and HR systems are 30 years old, and they need efficiencies. Current interactions with citizens rely too much on paper, and she pointed out that many state agencies websites provide nothing more than a download form to print out and mail in. There are nearly 2000 applications across all the state agencies, and at least 20 different types of architectures.",
      "The CAO runs VEAP, the Virginia Enterprise Applications Program, and Ms Feldman pointed out that this is the only agency in a position to push multiagency and collaborative solutions. The CAO does not want to force all state agencies to adopt a particular platform, but she does want to find ways for the Commonwealth to find technology efficiencies without a lot of upfront software costs.",
      "In the long term, she wants to see state agenices providing more self-service applications to citizens, to have agencies share more data using XML, and to reduce those 20 some architectures to a smaller pool of 3 or 4.",
      "Because the state agencies are very decentralized, data standards and management is a huge challenge. What the CAO wants is to put an enterprise level layer on top of existing systems, and she sees this as based on SOA principles.",
      "So why go the SOA route, and how does this help the Commonwealth? I think the CAOs strategy is a good one that lends itself to a gradual transformation of the states IT infrastructure and a relatively efficient use of resources (both internal agency IT staff and external contractors).",
      "Thomas Erls book \"SOA: Principles of Service Design\" lists the following benefits of an SOA infrastructure:\n*",
      "Increased Intrinsic Interoperability\n  Increased Federation\n  Increased Vendor Diversification Options\n  Increased Business and Technology Domain Alignment\n  Increased ROI\n  Increased Organizational Agility\n  Reduced IT Burden",
      "I think the Commonwealth will benefit from all of these advantages of SOA, but perhaps on top of Ms Feldmans mind is the Increased Intrinsic Interoperability.",
      "This week I had the pleasure of attending my first board meeting of the Virginia Technology Association, which I was recently appointed to by virtue of my work on the board of the Charlottesville Business Innovation Council. After the VTA board meeting at the Library of Virginia, the VTA and the GRTC hosted their annual legislative reception at the same location. It was a great reception with a number of interesting contacts, but it also gave me an opportunity to speak with Ms Feldman a little more one on one.",
      "Ms Feldman told me that there are many types of data that are common between state agencies, and that this information could be shared better. Specifically she pointed out the possible use of HR-XML as an XML standard for personal information about citizens that all agencies use. Thats an example of the Intrinsic Interoperability that SOA offers.",
      "She told me that most of the next year will be spent developing standards, before individual agencies in 2010 may start to publish RFPs for work, but that the various tax agencies, the DMV, and social services are likely candidates for some of the initial work.",
      "Based on my own recent experiences for both my own business and CBIC, I can say that its a real pain to get the Virginia Employment Commission to return my calls about some employment taxes, and I certainly would appreciate more self service apps that could answer the relatively simple queries I have. The VECs contact page for employers gives little more than phone numbers, and I havent had a real person answer the phone for several weeks – just a voice mail where I have to leave a message that no one will return. Im not feeling the love, and self service apps could go a long way to fixing that.",
      "Another interesting comment Ms Feldman made to me was about cloud computing. Ideally the commonwealth would like to centralize the hosting of more agency IT applications, and a cloud computing environment is ideal for that. However, the agencies are very hesitant to use commercial solutions for this, and instead the Commonwealth is more likely to create its own cloud computing implementation that all the state agencies could utilize. Ms Feldman assured me that this would still allow agencies to support multiple operating systems, and that the state does not want to force everyone to use the same technology or platforms, they just want to enforce the contracts between services.",
      "Thats good news, and the right way to architect an SOA system. If a centralized Commonwealth hosting solution dictated only the use of one or two technologies, it would work against some of the other main benefits of SOA: Increased Vendor Diversification and Increased ROI."
    ],
    "summary_t": ""
  },
  {
    "id": "23c4712c60f1a51a2de316116ffccd57",
    "url_s": "https://opensourceconnections.com/blog/2009/02/09/speaking-on-ci-at-betech-march-19th/",
    "title": "Speaking on CI at beTech March 19th",
    "content": [
      "Ill be speaking on CI for the beTech group March 19th:",
      "4:00 PM – 5:00 PM March 19, 2009\nLocation: Clemons Library room 407",
      "Please come hear Eric Pugh of Open Source Connections\n(/) discuss Continuous Integration (CI).\nThe goal of CI is to have \"a fully automated and reproducible build [of your\nproject], including testing, that runs many times a day.\" (I stole that from\nErics online slides.)",
      "Eric is an active participant in the open source world, and is a contributor\nto CruiseControl, a widely used open source CI solution. Eric has presented\nfor beTech several times in the past, and his talks are always excellent and\ninformative. He has also been quite active in organizing beCamp in the past,\nso we might also use this meeting for some brief preliminary planning about\nthis years beCamp event."
    ],
    "summary_t": "Ill be speaking on CI for the beTech group March 19th: 4:00 PM – 5:00 PM March 19, 2009 Location: Clemons Library room 407 Please come hear Eric Pugh of Open..."
  },
  {
    "id": "d11c3e1965a1f554957647bb28bc035b",
    "url_s": "https://opensourceconnections.com/blog/2009/02/13/celebrating-1234567890-day/",
    "title": "Celebrating 1234567890 Day!",
    "content": [
      "A couple weeks ago Youssef posted to our team mailing list that 1234567890 day was coming. I emailed my wife that she should Google for \"unix epoch + 1234567890″. Well, apparently she likes Geek humor that relates to Valentines day, because on Monday a pair of shirts from Cafepress show up:\n\n\nHere is a closeup of these amazingly cool shirts:",
      "Technicially 1234567890 seconds January 1st, 1970 happens Feb 13, 11:30 PM and 30 seconds. Should I be nervous that this critical time rollover is happening on Friday the 13th?",
      "Here is a real time counter from http://coolepochcountdown.com/:",
      "Kate and I will be out tonight celebrating 1234567890 day instead of Valentines day this year!"
    ],
    "summary_t": ""
  },
  {
    "id": "b88bed3b1fbee6da667cb3ac9c0190c4",
    "url_s": "https://opensourceconnections.com/blog/2009/02/13/client-side-javascript-implementation-of-the-haversine-formula/",
    "title": "Client Side (JavaScript) Implementation of the Haversine Formula",
    "content": [
      "Given two points on the surface of a sphere the Haversine Formula can be used to find the shortest distance between the points. Well what do ya know, the Earth is a sphere. Well not really its a spheroid, the poles are a bit flattened. Even that does not account for the various irregularities in the Earths shape. A geodesist can tell you more. To get really precise a calculation would need to use the datum from the World Geodetic System of 1984 (WSG84). But thats another post.",
      "Heres a link to the page: Haversine Calculation. Obviously, you can then use your browsers \"view source\" functionality to check out the source code. Turns out that posting the source code here really freaks out wordpress."
    ],
    "summary_t": ""
  },
  {
    "id": "306f0f5e49d9d3192fb38b292ef23f93",
    "url_s": "https://opensourceconnections.com/blog/2006/11/01/bug-4814103-submitted-to-apple/",
    "title": "Bug 4814103 submitted to Apple!",
    "content": [
      "I discovered a bug in Apples implementation of Java. When selecting multiple leaves in a JTree, and then ctrl + drag, the mouseDragged event should be fired. However, on a Mac, the mouseDragged event never fires!",
      "We tested on Linux and Windows, and everything works. Argh. At any rate, hopefully Problem ID 4814103 will soon be fixed:-)",
      "Not sure if others can see it, but the site is here https://bugreport.apple.com.",
      "The joys of doing Swing development!"
    ],
    "summary_t": ""
  },
  {
    "id": "1b5364e95ab29fe145499839d862a2a8",
    "url_s": "https://opensourceconnections.com/blog/2009/02/13/drupal-webforms-controlling-the-submit-button/",
    "title": "Drupal Webforms – Controlling the Submit Button",
    "content": [
      "The definition of frustration should be something about expecting a task to be short and simple but instead it takes a really long time. So it was with centering the submit button on a Drupal webform. Now, I dont want to give you the wrong impression, because the Drupal webforms module is fantastic. Right out of the box it did 95% of what I needed. Another 4% was fairly easy to accomplish by adding code to the Webform advanced settings -> Additional Processing section. The last 1% was a bit trickier to figure out, which was controlling the position and layout of the submit button.",
      "An implicit assumption is that most websites want to `brand or customize their buttons. Is this a fair assumption? Heres the default submit button of a Drupal webform.",
      "",
      "Please, someone correct me if Im mistaken, but I could not find a way to control the appearance and layout of the submit button in the webforms admin interface. Which was kind of surprising, because the webforms module is feature rich. I was expecting a way to designate an image for the button and perhaps basic position settings (left, center, right). Again, to be fair, Drupal module development is open source developed, so I could contribute instead of casting stones, no?",
      "Anyhow, a quick peek at the page source of a form created with Drupal webforms shows this HTML for the submit button:",
      "",
      "Which is inside this form tag:",
      "<form action=\"/all_your_base\" accept-charset=\"UTF-8″ method=\"post\" id=\"webform-client-form-3257″ class=\"webform-client-form\" enctype=\"multipart/form-data\">",
      "The good news here is that both these elements have a class attribute. Again kudos to webforms team for doing this. So CSS to the rescue. Heres the CSS Im using to theme the submit button",
      ".webform-client-form .form-submit {",
      "background: url(`/site/images/submit.gif);",
      "background-color:transparent;",
      "border:medium none;",
      "cursor:pointer;",
      "padding:0;",
      "width: 100%;",
      "height:32px;",
      "background-repeat:no-repeat;",
      "background-position:center;",
      "}",
      "As you can see, Im using a background image, centering that image within its block and not repeating it. So here it is:",
      "",
      "Two problems remain. Notice the text \"submit\" on top of the button image. As youve guessed, \"submit\" is the default text of the button. The workaround to get rid of the word \"submit\" is to put a single space ( ) in the Webform advanced settings -> Submit button text. Kudos to my colleague Youssef Chaker for finding this work around. An additional setting such as upload image button or url to button image would have been nice. Here is the final look of the submit button.",
      "",
      "The remaining problem is that the CSS setting \"width:100%\" makes the entire width of the block clickable, not just the area above the submit button image. Its not ideal, but getting the button centered was more important. Tradeoffs, such is our world…",
      "Actually I found a new definition of frustration – getting this WYSIWSG editor to properly format and position text."
    ],
    "summary_t": ""
  },
  {
    "id": "b627b7673cad831e8edd8092b9b5eb7a",
    "url_s": "https://opensourceconnections.com/blog/2009/02/13/hackathon/",
    "title": "Hackathon",
    "content": [
      "We like to get together periodically and hold \"Lightning Talks\" for an afternoon that focus on what weve been doing and learning. However this time we decided to take a full day and get as many of us who could make it to hack together on new things. From 9:00 to 10:00 we threw out ideas and then broke up into small teams of 2 or 3 folks. We started coding at 10:00, and frantically worked on our ideas till 4:00 PM. We did demos of our cool new projects form 4:00 to 5:00, and then adjourned to South Street Brewery to celebrate what wed learned. Here are just a couple of snaps: We had one team that wanted to \"Play with Google APIs\" using the new Ajax Playground, so we built a couple of little photo journal and map demos using the Google Earth Browser plugin."
    ],
    "summary_t": ""
  },
  {
    "id": "18078336d4b89474595c8dd8ceea5238",
    "url_s": "https://opensourceconnections.com/blog/2009/02/18/erics-predictions-for-the-future-of-the-web/",
    "title": "Erics Predictions for the Future of the Web",
    "content": [
      "So Ive been making these statements on the future of the web for quite some time, but just so I can prove I was \"there first\", I thought I should put them into writing!",
      "Prediction 1: Browsers will download complete environments from a web site to display a web site.",
      "And no, I dont mean like what we have today, where a browser downloads markup instructions, image assets, and some JavaScript to glue it together. Or, where a website is written in Flash, and runs in the Flash plugin in the browser. I mean a full environment, where I, the web site content developer control everything, and provide it all in a simple standardized method to you, the browser who consumes the site. I may write my sites client code in Ruby, Erlang, Lua, or build a strange 3D world. And the browsers will expect to visit a site, find the download links, and automatically start up these environments, in a safe clean sandboxed environment as simply as they find favicon.ico today. Heck, I could see a browser downloading and firing up a VMWare image of Windows 9 to browse \"My Extremely Enterprisy Website\".",
      "By allowing browsers to download and execute really rich code natively, without weird hacks like Google Gears, we eliminate the requirement that all advancements in HTML standards be matched in lock step by all the browsers. Especially with the rise of mobile devices, I expect to see more and more browser types, not fewer, so keeping them all up to date and current will be harder and harder. And we open up the ability of content developers to deliver their content in whatever way makes sense, not just in whatever way the browsers support.",
      "Prediction 2: \"Connect\" apps will become just part of the internet",
      "Credit for the word \"Connect\" apps goes to David Recordon, however Ive been thinking for a while that all these social graphs just duplicate the graphs of pages on the internet. And I predict that eventually something like OpenID will be your entry point into these graphs. Youll browse to epugh.myopenid.com and depending on who you are, Facebook, LinkedIn, or a random stranger, youll get a list of nodes that describes my links to other entities, regardless of if they are people, sites, content, etc."
    ],
    "summary_t": "So Ive been making these statements on the future of the web for quite some time, but just so I can prove I was there first, I thought I should put them into..."
  },
  {
    "id": "5714f688f49dd50e3e41b6532ae8502b",
    "url_s": "https://opensourceconnections.com/blog/2009/02/23/how-many-agile-practices-can-you-count-in-this-picture/",
    "title": "How many Agile practices can you count in this picture?",
    "content": [
      "My sons often get activity books with games in them like \"Can you find the 10 different things between these two photos\" or \"Find these 10 animals hidden in this picture.\" So rather than just post a picture of our uber-cool Agile development space with our new client Newswise, I figured instead Ill post the photo without a description of what were doing and see if you can figure it out.",
      "I count 8 items in this photo that are specifically Agile. How many can you find? Ill post my \"solution\" in a couple days, and I encourage you to post your own answers in the comments to this post. In addition to 8 Agile-specific items in this picture, I also count 6 other more general common traits of an OpenSource Connections project.",
      "For bonus points, can you think of anything missing from this photo?",
      "What sort of prize does the winner get, you ask? Hmmm, I dont really have one. How about free bowling lessons from our newly found OSC official \"Bowling Kingpin\" Jim Nist?"
    ],
    "summary_t": ""
  },
  {
    "id": "a7e2c32fea0702c7d105a8cc800d1972",
    "url_s": "https://opensourceconnections.com/blog/2009/03/02/a-picture-tells-an-agile-story/",
    "title": "A picture tells an Agile story",
    "content": [
      "This picture may not tell a thousand stories, but I count at least eight Agile related stories, as well as a few others about OpenSource Connections.",
      "Last week I posted a photo and asked people to comment with how many Agile practices they could count in the picture. There were a couple of very good replies – thanks!",
      "As promised, and long awaited, here is the list I made when posting those photos:",
      "8 Agile Practices",
      "Burndown chart on wall – Kind of hard to see, but there is a print out of past burndown charts on the wall. Good for seeing the burndown change over time.\n  Sprint goals on wall – Prominently displayed to remind us what were really trying to accomplish in the current sprint.\n  CI Server on Michaels monitor\n  Product owner in meeting – Weve got a great product owner on this project who is highly engaged in the project and participates in all stand up meetings. Ive been on projects where the product owner is not as engaged in the stand ups, and it really takes away a lot of the value.\n  Burndown/Task List on Google Docs on Arins monitor – In every stand up, I keep the current burndown displayed on my laptop for us to go over.\n  Remote team member in meeting on video chat (Stefan in Finland). While video chat is not strictly agile, having all your team members involved is. In this project, even though one of the clients key developers is in Helsinki, he participates in the daily stand ups via video or audio chat, and we also use TeamViewer to pair program with him.\n  Team working on a single table, pairing up as needed\n  Scrum poker cards – Nobody guessed this one, but thats my fault. You really cant see them in the picture because theyre white and next to a white roll of paper towels. But there are scrum poker cards there that we used in task estimation.",
      "In addition to those Agile specific parts of the project, heres a few other things to note about the photo that are not necessarily specific to Agile, but you will often see on an OSC project.",
      "User stories on wall – as was pointed out by James in the comments of my previous post, these are actually color coded to represent stories that relate to different types of users for the website we are working on.\n  Large monitors – they just make life so much easier, and are great for pair programming and demos.\n  Entity modeling on wall and whiteboard – We went through some great exercises with the clients modeling the entities in their business. We were originally going to be more dependent on a CMS in the project, but the modeling exercise made it clear some of their needs were too unique for the CMS, and so we scaled back (though didnt eliminate) the use of the CMS in the system.\n  Mountain Dew in Michaels drink – Michaels trying to be sneaky by putting his Mountain Dew in a travel mug, but I know that he was actually drinking the Dew as a breakfast drink. Very common to see on an OSC project.\n  OSC schwag on Arin – The world famous bright red OpenSource Connections logo shirt. Guaranteed to be a collectors item someday.\n  Windows and Mac living happily together. Since I dont have a big monitor like Michael and Youssef, I try to compensate by bringing two laptops. Sure, I could just use Parallels on my Mac, but then I dont get to have twice the computing power at my disposal."
    ],
    "summary_t": ""
  },
  {
    "id": "b4ed572d0a14cc05ddcb0a47f1b73692",
    "url_s": "https://opensourceconnections.com/blog/2009/03/05/looking-for-successful-charlottesville-business-social-media-stories/",
    "title": "Looking for Successful Charlottesville Business Social Media Stories",
    "content": [
      "Im going to be a panelist for an April workshop at the Charlottesville Chamber of Commerce discussing how to leverage social media to improve your professional and business profile.  One of the things that we want to do is show case studies of local companies and organizations that have used social media to grow their business.  If you know of a company that has done this or would like for us to highlight your efforts, please let me know, either by commenting below, going to my Facebook page or telling me on Twitter.",
      "@Barbara Hutchinson–Yes, youre one of the success stories Marijean Jaggers and I are talking about discussing!"
    ],
    "summary_t": ""
  },
  {
    "id": "3b6d5df267210fcc52421a137d5a1b85",
    "url_s": "https://opensourceconnections.com/blog/2009/03/09/a-new-deployment-tool-for-the-rails-world/",
    "title": "A new deployment tool for the Rails world?",
    "content": [
      "I think there is an opportunity for an alternative to Capistrano for managing deployments in the Rails world…. Jamis Buck recently blogged that that he was exhausted and done with Capistrano, hitherto the dominate deployment tool for Rails apps.",
      "On the local #rubycodejam IRC channel I was roundly criticized for my lack of sympathy towards Jamis, my point was that Jamis wouldnt have burned out if hed built a really solid community of contributors around Capistrano. The biggest lesson Ive learned from my involvement in ASF is that community is key to longevity for an open source project. While technical excellence opens the door to adoption, for a project to become long lasting you need community because community is what fosters new energy and new blood. Without community you have a collection of individuals who inevitably move on to new challenges or burn out. I point to my own experience as a committer on DBUnit as an example. While no longer active, as my interest and energy waned for the project I made sure to recruit new committers who have moved the project forward, including a point release done just a week and half ago.",
      "In the Capistrano world there isnt really the sense of community… While I am sure the folks who follow and forked Capistrano may disagree, I dont see anywhere on the Capify.org site a list of contributors, although it is in the changelog.",
      "At any rate, enough with the rant….. Is there the opportunity for a new project to take over????",
      "Ive been using Vlad the Deployer, and really loved it. Except that it doesnt run on Windows. Which doesnt bother me and my beloved MacBook. However Jim uses Windows, and does a fair amount of our systems work, which means deploying software better work for him! But it seems that he is finally giving up on Capistrano.",
      "Hopefully Vlad will gain support on Windows, and build on the community they already have!"
    ],
    "summary_t": ""
  },
  {
    "id": "6d1247e62779f49a0455cb7c431e97f9",
    "url_s": "https://opensourceconnections.com/blog/2009/03/09/add-search-to-any-drupal-block/",
    "title": "Add Search to any Drupal Block",
    "content": [
      "I recently needed functionality to search a Drupal site, but the Drupal search block was already being used in the header section. And, unless Im mistaken, the Drupal block admin page only allows a block to be placed in one section. Also I wanted a search that looked only in a specific content type, in my case a content type called \"osc_news\".",
      "My solution was to add html/php code to the body of a block I created. This code uses the existing search functionality, which is way easier than creating my own search functionality. I started by viewing the page source of the Drupal advance search form, which does allow the user to select a specific content type to search. This gave me the base html to add to my block. I had to add calls to the Drupal functions which create unique element ids: md5(uniqid(mt_rand(), true)). Then to search in a specific content type I added an hidden form element. I had to use attribute values which Drupal recognized and the \"checked\" attribute. Of course, this is not a standard element for a hidden element. But it did cause Drupal to search only in the specific type.",
      "Click the link below to view the html/php code I added to my block. This code can probably be used in any user created block.",
      "addsearchtodrupalblock"
    ],
    "summary_t": ""
  },
  {
    "id": "785533f20b1e415cc55d01e988ef0ee6",
    "url_s": "https://opensourceconnections.com/blog/2009/03/12/innovation-awards-nominations-open-please-submit-a-nomination/",
    "title": "Innovation Awards nominations open – please submit a nomination!",
    "content": [
      "The Charlottesville Business Innovation Council has announced that nominations are open for this years Innovation Awards. I chair the research committee that does the leg work necessary for the judges to pick the recipients of the 11th annual Innovation Awards, and so I want to personally encourage each and every one of our many blog readers to submit a nomination for your favorite client, employer, cool-company-you-once-heard-about, or even yourself.",
      "<a target=_blank href=\"http://www.cvillebic.org/cia/nominations\">Place your nominations here!</a>",
      "Not all awards are for companies either, so if you know of a really great teacher in the Charlottesville area using technology, or a business leader who is a \"Navigator\" for technology in this area, then please nominate them too! The more nominations we receive the better. Here is the press release from CBIC with more details and a description of each award:",
      "Charlottesville Innovation Awards â€\" Call for Nominations",
      "The Charlottesville Business Innovation Council (CBIC) invites nominations for the 11th annual Charlottesville Innovation Awards (CIA).",
      "Individuals and organizations are urged to use the nominating form on the CBIC website (http://www.cvillebic.org/cia/nominations) to submit nominations before the March 18 deadline to honor achievements in any of six categories:",
      "Peopleâ€™s Choice Navigator Award â€\" For the enterprise or individual who has demonstrated significant leadership in the local or regional business community, by making significant contributions toward the improvement or advancement of the high-tech, business and/or entrepreneurial environments, and thereby improving the quality of life for many.",
      "Rocket Award â€\" For the enterprise that has moved with noteworthy speed from concept toward commercialization, through development of a new technology, product or service in a new or existing business of any size.",
      "Spotlight Award â€\" To be presented to that enterprise or individual that has brought significant positive attention to the region spotlighting us as a world-class leader.",
      "Breakthrough Award â€\" For the enterprise or individual who has achieved the most remarkable breakthrough or quantum advance, stemming from an original discovery, a seemingly unlikely redirection or unforeseen application of an existing solution.",
      "Community Award â€\" For the enterprise or individual who exemplifies the greatest commitment to and impact upon improving the quality of life in Central Virginia through community involvement.",
      "Red Apple Award â€\" Educator, in K-12, who clearly and consistently is able to do the most with resources available in exciting and preparing students for the possibilities in technology.",
      "The award-selection process involves rigorous research by a panel of judges, who bring diverse expertise and a shared commitment to recognizing and advancing innovation in the Central Virginia region. Finalists in each category will be announced on or around March 25, with award recipients to be honored at a gala dinner event on May 21 at Farmington Country Club. Self nomination is encouraged, for individuals and organizations, and CBIC membership is neither required nor considered in the selection process.",
      "â€œIn this time of economic uncertainty, we are especially committed and excited to provide this extraordinary showcase to honor organizations and individuals who exemplify the innovation and leadership that continue to drive our region,â€ said Tracey Linkous, CBICâ€™s Chairman of the Board. â€œIn addition to its primary purpose of recognizing excellence, our CIA Gala has always been a great occasion of fun and inspiration for all.â€",
      "About the Charlottesville Business Innovation Council:\nCBIC is the preeminent private-sector catalyst and advocate for entrepreneurship and technology-based economic development in the Central Virginia region. It serves its members and the broader community by providing educational programs and opportunities for productive networking, through advocacy with governmental and media constituencies and by presenting events that inform and inspire our community.",
      "Media inquiries may be directed to:\nTracey Linkous\nChairman of the Board\nCharlottesville Business Innovation Council\n434.220.0718 (o) / 434.242.5886 (m)\n[email protected]",
      "<a target=_blank href=\"http://www.cvillebic.org/cia/nominations\">Place your nominations here!</a>"
    ],
    "summary_t": ""
  },
  {
    "id": "61a905fb127557b76b4894c06333c7e7",
    "url_s": "https://opensourceconnections.com/blog/2009/03/13/recent-teaching-demonstration/",
    "title": "Recent Teaching Demonstration",
    "content": [
      "Recently I gave a teaching demonstration at a local community college. This was part of the process of becoming an adjunct instructor/professor. If all goes well, Ill start as early as the coming summer semester. Im not sure what Ill be teaching yet, most likely an established, introductory computer science class. Some of these classes teach the use of common applications like spreadsheets and word processing. By the end of the semester the students are doing some advanced stuff. Or I may teach an introductory programming class – using java of course. After a little experience, I will propose some .NET classes, either C# or ASP.NET. Im excited to have the opportunity, wish me luck. I knew all those graduate CS classes would come in handy.",
      "Heres a link to the lecture I demonstrated: Object Oriented Programming – Encapsulation, Inheritance and Polymorphism."
    ],
    "summary_t": ""
  },
  {
    "id": "5ae13ec7fd3b0880287f06f9c2169857",
    "url_s": "https://opensourceconnections.com/blog/2006/11/02/roping-in-the-cowboy-coders/",
    "title": "Roping In the Cowboy Coders",
    "content": [
      "When an IT team needs help with a big project or a new task, they will often look to outsiders for help in delivering their objectives. Oftentimes, they will bring in an elite coder, one with superior skills and knowledge, and look to that coder to be the white knight and save the team.",
      "Unfortunately, managers are overawed far too often by the elite coder, and they take a far too hands-off management style, letting that person run roughshod and do whatever he or she wants. What managers forget is that theres a horizon beyond the current project, and when the external help is gone, the remaining IT team needs to maintain and update whatever was left behind. Often, this means that the team is left holding the bag, trying to figure out the code and how to work with it.",
      "Just as any company that hires a consultant, just gets a deck, and lets the consultant leave, it is incumbent on the manager to MANAGE the external resources as well.",
      "How so?",
      "1) Integrate the external resources with the internal team. For all intents and purposes, for as long as the contract lasts, the contractors are part of the team. Have a kickoff. Do some social events, even if they are on the company clock and the company dime. Managers need everyone to work together.\n2) Set expectations. Managers should demand well documented, clean code and walkthroughs of the code base so that when the team has to do maintenance, its not the first time anyone has seen the code.\n3) Get the contractors skin in the game. Put some percentage of the total payment into an escrow to be released a certain time after the code goes live. The contractor should be responsible for bugs, which often dont appear until the release. Demand testing, at least unit testing, and regression testing if possible.\n4) Pair program. This isnt so that someone is consistently watching over the contractors shoulder, but, rather, so that the internal team can learn from the person who was brought in with special knowledge. There should be knowledge transfer, be it through seminars, brown bag lunches, or observation.",
      "This means that a manager looking to bring in external resources needs to do more than look at a resume. Solo riders cure the symptom, but probably dont cure the root cause. This means that the manager needs to do a thorough job in screening; the problems of a bad relationship will last with the team much longer than the relationship will."
    ],
    "summary_t": ""
  },
  {
    "id": "701e5d3033cae9b6d806cdea65cd63e8",
    "url_s": "https://opensourceconnections.com/blog/2009/03/16/dont-throw-away-your-laptop/",
    "title": "Don’t throw away your laptop!",
    "content": [
      "Me `Ole Thinkpad was getting very noisy and running extremely hot which got me yearning for a shiny new one. I have configured a new one on the Lenovo site about 50 times over the last few months.",
      "Times being as they are, i decided to investigate what a reasonable DIYer would do and consequently, i spent a good part of Sunday reading articles about, watching videos on, taking apart, and then putting back together my trusty laptop with its beloved keyboard action, pointing stick and THREE mouse buttons.",
      "I basically did 3 things:",
      "Removed and dis-assembled the fan and cleaned a TON of dust out. Dust was totally blocking the exhaust. While the fan was apart, i put a little dab of white lithium grease on the \"axle\"\n  \n  \n    Made a mod to the heat sink wherein i removed the thermal \"pads\" that were (not) conducting heat very well to the heat-sink and fan bent the heat sink so it would sit on top of GPU an some other chip that i have no idea what it does, then taped a 1960 penny to the clip that holds it down and bent the springs a little so that there would be enough pressure to seat the the heat sink on the chips. If that was your prized penny that you somehow left at my house, you are SOL.\n  \n  \n    Replaced the thermal compound on the CPU with Arctic Silver . . .",
      "When Humpty Dumpty went back together again . . . both the CPU and the GPU are running about 20 to 25 degrees cooler. Centigrade for craps sake.",
      "When the fan runs . . . its almost silent.",
      "Hmm . . . now I have to decide (after I upgrade the hard drive) what to do with $1500."
    ],
    "summary_t": ""
  },
  {
    "id": "e447c871f3bf4e8006e073abfb4cab5c",
    "url_s": "https://opensourceconnections.com/blog/2009/03/22/arin-sime-to-moderate-cbic-panel-on-social-media/",
    "title": "Arin Sime to moderate CBIC panel on Social Media",
    "content": [
      "This coming Wednesday March 25th, I have the honor of moderating a luncheon panel on the uses of social media in business.Â  The panel is hosted by the Charlottesville Business Innovation Council, and is titled \"Incorporating Social Media into your business strategy.\"Â  You can learn more about the event and register here.",
      "On the panel will be:",
      "Ryan Adams, Intalgent\n  Suzanne Henry, Four Leaf Public Relations LLC\n  Scott Hildebrand, BoldMouth",
      "It should be a great discussion and Im looking forward to it.Â  As part of the event, Im planning on bringing a large monitor to setup somewhere in the room and rotate through some screen shots of how local businesses are using tools like twitter and facebook.Â  You can see the screenshots Ive collected so far on flickr.Â  Id love to hear of other examples in our area – since Im sure Ive missed a lot.Â  Just leave a comment here and Ill probably get your examples added to the rotation too!"
    ],
    "summary_t": ""
  },
  {
    "id": "5a190437173643f9dff7b0202f8b14ef",
    "url_s": "https://opensourceconnections.com/blog/2009/03/22/mysql-and-innodb-foreign-key-tip/",
    "title": "MySQL and InnoDB Foreign Key Tip",
    "content": [
      "If you are not a DBA, as is in my case, debugging SQL errors can get tricky and cryptic. An error message like",
      "MySQL has left you",
      "or",
      "General error: 1005 Cant create table `./db_name/table_name.frm",
      "wont mean much to you and trying to figure out the problem can be a pain. After some googling and messing around with my SQL statement, I now know some of the causes for the 1005 error. This error usually occurs if youre trying to create foreign key constraints in your table, so heres some of what you should look out for:",
      "The referencing column and the referenced column are of the same type\n  The referencing column and the referenced column are of the same length\n  The referencing column and the referenced column are both signed or unsigned",
      "basically the referencing column and the referenced column need to be identical. This may sound trivial, but its something you might not think about specially if there was a long period of time between when you created the first table and when youre creating the second one. Or if some one else on the development team created the table youre referencing and did not use the default values whereas you did.",
      "When using a SQL dump to create the tables, the referencing table is created before the referenced table (this could happen when the dump is generated because the tables are sorted by table name)",
      "to get passed that error add the following lines",
      "SET FOREIGN_KEY_CHECKS = 0;",
      "at the top of the dump and",
      "SET FOREIGN_KEY_CHECKS = 1;",
      "at the bottom of the dump."
    ],
    "summary_t": ""
  },
  {
    "id": "e8fa26cefb3c3f25fc34f641331d7dca",
    "url_s": "https://opensourceconnections.com/blog/2009/03/23/could-new-facebook-look-be-a-sign-that-its-still-looking-to-buy-twitter/",
    "title": "Could New Facebook Look Be a Sign That Its Still Looking To Buy Twitter?",
    "content": [
      "Recently, Facebook released a new look and feel.  Over 1 million users complained about the new interface, citing, among other things, that it looks too much like Twitter.  Given Twitters 1,689% year over year growth, maybe Facebook thought it was onto something.",
      "I was not surprised, then, to read that Facebook had recently attempted to purchase Twitter.  It rejected a $500 million valuation, much lower than its initial attempts at third round VC funding.  Perhaps cash in hand was better than Facebook stock in hand?",
      "I think that a few lessons are applicable here for those following the subject:",
      "If youre looking to be acquired, or, alternatively, to acquire, make sure that your technologies are compatible.  Facebooks new look and feel looks very similar to Twitter.  Since Facebook cant buy, perhaps it will build, or perhaps its trying to make a conversion of Twitter users less painful.\n  When you roll out a new look and feel, give the users the option to revert back to the previous version.  Inertia is a powerful force to overcome.  We use Google Mail as our mail service, and the website still has an \"Older Version\" link in its upper menu tab list.\n  Beta test with your users.  Nothing portends doom like rolling out a brilliant new idea that marketing cooked up without getting a subset of your users–and preferably the thought leaders among your users–to say what they think.  Its better to spend a small amount on a test case that fails than to roll it out to your grand audience and pay the price.",
      "Given the apparent haste and backlash with which Facebook has rolled out its new look and feel, it is reasonable to believe that the Facebook purchase of Twitter is still on the table, or its looking to assimilate Twitter users, the poker equivalent of a spite call."
    ],
    "summary_t": ""
  },
  {
    "id": "0311439f41d2c5d687442eddce7d0acd",
    "url_s": "https://opensourceconnections.com/blog/2009/03/26/examples-of-charlottesville-businesses-using-social-media/",
    "title": "Examples of Charlottesville businesses using Social Media",
    "content": [
      "On March 25th I moderated a panel on the use of social media by businesses, and it was a lot of fun.Â  Many thanks go to all of the roughly 90 attendees for being there, and my fellow CBIC members for all their work putting together the event.Â  The audience asked great questions, put up with my jokes, and the panelists had great advice.Â  So thanks again to our panelists:Â  Ryan Adams from Intalgent, Suzanne Henry from Four Leaf PR, and Scott Hildebrand from BoldMouth.Â  There was also a mention of the event on CBS19.",
      "At the event I had an iPhoto slideshow going of screen shots from local Charlottesville businesses who are already using social media to promote their businesses.Â  These are actually some pretty good examples I think, so as promised, Im including all those photos here along with a quick tag line describing the picture.",
      "In no particular order, here they are….",
      "",
      "Jim Duncan is a Charlottesville area realtor who uses Twitter to communicate with potential clients and the community as a whole about the real estate market.Â  His use of blogs and twitter have helped establish him as an expert locally and lead to more publicity for him and his clients.",
      "",
      "Business Bullpen is using a Facebook Fan Page to communicate with their potential clients.",
      "",
      "Trees On Fire is a local Charlottsville band that is using Facebook events to invite people to their shows. Perhaps your business should be using Facebook to invite people to your sales or events too?",
      "",
      "Newswise is a Charlottesville based company that distributes press releases on behalf of their clients. They are using Twitter as an additional channel to distribute releases to journalists.",
      "",
      "High Tech Cville is a creation of OpenSource Connections own Eric Pugh, and is a site that aggregates information about technology professionals in Charlottesville automatically from a number of social media sites.",
      "",
      "OpenSpace Coworking is using social media to promote the upcoming launch of their new coworking office space, and building excitement and interest ahead of time.",
      "",
      "The Charlottesville Business Innovation Council has a group you can join on LinkedIn, which is another way to keep up to date with upcoming CBIC events.",
      "",
      "Facebook Fundraising is a facebook app that I am building which is still under development, but Im using Twitter to keep potential users apprised of its upcoming release and to start to build a little excitement about it. After release, it will be a facebook app that helps indirectly promote my business Donor Town Square (though it can be used with my competitors services as well).",
      "",
      "Ryan Adams started the Charlottesville Tech LinkedIn Group as another place to go to discuss the tech community in Charlottesville.",
      "",
      "The Charlottesville Airport has its own blog on its website GoCho.com, where airport staff can keep the community up to date with whats going on at our eeggional airport and tell travel stories.",
      "",
      "Scott Hildebrand, CEO of BoldMouth, was one of our panelists and brought a lot of great insight about his experiences working with large companies on social media branding campaigns.",
      "",
      "Ryan Adams of Intalgent software was another one of our panelists, and brought a lot of great insight into how he has secured many leads and contracts through LinkedIn, and how to use social media to advance your personal career growth.",
      "",
      "Quadruplicity is a conference recently put on by the Charlottesville Chamber of Commerce, and they used Facebook to help promote the event. OSCs own Jason Hull ran a social media panel at that conference.",
      "",
      "OpenSource Connections uses their blog for technical posts, announcements of company events and speaking engagements, and for commentary on various technology areas related to our expertise. It helps to establish our credentials and we often point potential clients to blog posts that are related to their projects.",
      "",
      "Suzanne Henry, President of Four Leaf PR, was a panelist in our discussion, and provided a great perspective on how social media has been used by her clients, how businesses can get started developing their message, and unleashing the power of social media to promote their business.",
      "",
      "Local gym ACAC has setup a Facebook page for their gym that promotes special events, and provides a way for gym members to interact with staff.",
      "",
      "CBIC has incorporated a twitter feed into our homepage. If you twitter using the hashtag #cbic (which just means putting #cbic anywhere in your tweet), then your tweet will appear on CBICs homepage. You can use this to comment on CBIC events, and CBIC volunteers are using it as a quick way to post announcements and reminders on the CBIC homepage.",
      "",
      "My company Donor Town Square has started to use a twitter feed to announce updates to our services, feature changes, or updates on planned outages. This provides a quick way that customers can get up to the minute updates.",
      "",
      "The Southern Environmental Law Center, based in Charlottesville, uses a YouTube channel to promote vidoes about their advocacy efforts.",
      "",
      "Shergold Studio is a dance Studio in Charlottesville that has setup a Facebook page where their students interact and post videos of their performances. Panelist Suzanne Henry described them as having less than 100 supporters on their page, which is not a huge number, but is \"the right 100 people\" for their demographic, since it is a very devoted fan base of the business.",
      "Thats it! This was certainly not a comprehensive list of all the ways Charlottesville businesses are using social media to grow, but I hope its given you a range of ideas for how to grow your own business. And if those plans involve custom software development or Facebook Apps, dont forget to call us at OpenSource Connections and we can help. If you need help with general social media marketing tasks, you should also look up the panelists we had – they did a great job at the event and Im sure could help your business too.",
      "Thanks again to all who attended the CBIC event, and please feel free to leave comments about the event or other uses for social media in the comments below!"
    ],
    "summary_t": ""
  },
  {
    "id": "5ceff68b76202363f41066ede3f3e594",
    "url_s": "https://opensourceconnections.com/blog/2009/04/03/texas-senate-bans-microsoft-vista-use/",
    "title": "Texas Senate Bans Microsoft Vista Use",
    "content": [
      "On April 1, the Texas State Senate banned Microsoft Vista use by Texas state government agencies.  Citing issues with Micosoft Vista and their current level of satisfaction with XP, the Senate specifically called out Microsoft in its budget.  Will Texas be the canary in the coal mine for other government agencies?  I imagine that RedHat sent a thank you note or two to Texas state Senators.  It will be interesting to see if Microsoft can convince the Texas Senate that its issues have been sufficiently ironed out, or if other O/Ses such as Linux get a shot.  Given nationwide state budget shortfalls, it would not surprise me to see a trend in that direction, particularly smaller states who could use Texas as an example to justify a deeper evaluation of open source alternatives.  Texas near-neighbors Nebraska and Mississippi have already adopted Linux use.",
      "Whats next?  Ubuntu?",
      "Thanks to Steve Stedman for pointing this out!"
    ],
    "summary_t": ""
  },
  {
    "id": "9d6f681b3a0644c5059738153708ada5",
    "url_s": "https://opensourceconnections.com/blog/2009/04/15/time-for-a-time-based-database-tbdb/",
    "title": "Time for a Time Based Database (TBDB)?",
    "content": [
      "Yesterday Arin Sime and I saw Jean Bauers presentation on developing the Early American Foreign Service Database that is part of her disseration attempting to identify why the Foreign Service was so effective when it was intrinsicaly a very chaotic unorganized period for what we now call the Foreign Service.",
      "Jean gave a great brief on some of the challenges the Foreign Service faced in the late 1790s, and walked us through her database for modeling the social network of the Foreign Service. Source code available at http://projectquincy.rubyforge.org. She modeled both traditional relational data such as individuals and where they are posted, letters they wrote to who, as well as more GIS oriented data, such as what states belong to what countries and what countries were part of what empires (ie British Empire, Ottoman Empire).",
      "One of the things that struck me is that while the schema was very clean, all done up to 3rd normal form standards with good use of primary and foreign keys, what I didnt like was that many of the relationship tables such as the assignments table for various folks had begin and end dates. And it struck me that many of the queries that you would want to issue are all based around time spans. I want all individuals posted to Barbadoes between 1770 and 1780. Which can easily be done with a select statement that looks something like:",
      "SELECT * from INDIVIDUALS i, POSTINGS p, LOCATION l where l.name = 'barbados' and l.id = p.location_id and p.individual_id = i.id and p.start_date > 1770 and p.end_date < 1781",
      "However it strikes me that this extra section at the end \"p.start_date > 1770 and p.end_date < 1781\" is really the key part of the relationships, potentially on the level of the primary keys and foreign keys for identifying the data. How would I model a single posting to a country that was interrupted for a small time period? I guess the traditional answer is to have another table called something like \"Residency\" that would track actual time periods as well but that seems icky and roundabout adding extra layers of complexity to my data model.",
      "So part of me wonders, why dont the Primary Keys and Foreign Keys contain the time axis of data as well as the relationship between individual tables? Why dont we have a Time Based Database for storing data whose primary relationship is time? Document Based Databases have joined our pantheon of data stores, so what would a Time Based Database look like? I think it would deal with various calendars very simply.. Tell me when Rome was founded in Gregorian or Julian calendar. Or, what year did Rosa Parks stage her protest according to the Jewish calendar?",
      "I can imagine that a TBDB would need to understand the degree of confidence/accuracy in dates. If I selected when all the worlds cities were founded, it would natively let me know that Rome was founded middle of the 8th century BC, but that New York was founded in 1624 AD.",
      "And lastly, it would naturally handle time zones. No need to tediously convert all dates into UTC, just use whatever and know the TBDB would handle the selects.",
      "I can see this being very useful for data such as flight schedules, and figuring out the best route between cities. How do I get from Charlottesville Virginia to Brisbane Australia with multiple stops on the way? A TBDB would handle all that icky date handling so that a stop in Dubai wouldnt overlap with the departing flight from Dubai.",
      "While there dont seem to be any candidates yet, feel free to comment on what you would want in your TBDB!",
      "Â"
    ],
    "summary_t": ""
  },
  {
    "id": "63140c944af0cf1e7c9bc4b676534892",
    "url_s": "https://opensourceconnections.com/blog/2009/04/23/a-scrum-take-on-metrics-and-analysis-in-companies-at-different-maturity-levels-of-the-cmmi/",
    "title": "A Scrum take on \"Metrics and Analysis in Companies at Different Maturity Levels of the CMMI\"",
    "content": [
      "Last month Scott, Arin, and myself road tripped to attend the Richmond SPIN meeting, where Kris Puthucode of the Software Quality Center gave a talk on \"Metrics and Analysis in Companies at Different Maturity Levels of the CMMI\nModel\". The focus on the talk was what results you can expect out of metrics if you do the work of performing the analysis required.",
      "I was attending the presentation with a certain sense of trepidation… I consider myself a hard core developer( despite my \"Test Obsessed\" armband) who doesnt have time or patience for pointy headed manager paperwork. But I am also someone who focuses on process improvement and honing my craft of software development, so where can metrics inspired by CMMI be useful to a fast moving Agile team cranking out functioning software every three weeks? So I listened to Kris and tried to think about what he said in the context of Scrum.",
      "The first slide pointed out that there are three kinds of lies: \"lies, damn lies, and statistics\". You need to be careful about what your data says. When you are measuring velocity in Scrum, you need to have a couple of sprints, at least 3 to have any sense of your progress. If you say \"we get X done\" based on the first sprint, well often the first one is very conservative sprint. And, as you look at your average burndown, you need to have a couple of days of information before you can get a sense of average burndown as the first days often you find as many tasks as you accomplish. And, over the 15 days of your iteration, progress can be pretty spiky… Many teams have pretty flat burn down at the beginning, and then some steep drops… Ideally you are looking to see progress per day become flatter, less spiky, which would indicate that your estimating is improving. Or your team is being less affected by external factors that might hamper their productivity.",
      "He talked about whether to use average or median numbers in looking at series of numbers, such as looking at your burndown.. If you have a lot of outliers, then use median to get a better view, otherwise use average. So if you burndown 20, 25, 22, 40, 25 then using the average is good. But if you have 8, 20, 25, 22, 40, 12 then maybe median would be better.",
      "Kris talked about the cultural challenge of convincing people to provide the data required to build metrics. People need to know why the numbers matter, and even better why it will help then. Its why developers hate filling out TPS reports! And why I like Scrum and its low overhead, as well as more passive measurement tools like HackyStat and 6thSense(now part of RallyDev). I feel like mandating one metric, say your basic time tracking is viable, but if you add more on you start getting more push back or gaming of the metrics.",
      "He talked about getting metrics like Project Start Date and Project End Date. A local company splits up its year as 3 week iterations, and then apportions iterations across competing projects. These iterations then feed the project start and end dates.",
      "He stressed that you need to have a shared vision on metrics, and a shared vision of what \"on time, in budget, with quality\" really means. I know we the other day had a scrum team debate how to track found tasks. And this was a set of people that had worked together previously on different project having different visions of tracking found tasks and how they should affect the \"ideal burndown\" line! Covering periodically what that shared vision, and ensuring your team and stakeholders are all on the same page is very valuable.",
      "He stressed that your metrics need to be actual \"measurable\" things. You need to be able to quantify the metrics that you are using using a shared basis so that when you compare two things using the same metric that you are doing an apples to apples comparison, not an apples to kiwi comparison! For Scrum, it means you need to use the same time frame for iterations, and you can compare one sprint to another for a specific team on a specific project, but not across teams or projects.",
      "Scrum for us provides a very standarized metrics across the OSC organization, regardless of client or specific technology. As long as we are sharing the same vision for our metrics!",
      "When we do a retrospective, and look back at our burndowns, we are doing \"Progress Indicators\" that are lagging indicators. One of the things that the speaker was advocating was to look at forward looking indicators that predict into the future where we will be. But of course, identifying and seeing a leading indicator is difficult, and takes a lot more analysis. In scrum we would have to tie our tasks to various sprint goals which are appropriately estimated against to provide our velocity.",
      "Did highlight that you need multiple projects happening to be able to gather the variety of data points to be able to compare data points. Compare two scrum teams together and its tough to compare them because you cant see the outliers. But, if you have 10 scrum teams, who have a shared vision of the metrics, then you can start comparing them together. You may have to normalize across the teams, but with enough iterations you can compare and predict.",
      "So some things to show would be a histgram of how much the team burns down a day. Highlight what kind of deviation we have in our progress per day. Over multiple sprints you can maybe see what the first third, middle third, and final third look like. Can we characterize \"At OSC, we typically see this kinda of result in the thirds of the project?\" Hey, does this feed into our Waterfall projects in 3 weeks?",
      "First week is requirements solidification. Second week is development. Third week is testing and polish.",
      "Can we figure out how to predict the results for sprint 3? We could do this for sprint 1 and 2 and predict sprint 3.",
      "We measure progress per day as a ratio, and then sum it over a week. With that progress per week, then we can see what a sprint 3 would do."
    ],
    "summary_t": ""
  },
  {
    "id": "0d90cb2d8754b66ace82f2948d605aed",
    "url_s": "https://opensourceconnections.com/blog/2009/04/29/slides-available-from-charlottesville-chamber-of-commerce-social-networking-201-workshop/",
    "title": "Slides Available From Charlottesville Chamber of Commerce Social Networking 201 Workshop",
    "content": [
      "The slides from this mornings presentation of Social Networking 201 with the Charlottesville Chamber of Commerce, presented by me, Marijean Jaggers, and Sasha Farmer, is now available here.  Thanks to Sasha and Marijean for making this such a great presentation!  It was great to work with both of you!"
    ],
    "summary_t": ""
  },
  {
    "id": "72643b133fd50f7b5db87bf77450ac3a",
    "url_s": "https://opensourceconnections.com/blog/2009/05/01/arin-sime-speaking-at-agile-2009/",
    "title": "Arin Sime speaking at Agile 2009",
    "content": [
      "I was very excited to learn earlier this week that I had a session proposal for Agile 2009 accepted.Â  In Chicago on Thursday Aug 27, at 4pm, I’ll be discussing How to sell a traditional client on an agile project plan. Here’s how I described the session:",
      "You’re negotiating a project with a client or internal customer, but they balk when you don’t present a fixed budget and a predefined list of requirements. How do you convince them that the benefits of an Agile team outweigh a top heavy and fragile requirements document? Based on Agile experience with government and commercial clients, we will discuss ways to make your customer feel comfortable with process changes that don’t always result in the same set of documents they are used to.",
      "Process/Mechanics\n    Learning outcomes\n    Insight into different types of clients, and what they often expect\n    Strategies for finding a middle ground between what they are used to, and the freedom you need to implement Agile\n    Ideas for dealing with customer expectations throughout the project\n    Real world examples of how to deal with clients who require processes that are not Agile\n    Examples of how to structure a proposal to incorporate Agile methodology",
      "One of the reviewers posted a comment about my session which read \"This session addresses a very common question I come up against, and feel we should be offering the answers to.\"",
      "I was glad to see that comment, because I agree completely and thats why I proposed the session. While I wont claim that we here at OSC have all the answers, I do think weve developed a lot of experience in the area due to the wide range of clients that we work with. Some are very agile, some … not so much. Many are somewhere inbetween.",
      "OpenSource Connections has methods based on our experiences, but I would love to hear feedback from others too. Are there any particular Agile war-stories that you have about convincing a traditional client to implement agile practices?"
    ],
    "summary_t": "I was very excited to learn earlier this week that I had a session proposal for Agile 2009 accepted. In Chicago on Thursday Aug 27, at 4pm, I’ll be discussin..."
  },
  {
    "id": "649ffaad92b56be6cc3379b9c7d13996",
    "url_s": "https://opensourceconnections.com/blog/2006/11/08/rails-and-functional-testing-parameter-pain/",
    "title": "Rails and Functional Testing parameter pain",
    "content": [
      "Ruby on Rails once again lived up to my expectations that it is incredibly powerful, but can als cause real pain!",
      "I had a functional test that proved that my action for dropping days from a calender of estimates worked. However, then when running the action through the webapp, it never actually dropped the days!",
      "Well, thanks to Jamie Orchard Hays and Chris Hapgood at our local RubyJam, we discovered an \"oddity\" in how rails works. My action had logic like this:",
      "date = params[:date]\n....\nif (estimate.date == date)\n      estimate.destroy\nend",
      "And the estimates were never destoyed! But in my functional test call the estimates were destroyed:",
      "dropdate = @sprint.start_date + 2\n\nget :drop_day,{:id => @sprint.id, :date=> dropdate},{:current_user => @user}",
      "So, it turns out that in my functional tests I was passing in dropdate as an actual Ruby Date object, but in the real web action, the date was coming in as a String! Therefore the call:",
      "estimate.date == date",
      "was never true! I changed my call for getting date to:",
      "date = params[:date].to_date",
      "and that fixed everything. However, what I think should ALSO have happened was the call to get :drop_day method should have converted everything to String, or whatever else you would expect params[] to return."
    ],
    "summary_t": ""
  },
  {
    "id": "e2a78df5faaf128bd31ece83d307e107",
    "url_s": "https://opensourceconnections.com/blog/2009/05/01/opensource-connections-named-university-of-virginia-preferred-vendor/",
    "title": "OpenSource Connections Named University of Virginia Preferred Vendor",
    "content": [
      "Today, OpenSource Connections received notification that they had been chosen as a University of Virginia preferred vendor for Web Design, Development, and Programming Services.  The new contract should appear on the University of Virginia Preferred Vendor page within the next two weeks.",
      "The range of services covered under the contract include:",
      "Information design\n  Web site design/redesign/implementation\n  Programming and scripting, to include HTML, Java, C, C++, PHP, CSS, RSS, C#, ASP, XML, .NET\n  Interactive database design and programming (SQL Server, MySQL, PostgreSQL)\n  Flash development\n  Javascript development (JSON, DOM, AJAX)\n  Designing for handheld devices\n  Interactive design (polls, wikis, social media, learning management systems)\n  Webmaster services\n  Training\n  Web standards (Section 508, Dublin Core, WCAG 2, etc.)\n  Project management\n  Search engine optimization (SEO)\n  Online marketing, to include social media\n  Ruby on Rails programming\n  Search engine implementation (Solr, Lucene, Nutch)\n  Database design",
      "We are thrilled with the honor and look forward to working with many of our colleagues in the University on future opportunities."
    ],
    "summary_t": ""
  },
  {
    "id": "919f630e4667aa3f2d68b56dcf15492e",
    "url_s": "https://opensourceconnections.com/blog/2009/05/06/attending-futureruby09/",
    "title": "Attending FutureRuby09",
    "content": [
      "Conference season is on its way, as I am writing this RailsConf09 is taking place in Vegas. And the people who brought us RubyFringe will try to repeat their success with this years conference called FutureRuby. The conference will be taking place in Toronto, CA July 9-12th, 2009.",
      "From OSC, Youssef Chaker will be attending. So if you are interested in attending…",
      "",
      "…sign up fast, early bird tickets sold out in 5 hours. And be sure to introduce yourself to Youssef."
    ],
    "summary_t": ""
  },
  {
    "id": "4f92b2a3e90a531a79debae12c7eabd7",
    "url_s": "https://opensourceconnections.com/blog/2009/05/06/richmond-spin-on-continuous-integration/",
    "title": "Richmond SPIN on Continuous Integration",
    "content": [
      "I have the honor of speaking to the Richmond SPIN group on Continous Integration next week Wednesday, May 13. SPIN is the Software and Systems Process Improvement Network, and are the local groups sponsored by Carnegie Mellon Universitys Software Engineering Institute.\n\nMy presentation is going to be a bit different from some of the previous topics that have talked about process improvement, whereas I am talking about a specific improvement that enhances your process. A CI system can provide the base framework for layering on much more then just the basic automatic code/compile/test cycle, and well talk about what else it can be.",
      "More information and registration is available at http://www.richmondspin.org/home22332.",
      "Im looking forward to a good crowd, lots of questions, and drinks afterwords!"
    ],
    "summary_t": ""
  },
  {
    "id": "5546504204147f76ab808deafba4a377",
    "url_s": "https://opensourceconnections.com/blog/2009/05/08/zend-and-expressionengine-working-together/",
    "title": "Zend and ExpressionEngine Working Together",
    "content": [
      "Have you ever tried to make two technologies working together? Has it worked well for you? Was it easy, out of the box type of task that took you about an hour to complete? Or was a week long thing, and you got out of it with less hair? If you fall in the last case, then youre in the right place, I will give you a few tips about getting ExpressionEngine to work nicely (sort of) with Zend.",
      "In this post I am going to assume that you know what ExpressionEngine, Zend, CMSes, and MVC frameworks are. The challenge that well be trying to overcome is getting Zend to serve up static pages built in ExpressionEngine. Most people would probably have two applications and try to establish a connection between the two, where one application would be the custom built Zend application and the other would be the CMS loading static pages with ExpressionEngine. Or some sort complicated Apache RewriteRule filled solution. Not in this case.",
      "Note: We are using ExpressionEngine 1.6.7, ZendFramework 1.7.4 and PHP 5.3",
      "The first thing we did after installing EE and setting up our Zend directories (for more information on this check out the respective sites for EE and Zend), was to copy the index.php file into our public folder and named it ee.php, we also added a symlink in the public folder that points to the system folder of the EE code base. We copied and renamed the index.php file because we needed to rewrite some of the logic that deals with routing and where everything is found. We did also have to deal with some bugs and warnings, so you might have to do some of that yourself, I will only mention the changes that we made that are concerned with working with Zend.",
      "Introducing EE to Zend",
      "Since Zend is now responsible for all the routing for the application, we need to tell Zend how to behave when we want to display EE content. Since Zend is an MVC framework, the first place to start is in the controllers. We added a new controller called EEController which will take over the responsibilities of displayed EE content. In the controller we added the following function:",
      "public function __call($method, $args)\n{\n$this->_helper->viewRenderer->setNoRender();\n$this->_helper->layout->setLayout(\"site\");\n\n  global $root;\nrequire_once $root . \"/public/ee.php\";\n}",
      "In this function \"site\" represents the general site layout that is defined in our Zend code which allows us to encapsulate the content in EE with our navigation menus. This also means that our work does not have to be duplicated in two places by creating the layouts in both Zend and EE (keeping with the DRY principal). And $root is a global variable defined in our application that points to the directory that contains our app folder which is where the controllers and the models and the views reside, and the public folder and other stuff such as the testing directory and other custom code. We use that $root variable to include the custom ee.php file that we talked about earlier. This is all that Zend needs for it to figure out that when someone goes to http://domain/ee/something that it needs to let EE figure out what to do with the something part and display that appropriate content. Now that this part is done, we need to configure EE to deal with the changes that weve made.",
      "**The Secret Handshake **",
      "Remember that ee.php file? Well heres what we added to make sure everything works nicely. At the top of the file theres a variable called $qtype, and the comments above it instruct you to set it to either 0,1, or 2. This variable controls a switch statement in the code that determines what EE should do with the url coming in (you know, that something part). Well we dont like any of the options, so we set the variable to 3:",
      "$qtype = 3;",
      "and then added another case in the switch statement:",
      "case 3 :\n$uri = str_replace(\"ee.php\", \"\", ltrim($_SERVER[\"REQUEST_URI\"], \"/\"));\n$uri = str_replace(array(\"ee/\", \"admin_ee/\"), \"\", $uri);\nbreak;",
      "the first line comes from the default behavior in the switch statement but now we have ee.php in there instead of index.php. As for the second part, well we want to take away the controller name from the uri that we added in there to get Zend to cooperate. We use an array because we could have multiple controllers in Zend that reroute to EE, in our case we have the EEController, so we neep to strip \"ee\" out of the uri, you could also have and admin EE controller name Admin_EEController, and you would use the second example \"admin_ee\". This is necessary because EE uses relative paths and constructive urls to figure out what to do and we dont want to confuse it with our controller stuff, it is PHP after all.",
      "There is one more thing that needs to be done. I just said that EE uses relative paths, well that means that links for anything that is produced by EE (like comments …) would need some tweaking. We need to have the name of our controller in the links for Zend not to choke, but we remove it once we get to the EE level for EE not to choke, so we need to tell EE to add the controller name in its links. We do that by specifying in the admin interface of EE (under CP Home/Admin/System Preferences/General Configuration  ) that the name of the sites index page is the name of the EE controller, so we changed that field to `ee/.",
      "…and voila, everything works nicely. Good luck and happy hunting!"
    ],
    "summary_t": ""
  },
  {
    "id": "ea3f86aedcbdea0554e7da27214d2f01",
    "url_s": "https://opensourceconnections.com/blog/2009/05/11/a-more-agile-cia/",
    "title": "A more Agile CIA?",
    "content": [
      "This past weekend was my first as a graduate student at the University of Virginias McIntire School of Commerce, where I am working on a Masters degree in Management of Information Technology.Â  It was a great 3 days of intensive classes on IT strategy, and I really enjoyed it.Â  Over the next year as I continue my studies, I will try to blog regularly about topics we are learning about.",
      "As part of the program, the faculty regularly bring in interesting guest speakers with CIO experience.Â  This Saturday was a great example, since Jill Singer joined us.Â  Ms. Singer was formerly the Vice President for Project Management at SAIC, and is now the deputy CIO of the CIA.Â  She gave a great presentation on the role of the CIO, and the process they use at the CIA for evaluating, architecting and implementing their internal IT projects.",
      "The CIA, despite the mystique and the fact that Ms Singer was not free to answer all the questions we asked, is still a lot like any other IT shop.Â  The process they follow for IT initiatives could easily be found in any Fortune 500 company.Â  In short, they follow these steps: understand the mission, establish the vision, develop the architecture, define plans, resource plans, execute plans, and measure progress.",
      "Sounds pretty traditional, right?Â  Many other federal agencies probably follow a similar approach, which sounds a lot like a spiral development method.Â  But even within the constraints of this process, I was very pleased to hear Ms. Singer talk about regularly using Agile methodologies.",
      "According to Ms. Singer, the CIA regularly uses Scrum, most often in 4 week development cycles.Â  Their customers, which would generally be some sort of internal analyst, really like the fact that Scrum encourages regular and tangible deliveries.Â  This allows them to try out the prototypes, and their customers also enjoy being able to add features and change priorities during each iteration.",
      "This has worked very well for them on many projects, and Ms. Singer feels that the move from a more waterfall style to Scrum has really helped them improve many of their projects, though with an interesting side effect.",
      "The biggest challenge she has seen on their is knowing when they are done, or as she put it, \"defining what 1.0 is.\"Â  They cant fund their projects forever, just like any other IT shop.Â  Sometimes they end up doing iterations indefinitely though, and then realize they have gone longer than they originally thought because they keep adding features.Â  But unlike most project methodologies, if they slip on a project schedule using Scrum, Ms. Singer has found their customers are much more forgiving then when a waterfall project is late.",
      "The reason for this is simple, and it is one of the fundamental advantages of Scrum and Agile, regardless of whether you are a start up company, a government agency, or even the CIA.Â  By engaging your business owners with burndowns, daily stand ups, and short iterations for which the customer helps set the priorities, you are empowering your customer.Â  Its important to note that this is done in a way that does not infringe on the creativity of your development team.Â  Your developers are likewise empowered by choosing what they work on and in what order within a sprint, setting their own estimates, and providing regular feedback and ideas directly to the customers.",
      "Its never good when a project is late, but if the customer has seen constant progress along the way, and they are empowered to help decide what features should be added or removed, then you have successfully created a collaborative environment between your customers and your IT staff.",
      "Determining \"what 1.0 is\" can be a real challenge – we just had a good discussion on that today with one of our current clients.Â  By employing Scrum, it sounds like the CIA has also learned the advantages of a highly iterative and collaborative process, and it is helping them to stay efficient and productive.Â  By the very nature of what they do, the CIA must be innovative, and so it should come as no surprise that they are using the latest in software development methodology.Â  I hope that other federal agencies will follow their lead."
    ],
    "summary_t": ""
  },
  {
    "id": "ca4d9af8b726c0a532791498c5f4eac2",
    "url_s": "https://opensourceconnections.com/blog/2009/05/12/programatically-capturing-web-pages-as-images/",
    "title": "Programatically Capturing Web pages as Images",
    "content": [
      "I dont normally post blog articles that are reposts of other content, but this email thread answered a question that Ive struggled with, which is how do you render a web page and save it as an image. I do this on HighTechCville, and our Fish4Brains RailsRumble entry a couple of years ago via thumbshots.org, but Ive never been happy with that service:",
      "At Sun, 3 May 2009 11:19:17 -0400,\nEric Pugh wrote:\nCool!\n\n  Its one of those things that seems like everybody wants it, but no\none has quite figured out. And the various \"services\" like\nthumbshots all feel kinda \"seedy\", I am always expecting to see\nadvertisements for viagra stamped on top of the screenshots and\nother questionable business practices.\n\n  It seems like you should be able to have the pages be render inside\nof a library such as WebKit, but I guess rendering is very\nintertwined with monitor displays and resolutions etc.\n\n  I have a research projects that aggregates info about people,\nevents, and organizations and Id love a better solution for linking\nin screenshots of the organizations and individuals site. Here is an\nexample using the thumbshot service for now..:\nhttp://www.hightechcville.com/organizations/318-worrell-water-technologies\n\n  Here is the text (thanks to Mark Phillips for this):\n\n  Khtml2png – http://khtml2png.sourceforge.net/ â€œKhtml2png is a\ncommand line program to create screenshots of webpages. It uses\nlibkhtml (the library that is used in the KDE web browser Konqueror).\nIn khtml2png 2.0.5 to 2.5.0, \"convert\" from the ImageMagick graphic\nconversion toolkit is used to create the output files in various\nimage file formats. 2.6.0 and future development will use the built-in\nconversion of the Qt library.â€ â€\" from the Khtml2png website\n\n  Pearl Crescent Page Saver –\nhttp://pearlcrescent.com/products/pagesaver/ â€œPearl Crescent Page\nSaverâ€ is an extension for Mozilla Firefox that lets you capture\nimages of web pages. These images can be saved in PNG format or (with\nFirefox 2) in JPEG format. The entire page or just the visible portion\nmay be captured. Options let you control whether images are captured\nat full size (which is the default) or scaled down to a smaller size.\nPage Saver uses the canvas feature that was introduced in Firefox 1.5.â€\nâ€\" from the Pearl Crescent Page Saver website\n\n  Webkit2png – http://www.paulhammond.org/webkit2png/ â€œWebkit2png is a\ncommand line tool that creates PNG screenshots of webpages. â€¦\nwebkit2png makes use of webkit, the rendering engine used in Safari.â€\nâ€\" from the Webkit2png website This utility is only available for Mac\nOSX because of the dependence on Safari.\n\n  Webshot – http://www.websitescreenshots.com/ â€œWebShot is a program\nthat allows you to take screenshots and thumbnails of web pages or\nwhole websites. It comes with a command line interface for advanced\nusers. The following image formats are supported: JPG, GIF, PNG, BMP.â€\nâ€\" from the WebShot website WebShot uses Internet Explorer as the\nengine for creating thumbnails of HTML files.\n\n  best,\nErik Hetzner"
    ],
    "summary_t": ""
  },
  {
    "id": "cd64df6c58ac5431538e2b718eaa9111",
    "url_s": "https://opensourceconnections.com/blog/2009/05/20/monkeyci-super-light-weight-continuous-integration-for-small-teams/",
    "title": "MonkeyCI: Super light-weight Continuous Integration for small teams",
    "content": [
      "At OSC, we have a well developed methodology that we apply to our client work, and one of the core tenets is using Continuous Integration to ensure our code behaves the way we intend it to.",
      "However, recently weve had two projects were the usual CI solutions such as CruiseControl etc havent worked out well, and we had to develop our own internal CI tool that we are ready to publish to the world called MonkeyCI.",
      "On the first project, which was a PHP based application with 5 full time developers, we used CruiseControl with the phpUnderControl addon. However, we were running CruiseControl on what turned out to be an underpowered hosted Windows server, and we kept getting build failure errors related to environmental difficulties. Now, if youve seen my talk about CI, you know how big I am on speccing a beefy server for CI, and this experience reinforced that lesson. We decided that migrating the CI environment to a bigger server was something that we felt was in the \"nice to have\" category, and that it could wait till the next iteration. But we needed something immediate. Enter MonkeyCI.",
      "The heart of CI is all about building the code, running the tests, and publishing the results frequently. Everything else, the reports, the red/green lava lamps, the pretty JavaDocs etc are all gravy. To meet the needs of your developers, you need to know if the \"bar is green\". So MonkeyCI does that in a decidedly low tech way:",
      "Everytime someone runs the full suite of unit tests they record the day and time, and put their initials. If the build is failing then they immediately fixed it. Weve played with writing the results in red for failing builds and green for successful builds as well. Then, each day at the standup someone highlights how the CI is doing, and verifies that multiple folks are initialing, which means that the tests are running on multiple systems successfully.",
      "While this does mean you have an additional manual process, its also really easy to do, requiring just a whiteboard! And for small project teams, the overhead of maintaining a reliable CI system is too much.",
      "Were doing another two developer project right now, and at least so far MonkeyCI has been great. We havent seen integration issues yet such as database scripts that dont run, or busted code being checked in. Ill post a picture of our whiteboard once we have a bunch of checkoffs recorded!",
      "We call this simple low tech process MonkeyCI because typically we refer to anything manual, such as testing by pounding keyboards as Monkey testing. Also, somewhat of a reference to the great developers at the Primate Programming Institute who I am sure would use this approach to CI!:",
      "",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "825930f3768f3a508318972676b95d59",
    "url_s": "https://opensourceconnections.com/blog/2009/05/22/arin-sime-awarded-cbic-leadership-award/",
    "title": "Arin Sime Awarded CBIC Leadership Award",
    "content": [
      "On May 21, 2009 at the Charlottesville Business Innovation Council annual awards dinner, Arin Sime of OpenSource Connections was awarded the CBIC Leadership Award for demonstrating \"exemplary leadership and [contributing] most significantly toward the organization€™s mission.€  Arin has served for the past year as CBICs treasurer as well as serving as the Chairman of the Awards Committee for the past two years.",
      "We at OpenSource Connections are thrilled with the well-deserved recognition that Arin is receiving for a job well done.  We look forward to continued great things from Arin in the future!"
    ],
    "summary_t": "CBIC awards OSC’s Arin Sime an award for demonstrating exemplary leadership contributing to the organization’s mission"
  },
  {
    "id": "bfe383a7d0936b4749563fb699ea386e",
    "url_s": "https://opensourceconnections.com/blog/2009/06/03/cbs19-blog-of-the-week-for-tuesday-june-9/",
    "title": "CBS19 Blog of the Week for Tuesday, June 9",
    "content": [
      "Ill be presenting the CBS19 Blog of the Week next Tuesday, June 9.  If you want to watch it live, Ill be appearing between 6:30 and 7:00 PM, and you can also watch it streaming on http://www.charlottesvillenewsplex.tv/.  Fans of Marijean Jaggers need fear not, for shell be back in action on the 16th freshly tanned from her (20 years delayed) honeymoon in Maui.  Ive already picked a blog for next weeks feature.  Who do you think it will be?  Who would you like to see featured in future episodes?  Drop me a comment and let me know what you think!"
    ],
    "summary_t": ""
  },
  {
    "id": "f019ab82386fe212309fb0aaef697733",
    "url_s": "https://opensourceconnections.com/blog/2009/06/19/solr-1-4-enterprise-search-server-to-be-published-in-july/",
    "title": "Solr 1.4 Enterprise Search Server to be published in July",
    "content": [
      "The book Solr 1.4 Enterprise Search Server by David Smiley and OSCs own Eric Pugh will be published next month from Packt Publishing.  You can find out more about the book at the Packt Publishing website.  Some of the topics that the book covers include:",
      "Blend structured data with real search features\n    Import CSV formatted data, XML, common document formats, and from databases\n    Deploy Solr and provide reference to Solrs query syntax from the basics to range queries\n    Enhance search results with spell-checking, auto-completing queries, highlighting search results, and more.\n    Secure Solr\n    Integrate a host of technologies with Solr from the server side to client-side JavaScript, to frameworks like Drupal\n    Scale Solr using replication, distributed searches, and tuning Working with Packt has been a real pleasure, as they have been understanding about slipping deadlines and Erics propensity to abuse his apostrophe key.  Were looking forward to the feedback from the book and seeing growth of the adoption of Solr.  Also, thanks to Dave Smiley for allowing Eric to work with him on the book!"
    ],
    "summary_t": ""
  },
  {
    "id": "532d20ccabc24c359c80a48f0016b679",
    "url_s": "https://opensourceconnections.com/blog/2006/11/11/better-itunes-song-deduping/",
    "title": "Better iTunes song deduping",
    "content": [
      "**Update 4/24/2011. TidySongs is now Rinse! Same great product, but new vendor with a deeper music database that seems to match better. Ive updated the links. **",
      "*Update 5/16/2010. When I am old and grey, I am going to tell my children about the most popular blog post I ever wrote, this one about iTunes deduping! I finally have to pass the baton of deduping on to",
      "TidySongs, a product from a company that coincidentally is in my home town of Charlottesville, Virginia!</p>",
      "",
      "Im happy to share my hacked version of Doug Adams Corral iTunes Dupes AppleScript. You can grab my version from here.",
      "His version uses some wicked fast PERL to find all the duplicate songs based on info in the Library, and then puts them all in a Dupes playlist. However, I wanted to filter by Name, Artist, and Album, and have a single playlist where I could delete in one fell swoop all the duplicates. So I tweaked his AppleScript to have another prompt that asks if you want all dupes to go in the playlist, or just copies 2…X to go in.",
      "I run it once with \"All\" to make sure I have what I want, and then rerun it with \"Just Dupes\" and build the playlist of files to delete. Works great, and Im mailing him a copy, maybe itll be in the 1.1 version!",
      "I was also very happy with the application Seek and Destroy Music Duplicates. It is written in Java using WebStart, and gave some good results as well. It is better in that you can pick which copy of the duplications you want to keep, and color codes each set of duplicates.",
      "The tool iDupe really didnt do that much for me, and the demo is limited to just doing 3 deletes. Seek and Destroy is a better choice."
    ],
    "summary_t": "Update 4/24/2011. TidySongs is now Rinse! Same great product, but new vendor with a deeper music database that seems to match better. Ive updated the links. ..."
  },
  {
    "id": "8a2ff9f5833d3981eda8d98b2cb9db1a",
    "url_s": "https://opensourceconnections.com/blog/2009/06/23/using-scrum-on-grad-school-projects/",
    "title": "Using Scrum on grad school projects",
    "content": [
      "Last week I finished Module 1 of the UVa McIntire School of Commerce grad school program Im in, for an MS in Management of Information Technology.Â  It was a great 2 weeks, and a lot of fun and hard work.",
      "",
      "The MSMIT program at McIntire is a 12 month program for working professionals, where we typically meet for 3 long days a month, with group work, projects, and lots of reading and research in between the monthly sessions.Â  Module One (or Mod1 as we call it) is different however.Â  This is our full time 2 week residency at UVa, where we study various foundational IT topics in class for 8 or 9 hours.Â  Topics like:Â  Database design, Data network design, UML modeling, the Zachman framework, Business strategy, Web 2.0, Datawarehousing, IT security, and more.",
      "",
      "But thats just during the day.Â  At night we work on a group project, in addition to some homework assignments, guest lecturers (we had some great speakers visit), and social events to network with our classmates.",
      "The group project for Mod1 was to come up with a Web 2.0 initiative for a health insurance company, design the product, research and estimate its business value, and prepare a presentation for the class.Â  At the end of the two weeks, we had two things to deliver.Â  First, we had to turn in a complete Zachman framework design of the product.Â  Second, we gave a presentation to our classmates as if they were the Board of Directors of the health insurance company and try to convince them to fund our project.",
      "",
      "I had the pleasure of being on a great team.Â  At the beginning of Mod1, each group had to complete some team building exercises, where we decided what the ground rules for our group would be, what our group and individual goals were, and how we wanted to operate as a group.Â  I found this to be a more valuable experience then I expected, and I plan to use it more often in projects at work.",
      "One important decision that our group made right from the start was to use a lightweight version of Scrum to organize our team.Â  Our teams goals were to be very efficient, and to focus on the quality of our work, not the quantity of time we spent on it.Â  Since two of our team members had previously used Scrum, and a third is a rugby player (Tim), using Scrum was a natural fit.",
      "We ran 1 day sprints over the two weeks.Â  We didnt keep a burndown chart, since that would be too much overhead.Â  Our planning sessions basically were to update a list on the wall with everything we were going to do that night, and then go around the team and let everyone pick their own tasks until all tasks had been assigned.Â  We didnt bother with estimates on those tasks since the granularity of the sprint was already down to just one day.",
      "",
      "At our next meeting, we would do a quick stand up to update the team on where we stood on our assigned tasks, and then we usually did a demo of our tasks.Â  This \"lightweight Scrum\" helped keep us very focused on our work and the tasks assigned to each of us.Â  At OpenSource Connections, we did a similar thing for the Rails Rumble last year, which is a 48 hour coding competition.Â  During the Rails Rumble, we would plan sprints of 2 or 3 hours each, at the end of which we would demo our progress to each other.Â  It was lightweight enough to work in a very tight timeframe while still keeping just enough process that we all stayed focused on the job at hand.",
      "The other efficiency thing we did was to \"time box\" every discussion we had.Â  Many parts of the group work involved design discussions about what we wanted to do for our projects, debating the pros and cons of it, reviewing each others work, etc.Â  To keep these discussions from going on all night, or wasting our time debating things in detail that might not even end up in the project, we would put a time limit on everything.Â",
      "Every day we tried to rotate a facilitator role, and that persons job was to keep the discussions going smoothly, and they were usually the person who would pipe up with \"how long do we want to spend on this topic?\"Â  We might set a time limit like 15 or 30 minutes, and then Adil would set the timer on his iPhone.Â  When the timer went off, we would cease debate on the topic and try to make a decision.Â  If necessary, we would extend the time, but the goal was always to stay within in a timebox so we wouldnt be up all night and could get some rest (or at least head over to the bar!).",
      "We had a lot of different personalities on our team, and lots of different perspectives.Â  But we got along great as a team and I think the agreement on ground rules and the use of Scrum helped out a lot with that.",
      "I think the way we came up with ideas worked pretty well too. We started by encouraging everyone to write any idea they had on yellow post-it notes, and stick it up in one corner of the wall (you can see Adil pointing at the post-it notes in the photo below). It didnt matter if it was a good or bad idea, the point was to generate as many as possible and get them on the wall. Similar to the way people propose sessions at an open spaces conference, each person would read their idea as they posted it to the wall. No criticism or debate was allowed at that point.",
      "",
      "The next day we grouped the post-it notes together by ideas that were similar or could be easily combined together into the same project. It was clear that we had a lot of ideas around mash-ups, since that was the largest number of post-it notes. But we didnt choose the idea based on quantity.",
      "Arash had drawn a map on one whiteboard of the goals, strategies, and problems at the health insurance company (see pic below), and we began to move the post it notes onto the whiteboard, lining them up with what goals/problems they addressed.",
      "",
      "At that point we finally started voting on what idea to go with. Everybody picked their 3 favorite ideas, and the three with the most votes were the finalists. We debated those three ideas in more depth, bounced ideas off Prof. Grazioli, and then picked our project idea.",
      "Although it sounds a little complex, I liked this process because it encouraged us to all be creative, to consider all ideas, to map those ideas to the goals of the project, and ultimately to create a consensus around the final idea. I have a tendency to push for my favorite ideas, and this process helped make sure I didnt force my ideas on the team, and ensured that we carefully and creatively considered all the ideas.",
      "In the end, our team ended up proposing an incentives program based in part on an iPhone app that would record your exercise and upload that data to the health insurance company, which would then convert that data into \"health points\" for use in contests with fellow employees to encourage healthy lifestyle changes.",
      "",
      "All the presentations were given on Friday, and the professors split the class into three different classrooms of 4 or 5 teams each (there were 13 teams total).Â  Each team presented to their classroom, and then the room would pick which team should \"advance\" and be one of only three teams to present to the whole class in the afternoon.",
      "Our team was really honored to be picked as one of the final three teams to present to the whole class.Â  It was a lot of fun.Â  I have to also tip my hat to the other two final teams since their presentations were not only excellent, but I was also very impressed with the depth of financial analysis they incorporated. (Im looking forward to future modules where Ill learn the finance skills to be able to do the same since finance is not my background).",
      "We kicked off our presentation with a video to illustrate the idea of our product.Â  We had a lot of fun producing it, and I think our classmates enjoyed it too.Â  Youll have to pardon the poor resolution – it was my first time using iMovie and a flip camera, and I didnt get the conversion right.",
      "Just for fun, I also took a few clips from our filming and made an \"out takes\" video.Â  We got to show the outtakes to the whole class too after our presentation, which I hope added a little playful humor to the end of Mod1. (Prof. Grazioli showed some very funny retrospective slide shows that were the real hit of the afternoon however!)",
      "Mod1 was a great experience.Â  I advanced my IT skills, teamwork and leadership skills, but perhaps most of all made some great friends through the shared experience of great professors, very interesting classmates, and of course sleep deprivation.Â  Im looking forward to Mod 2!",
      "Team 11 was:Â  Tim Bucher (Booz Allen Hamilton), Adil Qazi (Freddie Mac), Arash Sadati (International Monetary Fund), Arin Sime (OpenSource Connections) and Mark Widener (Virginia Air National Guard)."
    ],
    "summary_t": ""
  },
  {
    "id": "ab39722c8c4168a631e0ed8559217086",
    "url_s": "https://opensourceconnections.com/blog/2009/06/29/upcoming-osc-appearances/",
    "title": "Upcoming OSC appearances",
    "content": [
      "In prepping for tomorrows presentation on Social Media for the Shenandoah Ruby Users Group, Jennifer Till asked me where else we were going to be appearing at.  I feel like a concert promoter whos failed to promote his bands concerts.  We have several upcoming appearances; here are a few of them!",
      "June 30, Harrisonburg, VA: Shenandoah Ruby Users Group, Social Media 201 (Jason Hull)",
      "July 23, Las Vegas, NV: National Veterans Conference, Leveraging Web 2.0 Applications for Productivity (Jason Hull)",
      "August 27, Chicago, IL: Agile2009, How to Sell a Traditional Client on an Agile Project Plan (Arin Sime)",
      "October 21, Cambridge, MA: Software Test & Performance Conference, Setup and Testing with Continuous Integration (Eric Pugh)",
      "If youre going to be at any of these conferences or meetings, give us a shout!  Wed love to meet up with you!"
    ],
    "summary_t": ""
  },
  {
    "id": "ce930132dfed97ace1a1e92257485035",
    "url_s": "https://opensourceconnections.com/blog/2009/07/10/fill-out-my-survey-on-agile-and-ill-make-you-famous/",
    "title": "Fill out my survey on Agile and I’ll make you famous",
    "content": [
      "Well … famous in one room of a conference at least.",
      "I just posted a google form survey looking for feedback on how people are \"selling\" the use of Agile in their companies and with their clients. You can see the survey and fill it out here:",
      "http://tinyurl.com/SellingAgileSurvey/",
      "I will be using the data and stories I gather from this survey in a presentation I am giving at the Agile 2009 conference in August entitled \"How to sell a traditional client on an Agile project plan.\" By default, all quotes I use from the survey will be used anonymously unless you specifically want me to make you famous.Â  So feel free to tell me what you really think.Â  :^)",
      "You dont have to be a developer or consultant to participate in this survey!Â  If you are a project manager and someone has tried to convince you to use Agile methods, there are questions for you here too.Â  In fact, if they were unsuccessful at convincing you to use Agile, you probably have a story even more valuable to me.",
      "If you have the chance to answer this survey before Wednesday July 15th that would be wonderful since Im doing a dry run of my presentation on the 16th to AgileCville.Â  But in reality, if you can answer anytime between now and the end of the month that would be awesome.Â  You are also more than welcome to forward this on to others you think may be interested.",
      "http://tinyurl.com/SellingAgileSurvey/",
      "Ill post some of the results on this blog later in August (respondents names and other identifying information will be removed from any results that are posted).",
      "Thank you!"
    ],
    "summary_t": ""
  },
  {
    "id": "1f4f0269015700f1f790cd76810b6800",
    "url_s": "https://opensourceconnections.com/blog/2009/07/14/arin-sime-to-speak-at-agilecville/",
    "title": "Arin Sime to speak at AgileCville",
    "content": [
      "",
      "Ill be speaking to this Thursdays #agilecville group meeting, about \"How to sell a traditional client on an Agile project plan.\" I have 11 strategies that I will go over for how to convince your clients that Agile methodologies are the way to go, and I think they cover a variety of different styles so there should be something for everyone.",
      "When: Thursday July 16th, 6:00 PM",
      "Where: Jefferson Room, 3rd Floor, Charlottesville Central Library (http://www.jmrl.org/li-meeting.htm)",
      "More info and signups are on the AgileCville Google groups page.",
      "This is all part of my lead up to speaking at Agile2009, and Im really looking forward to tapping into the experiences and brilliance of my fellow Agile practitioners here in Charlottesville.Â  If you havent already, please take a moment to fill out my survey on how to sell Agile.Â  Ill use the information I gather from this informal survey as part of my presentation.",
      "I hope to see you there!Â  OpenSource Connections will provide pizza and soda!"
    ],
    "summary_t": "Ill be speaking to this Thursdays #agilecville group meeting, about \"How to sell a traditional client on an Agile project plan.\" I have 11 strategies that I ..."
  },
  {
    "id": "5ff3a9a3f9f019f1348b7c0d9aeeb295",
    "url_s": "https://opensourceconnections.com/blog/2009/07/15/futureruby-characters-welcomed/",
    "title": "FutureRuby: Characters Welcomed",
    "content": [
      "To celebrate my one year in the \"real world\", I went to Toronto to attend the FutureRuby conference. The format of the conference is consistent with all other conferences Ive been to, the unConference with an unThink style of philosophy. This was also my second Ruby specific conference, and the first one I attended, RubyDCamp, was a great experience so I was looking forward to this one. There many things that got me excited about the conference, the first one was the website:",
      "",
      "another one was the conference motto:",
      "People who program in Ruby arent like other coders\n\n  We are the artists, philosophers, and troublemakers. We realize that the fringe of today is the mainstream of tomorrow. We grease the engines of progress, even when were working outside of the machine.\n\n  FutureRuby isnt a Ruby conference, but a conference for Rubyists. This is a call to order – a congress of the curious characters that drew us to this community in the first place. We have a singular opportunity to express a long-term vision, a future where Ruby drives creativity and prosperity without being dampened by partisan politics.",
      "The conference location was also an added benefit. Of course it was more costly to go to Canada, but I like to travel and this was a great chance to visit a new place. The conference itself, or activities related to the conference started as early as Thursday with a iPhone app development class. Friday was the early registration day and opening party time. I got into the city Friday night, so I missed that days events. But of course everyone was talking about the party and the roof top view the next morning at breakfast, so check out this video about it:",
      "",
      "",
      "",
      "For me the conference started Saturday morning when I picked up my bag of schwag:",
      "",
      "",
      "",
      "Then was the opening note by @peteforde who kicked off the weekend by expressing how he wishes that this conference is something that will help rubyists everywhere shape the future of the community. He was then followed by this great list:",
      "Speaker: Nathaniel Talbott\nDescription: The revolution isnt free – none ever is. If we want to keep Ruby real and not have the life sucked out of our community by soulless corporations we have to learn to take value and turn it into cash. And yes, we can learn!\nNotes: Great starting presentation, funny and insightful. This presentation eased my worries that the talks will be very technical and over peoples head and I wont be able to benefit from them. The speakers biggest point is owning your work and your tools. Do not let the corporates dictate the future. Developers are the ones who use the tools and theyre the ones who should decide what to use. See the presentation here.",
      "Speaker: Ilya Grigorik\nDescription: Tokyo Cabinet offers a great many features right out of the box: key-value store, ordered traversal, attribute search, schemaless data structures, and even indexing. Well explore these features with hands on examples and then delve into the advanced and little known feature of TC: ability to script it with Lua! Well explore a number of lean & mean recipes to take TC to the next level. A cache server you say? Perhaps a graph database?\nNotes: Of course everything conference I go to, theres a new database engine being introduced. This one was cool though and Im looking forward to see it get widely used.",
      "Speaker: Austin Che\nDescription: I will discuss a programming language that makes Ruby look like childs play. The language of life, DNA, has shown its robustness and expressiveness through billions of years of pervasive use. Engineers have recently begun to use DNA to reprogram life to create a myriad of novel biological systems. Biology is currently at the tip of a revolution similar to that of electricity and magnetism at the beginning of the 20th century. The electrical engineering revolution has allowed non-physicists to program in high-level languages like Ruby by distilling classical physics into a set of engineering design principles. Similarly, the emerging field of synthetic biology applies engineering principles to biology. Efforts to bring modularity, interchangeable parts, abstraction and standardization to biology is beginning to allow non-biologists to quickly and predictably design and build biological systems. Soon, it may become childs play to program with DNA.\nNotes: This talk was designed to blow your mind away, in many ways. Its one of those thing you go \"Hunh?!?!\" about because of both how hard and technical it is and how cool it is. After the talk everyone started googling for websites where they can program DNA to spell their name. I had a talk with Austin Che at the Pravda Gala and hes a very cool guy. It was great presentation about biology and programming combined.",
      "",
      "Speaker: Anita Kuno\nDescription: Usain Bolt revealed his nutrition sources to a Canadian journalist; KFC, McDonalds and Chinese Food. If the worlds fastest man doesnt bother eating healthy food, why should we? Ah, because we know something he may not. We know about Version Control!\nNote: This was an interesting talk. I personally was expecting Anita to tie it in to Ruby or at least programming more than what she actually did. It did fit the theme if you looked at it this way: the future of ruby depends on healthy rubyists and the health of rubyists depends on nutrition. I did learn one interesting fact though, I can influence the structure of my body and specifically my bones over the next 10 years with what I eat. Something to keep in mind for those of us who sit all day in front of a computer which damages our back.",
      "Speaker: Foy Savas\nDescription: In a world were the boundaries of bytecode define your allegiance, one speaker will challenge our assumptions and defy our prejudices while writing a talk summary that reads more like a movie trailer. Because though the future is coming, who knows if itll be ours? Will the right tool for the job prevail or are we facing what seems to be the inevitable rise of the virtual machines? Foy will tell us where he thinks were all going and how we might avoid such disaster.\nNotes: Hands down the best presentation of the day. Foy used his slides very well and the talk had a fast pace. The topic of his talk is something I have been discussing with people at OSC for a while. The ability to use multiple languages in our application to benefit from each languages strength is something that would please many developers. Of course Foy had everyone tweeting about crack during his talk when he mentioned how he went from C++ and Rack to C++Rack to CRack.",
      "Speaker: Misha Glouberman\nDescription: Misha Glouberman is a performer and artist based in Toronto. For the past several years, he has presented a series of events called Terrible Noises for Beautiful People. These are participatory improvised sound events, where groups of non-musicians make sounds together. Misha will talk a bit about these events, and see what sorts of sounds the FutureRuby conference can make. You can read more about these sound events at schooloflearning.org\nNotes: Cant tell you about it. Its a FutureRuby secret.",
      "Speaker: Ron Evans + Damen Evans\nDescription: No description available. FAIL.\nNotes: The future Wright brothers? The introduced us to flying robots and how it easy it is to download the flying_robot gem (http://github.com/deadprogrammer/flying_robot/tree/master) to use for controlling robots through RAD (Ruby Arduino Development: http://rad.rubyforge.org/). There were cameras and blimps involved too!",
      "Speaker: Brian LeRoux, Brock Whitten + Rob Ellis\nDescription: The future of Ruby is the same as the future for all computing: the mobile web. Ruby developers need to make sure they are prepared to take their skills mobile. And, currently, building mobile web applications is a pain in the ass. In this presentation, Brian LeRoux, Brock Whitten and Rob Ellis will introduce shortcuts for building device neutral mobile applications with PhoneGap and other techniques for smuggling our precious Ruby onto iPhones, Androids and elsewhere.\nNotes: Mobile development is probably the hottest topic these days. With that comes the debate about which platform is the best to use and develop for. Unfortunately, the iPhone, Android, Blackberry phones and Nokia phones virtually have nothing in common. What this means is that if someone wants to develop an application that works on all these platforms, he is forced to develop a specific version for each platform. What these guys presented is a framework that allows you to develop an application once and have it work the way you expect to on all these devices. The interesting note from this presentation was the philosophy behind their work. They stated that the goal behind PhoneGap is for PhoneGap to stop existing. Sounds counterintuitive at first but its a great way to approach things. What they want is for their work to be integrated to the core of the platforms, moving towards more of a standard.",
      "Speaker: Adam Blum\nDescription: Rhodes is an open source Ruby-based framework for building locally executing, device-optimized mobile applications for all major smartphone devices. These applications work with synchronized local data and also take advantage of native device capabilities such as GPS, PIM contacts, camera, and SMS. Yet you write the majority of your interface with high productivity in HTML and Ruby. Rhodes allows you to write an app once and it will then run on all iPhone, Windows Mobile, BlackBerry, Symbian and Android smartphones. During this session well build a sample app for all mobile devices, from scratch, in minutes.\nNotes: This was the second mobile development presentation in a row and was the last of the day, so by this time my brain was wondering off. I was ready to get out of that conference room. The presenter didnt help his case either. Eh, it happens!",
      "Now on to the juicy part, the Pravda Gala. Before walking in to the bar I realized that there was some sort of connection with the name and the vodka with the theme of the conference, but oh boy was in for a big surprise. This bar was completely themed after Soviet Russia. Now I did show images of the conference theme, but this took it to another level. This conference showed me the Ruby community in a different light. This bunch of people have a unique sense of humor, something that seems to be a requirement to get by in this field of work. This party allowed the alter egos of everyone at the conference to be free. You needed to be there for the conversations to understand this more but the images should suffice.",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "I screwed myself when I booked my return flight before the schedule for the conference was out. I booked my flight for 2:30pm on Sunday. Which meant that I missed a good part of the Sunday lineup which was:",
      "Speaker: Collin Miller Description: Much of what code editors do cover up for the gross impedance mismatch between machines and code. The artifacts of code misshape language design, system architecture, \"the community\", and the craft as a whole. Cast off the robes of the textual high-priesthood and think about constructing programs instead of encoding them. Notes: Did you even think that text was next the best mean of representation? Collin thinks so, and hes not talking about using images to describe something visual, hes talking about programming and writing code in something other than text! Interesting talk.",
      "Speaker: Dr. Nic Williams Description: I will be a developer for another 37 years; and by that time I might have created or maintained 500 to 1000 open source projects. In the last 3 years it is already a become maintenance challenge and in another 37 years I may be begging to retire. That is, unless we can solve the problem today: how to go from 1 to 1000 open source projects and still enjoy yourself. This talk will challenge much of the dogma of open source and will make you rethink what is open source? And how the hell do you live with it once its written? Notes: awesome presentation! I wish I could get to a point where I can do what Dr. Nic was preaching, which simply was: develop a project that you want to use to solve a problem you want to solve, then ditch it. If youre the creator of a project it doesnt mean youve inherited it for life and it doesnt mean that the project cant survive without you. You CAN retire!",
      "Speaker: Matthew Knox Description: The Milgram experiments revealed a number of exploitable weaknesses in human psychology, and demonstrated that our collective human intuition drastically overestimates the difficulty of getting ordinary people to do extraordinarily awful things. Im going to talk about those weaknesses, their exploitation, consequences, and aftermath. Notes: Good talk about owning what you do and being responsible for the code we unleash on the world. I had to leave before the end of this talk, sorry Mat!",
      "Speaker: Paul Dowman Description: If you dont engineer a battleship will you be swallowed by the Failwhale? Wheres the middle ground? A discussion on the philosophy and practice of staying light and nimble without falling down under load.",
      "Speaker: Joseph Wilk Description: Saucy Multilingual Cucumber seeks Fun and Frolics. Good-looking plaintext acceptance testing framework seeks meaningful relationship with devs, testers and non-techies too. Cucumber to my friends, Cuke to my lovers, I yearn to help you strive towards your business value. Very open minded, I enjoy a good web framework but am willing to get funky with whatever tickles your fancy (iPhone, Erlang, GTK and even Java). Most of all, Id love to watch you, erm, refactor, and I can keep you safe while you achieve your business and coding dreams. Must speak one of my 23 languages or help me learn a new one. P.S: Dont be shy, Joseph Wilk knows me as intimately as anyone – hes been like a father to me – hell show you how to treat me right. Dont worry though – Im easy to pick up and I dont bite (unless you want me to).",
      "Speaker: Avi Bryant Description: No description available. FAIL.",
      "Speaker: Jonathan Dahl Description: Art, music or words, and software too find limits freeing. Is less more?",
      "Speaker: Francisco Tomalsky Description: Francisco is a co-founder of 280 North and the creator of the Objective-J programming language. 280 North is bringing desktop-class applications to the browser with their new open source framework, Cappuccino. They recently launched 280 Slides, the first application built on Cappuccino. Before 280 North, Francisco was an early member of the iPhone team at Apple, working on Mobile Safari and Maps.",
      "Speaker: Jesse Hirsh Description: It is important that we understand the history of Imperial California and the means by which its ideology infects us all. From Hearst and De Young to Kevin Kelly and Chris Anderson the Californian Ideology is the hegemony that prevents other great cities or ideas from rising. Intrinsically we fight these ideas with our own, yet doing so blindly prevents us from seeing who struggles with us, and with whom we stand in solidarity. This presentation will help Future Ruby understand its role in ending Californias reign.",
      "All in all it was great conference. Those who werent there missed out. I hope there will be a future FutureRuby conference (no pun intended)!",
      "",
      "Credit is where credits due: Thank you @peteforde and @meghatron and @juliehache and @hyfen and all the other organizers/volunteers for a great conference. Thanks for all the speakers and presenters. Thanks for all the rubyists and non-rubyists who attended. And thanks to everyone on Flickr who provided great pics, including: aquateamhungryfort, luismi_cavalle, soukias, Edward OG, rtlechow, Austin Ziegler and Leftist (in no particular order). For more images check out: Flick",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "05a1cd46baf65e3dc81645aea316e104",
    "url_s": "https://opensourceconnections.com/blog/2009/07/18/software-theology/",
    "title": "Software Theology",
    "content": [
      "You might be aware of its existence, even if only on a subconscience level. It might even be a scary thought that Software Cults exist or worse that you could be a part of one. Whether its political, social, religious or even software related, people tend to cling to their opinions, beliefs no matter how illogical they are at times. Not only that, they sometimes fail to see how annoying or how dangerous being blindly overly passionate coercion of others can be. The point of this blog is to point out how people get religiously caught up in software and that can negatively impact a product, a persons image, business value, and the bottom line.",
      "####",
      "SoftwareÂ Zealots",
      "You have probably run into them or perhaps you are one. They are not technology evangelists or experts who know the ins or outs of a certain hardware/software and hired to sell the product, they are the nutcases, overly righteous zealots for their software cause. Beware of having your own original opinion or preference around these people, especially if it differs from their own opinion, it can be detrimental. They spout company rhetoric like it is sacred text and they will spend hours and loose days suffering in pain to fix their beloved software and not speak a word against it.",
      "They either constantly build up the image of their beloved software or tear up anything that isnt up to their understanding or standards. It comes in many forms, they detest pieces of software where its websites like myspace, facebook, linked-in, Â or software like aim, twitter, wordpress, expression-engine, etc. Or they highly praise their software and then mock others in a mighty python like fashion.",
      "One of the bigger groups that tend to stray into this area, especially in Charlottesville, are Mac Users. \"You never have to reboot a mac\". \"Macs are easier to use than windows/linux\". \"Macs are more secure\". While I can tear 90% of a macs zealots comments to shreds, its not worth the waste of thought or life. However I would like to point out the negative affects. For me personally, I spent 4 years in a mac lab so I know how to use a mac (esp after living on photoshop, cool edit pro, Quark express, etc). However, I use a pc because I tend to develop using C#/Ruby/Javascript and I like to use visual studio when possible. It is a preference. But every time a zealot makes a serious snide remark against anything non-mac, my opinion and my trust in their judgement and ability to logically weigh things drops tremendously. Their value and ability to make the right decision for clients rather than themselves dramatically falls short after seeing something like this.",
      "In light of the above Ill probably never buy a mac for 2 key reasons at the moment. (but hey apple if you want to buy me one, ill use it to make screen casts).",
      "I cant run OSX on virtual box while running windows/linux as the host, not only is it not currently possible, its also illegal.\n  The remarks that typically come from overly zealous fans of mac have left a bitter taste and venomous vibe, that I do not want to ever really be associated with them. (which is the opposite of affect of what some zealots hope to accomplish).",
      "But extreme occult like fandom isnt the only place where strong belief systems and software intersect. Legality of software, source code, languages, tools, architechture, software theory and strong opinions often cause heated discussions, debate, even split in software teams or even cost tax payers huge sums of money because someone let personal preference or pride rein where it should have let logic prevail. It is actually interesting to see how much of human emotion and systems of beliefs still comes into play even among geeks and programmers. You would think people of this nature would have more detachment from these things that hold only so much meaning in life.",
      "####",
      "Prejudice against Software due to its Company",
      "An example of recent biased behavior from a legality/source code sense, thats been making public waves is Richard Stallmans stance against Mono. Microsoft is a corporation out to make money (obviously), they can no longer afford to battle opensource software, they have to embrace it. In fact, theyve launched sites codeplex, port 25 and helped Mono development moonlight so that they could bring silverlight to the linux platform and  released C# and CLI under the community promise. They would stand to lose and enrage their developer base at this point should Microsoft decided to ever go back to trying to destroy Open Sourced software. Also with such markets as Software as a Service, Operating in the Cloud, and Selling Advertising at stake, they dont have the time or resources to waste on it. Granted this shift has probably cause heartache to hardcore proprietary ms fans of old. oh well. However Stallman still lives in a world where the evil empire patiently waits to spring its trap and forever dominate software so that it can never be free.",
      "Software Language (This translation must be Gospel)",
      "People get in a habit of saying this language beats all. But you know, I just dont see ruby beating out Java in building a performance search tool like lucene. I dont see .Net beating out Java in available open source projects (though .net has come along way). I dont seeing Java beating out .Net or Mono for building user friend thick client desktop applications. Each tool or language has its uses, which should be considered depending on the business value and the goals of the company, not what the developer prefers. Like Twitter using Scala, its not entirely replacing the rails application, but it is using Scala to help scale twitter, cause its compiled into byte code and runs faster. It has business value and its using the tool for its particular strengths for certain part of the whole. Though you cant beat using ruby and rails for prototyping a website quickly.",
      "####",
      "Free Software/Tools, (the best things in life are free)",
      "If you believe that, you obviously skipped economics in college or missed out on the discussion of opportunity cost. Paying $200 for a piece of software that will save you hours of work during a project or daily will actually having a higher return on investment in opening up time which is a valuable commodity. So many people are stuck on the concept of free they dont realize free is costing them precious time of their lives they could be doing something else more important. So weight the actual cost of time, efficiency, vs the importances of free software tools that dont work as well or free open source solutions when you can buy source code that has a better performance. Also weigh the converse.",
      "####",
      "Software Commandments",
      "Some software commandments to maul over in your moments of free thought (thats if you can think for yourself).",
      "thou shalt not slander thy neighbors preference in software/hardware.\n  thou shalt not force a new language dependency on a project, if it does not provide business value. i.e. a ruby script in otherwise totally php project.\n  thou shalt not cost tax payers or businesses extra money just because you like a certain os or language.\n  thou shalt detach software predjuice for your clients well being.\n  thou shalt weight opportunity cost and business value before ruling out open source or properitary software.\n  thou shalt only use logic when comparing technology versus spouting marketing proproganda unless you just want to look stupid (cough ie cough)….\n  thou shalt get a life.\n  thou shalt take out the Holy Pin, then shalt thou count to three, no more, no less. Three shall be the number thou shalt count, and the number of the counting shall be three. Four shalt thou not count, neither count thou two, excepting that thou then proceedest on to three. Five is right out. Once the number three, being the third number, be reached, then lobbest thou thy Holy Hand Grenade of Antioch towards thy foe, who being naughty in my sight, shall snuff it. Amen",
      "Any disregards for these commands and thou shalt be forced to build a giant gundam without food or sleep within 30 days or sent to the rain forest without your iphone, mac, pc, palm pilot, black berry, etc for a year.",
      "any commandments that should be added? thoughts?"
    ],
    "summary_t": ""
  },
  {
    "id": "e1590ce21c43d85f650e15801cf9bed9",
    "url_s": "https://opensourceconnections.com/blog/2009/07/19/my-first-plugin/",
    "title": "My First Plugin",
    "content": [
      "Working at a small company like OSC allows for group bonding \"work\" activities such as RailsRumble, which create a good working environment and helps develop our skills. If you dont know what RailsRumble is, its a competition where teams of 1 up to 4 members develop a Ruby on Rails application in 48 hours from scratch. If you dont know what Ruby on Rails is, you cant be saved :P. The competition is fun, or at least the OSC team is fun (no bias what so ever) because you get together with a group of people, sometimes teams share office space, and work your brain until you pass out. One of the things this competition teaches us is managing time. What you can do in 48 hours might vary from person to person, but its still 48 hours people!",
      "Our entry for 2008, see blog posts here and here, used a SMS service called Zeep Mobile. We needed to be able to communicate with our users, specially those who would be using our app on the go, and texting was the best solution. We came up with the idea for our app the day the competition started, so we didnt have time to do a lot of research on providers and available APIs for such a service. In retrospect, Zeep Mobile still seems the best free solution there. The one problem with this service is that the learning curve turned out to be too steep for the 48 hours we had.",
      "One of my teammates was charged with handling the integration between Zeep Mobile and our application. That task took about a day, half the time we had to build EVERYTHING! That is too much time. So a light bulb lit above my head. I always wanted to contribute to the rails community but have never had the way to do it. I have no experience building plugins or gems, so this was a good project to learn. Unfortunately, I discovered what my teammate had a year ago, how hard it is to integrate Zeep Mobile!!!",
      "No Worries! ZeepIt is here to help :D",
      "My first plugin, ZeepIt, is designed to get your app ready to go with Zeep Mobile. All you have to do is install the plugin and you automatically have a URL for Zeep Mobile to forward SMS to and a way to parse those texts. The plugin uses the zeep/messaging gem provided by Zeep Mobile to provide a the MVC structure needed to leverage the service.",
      "The plugin is available on GitHub: http://github.com/ychaker/zeep_it/tree/master\nDocumentation is available on Rdoc.info: http://rdoc.info/projects/ychaker/zeep_it \nReferences:",
      "RailsCasts\n  Ruby on Rails guides\n  RailsMagazine\n  Zeep Mobile\n  Google (of course)",
      "If you have any comments, questions or concerns please feel free to contact me which ever way you like. I would love to hear your feedback. ZeepIt is also on twitter @ZeepIt. (by the way, as I said, its my first plugin, so be gentle)"
    ],
    "summary_t": ""
  },
  {
    "id": "6c79a6389da875a800780ec1ed63a9fb",
    "url_s": "https://opensourceconnections.com/blog/2009/07/21/wrap-up-from-agilecville-meeting-selling-agile/",
    "title": "Wrap up from AgileCville meeting – \"Selling Agile\"",
    "content": [
      "Last Thursday I had a great opportunity to speak at AgileCville, and give a dry run of my upcoming presentation at Agile 2009.",
      "Eric took this photo of me speaking.Â  It looks like Im doing The Robot.Â  Im not, but that would have been cool.",
      "Id like to thank all those who attended. I really enjoyed the chance to share my thoughts on \"How to sell a traditional client on an Agile project plan.\" I got a lot of great feedback from everybody. Im flattered how much of it was positive, and I also got a lot of very helpful constructive feedback that I will definitely incorporate as I polish my slides for Agile 2009. Most notably perhaps, Im going to take a crack at improving the quality and consistency of the graphics in the slides.",
      "The slides are available online here. I also brought my voice recorder with me, and Ive posted very roughly edited audio from the meeting here. The presentation was about 35 minutes long, and then we had a very interesting and valuable discussion session for about an hour after that. If you try to listen to the audio, be forewarned that not everyones voice comes through clear in the second half when were having our discussion.",
      "Im really looking forward to seeing everyone at Agile 2009!"
    ],
    "summary_t": ""
  },
  {
    "id": "ef792f2a5ae58f3865519767aebeb7a7",
    "url_s": "https://opensourceconnections.com/blog/2009/07/29/arin-sime-to-speak-at-edui2009-on-facebook-applications/",
    "title": "Arin Sime to speak at EdUI2009 on Facebook Applications",
    "content": [
      "Im very pleased to be co-speaking with Wayne Graham at EdUI 2009 at the University of Virginia this September 22nd.Â  EdUI 2009 posted the full conference schedule today, which included the talk Wayne and I will give.Â  Heres the description for our talk:",
      "The Facebook API:Â  Thinking about UI in a social way\n\n  Building an application for the Facebook API is very different than your standard application. The basic concepts and flow of your application need to conform to underlying principles of social media in order for people to use your application and share it with their friends. We will discuss the development and implementation of Facebook applications based on our own experiences and drawing on the best practices of other projects. Wayne will discuss his implementation of the Facebook-Athenaeum project, and Arin will discuss his experiences building an application for fundraising on Facebook. The presentation will be a mixture of high level design concepts, details on the Facebook API, and code examples.Â",
      "Well be speaking on Day 2, September 22, from 2:15 – 3:00 pm.Â  Im really looking forward to hearing all the great speakers at this conference, and Im honored to have a chance to present with Wayne at EdUI!"
    ],
    "summary_t": ""
  },
  {
    "id": "d3de21b4642f1e70b34a693ce879b241",
    "url_s": "https://opensourceconnections.com/blog/2009/07/29/have-you-ever-written-an-rfp-the-agile-way/",
    "title": "Have you ever written an \"RFP the Agile Way\"?",
    "content": [
      "Scott Ambler wrote a very interesting article for Dr Dobbs Journal recently entitled \"RFPs the Agile Way – or – Fear and Loathing in the Procurement Department.\"Â  I first came across the article when Deborah Hartman Preuss wrote an excellent commentary on InfoQÂ about Scotts article, entitled \"Using the RFP Process to Hire Agile\".",
      "As Ambler points out, the use of a formal RFP often implies that the client uses a traditional development method like Waterfall.Â  Procurement writes an RFP, and then the consultant responds with a detailed proposal dictating timelines and cost estimates, and perhaps an up front design.Â  All before ever meeting with the client.",
      "The RFP process is a necessary evil of the consulting world, particularly in government contracting.Â  ButÂ RFPs dont lend themselves well to the very collaborative discussions you can have with a client when you are using an Agile process, or if you are securing the contract in person rather than electronically.",
      "This topic relates somewhat to my upcoming presentation at Agile 2009, \"How to sell a traditional client on an Agile project plan.\"Â  Although a lot of the advice I give in that talk can be done in proposals, most of it works better in person.Â  And so I think thats a real challenge to accurately respond (in anÂ Agile way)Â to an RFP that assumes a waterfall development model.",
      "At firstÂ I didnt quite find the answers I was looking forÂ in Amblers article.Â  I dont say that as a knock against the article at all, because it is well worth the read and I think it frames very wellÂ the problems with RFP processes.Â  However, Ambler is mainly encouraging procurement people to change the way they structure RFPs so that Agile people can respond to it better.",
      "Im not sure the procurement people are going to read his column or see the incentive to do so (which he acknowledges, and asksÂ readers to send this article to procurement people they know).Â  And so while IÂ believe my Agile 2009 talk offers some strategies for how to positively discuss Agile in a proposal, Im still in search of the holy grail.Â  How exactly do you respond to an RFP \"the Agile Way\"?",
      "But after reading Amblers article again, perhapsÂ he does lay out some of the answers Im looking for.Â  He just does it from the opposite perspective.Â  He suggests this advice for what procurement people should look for in an Agile response to an RFP:",
      "(rather than lift all his text, Ive just included the highlights below.Â  I suggest you *read his article for the full explanation)*",
      "The supplier should provide resumes\n    Follow a reasonable set of rights and responsibilities for both the customer and the supplier to adhere to … how they will collaborate together.\n    Propose how they will work with the customer.\n    Produce potentially shippable software to the customer every X weeks.\n    Allow the customer to monitor their work and work products.\n    Follow the customers corporate development guidelines.\n    Do full regression testing throughout the lifecycle.\n    Provide a minimal set of deliverable documentation, particularly user documentation, operations documentation, support documentation, and high-level systems overview documentation. The customer should provide examples of such documentation if theyre available.\n    Shared-risk strategy [for payment]Â which is fair to both customer and supplier.\n    Indicate rough estimates and schedules, in the +/- 30% range.\nThats a great list of ideas for consultants to incorporate into proposals, even if the customer is not looking for them.Â  Im glad to see that we already do many of them in our standard proposal template at OpenSource Connections.Â  In particular, we always talk about our \"Open Approach\" to projects,Â ie, the highly collaborative and open way that we work with our customers using Scrum.",
      "This is all a sign of the increasing maturity and adoptionÂ of Agile processes, and if nothing else, thats a good thing.Â  Perhaps the procurement people will sign on eventually after all."
    ],
    "summary_t": ""
  },
  {
    "id": "26de0b9df0142f480c294867c27a9cbb",
    "url_s": "https://opensourceconnections.com/blog/2006/11/29/labsapache-now-open/",
    "title": "[email protected] now open!",
    "content": [
      "During 2006 the Apache Incubator process has really taken off. Incubator is a place for existing projects to enter the Apache Software Foundation, and build up community around them. Once projects have attracted committers they then graduate.",
      "However this has led to many Apache committers (like me!) who have new ideas going off to places like Google Codes Project Hosting and SourceForge, versus going through the paperwork of the Incubator process.",
      "So, to provide a low cost place to start a project, Apache Labs was born. It is meant as a place that any ASF committer can start their own projects, but still leverage all the community and infrastructure that Apache has to offer!",
      "Hats off to the Apache Labs PMC !"
    ],
    "summary_t": ""
  },
  {
    "id": "1e6af8048b80e5fd30aa21b1a8c8d869",
    "url_s": "https://opensourceconnections.com/blog/2009/07/31/matthew-sposato-upcoming-presentations/",
    "title": "Matthew Sposato – Upcoming Presentations",
    "content": [
      "This fall I will present at the following events:",
      "October 3rd – Richmong Code Camp\n  October 8th – Roanoke Valley .NET User Group\n  October 15th – Charlottesville .NET User Group",
      "Hope to see yall there. Thank You"
    ],
    "summary_t": "This fall I will present at the following events: October 3rd – Richmong Code Camp October 8th – Roanoke Valley .NET User Group October 15th – Charlottesvill..."
  },
  {
    "id": "a9bb01fe58452511fbb73c9d27b972e0",
    "url_s": "https://opensourceconnections.com/blog/2009/07/31/presented-to-the-richmond-software-craftsmanship-group-073009/",
    "title": "Presented to the Richmond Software Craftsmanship Group – 07/30/09",
    "content": [
      "Last night I had the pleasure of presenting ASP.NET Dynamic Data to the Richmond Software Craftmanship Group. Dynamic Data is a rich scaffolding framework that instantly web enables a database. As you might expect, it creates a database centric website. That was my first demo. My second demo showed how a Dynamic Data website can be customized and extended to become more user centric, perhaps following the users normal workflow. The slides and demo projects can be downloaded from my blog. The group discussedsome useful ways to apply Dynamic Data, such as simple reporting site, as a temporary application or perhaps to demonstrate a database."
    ],
    "summary_t": ""
  },
  {
    "id": "2479087921ce6a94882c4309d825299c",
    "url_s": "https://opensourceconnections.com/blog/2009/08/04/5-reasons-the-marines-shouldnt-ban-social-networking/",
    "title": "5 Reasons the Marines Shouldnt Ban Social Networking",
    "content": [
      "Today, the United States Marine Corps announced that it was banning its Marines from using social networking on the job.  Citing security risks, the Marine Corps has said that it will block and ban social network sites while Marines are on the job, but doesnt stop them from doing so when theyre not at work.",
      "To me, this seems to be a case of letting a small bad outweigh a large good.  While I understand the desire not to expose Marine networks to hackers and not to expose secrets to the world at large, these are problems that the Marines and the military face on a daily basis.  Simply banning the use of social networks while at work will only move the problem to a less scrutinized venue.",
      "Here are 5 reasons that the Marines shouldnt ban social networking:",
      "The security risks arent going away.  People are going to use their social networks at home if theyre not being used at work, so the same secrets will get out regardless.  These transgressions will range from a slip of the tongue to intentional OPSEC violations but if these occur on government computers, then theres a better chance that theyll be caught sooner than if they occur at home.\n  It sends the wrong message to todays Marines.  The bottom line interpretation of this message, to the average Marine is \"we dont trust you.\"  While todays Marines and soldiers are being asked to do more at lower ranks than they were in the past–a large amount of responsibility being placed in the hands of teenagers and twenty-somethings–this message implicitly says that for all of the responsibility, there is still no trust.\n  It sends the wrong message to potential recruits.  Recruiters want to go where the recruits are, and most of them are on social networks.  Not allowing social networking sends the message that the Marine Corps is backwards and antiquated and doesnt care about what its like to be a youth in society.\n  It keeps the Marines from knowing whats being said about them.  Social media is turning into a major media outlet, and its often one that has a different take on events than mainstream media.  By banning the use of social networks, the Marine Corps has hamstrung their own public affairs office from both telling a good story about the Marines and knowing what is being said in the arena of public opinion about them.\n  It stifles a culture of innovation.  Terrorists continue to innovate, and as is pointed out in the NDIAs own magazine, \"to defeat terrorists, the military must innovate and disrupt [them].\"  By banning the use of an innovative means of collaboration and interaction, the Marines suppresses the culture that leads to the innovations needed to win post unilateral hegemonic conflicts.",
      "I predict that in the end, the Marines, the Pentagon, and the government will come to some sort of compromise that allows limited usage.  Giving up rights is part and parcel for live in the military; hopefully the sacrifice doesnt come at the cost of leaving a generation of future cyberwarriors behind."
    ],
    "summary_t": ""
  },
  {
    "id": "e6cc9e01ecba1871ce57837e6cccd630",
    "url_s": "https://opensourceconnections.com/blog/2009/08/19/solr-1-4-enterprise-search-server-book-is-released/",
    "title": "Solr 1.4 Enterprise Search Server Book is Released!",
    "content": [
      "",
      "I am very proud to announce the first book on Solr has been published by Packt. This has been a labor of love for myself and my co-author David Smiley, and we are excited to see the book now \"in the wild!\". Below is a copy of the email sent to the Solr community:",
      "Fellow Solr users, Ive finally finished the book \"Solr 1.4 Enterprise Search Server\" with my co-author Eric. We are proud to present the first book on Solr and hope you find it a valuable resource. You can find full details about the book and purchase it here: http://www.packtpub.com/solr-1-4-enterprise-search-server/book\n\n  It can be pre-ordered at a discount now and should be shipping within a week or two. The book is also available through Amazon. You can feel good about the purchase knowing that 5% of each sale goes to support the Apache Software Foundation. For a free sample, there is a portion of chapter 5 covering faceting available as an article online here: http: //www.packtpub.com/article/faceting-in-solr-1.4-enterprise-search-server\n\n  By the way, we realize Solr 1.4 isnt out [quite] yet. It is feature-frozen however, and theres little in the forthcoming release that isnt covered in our book. About the only notable thing that comes to mind is the contrib module on search result clustering. However Eric plans to write a free online article available from Packt Publishing on that very subject. \"Solr 1.4 Enterprise Search Server\" In Detail: If you are a developer building a high-traffic web site, you need to have a terrific search engine. Sites like Netflix.com and Zappos.com employ Solr, an open source enterprise search server, which uses and extends the Lucene search library. This is the first book in the market on Solr and it will show you how to optimize your web site for high volume web traffic with full-text search capabilities along with loads of customization options. So, let your users gain a terrific search experience.\n\n  This book is a comprehensive reference guide for every feature Solr has to offer. It serves the reader right from initiation to development to deployment. It also comes with complete running examples to demonstrate its use and show how to integrate it with other languages and frameworks This book first gives you a quick overview of Solr, and then gradually takes you from basic to advanced features that enhance your search. It starts off by discussing Solr and helping you understand how it fits into your architectureâ€\"where all databases and document/web crawlers fall short, and Solr shines. The main part of the book is a thorough exploration of nearly every feature that Solr offers.\n\n  To keep this interesting and realistic, we use a large open source set of metadata about artists, releases, and tracks courtesy of the MusicBrainz.org project. Using this data as a testing ground for Solr, you will learn how to import this data in various ways from CSV to XML to database access. You will then learn how to search this data in a myriad of ways, including Solrs rich query syntax, \"boosting\" match scores based on record data and other means, about searching across multiple fields with different boosts, getting facets on the results, auto-complete user queries, spell-correcting searches, highlighting queried text in search results, and so on.\n\n  After this thorough tour, well demonstrate working examples of integrating a variety of technologies with Solr such as Java, JavaScript, Drupal, Ruby, XSLT, PHP, and Python. Finally, well cover various deployment considerations to include indexing strategies and performance-oriented configuration that will enable you to scale Solr to meet the needs of a high-volume site.\n\n  Sincerely, David Smiley (primary-author) [email protected] Eric Pugh (co-author) [email protected]\n\n  A huge round of thanks goes to David for bringing me into this project and being such a great partner on it! With 5% of the proceeds going to the Apache Software Foundation, heres hoping its a great success!"
    ],
    "summary_t": "I am very proud to announce the first book on Solr has been published by Packt. The book is also available through Amazon. You can feel good about the purcha..."
  },
  {
    "id": "ba813619b957a5b74373a620c5698487",
    "url_s": "https://opensourceconnections.com/blog/2009/08/23/osc-will-attend-and-sponsor-edui-conference-2009-in-the-university-of-vir/",
    "title": "OSC will attend and sponsor EdUI Conference 2009 in the University of Vir",
    "content": [
      "",
      "OSC is proud to announce that we will attend and sponsor this years EdUI Conference 2009 which is bein held at the University of Virginia on 21st-22nd September 2009. A number of folks from the OSC team will be attending, and stop by our booth in the Vendor Hall on the second day and introduce yourself!",
      "EdUI 2009 boasts a powerhouse lineup of renowned and popular headliner speakers, most often found at the Web industryâ€™s premier events. In addition to these, it features a series of presentations, selected through a proposal process, to allow peers, colleagues, and geek kindreds to enlighten one another with their expertise and ideas. Our very own Arin Sime will be speaking on The Facebook API: Thinking About UI in a Social Way."
    ],
    "summary_t": "OSC is proud to announce that we will attend and sponsor this years EdUI Conference 2009 which is bein held at the University of Virginia on 21st-22nd Septem..."
  },
  {
    "id": "50fa6dbef5017dae865749bc43594139",
    "url_s": "https://opensourceconnections.com/blog/2009/08/24/rj-bruneel-presenting-at-the-next-adobe-user-group-meeting/",
    "title": "RJ Bruneel presenting at the next Adobe User Group Meeting",
    "content": [
      "When: August 25, 2009 from 12:00pm – 1:00pm",
      "Where: Michie North Building Room 324 across from Barracks shopping center.",
      "RJ Bruneel will show his Stock Watch Flex app which uses Google Finance for customers to be able to watch their stock portfolios.",
      "For more information about the meeting: http://groups.adobe.com/posts/93f0df3964",
      "For more information about the CVille Adobe User Group: http://groups.adobe.com/groups/2aec1514da"
    ],
    "summary_t": ""
  },
  {
    "id": "b92fa0c0a6b6f4433407ce13d611ad82",
    "url_s": "https://opensourceconnections.com/blog/2009/09/01/arin-simes-agile2009-presentation-receives-praise/",
    "title": "Arin Simes Agile2009 Presentation Receives Praise",
    "content": [
      "Arin Simes presentation last week at Agile 2009 on how to sell a traditional client on an agile plan made a good impression, apparently.  First, his presentation was featured on the front page of SlideShare.",
      "Now, hes received an excellent writeup from Adam Goucher (we were pointed to Adams writeup from here).  Many thanks to Adam for the kind words about Arins writeup!",
      "Arins next speaking engagement is at the University of Virginias EdUI conference on September 22, covering the Facebook API.  Hed love to see you there!"
    ],
    "summary_t": ""
  },
  {
    "id": "f7364efdd673b144a2ed1e86fef9a629",
    "url_s": "https://opensourceconnections.com/blog/2009/09/24/edui-2009-conference-workshops-recap/",
    "title": "EdUI 2009 Conference Workshops Recap",
    "content": [
      "What is the EdUI Conference?",
      "The EdUI Conference is a new conference held by the University of Virginia for the niche of web professionals in higher education. Do not let the tag line fool you. A good portion, if not all information, provided by on-the-front-lines professionals easily translates into other domains of business on the web. The various headlining speakers were all top notch.",
      "I had the privilege of going to one workshop and visiting the condensed version of another. Both were wise investments of time. This website, my BHAG website (amptools.net), and O.S.C. (opensource connections) clients will be able to reap the rich benefits from EdUI in the near future.",
      "Workshop Beyond Blah Blah: Creating Great Content for the Web.",
      "This great workshop was created by David Poteet of  NewCity, a company Mr. Poteet founded. The workshop was centered around the `best of. Concentrating in areas of creating/writing great content and essentially its architecture.",
      "Some of the sources used to put together the workshop were the \"Wizard of Ads Trilogy\", Made to Stick by Chip & Dan Heath, and a white paper called \"Designing for the Scent of Information\" by Jared Spool. Anyone related to or responsible for marketing in your business should/must read the Wizard of Ads trilogy if they have not already.",
      "The notes that I took from the workshop are pretty much in the actual slides. They center around 6 tips for developing and organizing content on your site. That being said: the notes and slides are pale in comparison to actually doing the workshop with David. We actually were lead through the KJ session, writing exercises, and more during the workshop. All of these were invaluable experiences.",
      "Beyond Blah Blah: Creating Great Content for the Web. takeaways",
      "See the notes are at the end of this blog**",
      "Workshop New Insights In Web Standards",
      "Molly E. Holzschlag, former group lead of WaSP, co-author of The Zen of CSS design, and all around free spirit, did the workshop on new insights in web standards. The focus here was definitely on HTML5. The results of even the condensed workshop was eye opening.",
      "Unfortunately, I was only able to see the condensed 2nd day version of this workshop. I generally keep tabs on emerging software technology, HTML 5 and CSS 3 being a few of those. I even read W3C specs, which read like legal documents on a good day.",
      "The opportunity of getting the word of mouth version from someone like Molly, saves me a few headaches and hours of reading. Plus gaining another perspective from someone never hurts.",
      "New Insights In Web Standards Takeaways",
      "HTML 5 is an application markup language.\n  HTML 5 is here (partially). Google Wave is an HTML 5 application.\n  Backwards compatibility on the web is a must. (however this should be pushed more onto the browsers supporting old documents, not developers/designers)\n  The web means open standards, HTML 5 is pushing that, with forcing browsers to provide things like video and audio codecs, animation, etc.\n  First html spec that all browser vendors are behind, which is almost scary.\n  XHTML 2.0 and its w3 group is gone at the end of this year, no more xhtml. yes. im serious.\n  XHTML is considered to be a failure.\n  IE still slowing down the web. (Well weve know this for years, but it might be wise for everyone to charge extra to clients who require IE 6 compatibility or just render plain white document style content to IE 6 users).\n  HTML 5 will come in 2 flavors SGML/HTML syntax & XML.\n  Opera has the lead on HTML 5 completion, including web forms 2.0 at the moment.\n  Javascript is the glue of the web. (fellow Javascript developers, can we say job security?)\n  Designers who work CSS and HTML, will now have to be pseudo developers with HTML 5. For Hybrids like me who can do both, that is not a big deal. However being able to do design and development is a rarity. So this will place a burden on many who see themselves as designers.",
      "My Notes from David Poteets workshop**",
      "Give them what they want.\n    \n      \"people come to your web site running\"\n      what are you audiences key goals.\n        \n          decide what is important.\n          take away choices where i do not need them.\n          make choices clear & distinct.\n        \n      \n      how do you know what theyre looking for?\n        \n          listen to them.\n            \n              interviews & focus groups.\n              social media sites, blogs, forums.\n              search logs.\n              mental models.\n              carewords survey.\n              KJ Session.\n            \n          \n        \n      \n    \n  \n  Use words that smell like goals.\n    \n      Readers are like bees\n        \n          \n            \n              people hunt for information like bees seeking nectar or hounds on the trail of a fox.\n            \n          \n          \"Scent\" or \"Trigger\" words\n            \n              What worlds would be in someones mind if they were pursuing a particular goal?\n    *   information Scent Theory\n            \n          \n        \n      \n      What they dont do\n        \n          read left to right, top to bottom.\n          look at all the options and choose the best one.\n          Instead, they SCAN and SATISFICE.\n        \n      \n      Write for scent\n        \n          each link needs to have a strong \"scent for the content that lies beyond it.\"\n          5-7 words are optimal.\n          Users expect each click to lead to information that is more specific.\n          when users click on triggers words, they expect to see those words on the next page.\n          **Dont let clever kill clear. ** Trigger words need to be readily understandable\n          Users search when they cant find the words on the page.\n        \n      \n      Am I in the right place? Are your credible?, where can i go from here?\n    \n  \n  Write \"visually.\"\n    \n      Do they ever read?\n        \n          yes, when they get to the content theyre looking for.\n          % of the story read by format:\n            \n              75% o\n            \n          \n        \n      \n      Shorter sentences and short paragraphs\n        \n          in most cases not more than 50 words per paragraph\n          one sentence paragraph is ok\n          So are fragments.\n        \n      \n      Lots of headings/sub headings.\n      Using Images\n        \n          User pictures that mean something on context\n          Use icons if meaningful.\n        \n      \n      Meet users expectations for visual formats, for example:\n        \n          address\n          game stores\n        \n      \n      Use lists\n      Use tables. collage mural\n        \n          data compare\n          cross reference\n          options\n        \n      \n    \n  \n  Show dont tell.\n    \n      give sensory details and substantive facts.\n      let them come to their own conclusions.\n      Theyll realize it with greater conviction.\n    \n  \n  Not everyone thinks like you.\n    \n      Write for temperaments\n        \n          four temperaments\n          Guardian(sj) idealist(nf), artisan(sp), rational(nt)\n          Methodical, humanistic, spontaneous, competitive.\n        \n      \n      Methodical\n        \n          details\n          fine print\n          how does it work\n        \n      \n      spontaneous\n        \n          quickly\n          superior\n          customize your product/service\n          narrow your choice\n          enjoy life more?\n        \n      \n      Humanistic\n        \n          How will the product make you feel\n          who uses your product service\n          who are you, let me see bios\n          what will it feel like to work with you\n          what experience other have with you?\n          Can I trust you?\n          What are you values?\n          How will this help me strengthen relationships?\n        \n      \n      Competitive\n        \n          What are you competitive advantages?\n          Why are you superior choice.\n        \n      \n    \n  \n  Say something theyll remember (and care about)\n    \n      Left Brain vs Right Brain\n        \n          logical vs intuitive\n          Sequential vs Chaotic\n          Objective vs Subjective\n          Analytical vs Holistic\n          Right or Wrong vs Likes or Dislikes.\n          Grammar & Vocabulary vs Intonation & Accentuation\n          Exact Numeric Computation vs Approximates, Estimates\n          Tempo, tone, & interval vs Music\n        \n      \n      Implications\n        \n          Intellect and Emotion are partners who do not speak the same language. The intellect finds logic to justify what the emotions have decided. Win the hearts of the people.\n          Keys to the emerald city\n            \n              storytelling\n              the unexpected\n              verbs\n              poetic meter\n              humor\n              leave something to the imagination\n            \n          \n          Storytelling\n            \n              we are hardwired to remember stories.\n              Adrenaline is the biochemical adhesive that turns short term memories into long-term memories.\n              Stores are a great way to both SHOW an idea and engage the reader mentally and emotionally, resulting in:\n                \n                  transfer to long-term memory.\n                  Persuasion / conviction that something is true.\n                  motivation to act\n                \n              \n              Who is your story about?\n              simple\n              unexpected\n              concrete\n              credible\n              emotional\n              stories\n                \n                  the sooner you can put a verb in the better.\n                  put words to music.\n                    \n                      music enters through Right Brain, bypassing Broca entirely.\n                      Poetry allows us to put music to words in our minds.\n                    \n                  \n                \n              \n              What do you remember?"
    ],
    "summary_t": ""
  },
  {
    "id": "d1bb9c2ff0ec5bb270e5d25fdc357ba4",
    "url_s": "https://opensourceconnections.com/blog/2009/10/01/installing-older-version-of-a-language-using-macports/",
    "title": "Installing Older Version of a Language Using MacPorts",
    "content": [
      "I personally like to keep up to date with technologies and use the latest and greatest of whats available. Not everyone is the same though, and it is not always a possibility. In my case, using the latest version of PHP at the moment (PHP 5.3.0) means not being able to work with Drupal because the latest version of Drupal (v6.x) is built for PHP 5.2.6 and there exists some incompatibilities.",
      "Like most people developing on a mac, I use MacPorts to install packages. I like how it easy and simple. Unfortunately, once a newer version of a package gets on the list, the older one disappears. But thanks to Joe Homs and Stephen Chu I now know of a way around this. In their posts they explain how to get older version of ruby, check their posts at:",
      "<a href=\"http://www.stephenchu.com/2006/12/specifying-ruby-184-to-install-using.html\" target=\"_blank\">http://www.stephenchu.com/2006/12/specifying-ruby-184-to-install-using.html</a><br>\n<a href=\"http://journal.bitshaker.com/articles/2007/10/20/install-old-versions-of-ports-using-macports/\" target=\"_blank\">http://journal.bitshaker.com/articles/2007/10/20/install-old-versions-of-ports-using-macports/</a>",
      "I will go through the same steps but using PHP instead of ruby and I will be looking for PHP 5.2.10.",
      "1) Find out the latest svn revision number of the Portfile before upgrading to 5.3.0 at:",
      "http://trac.macports.org/log/trunk/dports/lang/php5/Portfile",
      "In this case it is 53555.",
      "2) Set up a local port repository. In the file /opt/local/etc/macports/sources.conf, add this line before the rsync line:",
      "file:///Users/Shared/dports",
      "and create that directory.",
      "3) Install the port into your local repository.",
      "cd /Users/Shared/dports<br>\nsvn co http://svn.macports.org/repository/macports/trunk/dports/lang/php5/ -r 53555 lang/php5",
      "4) Run portindex so that ports now finds your new (old) version of ruby.",
      "portindex /Users/Shared/dports",
      "5) Now you should be able to see php5 @5.2.10 by running:",
      "port list<br>\nor port search php5",
      "6) Install PHP5",
      "sudo port install php5 @5.2.10",
      "You should be up and running now, so to check, run:",
      "php -v",
      "If you already have another version of php activated you might get an error at the activation part of the port, deactivate the active version and manually activate the one you would like to work with:",
      "sudo port deactivate php5 @5.3.0<br>\nsudo port -f activate php5 @5.2.10"
    ],
    "summary_t": ""
  },
  {
    "id": "df50fba16634b407bb0e3136ed366f47",
    "url_s": "https://opensourceconnections.com/blog/2009/10/08/12-strategies-for-selling-a-traditional-client-on-an-agile-project-plan/",
    "title": "12 Strategies for Selling a Traditional Client on an Agile Project Plan",
    "content": [
      "Why do we need to sell Agile?",
      "For those of us who know and love Agile, the benefits of Agile seem obvious.Â  It can be frustrating when someone doesnt inherently see those benefits, and we sometimes forget that we need to sell our clients or our managers on our process.",
      "For some perspective on the gut reaction many people have to Agile, consider the following quote.Â  John Zachman penned a famous paper on software architecture design in the 1980s, where he said:",
      "â€œSome kind of structure (or architecture) is imperative because decentralization without structure is chaos.â€ – J.A. Zachman, 1987, â€œA framework for information systems architectureâ€",
      "As practitioners of Agile, we need to remember that someones first reaction may be that we are engaging in chaos, and so we need to consider beforehands the needs of our client or manager and how we can convince them to follow a better process.",
      "In this post, I will briefly present 12 techniques for selling a traditional client on an Agile project plan.Â  This post is based on a talk I gave at Agile 2009 and to a number of other user groups, and which you can see on Slideshare here.",
      "As part of preparing this presentation, I created a survey of fellow grad students in UVas MSMIT program and other colleagues in the field.Â  I asked them how they have sold Agile or been sold on Agile.Â  Not everyone who answered the survey was a fan of agile and many of their quotes are interesting.",
      "You can see the survey yourself here, and add your own feedback to it:\nhttp://www.tinyurl.com/SellingAgileSurvey/",
      "Those who answered the survey are all experienced IT veterans who have worked for a wide range of organizations, including Booz Allen Hamilton, SAIC, Capitol One, the International Monetary Fund, Fannie Mae, Freddie Mac, and more.",
      "First, a couple of general comments from the survey that I found interesting:",
      "â€œAgile seems to carry the connotation of codelike-hell or just, work faster.\"",
      "â€œI am skeptical of any methods that that could be interpreted as â€˜cutting cornersâ€™â€",
      "These are common first impressions of Agile, and I hope the following 12 techniques will give you ideas how to convince your clients or managers that Agile is the way to go and their first impressions are not accurate.Â  These techniques are based on a survey of current literature, my own consulting experiences at OpenSource Connections, and the feedback I received from the survey mentioned above.",
      "1. Trial by Sprint",
      "â€œYou need to show a success to get adoption.â€",
      "Consider giving your boss an option.Â  Ask them to give you a sprint or two to try Agile out, and if they dont like it, then you will go back to the old ways of doing things.Â  Make sure that you pick tasks for those trial sprints which can be successfully accomplished, and will show your boss the value of the incremental delivery of Agile.Â  You should also encourage your boss to play a role in the daily standups and see how communication in your team improves.",
      "2.Â  Case Studies in Success",
      "Do some research on similar companies to yours, and what methodologies they are using.Â  By googling around online, you should be able to find some case studies on Agile success and present those to your boss.",
      "3.Â  Client Testimonials",
      "For consultants, make sure you go back to your clients after a project and interview them.Â  Ask them how your process worked, and gather some quotes you can use in future proposals or client discussions.Â  I did this for one of our recent projects, and the customer gave us some great feedback like:",
      "\"Certainly one of the most successful projects ever here … Scrum eliminated my biases of what developers could do by letting them self-select.\"",
      "4. Find a Champion",
      "One approach is to let others do the selling for you.Â  This is particularly useful for consultants who already have a relationship with a client, and you can let some of the clients employees talk about how they want you to use Scrum.Â  Or, as one of my classmates did, he became the internal champion for Agile:",
      "â€œI highlighted the benefits to the Project Manager: higher productivity and less team management stuff since the team will take care of lots of team-management and updating (burn charts) instead of PMs managing those details.â€",
      "Basically, identify a stakeholder in the project who is most in need of the benefits of Agile, and enlist their support to help you sell management on the use of Agile.",
      "5. Use Metrics of Success",
      "Many traditional clients like their current methods because they provide metrics that they can graph and look at.Â  They dont always immediately understand how they can measure a project using methodology without a lot of documents.",
      "But Agile has its own metrics too, and perhaps you can use those to sway the client.Â  Consider presenting them with information on velocity, story points completed vs waiting, test coverage, unit test success, etc.Â  Some Agile tools will provide these in pretty reports, but you can also fashion them yourself.Â  And dont forget to show them the benefits of the burndown chart if youre using Scrum, and emphasize how this gives them a real time view into the health of a sprint without encumbering the developers with lots of reporting.",
      "6. Show how Agile combats common IT failures",
      "One of my professors, Ryan Nelson, has been running a study of project retrospectives over the last 10 years, and has published articles in MIS Quarterly Executive detailing the Top Classic Mistakes in IT projects.Â  The top 3 are consistently:Â  Poor estimation and scheduling, Ineffective Stakeholder Management, and Insufficient Risk Management.Â  Agile can be used to address all those classic mistakes.",
      "Consider why projects normally fail in your organization, and present the ways that Agile combats those common failures.\n**",
      "Examples of government/industry leaders using Agile**",
      "As one of my classmates pointed out:",
      "â€œClients, especially the military, are wary of catch phrases and sometimes unwilling to change their habits.â€",
      "If your client or boss fits that description, then you need to convince them they are not the first to try Agile, and that it has been used elsewhere successfully.Â  Try to find examples of their peers in industry or government using Agile, and point to them as an example.",
      "One example you can use is my previous blog post about the CIA using Agile and Scrum.",
      "8. Comparison to other methodologies",
      "Compare how Agile/Scrum work compared to areas that your client or boss is already familiar with.Â  Try to point out both the similarities and the differences.Â  For example, one of my classmates took the following approach:",
      "â€œI gave an overview of the Scrum process and highlighted the ease of transition since iterative/incremental development has been in practice for a long time (in other forms such as a spiral approach)â€\n**",
      "**\n9.Â  Listen to their needs and address them.**",
      "â€œI am always skeptical of anything that promises it is the only or the best [methodology].â€ – Comment from a development manager",
      "Instead of going into the meeting and kicking off the discussion with your rant on why Agile is the best development methodology, take time to listen to your client or manager first.",
      "Ask them how their projects have been going, what challenges they face, and take notes on their common problems.Â  Then suggest how Agile will address some of these issues.",
      "The key here is to spend most of your time listening, and only then talk up the benefits of Agile.Â  This will assure your audience that you have heard their concerns, and that you are trying to present positive solutions to their problems.Â  This can be much more effective than trying to force a methodology on them.",
      "10.Â  Sneak it in",
      "This is a very common approach, and one that I have done before too.Â  One of my classmates is a technology program manager, and they commented that:",
      "â€œI make sure I utilize agile practices where ever I can – I just dont use the agile terminology.â€",
      "The point here is not to be deceptive, its just that sometimes you dont need the sign off from the boss to do something.Â  After you successfully conclude your project, you can then point out to your boss that \"by the way, all that stuff we did that really impressed you – that was Agile.\"Â  Now that they have a positive impression and have witnessed some success, you can work on letting them formally adopt the process.",
      "Note that doing this might require you to temporarily also fill out the same reports or status updates your current process requires, but hopefully you can get those dropped after having showed the benefits of Agile.",
      "11.Â  Compromise",
      "An agile purist may not like the idea of compromising, but I found the following sentiment to be pretty common:",
      "â€œThe methodology that has worked in my experience has been to incrementally introduce Agile … Start using a limited set of the practices and gradually start bringing in more.â€",
      "Similar to the \"Sneak it in\" approach, this can be done with or without your bosss knowledge.",
      "12.Â  Agile Project Management Office",
      "One other thought to consider is setting up an Agile Project Management Office, or PMO.Â  A traditional PMO is a very document and regulation intense group, but some Agile practitioners have started to promote the idea of a lighter weight PMO.Â  An Agile PMO focuses less on micro managing teams, and more on providing an interface to traditional processes.Â  The Agile PMO can help Agile development teams by taking ownership of any burdensome compliance requirements which go against Agile processes.Â  This allows the developers to concentrate on developing code how they want, and not filling out checklists of documents.",
      "They can also serve as an educator and coach to the client.",
      "Conclusion",
      "There are many variants on the above ideas, but I hope this quick list has given you some direction on different strategies you can use for convincing people to adopt Agile.Â  Keep in mind that each of these strategies has pros and cons, and you should consider the needs of your situation before adopting any particular strategy.",
      "If you have other suggestions, please leave a comment and I look forward to hearing your feedback!"
    ],
    "summary_t": ""
  },
  {
    "id": "05103abbef40f513683b7f028e84c7d6",
    "url_s": "https://opensourceconnections.com/blog/2006/07/12/what-are-the-costs-and-benefits-of-integration/",
    "title": "What are the costs and benefits of integration?",
    "content": [
      "We are working to foster a relationship with a potential client. They have clients who use tools which do not integrate with each other, causing these clients to enter in information manually.",
      "Some of these companies have a relatively low price point that they are willing to pay for the integration of the tools that they use. However, by not looking at the long-term opportunity costs of not using the integration tools, these companies are understating costs of inaction.",
      "Lets assume that failure to integrate the tools means that once a month, someone has to enter in 75 entries into two different databases rather than one. Furthermore, lets assume that each set of entries takes 2 minutes, meaning that 2.5 hours per month is spent in duplicate entry. This is 30 hours a year that someone is spending entering in duplicate information. Finally, lets assume that the fully-loaded cost of this employee is $40/hour. Thats a $1,200 cost/year that the company has which it does not need.",
      "Are there examples in your company where manual processes are costing time, and ultimately, money? Are you costing your company more money in the long run by not investing to fix the process issues?"
    ],
    "summary_t": ""
  },
  {
    "id": "81a781dae64829c42471d51909338100",
    "url_s": "https://opensourceconnections.com/blog/2006/12/03/more-reports-means-less-information/",
    "title": "More reports means less information",
    "content": [
      "Just as people spend far too much time in meetings, they also spend far too much time sifting through reports trying to determine what the most important information to running a business is. With the proliferation of database technology and the ease of creating reports to \"summarize\" that information, decision makers in an enterprise often find themselves unable to get their hands on what they need to drive business forward. This is not because the information is not available, but, rather, it is not available in a useful format.",
      "What is necessary to complement the proliferation of information, or, as analysts like to say, \"slicing and dicing\" information, is a way of filtering that information. More reports exist today with an equivalent amount of utility. Therefore, filtering and sifting through the reports has become todays equivalent to filtering and sifting through the raw data 10-15 years ago.",
      "The key to effective management is getting the right reports to the right people at the right time. Often business analysts feel the need to justify their jobs through creating and proliferating reports, most of which either waste leaders time or fill up deleted items folders and trashcans. Test what reports are necessary by eliminating them. See how many people in the organization can no longer make decisions because the reports are no longer there.",
      "How can IT help? Catalog and cordon off the reports. Make them accessible from a central location, but create a pull demand system instead of a push supply system. By having a central repository of reports and opt-in subscriptions, an organization can determine what it has and what it needs in a rapid fashion. Furthermore, it can cut down on the clutter in senior managements inboxes and free up time for strategic leadership and management, rather than in the weeds detail orientation.",
      "Another benefit of this way of managing reports is that it creates a market economy for reporting. Let the body of the organization determine what is important, much as web 2.0 sites allow for filtering of the best what is available. Find out the most popular reports, the most popular author, or trace down a trail just as you might someones movie or book preferences.",
      "Does your company suffer from the curse of a cornucopia of information? Contact us and see how we can help."
    ],
    "summary_t": ""
  },
  {
    "id": "4a341e70ba3e39ea2283522a2d138871",
    "url_s": "https://opensourceconnections.com/blog/2009/10/08/arin-sime-to-speak-on-agile-at-richmond-spin-october-14th/",
    "title": "Arin Sime to speak on Agile at Richmond SPIN October 14th",
    "content": [
      "Im very pleased to be speaking to Richmond SPIN (Software Process Improvement Network) next Wednesday October 14th, at 6:00 pm at Anthem BCBS in Richmond.Â  The address is 2015 Staples Mill Road in Richmond.Â  You can find more details and register online at:",
      "http://www.richmondspin.org/home22222",
      "Heres the brief on my talk, which is based on my recent talk at Agile 2009.Â  I hope to see you there!",
      "*\"Strategies for Persuasion on Adopting Agile\"\nBy Arin Sime, Senior Consultant with OpenSource Connections</p>\nDo you want to know why Agile is sometimes a preferred methodology over a Waterfall method?Â  Arin will present 12 different methods for convincing a \"traditional client\" to use an Agile project plan. Based upon his own experiences with a wide variety of project styles, he will also present several definitions of each of the environments, well as results gathered from a survey on how to sell Agile. Some stories and best practices from other sources will also be covered."
    ],
    "summary_t": ""
  },
  {
    "id": "0586b92b4608dd6ae36d69498fc89900",
    "url_s": "https://opensourceconnections.com/blog/2009/10/09/good-will-coffee-help-desks-and-software/",
    "title": "Good Will, Coffee, Help Desks and Software.",
    "content": [
      "There is a true story behind this odd ensemble of a title. I remember it like it was yesterday. In fact, it was yesterday…",
      "The Story",
      "I was packing up my laptop when I was abruptly cornered by an energetic fellow. He rattled off his problem in such a degree I could only process bit and pieces of his story. He had spent 9 hours working with a software vendor who-must-not-be-named for credit card processors. They had gotten no where. The first question screaming in my mind was why I being singled out to help him with a computer problem? I hide my geekness very well. I strive to to prevent the typical questions like: will you fix my computer, vcr, dvd player, etc? Will you build me a website? Teach me HTML, please? People fail to realize the differences between developers and IT. They also fail to see the different roles and specialization of certain knowledge for various parts of software development. Touching a computer makes you liable for scape-goat-itis to consumers. Not only that, people have a bad habit of donning you to become their free tech support [expletive deleted], whether you want it or not. I turned around to see a co-worker, that I now call Brutus, paying for coffee at the counter. Having this flaw called empathy, I caved to energetic man in red. I agreed to look at the program and see what I could do. Surprising the daunted man in red and the owner of this awesome local coffee house in the downtown mall of Charlottesville: I fixed the issue in about 3 minutes. Ive never seen the software before. The help desk and the developers of the software who connected remotely could not fix it. Why could I? *And no, Im not super-developer-guy in skin tight and equally frightening spandex.",
      "The Real Problem",
      "The first part of the problem of is because many software vendors use a broken system. The help desk personnel reads scripted dialog, they dont know the ins an outs of the software. Some of help desk personnel at various companies even refuse to take initiative to go past the script. Even if its simple as searching google, they wont do it. Software developers are seldom familiar with the nuances of the operating system or system environments. They are paid to develop software, not administrate or navigate all the pitfalls of minor differences in environment. Thats why testing software and working *with IT staff, users, clients, and people who know the business is so important. Its important to have people who specialize in different aspects, like the desktop environment, performance and usability testing. The attitude and mindset of the developers you hire is also important. If they stamp it \"well it works on my machine\" or \"it the users fault\", its going to hurt your business and even hurt your clients/users. The other major issue was that the developers did not take the time to really analyze the error message. Instead they chased Alice down the rabbit hole, rather than simply listening.",
      "The Solution.",
      "I used the tech support cheat sheet that most of us savvy developers use to fix family computers and other things.  Taking the key parts of the error message, I put them into google. I glanced over a couple of posts. I saw a feasible issue. I verified the issue. The program was being started in a compatibility mode for windows 2000 on windows XP. I tweaked the folder settings so the compatibility mode was showing. Changed the compatibility mode. Restarted the application. They were now on their way. So the solution was really listening to the problem and resolving to fix it.",
      "The Story. Second Act.",
      "It was an inconvenience to me. Helping people generally is. But Ive lost hours of life to soul-sucking computers and software issues. I know pain and thy name is crappy software. Besides with only a few minutes of my time, I was able to save a few people hours of pain and possible even a few premature grey hairs.  It was a good feeling to help someone and not deal with scape-goat-itis. The man in red was certainly gracious. The store owner was kind to present me a token. A gift certificate for coffee. I really did not want anything, but its also rude to refuse a gift. It was kind of him. With a few minutes of my time, I was able to help a local business owner and the man in red. The man in red, turned out to be a recent veteran, who was in business of credit card processors. It always cool to be able to help someone who served for this country. They work hard and almost never expect anything in return. They deserve much more than they ever receive. They never complain about it. Its humbling. Even during lunch at a near by restaurant, the man in red came by to thank me again. Then voiced his disheartening opinion of the software that was just installed. I have a sneaking suspicion that the man in red, wont be using that particular brand of software again anytime soon.",
      "Things to ponder",
      "If you are using consultants, hiring software developers, or buying software; the question you should ask is who are you actually doing business with? How much is your time worth? Are they honest and transparent? Do they cost more because of quality or support they provide, or because they are a brand name?"
    ],
    "summary_t": "There is a true story behind this odd ensemble of a title. I remember it like it was yesterday. In fact, it was yesterday….. The Story. I was packing up my l..."
  },
  {
    "id": "d9735aa4cbe9c37d3ad49232a3c7d55f",
    "url_s": "https://opensourceconnections.com/blog/2009/10/20/eric-pugh-to-speak-on-solr-at-shenandoah-ruby-users-group-october-27th/",
    "title": "Eric Pugh to speak on Solr at Shenandoah Ruby Users Group October 27th",
    "content": [
      "From the Meetup site:",
      "Well look at the thriving Ruby ecosystem that has grown up around integrating with Solr. From Ruby gems that integrate with Solr like solrb and rsolr, to general search solutions like acts_as_solr and sunspot. Well also look at a complete \"shrink wrapped\" catalog solution for Solr using BlacklightOPAC.\n\n  Youll lean the basics of getting started with Solr, and an understanding of what Ruby solutions are available to simplifying adding great search to your site!\n\n  As usual, food and beverages will be provided.",
      "</embed>\n\n  Click here to check outThe Shenandoah Ruby Users Group!"
    ],
    "summary_t": ""
  },
  {
    "id": "1881c85edf5b15ddcb9ed7f48e08f273",
    "url_s": "https://opensourceconnections.com/blog/2009/10/20/opensource-connections-announces-matching-donation-for-american-freedom-foundation/",
    "title": "OpenSource Connections Announces Matching Donation For American Freedom Foundation",
    "content": [
      "As many of you know, I am a disabled veteran, but I was fortunate in that my injuries were very minor, particularly compared to what many of our Wounded Warriors go through on a regular basis.  OpenSource Connections is a sponsor of the November 7 American Freedom Festival, presented by the American Freedom Foundation.  Proceeds for that foundation go directly to help Wounded Warriors get back on their feet after their service for us.",
      "Today, were announcing a little effort to help raise some funds for the American Freedom Foundation.  Its VERY simple.  All you have to do is go to their Facebook page at http://www.facebook.com/pages/American-Freedom-Foundation/132999207220?ref=ts and become a fan!  OpenSource Connections will donate $1 to the cause for each new fan up to 500 new fans between now and 5:00 PM on October 30.  Please go to the fan page and become a fan and find out more about the great work that the American Freedom Foundation is doing for our Wounded Warriors!  Thanks!"
    ],
    "summary_t": ""
  },
  {
    "id": "898f9e70d1830ae190cfbaffd8812a14",
    "url_s": "https://opensourceconnections.com/blog/2009/10/26/james-bach-the-bad-boy-of-testing/",
    "title": "James Bach, the bad boy of Testing?",
    "content": [
      "So, is James Bach (@jamesmarcusbach) the bad boy of testing?",
      "I flew up to Boston on Monday to lead some workshops on Continuous Integration. I checked into my room at the Hyatt and then went downstairs to see who was around. I ran into a couple of speakers milling about, and eventually joined one of them, and we headed over to the MIT Press bookstore, me to look for my Solr book. I wasnt too sure of the name of the other speaker I was with (I asked once, but couldnt quite remember what it was…). So we got to the book shop, I ask my fellow speaker again: James Bach. The name I was familiar with, but couldnt quite place it… I ended up buying Parentonomics, and then we go for coffee.",
      "So, over coffee, he asks me about what my topic is, and I gave him the brief summary of my two CI related workshops. Wow.. Little did I realize that I was sitting with the guy who rails against the \"fetish\" that Agile folks have for automated testing! That his entire approach to \"testing\" is to use skilled, motivated folks who do \"sapient testing\". And Im the guy whos selling an approach that REQUIRES automated tests! That encourages expanding the use of automated testing!",
      "He actually walked me through a process of talking about how to \"think like a tester\", and it was really great mini-workshop.. He definitely subscribes to the socratic approach, and believes in his message, I was sweating at the end of it! That chat probably sparked more ideas in less time over that coffee then anything else this week. I also heard a lot of ideas and phrases that were echoed in Michael Boltons keynote later on in the week. Clearly a lot of collaboration between the two!",
      "Probably the biggest idea that James and chatted about was the idea that automated tests really arent automated tests, they are automated checks. They verify that the expected behavior of the code was met. His argument that if you want to do testing, real testing, then computers, automated processes cant meet that need, only people can.",
      "Now, I dont know if I believe that is completely true, but I am very aware that the \"manual testing\" where long test scripts written as Word documents are executed by human beings by hand are really a waste of human potential. And that those test scripts are really, to use James terms, \"check scripts\" because the people are not using any creativity! In fact, a lot of my interest in CI comes from the idea that people should not do monkey testing, that machines can do it much better, and my frustation with the perception that testing is a low value activity and can be easily shipped off to low skilled folks.",
      "I think that this shift away from the term \"test\" for automated tests is actually happening in many places. In the Ruby world, we have libraries like Shoulda that are moving from using words like assert to other words like should. A Cucumber test really shows how controlled the space that a test needs to be to work well in an automated fashion:",
      "Scenario: See all vendors\nGiven I am logged in as a user in the administrator role\nAnd There are 3 vendors\nWhen I go to the manage vendors page\nThen I should see the first 3 vendor names",
      "So while I dont know if I am bought in on the idea that only people can do \"testing\", and machines can only do \"checking\". Tools like Heckle try to simulate aspects of what a human can do. While not suggesting that we can automate the \"does my website look okay after someone changed the CSS\" type of work today, in the future our automated testing will be more capable then just \"checking\" because we will move beyond the very constrained tests we have today to ones that mimicing the richness of the simulators that Airline Pilots use. Instead of testing the training given to pilots, well be testing the robustness of software via simulations!",
      "At any rate, James Bach, while taking a rather provocative approach to sharing his ideas, does subscribe to my favorite bullet in the Agile Manifesto: Individuals and interactions over processes and tools.",
      "Here is him giving a great presentation with the subversive title of \"How to Fake a Project\" that was incredible entertaining, and also quite thought provoking:",
      "",
      "What do you think? Automated testing is a fetish of the Agile community?"
    ],
    "summary_t": ""
  },
  {
    "id": "0b4b057af56c941d038fe65ce0651d9b",
    "url_s": "https://opensourceconnections.com/blog/2009/10/27/department-of-defense-cio-publishes-open-source-guidance/",
    "title": "Department of Defense CIO Publishes Open Source Guidance",
    "content": [
      "In a memorandum dated October 16, 2009, the CIO of the Department of Defense published guidance on the use of open source software within the DoD.  The CIOs assessment of the perception of open source sums up quite well the obstacles we have seen time and time again:",
      "\"Unfortunately, there have been misconceptions and misinterpretations of the existing laws, policies and regulations that deal with software and apply to OSS, that have hampered effective DoD use and development of OSS.\"",
      "I wonder how long it will take for contracting officers to get the memorandum (so to speak) and to try to find ways around the guidance, spelled out in Attachment 2:",
      "\"In almost all cases, OSS meets the definition of commercial computer software and shall be given appropriate statutory preference in accordance with 10 USC 2377 (reference (b)) (see also FAR 2.101(b), 12.000, 12.101 (reference (c)); and DFARS 212.212, and 252.227-7014(a)(1) (reference (d))).\"",
      "We also eagerly await broader acceptance within the DoD information assurance community, but do not expect rapid change, despite the following guidance:",
      "\"DoD Instruction 8500.2, Information Assurance (IA) Implementation, (reference (g)) includes an Information Assurance Control, DCPD-1 Public Domain Software Controls, which limits the use of binary or machine-executable public domain software or other software products with limited or no warranty, on the grounds that these items are difficult or impossible to review, repair, or extend, given that the Government does not have access to the original source code and there is no owner who could make such repairs on behalf of the government. This control should not be interpreted as forbidding the use of OSS, as the source code is available for review, repair and extension by the government and its contractors.\"",
      "In sum, this is good news for those of us who believe that open source has a place in the Department of Defense.  However, word spreads slowly in the Government, and I expect that this policy will take many months, if not years, before gaining significant traction in DoD contracting policy.  A good place to start would be providing correct links in the policy memo.  Clicking on the link currently leads to a 404 error.",
      "*Edit: This is the correct link: *Clarifying Guidance Regarding Open Source Software (OSS)",
      "Thanks to the Powdermonkey blog and Dr. Mark Drapeau for sharing this!"
    ],
    "summary_t": ""
  },
  {
    "id": "a279475e9bbf5baef4e701978bb61b0a",
    "url_s": "https://opensourceconnections.com/blog/2009/10/31/devdays-dc/",
    "title": "DevDays DC",
    "content": [
      "Three OSCiers attended DevDays DC and for a one day show there were some interesting moments and quite a bit of information to be absorbed.",
      "The conference started with a funny clip by the people at Fog Creek and had a nice lineup of speakers.",
      "You can find my notes from most of the presentations here."
    ],
    "summary_t": ""
  },
  {
    "id": "ca4a511dfce688624d0558ea3a24086d",
    "url_s": "https://opensourceconnections.com/blog/2009/11/04/estimation-the-cone-of-uncertainty-vs-the-wormhole-of-reality/",
    "title": "Software Estimation: The Cone of Uncertainty vs The Wormhole of Reality",
    "content": [
      "Earlier this year in my MSMIT classes at UVa, we talked about software estimation techniques, which was a very interesting topic.",
      "One thing we talked about was the \"Cone of Uncertainty\", as described in Steve McConnells book \"Rapid Development.\"  You can read the full description of the cone of uncertainty on the Construx website.  Here is a drawing of it as depicted on that website.",
      "The Cone of Uncertainty is a convenient way to think of how software estimation works.  When a project is first beginning, you know very little about it and so you should never give customers a single point estimate.  A single point estimate just means that you supply one number, like saying \"this project will take 6 months.\"",
      "Giving estimates in ranges instead is much more valuable and realistic.  This more accurately conveys to your customer the uncertainty of an estimate, and therefore the risk.  For example, if I give you a wide estimate like \"this will take 3-12 months\", that is a very clear way to communicate that we dont know enough yet to give a firm estimate.",
      "The Cone of Uncertainty shows that the more you know about a project, the more you can narrow that range.  So while I might initially think its 3-12 months, after spending time analyzing the project, I can revise that to a more narrow range, like 6-8 months perhaps, or 10-12 months.",
      "But what about the end of a project?  The Cone of Uncertainty says that the closer you get to the end of a project, the more likely you are to know exactly when it ends.  That makes sense in theory, but is it realistic?",
      "I would suggest that many times, there is a large amount of uncertainty at the end of a project, even if its well run.  Imagine that you are two weeks away launching a new site, and you are still working out some bugs that your testers have found.  You know you only have a few bugs left, so you confidently assert to your boss or customer that youll be ready to launch in two weeks.  Each day you are fixing bugs, but the testers keep finding new bugs.  Perhaps they are even finding new bugs faster than you are fixing them, or their rate of finding bugs is not decreasing.",
      "When you had a short bug list in front of you, you felt very confident about two weeks, but now you are less certain.  How many more bugs will testing uncover?  Your confidence about the time left to launch is now decreasing, and you have to go back to your boss or client and ask for additional time.",
      "The Cone of Uncertainty captures risk at the front end of a project very well, but it doesnt capture the risk and uncertainty at the back end of a project.",
      "Theres one other thing that also makes me feel like the Cone of Uncertainty is perhaps a bit simplistic.  In my classes, we discussed using the Cone of Uncertainty as a way to educate our bosses and clients about why estimates are inaccurate.  The idea was to sketch it out on a napkin or whiteboard, and use it to convince them why you cant give them a set estimate up front (\"6 months\"), but instead need to estimate in a range (\"3 – 9 months\").",
      "However, I believe the problem is that the Cone of Uncertainty implies that you will converge on a middle point most of the time.   While that is not the intention of the cone, it still can appear that way because of the way the cone is drawn.  So if your boss wants to be tricky, he can play games with the numbers.  If you tell him that a project will take 3-9 months, then he may just translate that to mean 6 months (the midpoint of your range.)",
      "But how often have your range estimates actually converged on the midpoint?  I would speculate that most of the time, they actually converge on something above the midpoint.  Thats because our initial reactions are almost always optimistic, so as we learn more about the projects complexity which we had not originally anticipated, then we are likely to settle on a point higher in our initial range estimate.",
      "So I would further suggest that its better to draw the cone as not converging on a midpoint.  At some point in time, your estimates are likely to converge on an estimate that is higher than your original midpoint.  This leads me to my humbly suggested variation of the cone of uncertainty, which I like to call \"The Wormhole of Reality.\"",
      "",
      "Why the wormhole of reality?  When the cone is redrawn like Ive done above, it just looks more like a wormhole than a cone. Im referring to the <a href=\"http://en.wikipedia.org/wiki/Wormhole\" target=_blank>wormholes</a> of physics and astronomy, where it is a shortcut through space and time. Theres no correlation between the meanings of wormholes in space and my estimation wormhole, its just that they look similar.  Plus I think its a cool name.",
      "But more importantly, how do we avoid getting sucked into \"The Wormhole of Reality\"? If the Cone of Uncertainty is ideally the way we want our estimates to work, then we want to avoid the \"fuzzy back end\" of a project.  As I already suggested, the fuzzy back end is driven by continual bug reports that seem like they may never end.  Here are some tips to try and close out your projects more predictably:",
      "Use test driven development to write your unit tests early.\n  Use browser based automated testing tools like Selenium to automate user level testing.\n  Keep your continuous integration server happy and well-fed with frequent code check ins, so it can feed you bug information earlier rather than later.\n  Engage users in your system testing early on.  Make sure that you deliver incremental releases of your code, at least to internal users and testers, well before the final weeks of your project.\n  Engage customers in those incremental releases too.  If your internal users are not an accurate representation of external users to your code, then identify some beta customers who would be willing to try out future versions of your product.\n  Consider incorporating regular user-experience testing with beta releases of your product, on a feature by feature basis.  This is another way to get additional feedback about the code you are going to deploy.\n  Use agile methodologies to drive all of the above recommendations.",
      "While the \"Cone of Uncertainty\" is an easy way to explain to managers and clients how to estimate software development, too often we fall into development traps and end up in dangerous territory like \"The Wormhole of Reality.\"  But by following best practices, and clearly communicating to customers the risk at the front and back end of a project, you will be more likely to avoid these pitfalls and deliver your software on time, on scope, and on budget."
    ],
    "summary_t": ""
  },
  {
    "id": "be3bcdffef65d617654eabd3a5495903",
    "url_s": "https://opensourceconnections.com/blog/2009/11/04/take-time-to-learn-something-new/",
    "title": "Take Time to Learn Something New",
    "content": [
      "This past Monday, OSC had a company hackathon day. It is not our first, but we have been trying to make this sort of activity a more regular thing for us. Unlike our previous hackathons, this one was more flexible on the rules. Previously we had the entire team work on the same technology or goal whereas this time was more free form. We each had the opportunity to work on either a new or old technology (by new and old I mean something weve worked with before or not), on new or old projects and to collaborate or work individually.",
      "The day featured work on things like Google App Engine and the Android application platform. I took this chance to get myself out of the PHP world Ive been stuck in for about a year now and get back into some Rails development. I had a little tool in mind that I wanted to develop which would help us internally do some things a bit more efficiently. I used that as an excuse to try out some of the Rails plugins or gems that I have wanted to use for a while, which are:",
      "Mocha (http://github.com/mislav/rspec-rails-mocha http://mocha.rubyforge.org/)\n  Factory Girl (http://github.com/thoughtbot/factory_girl)\n  GChart (http://github.com/jbarnette/gchart)\n  Open Flash Chart (http://github.com/pullmonkey/open_flash_chart)",
      "I have to say, the buzz around Mocha and Factory Girl is very old and I wish I had the opportunity to use these tools before now. New things come out everyday and it is tough to keep up with everything. My advise to you is to dedicate a \"playing time\" often for you to try out the newest and greatest thing that comes out. Do this in groups where each one can be working on a different things and share your findings at the end of the day. Even though I didnt work on the Android app, I did get a feel for it from Arin through his experience with it. One of the things that helped take this experience further is our presence all together in one conference room feeding out of each other.",
      "Looking forward to the next OSC hackathon."
    ],
    "summary_t": ""
  },
  {
    "id": "ef357ad9a74b0fe731d8fba7ec755f0c",
    "url_s": "https://opensourceconnections.com/blog/2009/12/03/osc-appearances-in-december-2009/",
    "title": "OSC Appearances in December, 2009",
    "content": [
      "Here are the places where you can find OpenSource Connections in the next month:",
      "December 3: Company of Friends holiday party, Siips Wine Bar",
      "December 7: Charlottesville Social Media Club Steering Committee Meeting",
      "December 8: Charlottesville CxO Round Table",
      "December 8: Jason Hull presents the CBS19 Blog of the Week, CBS19, 6:45 PM",
      "December 9: CBIC Holiday Party",
      "December 18: OSC Holiday Party",
      "December 24-25: Company holidays",
      "We look forward to seeing you out and about this December!"
    ],
    "summary_t": ""
  },
  {
    "id": "e1899a2d6f4970a18a8be2639fc13928",
    "url_s": "https://opensourceconnections.com/blog/2006/12/13/i-got-a-great-time-slot-for-speakingcodemash/",
    "title": "I got a great time slot for [email protected]",
    "content": [
      "I just checked the Agenda page for CodeMash and I am on Friday just before lunch! I was worried I would be on Friday after lunch, which is always a tough timeslot as people are thinking about cutting out early because their brains are too full!",
      "CodeMash is only $99 and has an incredible list of presenters. Im looking forward to seeing Bruce Eckel talk about why dynamic languages offer so much, and Mary Poppendieck talk about \"Lean\" software development. Beyond Agile is Lean?"
    ],
    "summary_t": ""
  },
  {
    "id": "a78fd6827a1eab8e6d87f8bc51d1ceb1",
    "url_s": "https://opensourceconnections.com/blog/2009/12/07/a-random-walk-down-web-street-can-we-predict-trends-for-2010/",
    "title": "A Random Walk Down Web Street: Can We Predict Trends for 2010?",
    "content": [
      "The exponential growth of technology makes me think of Burton Malkiels book A Random Walk Down Wall Street, whose main thesis was that the average investor cannot beat the market because without perfect information, it is impossible – or nearly so – to predict future asset prices.",
      "Thus, when I read the 2010 predictions of Mashables Peter Cashmore, my gut reaction was to at least somewhat discount the predictions, since, after all, who can tell whats going to be hot one year from now?  After introspection,  I realized the analogies to Malkiels book still apply, except people like Cashmore play the Peter Lynch and Warren Buffet roles to my average investor role.",
      "While I will get to my thoughts on Cashmans predictions, I should first actually answer the question posed in the title: can we predict trends for 2010?  I think that we can predict the trends that will be forthcoming, but we cannot predict how those trends will come to fruition.  Those of us who are on the cutting, but not bleeding edge of technology can generally see where there are market needs, but are far less successful at identifying which of a handful of nascent technologies will be the final answer to meet the market need.  Its why venture capitalists have portfolios of investments rather than betting the farm on one investment; they diversify risk because they can only imperfectly predict the future.",
      "I often get posed the question \"Whats going to be the next [Twitter/Facebook/YouNameIt].\"  Often , the answer is exactly the same application.  First mover advantage yields a pretty strongly defensible market position.  Even successful technologies dont die overnight.  They usually face a long, slow, steady decline.   The decline of MySpace is a good example.",
      "",
      "As Facebook as grown, it has both gained new users and cannibalized MySpace users.",
      "",
      "Compare both of these to the almost overnight growth of a popular, rapidly growing, and, mostly, virally spread website, PeopleofWalmart.",
      "",
      "In the first two, the trends were predictable at the beginning of 2009, but NOBODY could have predicted the third site because it didnt even come out until August, developed nearly overnight by a group of friends.  What could have been predicted, though, was that virally spread socially edgy websites would gain more traction, due to the success of LOLcats, FailBlog, and others.",
      "Thus, predicting the \"next\" Twitter or Facebook is a nearly impossible task, but predicting that there will be a new website or application which consumes more than 10 minutes of your time daily that currently doesnt exist is a pretty easy one.  I know.  Im going out on a limb.",
      "Now, let me give my thoughts on Cashmores projections.  Ill categorize them into three buckets: the read-write-share web (a definition of semantic web), the cannibalization of devices and providers by subsuming technologies, and the online socialization of free time.",
      "The read-write-share web.  The growth in the use of Twitter means that more users of technology are expecting real-time responses rather than delayed, or even near real-time responses.  As more information becomes available, the need for filters of that information grows.   Users want to know that not only is the information they search for relevant in both content AND time, but that the source of the information is trustworthy.  I expect recency to become more of a factor in search content and identify verification services will grow into a sustainable niche industry, rather than having to trust that @iamtherealbrettfarve is actually Brett Favres Twitter account.  I also expect the growth of myopenid as a centralized arbiter of identity across multiple applications.\n  Cannibalization. The Holy Grail of personal devices is a usable phone cum music player cum GPS cum book cum computer cum entertainment station cum television cum [every other piece of technology you could imagine].   The iPhone and Kindle have started this collapsing chain.  The iPhone has cannibalized iPod sales, since the iPod also is contained within the iPhone.  The Google Android offers turn-by-turn directions, eliminating the need for a GPS device.  The list goes on.  Consumers will expect their portable technologies to do more and cost less and could signal a decline in specialized devices.\n  The online socialization of free time.  Who was playing Farmville this time last year?  Nobody.  It didnt even start until June of 2009.   How many play now?  63.7 million as of November, 2009. (Information cited here.)  The kids of 40 years ago got all of the kids in the neighborhood to play football or baseball.  The kids of 20 years ago got their friends over to play Atari or computer games.  Now, people play games online with all of their friends.  We couldnt have predicted FarmVille specifically, but the growth of multiplayer online roleplaying games such as World of Warcraft and the strength of fantasy sports leagues pointed to the trend.  I predict not only new games, but also the socialized attempts to corral free time for good – solving difficult problems, organizing for causes, and trying to make the world a better place.",
      "There is one prediction that Cashmore doesnt appear to make which I will – the appearance of the app which manages the work/personal life identity divide.  Since many people now use social media for professional purposes just as much as for personal purposes, they get overwhelmed with the notion of separating work from personal life.  It blends into one amorphous existence where someone can never quite get away from work.  I predict that by January 1, 2011, when someone asks me how they can separate their personal and professional social media use, I can answer confidently, \"Theres an app for that!\""
    ],
    "summary_t": ""
  },
  {
    "id": "c9c7e8c48fbe862120229f3802915d1c",
    "url_s": "https://opensourceconnections.com/blog/2009/12/11/streaming-index-progress-results-to-browser/",
    "title": "Streaming Index Progress Results to Browser",
    "content": [
      "I recently needed to index from a local filesystem several thousand static webpages into Solr. I was already using Ruby on Rails for the admin interface, so I quickly threw together an action to index the documents using HPricot and RSolr. To monitor the progress I just output to standard out using puts",
      "def index_bulk_html\nsolr = RSolr.connect :url=>SOLR_URL\ncount = 0\nfiles = Dir.glob(\"/Users/epugh/Documents/code/www.somesite.com/**/*.{html,htm}\")\nfiles.each do |file|\npath_ends_at = file.index(\"www.somesite.com\")\nunless path_ends_at.nil?\nputs(\"<strong>Processed #{count} of #{files.size}</strong>\") if count % 100 == 0\n\nurl = \"http://#{file[path_ends_at,file.size]}\"\ntitle, content = parse_html(file, title, content)\n\nputs \"Bad Content:#{!page_content.blank?} #{url} #{title}\"\n\nbegin\nsolr.add :id=> url, :url=>url, :mimeType=>\"text/html\", :title => title, :docText => page_content\nsolr.commit\ncount = count + 1\nrescue RSolr::RequestError\nputs \"<strong>Could not index #{file}</strong>\"\nend\nend\nend\nputs \"Imported #{count} webpages successfully.\"\nsolr.optimize\nredirect_to root_path\n\nend",
      "This worked great, but I realized that indexing over 10,000 documents takes a long time, and meanwhile the user is staring at the browser slowly loading, wondering if things had frozen or not! So I wondered if I could somehow stream some info back to the user. Fortunately Rails has already solved that problem! ActionController has the ability to render as text a proc object, and stream the output:",
      "# Renders \"Hello from code!\"\nrender :text => proc { |response, output| output.write(\"Hello from code!\") }[/code]",
      "So I quickly wrapped my existing code in a large proc, changed the puts to output.write, and now stream out to the browser constant progress reports:",
      "def index_bulk_html\nsolr = RSolr.connect :url=>SOLR_URL\ncount = 0\nfiles = Dir.glob(\"/Users/epugh/Documents/code/www.somesite.com/**/*.{html,htm}\")\nrender :text => proc { |response, output|\nfiles.each do |file|\npath_ends_at = file.index(\"www.somesite.com\")\nunless path_ends_at.nil?\noutput.write(\"<strong>Processed #{count} of #{files.size}</strong>\") if count % 100 == 0\n\nurl = \"http://#{file[path_ends_at,file.size]}\"\ntitle, content = parse_html(file, title, content)\n\noutput.write \"Bad Content:#{!page_content.blank?} #{url} #{title}\"\noutput.flush\n\nbegin\nsolr.add :id=> url, :url=>url, :mimeType=>\"text/html\", :title => title, :docText => page_content\nsolr.commit\ncount = count + 1\nrescue RSolr::RequestError\noutput.write \"<strong>Could not index #{file}</strong>\"\noutput.flush\nend\nend\nend\noutput.write \"Imported #{count} webpages successfully.\"\n}\nsolr.optimize\n\nend",
      "Thank you Rails, Hpricot, and RSolr for making life so simple!"
    ],
    "summary_t": ""
  },
  {
    "id": "ac80636863ae3d7334c7eb20b0fca766",
    "url_s": "https://opensourceconnections.com/blog/2009/12/11/the-white-houses-open-government-directive-still-a-long-way-from-reality/",
    "title": "The White House’s Open Government Directive: Still a Long Way From Reality",
    "content": [
      "On December 8, 2009, the Executive Office of the President of the United States issued a memorandum providing guidance and directives for implementing the Open Government policy.  While the directive from Mr. Peter Orszag did direct some clear action to be taken in the immediate future, much of the directive simply instructed agencies on providing future guidance on how to implement the Open Government policy.  Given that 11 months has passed since the initial Presidential directive, it seems optimistic to expect much movement in the next 90 days on implementing the policy.",
      "There are some clear deliverables for agencies to meet:",
      "Websites which publish usable information to the public.  Generally, this will follow the www.agency.gov/open format.  Agencies will be required to provide a feedback mechanism for the public to interact with the agencies regarding what information is published.  Agencies are also required to publish 3 \"high value\" data sets which have not been previously viewable to the public.\n    \n      What I expect.  This is the clearest specific deliverable set forth in the entire memorandum.  I expect varying levels of compliance, but I do think that most agencies will have pages up in differing states of completion.  I also expect pretty serious data integrity issues.  Recovery.gov had noteworthy data integrity issues, as documented in this Government Accounting Office report, and I anticipate similar issues within the agencies.\n    \n  \n  Publication in open formats.  While not directing what formats are required, the directive does specify that information must be published in open, searchable, parsable formats.  The publication requirement is subject to \"valid privacy, confidentiality, security, or other restrictions.\"\n    \n      What I expect.  The \"other restrictions\" requirement leaves a very large loophole for interpretation, so until more specification is given, I do not anticipate a flood of information flowing from agencies.  There will also be a debate on the interpretation of open formats, so I expect publication in a wide variety for formats including, among others, XML, XBRL, PDF, and Microsoft Office formats.\n    \n  \n  Integration with existing dashboards and reporting mechanisms.  Agencies must publish information in a way that is integrable with dashboards such as Data.gov, the IT Dashboard, Recovery.gov, and USASpending.gov.\n    \n      What I expect.  Until more specific guidance comes out, this may be a hand-waved requirement, particularly given the existing data integrity problems previously mentioned.  The memorandum identifies information quality as a concern, but provides no concrete information on how to address the issue.  Until there is a mechanism for vetting and validating information publication, this process will be subject to gaming.\n    \n  \n  A presumption of openness.  While this presumption is specifically laid out in covering FOIA issues, the theme runs throughout the directive – be more open.  For the smaller agencies, this might not be as difficult to execute as for the larger agencies.\n    \n      What I expect.  This will be a long slow road to go down.  The technology exists today to make publication (and validation) of information viable.  Its existed for quite some time.  Technology is not the hurdle to overcome; culture is.  The attitude of the administration is laudable – they are trying to bring transparency, accountability, and collaboration to the government for the people whom the government serves.  Penetrating the entrenched layers of inertia and process will take more than speeches and broad, limited specification memoranda.",
      "The effort towards more open government is a signal of the right things coming from the administration.  However, particularly in understaffed agencies, this directive may create more wheel-spinning than action, or, as my old Ethics professor Ed Freeman would say, more heat than light.",
      "Given the short timelines involved for some of the deliverables, I expect systems which can rapidly stand up and rapidly scale to gain ascendancy.  This means, in my mind, either Microsoft Sharepoint portals will pop up everywhere (a real possibility given the conversion of recovery.gov to Sharepoint and the citation of a Microsoft executive in Washington Technology), or open source proponents Kundra and Chopra will get their way, and a system such as Drupal will appear in many .gov domains.  Regardless, I do not see this as a great entry point for new services to the government.",
      "There is also an opportunity for a data aggregation service to appear from the publication of previously unpublished information.  Solr would be a great engine for finding  information across agencies, and performing analytics against that information is only a step further.  I expect lobbyists to demand value-added analytics of what comes out of the agencies as a result of this directive, and if the information is truly published in an open format, then providing the technology to meet the demand is not difficult.  I do not anticipate a long wait for K Street to begin using agencies information to its advantage, if and when we truly start seeing accurate information published as per the intent of this directive."
    ],
    "summary_t": ""
  },
  {
    "id": "b5a4dda720e8cc870118b0206aceb85f",
    "url_s": "https://opensourceconnections.com/blog/2010/01/11/erik-hatcher-solr-committer-reviews-solr-1-4-enterprise-search-server/",
    "title": "Erik Hatcher, Solr Committer, reviews Solr 1.4 Enterprise Search Server",
    "content": [
      "When I first got involved in writing Solr 1.4 Enterprise Search Server I knew that one of the folks I wanted to have review the book was Erik Hatcher, a Solr committer, and who introduced me to the project. He has written a very indepth review, that Ill admit I was nervous to read! But he summed it up as:",
      "Grand Finale I spelled out a lot of fiddly feedback above, and I expect the great addendum wiki page will factor in any keepers from this review. Of course most of the review points out mistakes or differences of opinion, thats what a review is for, though this is a solid, useful book. So, if youre considering using Solr, this book is for you. If youre already using Solr, youll likely pick up a useful trick or three. Go get it! As you can see from the level of detail in his post, when we come out with a second version of the Solr book, updating it for changes between when we published it and the final release of Solr 1.4 will be very easy!"
    ],
    "summary_t": ""
  },
  {
    "id": "8821a0bdef87807453d63d5df5bf1250",
    "url_s": "https://opensourceconnections.com/blog/2010/01/28/notes-from-using-lucidworks-for-solr-distro/",
    "title": "Notes from using LucidWorks for Solr Distro",
    "content": [
      "Ive been playing with the LucidWorks for Solr distribution of Solr 1.4, and wanted to share some of things I had noticed about it. The LucidWorks distro is Solr 1.4 with patches and enhancements from Lucid added in.",
      "Installer",
      "The first thing youll notice is that an installer (and uninstaller) is provided that walks you through the basic steps of installing Solr. Now Solr itself is pretty darn simple to work with already, but you do need to compile the code, which means you need Ant to be installed. The Lucid installer avoids that need, and Â adds support for running Solr in Tomcat as well as Jetty. And, assuming you have a support agreement with Lucid, it supports downloading plugins from Lucid to extend your Solr platform. Right now the only free plugin is the Reference Guide PDF. Having an installer available definitely checks a box for the systems type folks who may be installing Solr, but it doesnt really do anything crazy special. Also, one nit is that if you install into /opt/dirA, and then want to install into /opt/dirB, you have to delete ~/.LucidWorks/ directory as the install dir is cached! Â But it does demonstrate what might be coming from Lucid in future updates!",
      "Installer Targets Screen",
      "Another enhancment from Lucid is a Tray Application for managing your Solr instances. However, this turns out to just be a basic (on OSX at least!) menubar application that allows you to start/stop a local Solr server. There doesnt seem to be any options to stop and start remote servers, or monitor the health of running Solrs, so I think this is something you use once and never again! Hey Lucid, it would be great though if the Tray App integrated stoplight monitoring of Solr instances and popped open web pages to admin pages to perform various tasks on your collection of Solr servers!",
      "Directory Layout",
      "The directory that youve installed Solr into should look very familiar. In fact, too familiar to me! Ive gone back and forth on the way that Solr is distributed with source code as well as compiled jars. While Solr used to be a tool that only Java centric shops would look at, its now gone mainstream, to where many, if not most, organizations that use Solr are not traditional Java shops! I really wish I could download a version of Solr that didnt have the src directory, was just a stripped down ready to go application. Admittedly, the example application that is part of the source functions as a template, but it has been bemoaned by myself and others that folks just use and abuse the configuration of what was meant as an example app, to their detriment!",
      "So I was hoping that the LucidWorks distros Installer would function as that smart template by walking me through including/excluding various extensions like DIH, Clustering, and Extraction. But at least in this first version, no such luck. The support though for for picking either Tomcat or Jetty as a container shows what could be in the offing though!",
      "While the LucidWorks distro still ships with the hoary old example directory is still there, there is now a lucidworks directory. When you run the new toplevel start.sh shell script it starts Solr with solr.solr.home=lucidworks/solr directory. Something to note is that the start.sh has complete paths defined in it from the installer:",
      "[sourcecode language=\"text\"]\ncd /Users/epugh/solr/solr2/LucidWorks/lucidworks/jetty/../\n[/sourcecode]",
      "It really should at least have a single variable at the top that you can changing depending on what environment you are in.",
      "The lucidworks project is also setup as a single index project. Â Since the future is multicore configurations, Id like to see that as the default in more examples. Â (The example app needs a bit of work as well to better show off multicore as a first class feature!)",
      "solrconfig.xml",
      "Doing a diff on the example and lucidworks versions of solrconfig.xml shows its pretty much the same as the one from the example app, but with the correct configurations for DataImportHandler and the Velocity based search UI calledÂ Solritas. Solritas is a nice tool for helping you \"wedge\" Solr into places by providing a simple Velocity template based translation layer, and even build a GUI, within your Solr environment. Solritas hasnt received a lot of buzz, so its nice seeing it turned on by default! The clustering functionality is also specified, but not sure if the solr.cluster.enabled=true startup parameter is actually required or not.",
      "The other oddity is that the Lucid monitoring product for Solr, SolrGaze, isnt enabled by default! Doesnt seem like the most ringing endorsement for the software. Im excited by the prospect of better visiblity into the internals of Solr, so I enabled it.",
      "schema.xml",
      "Diffing the two schema.xml files reveals the addition of the Lucid KStemmer com.lucidimagination.solrworks.analysis.LucidKStemFilterFactory for fast non-aggresive text stemming. According to Lucid it provides:",
      "Large field performance shows a 220% performance increase, while small fields show a 1140% increase compared to the original UMASS code.",
      "SolrGaze",
      "SolrGaze promises to make it easier to see what is going on inside of Solr. Anything that makes it simpler for operations folks instead of developers to manage Solr is good in my book. I ran into one nit which was I opened up SolrGaze using the url http://localhost:8983/gaze/index.html. It barfed connecting to Solr to display gathered metrics, but if I used http://127.0.0.1:8983/gaze/index.html then everything was fine.",
      "I havent had to chance to really play with Gaze yet, so Ill post a more in-depth review soon.",
      "Summary_t",
      "All in all, the Lucid distro would be what I would recommend for a first timer to download, or someone doing a spike of development and needing a quick install of Solr. Â Not requiring Ant to be installed is a wonderful thing, and being pre-configured for Clustering, DIH, and Solritas means you get to see a working Solr install, complete with a full featured GUI, right out of the box. Â In terms of using for a production deploy, there is less to recommend it since youre going to want to strip down to just the bits and bobs that your require for your specific needs. Â I havent delved down into what SolrGaze provides, so that feature may be the tipping point for deciding to use the Lucid distribution."
    ],
    "summary_t": "I’ve been playing with the LucidWorks for Solr distribution of Solr 1.4, and wanted to share some of things I had noticed about it."
  },
  {
    "id": "2a7a5a88a3fb0e55b18ece835882a38f",
    "url_s": "https://opensourceconnections.com/blog/2010/01/30/working-with-groovy-and-grails-the-funnies/",
    "title": "Working with Groovy and Grails: The Funnies!",
    "content": [
      "1. Documentation",
      "Have you ever wished that you could take Google with you on the road when you dont have access to the INTRANET? or at least the documentation for whatever programming language you are currently using?",
      "Groovy provides you with the option to download the documentation:",
      "as a PDF (Very Old)\n  as a zipped HTML (Broken)",
      "or does it?",
      "2. Installation",
      "So lets try installing Groovy on a Mac OS X:",
      "Download the source\n  cd into the directory and run `ant install\n  simple right? not until you get this lovely message:",
      "youssef-chakers-macbook:groovy-1-1.7.0 youssefchaker$ ant install\nBuildfile: build.xml\nTrying to override old definition of task javac\n\n-excludeLegacyAntVersion:\n\n-checkAntVersion:\n\n-banner:\n     [echo] Java Runtime Environment version: 1.5.0_22\n     [echo] Java Runtime Environment vendor: Apple Inc.\n     [echo] Ant version: Apache Ant version 1.7.1 compiled on June 27 2008\n     [echo] Operating system name: Mac OS X\n     [echo] Operating system architecture: i386\n     [echo] Operating system version: 10.5.8\n     [echo] Base directory: /opt/local/src/groovy-1-1.7.0\n     [echo] Java Home: /System/Library/Frameworks/JavaVM.framework/Versions/1.5.0/Home\n\n-initializeReports:\n    [mkdir] Created dir: /opt/local/src/groovy-1-1.7.0/target/reports\n\n-mavenTaskdef:\n\n-mavenPomDefinitions:\n     Processing /opt/local/src/groovy-1-1.7.0/pom.xml to /opt/local/src/groovy-1-1.7.0/target/groovy-all.pom\n     Loading stylesheet /opt/local/src/groovy-1-1.7.0/config/maven/groovy-all.xsl\n\n-mavenInit:\n\n-mavenFetchAllModules:\n[artifact:dependencies] Downloading: asm/asm/3.2/asm-3.2.pom from central\n[artifact:dependencies] Downloading: asm/asm/3.2/asm-3.2.pom from central\n[artifact:dependencies] Downloading: junit/junit/4.7/junit-4.7.pom from central\n[artifact:dependencies] Downloading: junit/junit/4.7/junit-4.7.pom from central\n[artifact:dependencies] Downloading: asm/asm-commons/3.2/asm-commons-3.2.pom from central\n[artifact:dependencies] Downloading: asm/asm-commons/3.2/asm-commons-3.2.pom from central\n[artifact:dependencies] Downloading: asm/asm-util/3.2/asm-util-3.2.pom from central\n[artifact:dependencies] Downloading: asm/asm-util/3.2/asm-util-3.2.pom from central\n[artifact:dependencies] Downloading: asm/asm-analysis/3.2/asm-analysis-3.2.pom from central\n[artifact:dependencies] Downloading: asm/asm-analysis/3.2/asm-analysis-3.2.pom from central\n[artifact:dependencies] Downloading: asm/asm-tree/3.2/asm-tree-3.2.pom from central\n[artifact:dependencies] Downloading: asm/asm-tree/3.2/asm-tree-3.2.pom from central\n[artifact:dependencies] Downloading: commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.pom from central\n[artifact:dependencies] Downloading: commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.pom from central\n[artifact:dependencies] Downloading: org/livetribe/livetribe-jsr223/2.0.6/livetribe-jsr223-2.0.6.pom from central\n[artifact:dependencies] Downloading: org/livetribe/livetribe-jsr223/2.0.6/livetribe-jsr223-2.0.6.pom from central\n[artifact:dependencies] Downloading: xmlunit/xmlunit/1.3/xmlunit-1.3.pom from central\n[artifact:dependencies] Downloading: xmlunit/xmlunit/1.3/xmlunit-1.3.pom from central\n[artifact:dependencies] Downloading: hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.pom from central\n[artifact:dependencies] Downloading: hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.pom from central\n[artifact:dependencies] Downloading: com/thoughtworks/xstream/xstream/1.3.1/xstream-1.3.1.pom from central\n[artifact:dependencies] Downloading: com/thoughtworks/xstream/xstream/1.3.1/xstream-1.3.1.pom from central\n[artifact:dependencies] Downloading: org/apache/ivy/ivy/2.1.0/ivy-2.1.0.pom from central\n[artifact:dependencies] Downloading: org/apache/ivy/ivy/2.1.0/ivy-2.1.0.pom from central\n[artifact:dependencies] Downloading: asm/asm/3.2/asm-3.2.jar from central\n[artifact:dependencies] Downloading: asm/asm/3.2/asm-3.2.jar from central\n[artifact:dependencies] Downloading: junit/junit/4.7/junit-4.7.jar from central\n[artifact:dependencies] Downloading: junit/junit/4.7/junit-4.7.jar from central\n[artifact:dependencies] Downloading: asm/asm-commons/3.2/asm-commons-3.2.jar from central\n[artifact:dependencies] Downloading: asm/asm-commons/3.2/asm-commons-3.2.jar from central\n[artifact:dependencies] Downloading: asm/asm-util/3.2/asm-util-3.2.jar from central\n[artifact:dependencies] Downloading: asm/asm-util/3.2/asm-util-3.2.jar from central\n[artifact:dependencies] Downloading: asm/asm-analysis/3.2/asm-analysis-3.2.jar from central\n[artifact:dependencies] Downloading: asm/asm-analysis/3.2/asm-analysis-3.2.jar from central\n[artifact:dependencies] Downloading: asm/asm-tree/3.2/asm-tree-3.2.jar from central\n[artifact:dependencies] Downloading: asm/asm-tree/3.2/asm-tree-3.2.jar from central\n[artifact:dependencies] Downloading: org/livetribe/livetribe-jsr223/2.0.6/livetribe-jsr223-2.0.6.jar from central\n[artifact:dependencies] Downloading: org/livetribe/livetribe-jsr223/2.0.6/livetribe-jsr223-2.0.6.jar from central\n[artifact:dependencies] Downloading: com/thoughtworks/xstream/xstream/1.3.1/xstream-1.3.1.jar from central\n[artifact:dependencies] Downloading: com/thoughtworks/xstream/xstream/1.3.1/xstream-1.3.1.jar from central\n[artifact:dependencies] Downloading: org/apache/ivy/ivy/2.1.0/ivy-2.1.0.jar from central\n[artifact:dependencies] Downloading: org/apache/ivy/ivy/2.1.0/ivy-2.1.0.jar from central\n[artifact:dependencies] An error has occurred while processing the Maven artifact tasks.\n[artifact:dependencies]  Diagnosis:\n[artifact:dependencies]\n[artifact:dependencies] Unable to resolve artifact: Missing:\n[artifact:dependencies] ----------\n[artifact:dependencies] 1) asm:asm:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies]   Try downloading the file manually from the project website.\n[artifact:dependencies]\n[artifact:dependencies]   Then, install it using the command:\n[artifact:dependencies]       mvn install:install-file -DgroupId=asm -DartifactId=asm -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n[artifact:dependencies]\n[artifact:dependencies]   Alternatively, if you host your own repository you can deploy the file there:\n[artifact:dependencies]       mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[artifact:dependencies]\n[artifact:dependencies]   Path to dependency:\n[artifact:dependencies]         1) org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]         2) asm:asm:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies] 2) junit:junit:jar:4.7\n[artifact:dependencies]\n[artifact:dependencies]   Try downloading the file manually from the project website.\n[artifact:dependencies]\n[artifact:dependencies]   Then, install it using the command:\n[artifact:dependencies]       mvn install:install-file -DgroupId=junit -DartifactId=junit -Dversion=4.7 -Dpackaging=jar -Dfile=/path/to/file\n[artifact:dependencies]\n[artifact:dependencies]   Alternatively, if you host your own repository you can deploy the file there:\n[artifact:dependencies]       mvn deploy:deploy-file -DgroupId=junit -DartifactId=junit -Dversion=4.7 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[artifact:dependencies]\n[artifact:dependencies]   Path to dependency:\n[artifact:dependencies]         1) org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]         2) junit:junit:jar:4.7\n[artifact:dependencies]\n[artifact:dependencies] 3) asm:asm-commons:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies]   Try downloading the file manually from the project website.\n[artifact:dependencies]\n[artifact:dependencies]   Then, install it using the command:\n[artifact:dependencies]       mvn install:install-file -DgroupId=asm -DartifactId=asm-commons -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n[artifact:dependencies]\n[artifact:dependencies]   Alternatively, if you host your own repository you can deploy the file there:\n[artifact:dependencies]       mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm-commons -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[artifact:dependencies]\n[artifact:dependencies]   Path to dependency:\n[artifact:dependencies]         1) org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]         2) asm:asm-commons:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies] 4) asm:asm-util:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies]   Try downloading the file manually from the project website.\n[artifact:dependencies]\n[artifact:dependencies]   Then, install it using the command:\n[artifact:dependencies]       mvn install:install-file -DgroupId=asm -DartifactId=asm-util -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n[artifact:dependencies]\n[artifact:dependencies]   Alternatively, if you host your own repository you can deploy the file there:\n[artifact:dependencies]       mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm-util -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[artifact:dependencies]\n[artifact:dependencies]   Path to dependency:\n[artifact:dependencies]         1) org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]         2) asm:asm-util:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies] 5) asm:asm-analysis:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies]   Try downloading the file manually from the project website.\n[artifact:dependencies]\n[artifact:dependencies]   Then, install it using the command:\n[artifact:dependencies]       mvn install:install-file -DgroupId=asm -DartifactId=asm-analysis -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n[artifact:dependencies]\n[artifact:dependencies]   Alternatively, if you host your own repository you can deploy the file there:\n[artifact:dependencies]       mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm-analysis -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[artifact:dependencies]\n[artifact:dependencies]   Path to dependency:\n[artifact:dependencies]         1) org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]         2) asm:asm-analysis:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies] 6) asm:asm-tree:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies]   Try downloading the file manually from the project website.\n[artifact:dependencies]\n[artifact:dependencies]   Then, install it using the command:\n[artifact:dependencies]       mvn install:install-file -DgroupId=asm -DartifactId=asm-tree -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n[artifact:dependencies]\n[artifact:dependencies]   Alternatively, if you host your own repository you can deploy the file there:\n[artifact:dependencies]       mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm-tree -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[artifact:dependencies]\n[artifact:dependencies]   Path to dependency:\n[artifact:dependencies]         1) org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]         2) asm:asm-tree:jar:3.2\n[artifact:dependencies]\n[artifact:dependencies] 7) org.livetribe:livetribe-jsr223:jar:2.0.6\n[artifact:dependencies]\n[artifact:dependencies]   Try downloading the file manually from the project website.\n[artifact:dependencies]\n[artifact:dependencies]   Then, install it using the command:\n[artifact:dependencies]       mvn install:install-file -DgroupId=org.livetribe -DartifactId=livetribe-jsr223 -Dversion=2.0.6 -Dpackaging=jar -Dfile=/path/to/file\n[artifact:dependencies]\n[artifact:dependencies]   Alternatively, if you host your own repository you can deploy the file there:\n[artifact:dependencies]       mvn deploy:deploy-file -DgroupId=org.livetribe -DartifactId=livetribe-jsr223 -Dversion=2.0.6 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[artifact:dependencies]\n[artifact:dependencies]   Path to dependency:\n[artifact:dependencies]         1) org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]         2) org.livetribe:livetribe-jsr223:jar:2.0.6\n[artifact:dependencies]\n[artifact:dependencies] 8 ) com.thoughtworks.xstream:xstream:jar:1.3.1\n[artifact:dependencies]\n[artifact:dependencies]   Try downloading the file manually from the project website.\n[artifact:dependencies]\n[artifact:dependencies]   Then, install it using the command:\n[artifact:dependencies]       mvn install:install-file -DgroupId=com.thoughtworks.xstream -DartifactId=xstream -Dversion=1.3.1 -Dpackaging=jar -Dfile=/path/to/file\n[artifact:dependencies]\n[artifact:dependencies]   Alternatively, if you host your own repository you can deploy the file there:\n[artifact:dependencies]       mvn deploy:deploy-file -DgroupId=com.thoughtworks.xstream -DartifactId=xstream -Dversion=1.3.1 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[artifact:dependencies]\n[artifact:dependencies]   Path to dependency:\n[artifact:dependencies]         1) org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]         2) com.thoughtworks.xstream:xstream:jar:1.3.1\n[artifact:dependencies]\n[artifact:dependencies] 9) org.apache.ivy:ivy:jar:2.1.0\n[artifact:dependencies]\n[artifact:dependencies]   Try downloading the file manually from the project website.\n[artifact:dependencies]\n[artifact:dependencies]   Then, install it using the command:\n[artifact:dependencies]       mvn install:install-file -DgroupId=org.apache.ivy -DartifactId=ivy -Dversion=2.1.0 -Dpackaging=jar -Dfile=/path/to/file\n[artifact:dependencies]\n[artifact:dependencies]   Alternatively, if you host your own repository you can deploy the file there:\n[artifact:dependencies]       mvn deploy:deploy-file -DgroupId=org.apache.ivy -DartifactId=ivy -Dversion=2.1.0 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n[artifact:dependencies]\n[artifact:dependencies]   Path to dependency:\n[artifact:dependencies]         1) org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]         2) org.apache.ivy:ivy:jar:2.1.0\n[artifact:dependencies]\n[artifact:dependencies] ----------\n[artifact:dependencies] 9 required artifacts are missing.\n[artifact:dependencies]\n[artifact:dependencies] for artifact:\n[artifact:dependencies]   org.codehaus.groovy:groovy:jar:1.7.0\n[artifact:dependencies]\n[artifact:dependencies] from the specified remote repositories:\n[artifact:dependencies]   central (http://repo1.maven.org/maven2)\n[artifact:dependencies]\n[artifact:dependencies]\n\nBUILD FAILED\n/opt/local/src/groovy-1-1.7.0/config/ant/build-maven.xml:79: The following error occurred while executing this line:\n/opt/local/src/groovy-1-1.7.0/config/ant/build-maven.xml:52: The following error occurred while executing this line:\n/opt/local/src/groovy-1-1.7.0/config/ant/build-maven.xml:44: The following error occurred while executing this line:\n/opt/local/src/groovy-1-1.7.0/config/ant/build-maven.xml:29: Unable to resolve artifact: Missing:\n----------\n1) asm:asm:jar:3.2\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command:\n      mvn install:install-file -DgroupId=asm -DartifactId=asm -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there:\n      mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency:\n        1) org.codehaus.groovy:groovy:jar:1.7.0\n        2) asm:asm:jar:3.2\n\n2) junit:junit:jar:4.7\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command:\n      mvn install:install-file -DgroupId=junit -DartifactId=junit -Dversion=4.7 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there:\n      mvn deploy:deploy-file -DgroupId=junit -DartifactId=junit -Dversion=4.7 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency:\n        1) org.codehaus.groovy:groovy:jar:1.7.0\n        2) junit:junit:jar:4.7\n\n3) asm:asm-commons:jar:3.2\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command:\n      mvn install:install-file -DgroupId=asm -DartifactId=asm-commons -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there:\n      mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm-commons -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency:\n        1) org.codehaus.groovy:groovy:jar:1.7.0\n        2) asm:asm-commons:jar:3.2\n\n4) asm:asm-util:jar:3.2\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command:\n      mvn install:install-file -DgroupId=asm -DartifactId=asm-util -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there:\n      mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm-util -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency:\n        1) org.codehaus.groovy:groovy:jar:1.7.0\n        2) asm:asm-util:jar:3.2\n\n5) asm:asm-analysis:jar:3.2\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command:\n      mvn install:install-file -DgroupId=asm -DartifactId=asm-analysis -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there:\n      mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm-analysis -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency:\n        1) org.codehaus.groovy:groovy:jar:1.7.0\n        2) asm:asm-analysis:jar:3.2\n\n6) asm:asm-tree:jar:3.2\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command:\n      mvn install:install-file -DgroupId=asm -DartifactId=asm-tree -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there:\n      mvn deploy:deploy-file -DgroupId=asm -DartifactId=asm-tree -Dversion=3.2 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency:\n        1) org.codehaus.groovy:groovy:jar:1.7.0\n        2) asm:asm-tree:jar:3.2\n\n7) org.livetribe:livetribe-jsr223:jar:2.0.6\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command:\n      mvn install:install-file -DgroupId=org.livetribe -DartifactId=livetribe-jsr223 -Dversion=2.0.6 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there:\n      mvn deploy:deploy-file -DgroupId=org.livetribe -DartifactId=livetribe-jsr223 -Dversion=2.0.6 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency:\n        1) org.codehaus.groovy:groovy:jar:1.7.0\n        2) org.livetribe:livetribe-jsr223:jar:2.0.6\n\n8 ) com.thoughtworks.xstream:xstream:jar:1.3.1\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command:\n      mvn install:install-file -DgroupId=com.thoughtworks.xstream -DartifactId=xstream -Dversion=1.3.1 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there:\n      mvn deploy:deploy-file -DgroupId=com.thoughtworks.xstream -DartifactId=xstream -Dversion=1.3.1 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency:\n        1) org.codehaus.groovy:groovy:jar:1.7.0\n        2) com.thoughtworks.xstream:xstream:jar:1.3.1\n\n9) org.apache.ivy:ivy:jar:2.1.0\n\n  Try downloading the file manually from the project website.\n\n  Then, install it using the command:\n      mvn install:install-file -DgroupId=org.apache.ivy -DartifactId=ivy -Dversion=2.1.0 -Dpackaging=jar -Dfile=/path/to/file\n\n  Alternatively, if you host your own repository you can deploy the file there:\n      mvn deploy:deploy-file -DgroupId=org.apache.ivy -DartifactId=ivy -Dversion=2.1.0 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\n  Path to dependency:\n        1) org.codehaus.groovy:groovy:jar:1.7.0\n        2) org.apache.ivy:ivy:jar:2.1.0\n\n----------\n9 required artifacts are missing.\n\nfor artifact:\n  org.codehaus.groovy:groovy:jar:1.7.0\n\nfrom the specified remote repositories:\n  central (http://repo1.maven.org/maven2)\n\nTotal time: 18 seconds",
      "Ok, how about we try MacPorts?",
      "youssef-chakers-macbook:~ youssefchaker$ sudo port install groovyPassword:--->  Fetching apache-ant--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://www.ibiblio.org/pub/mirrors/apache/ant/binaries--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://apache.mirror.rafal.ca/ant/binaries--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://apache.adcserver.com.ar/ant/binaries--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://www.mirrorservice.org/sites/ftp.apache.org/ant/binaries--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://apache.mirroring.de/ant/binaries--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://apache.multidist.com/ant/binaries--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from ftp://ftp.infoscience.co.jp/pub/net/apache/dist/ant/binaries--->  Verifying checksum(s) for apache-ant--->  Extracting apache-ant--->  Configuring apache-ant--->  Building apache-ant--->  Staging apache-ant into destroot--->  Installing apache-ant @1.7.1_0--->  Activating apache-ant @1.7.1_0--->  Cleaning apache-ant--->  Fetching groovy--->  Attempting to fetch groovy-src-1.6.3.zip from http://distfiles.macports.org/groovy--->  Verifying checksum(s) for groovy--->  Extracting groovy--->  Configuring groovy--->  Building groovyError: Target org.macports.build returned: shell command \" cd \"/opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_ports_java_groovy/work/groovy-1.6.3\" && ant install -DskipTests=true \" returned error 1Command output: /opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_ports_java_groovy/work/groovy-1.6.3/config/ant/build-maven.xml:90: The following error occurred while executing this line:/opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_ports_java_groovy/work/groovy-1.6.3/config/ant/build-maven.xml:54: The following error occurred while executing this line:/opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_ports_java_groovy/work/groovy-1.6.3/config/ant/build-maven.xml:29: Unable to resolve artifact: Missing:----------1) biz.aQute:bnd:jar:0.0.258Try downloading the file manually from the project website.Then, install it using the command:mvn install:install-file -DgroupId=biz.aQute -DartifactId=bnd -Dversion=0.0.258 -Dpackaging=jar -Dfile=/path/to/fileAlternatively, if you host your own repository you can deploy the file there:mvn deploy:deploy-file -DgroupId=biz.aQute -DartifactId=bnd -Dversion=0.0.258 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]Path to dependency:1) groovy:groovy-tools:jar:internal2) biz.aQute:bnd:jar:0.0.258----------1 required artifact is missing.for artifact:groovy:groovy-tools:jar:internalfrom the specified remote repositories:central (http://repo1.maven.org/maven2),aQute (http://www.aQute.biz/repo)Total time: 2 minutes 51 secondsError: Status 1 encountered during processing.\nyoussef-chakers-macbook:~ youssefchaker$ sudo port install groovy\n\nPassword:\n\n--->  Fetching apache-ant\n\n--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://www.ibiblio.org/pub/mirrors/apache/ant/binaries\n\n--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://apache.mirror.rafal.ca/ant/binaries\n\n--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://apache.adcserver.com.ar/ant/binaries\n\n--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://www.mirrorservice.org/sites/ftp.apache.org/ant/binaries\n\n--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://apache.mirroring.de/ant/binaries\n\n--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from http://apache.multidist.com/ant/binaries\n\n--->  Attempting to fetch apache-ant-1.7.1-bin.tar.bz2 from ftp://ftp.infoscience.co.jp/pub/net/apache/dist/ant/binaries\n\n--->  Verifying checksum(s) for apache-ant\n\n--->  Extracting apache-ant\n\n--->  Configuring apache-ant\n\n--->  Building apache-ant\n\n--->  Staging apache-ant into destroot\n\n--->  Installing apache-ant @1.7.1_0\n\n--->  Activating apache-ant @1.7.1_0\n\n--->  Cleaning apache-ant\n\n--->  Fetching groovy\n\n--->  Attempting to fetch groovy-src-1.6.3.zip from http://distfiles.macports.org/groovy\n\n--->  Verifying checksum(s) for groovy\n\n--->  Extracting groovy\n\n--->  Configuring groovy\n\n--->  Building groovy\n\nError: Target org.macports.build returned: shell command \" cd \"/opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_ports_java_groovy/work/groovy-1.6.3\" && ant install -DskipTests=true \" returned error 1\n\nCommand output: /opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_ports_java_groovy/work/groovy-1.6.3/config/ant/build-maven.xml:90: The following error occurred while executing this line:\n\n/opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_ports_java_groovy/work/groovy-1.6.3/config/ant/build-maven.xml:54: The following error occurred while executing this line:\n\n/opt/local/var/macports/build/_opt_local_var_macports_sources_rsync.macports.org_release_ports_java_groovy/work/groovy-1.6.3/config/ant/build-maven.xml:29: Unable to resolve artifact: Missing:\n\n----------\n\n1) biz.aQute:bnd:jar:0.0.258\n\nTry downloading the file manually from the project website.\n\nThen, install it using the command:\n\nmvn install:install-file -DgroupId=biz.aQute -DartifactId=bnd -Dversion=0.0.258 -Dpackaging=jar -Dfile=/path/to/file\n\nAlternatively, if you host your own repository you can deploy the file there:\n\nmvn deploy:deploy-file -DgroupId=biz.aQute -DartifactId=bnd -Dversion=0.0.258 -Dpackaging=jar -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id]\n\nPath to dependency:\n\n1) groovy:groovy-tools:jar:internal\n\n2) biz.aQute:bnd:jar:0.0.258\n\n----------\n\n1 required artifact is missing.\n\nfor artifact:\n\ngroovy:groovy-tools:jar:internal\n\nfrom the specified remote repositories:\n\ncentral (http://repo1.maven.org/maven2),\n\naQute (http://www.aQute.biz/repo)\n\nTotal time: 2 minutes 51 seconds\n\nError: Status 1 encountered during processing.",
      "meh!",
      "sudo apt-get install groovy",
      "to the rescue : D",
      "3. Compiling!",
      "I attended a presentation by Andy Hunt, he was talking about some of the stuff that he wrote in his book \"Pragmatic Thinking & Leaning\", which is very good by the way, and he mentioned context switching a few times. Naturally, that came up in often afterwards, specially on twitter and I brushed it off mostly. I did some context switching but only when taking breaks and I didnt think it was a big deal. Not until I saw this line:",
      "\"[groovyc] Compiling 1 source file …\"",
      "and it hit me. Most people are tempted to do some context switching when they are dealing with time wasting tasks such as compiling! I had completely forgotten how it felt like. I was so immersed in the Ruby/Python/PHP world that I totally erased the memories of \"Its compiling…\" from my brain!!!",
      "But in all seriousness, if you like Java but also appreciate what a language like Ruby provides and what a framework like Ruby on Rails adds to the table, Groovy and Grails are worth a shot. And if you want a quick overview and introduction to both, check out my friend, Mohamed Seifeddines thesis:",
      "This document is written as my Masters thesis and is based on a long individual learning period of Groovy and Grails through the developement of what later became SoukLubnan.com. A Website in its younger days that allows people in Lebanon to Buy & Sell products online.",
      "and heres the link: Introduction to Groovy and Grails"
    ],
    "summary_t": "Documentation Have you ever wished that you could take Google with you on the road when you dont have access to the INTRANET? or at least the documentation..."
  },
  {
    "id": "7b46a31d28a9e8311b95aea4c93a9097",
    "url_s": "https://opensourceconnections.com/blog/2010/02/05/small-business-revitalization-act-falls-short/",
    "title": "Small Business Revitalization Act Falls Short",
    "content": [
      "Yesterday, the House of Representatives introduced S.2989, the Small Business Revitalization Act, which punishes government prime contractors for failing to pay their subcontractors after the government has paid them for work performed.",
      "While noble in trying to ensure that small businesses get compensated for work performed, the bill seems to miss the mark on the bigger issue with small business subcontracting.  I admit, I cannot find the text of the bill (and would love it if someone could point it out to me), so I could be wrong, but based on the article linked above, it appears that a couple of key issues are not addressed:",
      "Prime contractors actually meeting their subcontracting requirements.  In most large contracts, prime contractors are required to have a subcontracting plan and meet certain thresholds of work sharing amongst disadvantaged groups.  Very few enforcement mechanisms exist to ensure these thresholds are met, and this act does not address the issue.\n  Prime contractors actually using the teams that they proposed in RFP responses.  If a prime wins a contract by describing teaming arrangements with certain companies, then it should actually utilize them as proposed to the government originally, or replace like-for-like on the team.  Again, no enforcement mechanism exists unless a contracting officer writes a constraining and binding contract.",
      "As I have written before, set-asides and subcontracting goals go against my libertarian nature, as I want the most value for my money as a taxpayer; however, if they are going to exist, then the least the government can do is make sure that the rules are enforced.  Unfortunately, the Small Business Revitalization Act, it appears, fails to help meet those goals."
    ],
    "summary_t": ""
  },
  {
    "id": "2f1d74744e81dbc420e144510cc650ac",
    "url_s": "https://opensourceconnections.com/blog/2010/02/08/microsoft-abandons-fast-on-linux-and-unix-and-opens-the-door-for-solr/",
    "title": "Microsoft Abandons FAST On Linux and Unix and Opens the Door For Solr",
    "content": [
      "Today, Microsoft announced that it was abandoning development of the FAST search engine for Linux and Unix. Given that Microsoft paid $1.2 billion for FAST, the move is an apparent revelation of a strategy to get non-Windows based users to move to an enterprise Windows platform rather than to continue to support FAST.",
      "This move seems to be risky. The Microsoft bet is that its FAST customers are more loyal to FAST than they are the operating platform, and the perception of switching costs are higher for moving from FAST to another enterprise search engine rather than the opposite–a loyalty to the operating system and a perception that search engines are interchangeable.",
      "Microsoft might be right for most of its customers, but this announcement will certainly be grist for the mill in IT departments over the coming weeks.  Many companies built their IT infrastructure around a Linux-based platform, and being forced to change to a Windows environment may be a pill that is too hard to swallow.  The alternative will be to look to other search engines, which can do nothing but help Solr and Lucene.  With an established user base, enterprise grade support packages from companies like Lucid Imagination, and a significantly lower total cost of ownership than the FAST + Windows package, Solr will appeal to many a CTO who might otherwise have continued to gladly pay the licensing costs for FAST but is now forced to reconsider his or her decision.",
      "Rather than supporting FAST on both platforms at the cost of a few developers, Microsoft may lose many more customers and revenues because of its insistence on one platform.  It will be interesting to see how companies like Lucid respond to the new opportunity."
    ],
    "summary_t": ""
  },
  {
    "id": "da51fbb0f6bc87668126e448f403795b",
    "url_s": "https://opensourceconnections.com/blog/2010/02/23/will-foss-damages-finding-spook-government-agencies/",
    "title": "Will FOSS Damages Finding Spook Government Agencies?",
    "content": [
      "Last week, the U.S. Federal District Court of Northern California found a software developer liable for unattributed use of a Java Model Railroad Interface.   The lack of attribution, as required in the license, the Court found, violated the Digital Millennium Copyright Act.  While the Courts finding has limited legal jurisdiction, it does set a precedent which will surely be cited in future cases looking for both damages and to set further legal precedent and define more fully the cases in which stare decisis will apply.",
      "While, analogously, no different than any other license violation, I predict that this finding will have a ripple effect in government adoption of open source projects in their applications, further dampening the adoption rates.",
      "Many government agencies are skeptical of open source in the first place.  Open source projects, rightly or wrongly, have a blanket aura of the two high school kids in a garage image.  Most government agencies like controls and process, which they dont perceive exist in open source projects.  If some of them tried to become committers on major projects, perhaps the perception would change.\n  Very few open source projects have sales representatives.  The sales representatives at major software companies such as Microsoft do an excellent job of the appropriate education and shepherding to provide assurances to the government buyers.\n  Walking through a purchase process gives the perception of license compliance.  A contracting officer can say that they have made a purchase and have a license and feel like they are in compliance, although audits oftentimes reveal shortcomings.  With an open source project, anyone can download source code and deploy the software onto a system, creating manifold increases in risk of license violations.\n  Most legal counsel isnt familiar with open source licensing requirements and restrictions.  Rather than digging into and understanding GPL or \"copy-left\" restrictions, its easier to just stay away from it.  Again, there are probably as many, if not more, EULAs than accredited and accepted open source licenses, but there are also sales representatives to walk through EULAs whereas the open source projects do not have such champions.\n  Its easier to create blanket restrictions than to make judgments on each case based on the specific merits of the case.  The finding in Jacobsen v. Katzer makes it easier to point to a known and potentially quantifiable risk to deny use.  The Government is, and in most cases rightly so, risk-averse, and this finding creates a potentially big risk.  The risk may be a black swan, but it is very hard to measure and, in a perfect (in the Governments view probably an imperfect) storm, could be potentially exceptionally large.",
      "While it wont have an immediate effect, the end result, I expect, will be an even more rigorous set of processes to complete for approval of the use of most open source software within the government.  I hope this is not the case, because there are many cases where open source software is the best solution.  Furthermore, I truly believe that:",
      "By and large, open source developers are not a litigious group.  They want to write software which makes life easier, and as long as they get the credit and attribution they want, theyre happy to share.  Its whats written in the licenses.  Plus, legal paths are expensive, and very few can afford to go down that path.  Compliance with a very few legal requirements or selection of a different platform will avoid any potential legal pratfalls.",
      "Thanks to Onlyopensource for pointing out this article!"
    ],
    "summary_t": ""
  },
  {
    "id": "784511076dfda889305bf9bccdec6e54",
    "url_s": "https://opensourceconnections.com/blog/2010/03/09/vthings-i-learned-about-last-week/",
    "title": "Things I Learned About Last Week",
    "content": [
      "Last week was the crucial week on my current Lucene -> Solr conversion project for making our goals. A lot of work the previous couple of weeks came together. I wanted to take a couple of minutes and just record some of the little things that Ive been learning about:",
      "Solr",
      "Sunspot is the up and coming solution for integrating Solr into Ruby on Rails, and fortunately enough, the 1.0 release (followed quickly by 1.0.1!) has just come out last week. Between acts_as_solr and Sunspot, Sunspot wins hands down for its support of a master/slave Solr configurations, embedded Solr for testing, richer indexing semantics, and not being tied to ActiveRecord. The companion sunspot_rails gem does give wonderful ActiveRecord integration however.",
      "Solr cores are the bees knees! Weve built a simple RoR webapp using HTTParty and the Solr API that allows you to perform all the admin functions for cores, and allows you to quickly clone a core for your own nefarious purposes! Simplifies hacking around with a new schema or configuration without having a local copy of Solr running. Allows multiple QA environments to potentially share a single Solr infrastructure.",
      "Solr master and slave setup in a single VM. While pointless from a scaling perspective, its a really great way to work out the kinks! Its funny to see a slave core polling the same Solr VM its in for updated segments!",
      "JRuby",
      "Doesnt suck after all. Actually, maybe I should say that JBoss, when combined with JRuby, means that JBoss doesnt suck so much. I had the aforementioned Solr core admin tool bundled up as a WAR file with JRuby, and was able to deploy it to an existing environment that had JBoss installed! I didnt have to install ruby on the box, (or JRuby for that matter!) I just deployed the WAR file and bamn, off to the races. Ops folks get the JBoss they love, I get the Ruby on Rails that I love.",
      "And on a related note, Warbler was the key to thinking JRuby is cool. Id never actually had to package up a RoR app, so Warbler came to the rescue. And you know what? It was nice to build a single file that I knew had everything that I needed in it that could be scped around! And thanks to some cool code in the environment.rb, my app was able to load up the right configuration file for the environment based on an environmental variable set in JBoss.",
      "Virtual Machines",
      "I recently migrated a Linux VPS based RoR + Solr app (see a trend in tech choices ;) ) to a Windows environment. And to deliever the new Windows environment, I used VirtualBox to host the Windows Vista environment on my Mac laptop.",
      "A couple of notes:",
      "VirtualBox may not have all the snazzy integration points of Parallels with the host computer like seamless application sharing, but it seems to be much lighter weight. Starts up quicker, and I dont get the spinning beach ball of death as much.\n  If you are shipping a 11 GB file, you cant use a 16 GB USB Memory Stick… Turns out the biggest file is 4 GB. Â (Although I never tried formatting the stick as NTFS, maybe that would have allowed a single 11 GB file???)\n  Uploading 11 GB to a remote out on the internet server will take a long long long time. Even on a really fast network. connection.\n  If you need to format an external USB hard drive as NTFS on a Mac, it is possible! Just fire up your trusty Windows Vista image in Parallels, plug the USB drive in, download and install the correct USB drivers so the drive doesnt show up as a network share mapped to the Mac, and then use the built in reformatting tools! Warning: This will take a loooong time!\n  Lastly, if you are using VirtualBox, and you attempt to create a Windows XP machine, and attach a Windows Vista hard disk image to it, VirtualBox will let you! And then Windows wont start. sigh."
    ],
    "summary_t": ""
  },
  {
    "id": "bd3f11ef72237beef2be59c4db13bd3a",
    "url_s": "https://opensourceconnections.com/blog/2010/03/18/the-state-of-opensource-on-the-microsoft-stack/",
    "title": "The state of open source on the Microsoft stack.",
    "content": [
      "This is like the state of the union address, except in mid march, and the only thing Im president of is my current residence. If you have ever studied Science, you know about potential energy vs kinetic, Or maybe a better metaphor for the reality TV generation, is the Swan.",
      "Open source software on the Microsoft stack has tons of stored potential, even some movement, but it is still left wanting. The evil empire has embraced open source software, releasing jQuery with Visual Studio, starting codeplex.com and the Codeplex Foundation. The are tons of abandoned projects or ones that have gone stale (log4net anyone?).",
      "Compared to java or even the new kid ruby, were lagging behind. We dont even have a fully managed open source enterprise web server, compared to Javas n-th variety of containers to pick and use.",
      "Even rails has a built in server. Of course there is kayak http web server framework and  webserver on codeplex, but theyre new, far from mature and their not an industry defacto standard like jboss or tomcat. Dont get me wrong there are some amazing open source projects out there. Gallio, db4o, Subtext, blogengine.net, facebook developer toolkit to name a few.",
      "However, there are gaps in having a full open source Microsoft stack. We have plenty of unit testing and mock testing libraries, but with NDoc gone, left to SandCastles release schedule, libraries left to rust like log4net wit not even .net 4.0 beta build or silverlight build. CruiseControl.Net is in dire need of revamp and version 2, at the very least it needs some decent competition that isnt java.",
      "With plenty of single person projects out there who just end up getting burnt out, seemingly stagnated public dialog from the likes of the 14, its hard to really get developers to rally and get some much needed things done. The community need some decent leaders, organizers, and some company backing.",
      "Most software vendors and clients get a great productivity boost from these projects. It would only make sense to invest in their growth, even pool resources for joint projects. Organize some hack-a-thons days with some cool prizes for work top-notch work. Even put together a small guild of programmers, just have 20 or so companies pitch in, put their banners on a website and churn out some decent open source projects that everyone can use. .Net isnt going away and its time the community and companies invest more into the open source community instead of letting all that potential go to waste."
    ],
    "summary_t": ""
  },
  {
    "id": "1d03d09256438e0445fc93221084099e",
    "url_s": "https://opensourceconnections.com/blog/2010/03/18/things-i-learned-last-week-part-2/",
    "title": "Things I learned Last Week Part 2",
    "content": [
      "Favicon and Rails?",
      "Want to use a favicon.ico but dont want to put it at the root as favicon.ico? Then add . The use of image_path means that you get all the goodness of Rails routing to generate a complete image path that will work. Even if you deploy under some sub URL, like a war file in JBoss!",
      "Are you working with ISO-8859-1 encoded text, (more info at http://www.w3schools.com/tags/ref_entities.asp).",
      "ISO 8859-1 Characters",
      "And look at the character Â (should be a capital A with a caret on top called a grave accent). That will kill Solr, regardless of container deployed in, like Jetty versus Tomcat. But the other ways of representing this entity work great:",
      "À\n    \n\n    \n      À",
      "Of course, there is a bit of confusion on this, as supposedly if the XML document posted to Solr is UTF-8 encoded, then Solr shouldnt have any issues. So, still some digging to do!",
      "Solritas and JBoss and Velocity Oh My",
      "I recently ran into a Java Classloader issue between JBoss and Solr when loading Velocity. If you are getting in the browser:",
      "TTP Status 500 – loader constraint violation: when resolving method \"org.apache.velocity.Template.merge",
      "or messages like \"SEVERE: java.lang.LinkageError: loader constraint violation: when resolving method \"org.apache.velocity.Template.merge\" in the logs, then that means a conflict in the velocity jars. Oddly enough, I could not actually find a velocity.jar anywhere in my JBoss app. However, the fix was to copy the velocity jar from Solr into my JBoss ./lib/ directory."
    ],
    "summary_t": "Favicon and Rails? Want to use a favicon.ico but dont want to put it at the root as favicon.ico? Then add <LINK REL=\"SHORTCUT ICON\" href=\"<%=image_path..."
  },
  {
    "id": "d0fb56b0cd53aaeab9a91713a62b0e12",
    "url_s": "https://opensourceconnections.com/blog/2010/03/22/annotations-what-they-are-and-why-i-want-them/",
    "title": "Annotations:  What they are and why I want them",
    "content": [
      "What they are",
      "Annotation is the process of selecting portions of an original document and adding additional information. These additions are not necessarily saved with the original and are commonly used by editors and scholars to include background citations. In Computer Science, annotations are used in Semantic Web and work collaboration technologies. The Comment function in MS Word is an example of using annotations for collaboration.",
      "Business uses",
      "Aside from these functions I think that there is a whole class of business processes that could benefit from annotation technology. For example, we often copy line items from a Request For Proposal (RFP) and paste them into our proposal. Later, these line items may get pasted into a Product Backlog, Production Readiness Checklist, or ticketing system.",
      "As a programmer, I shudder whenever I see so much copy-and-pasting. If a block of text is important enough to be present in multiple documents, it should be extracted as its own entity and reused. This eliminates the problem of maintaining that block in multiple places and ensures that each instance of the block is accurate. More importantly, it helps you concentrate on adding value rather than repeating yourself (see the DRY principle.)",
      "Categories of technologies",
      "Annotation technologies can be split into two broad categories based on where the annotations are saved: In a server or in the document. Annotations that are saved centrally in a server lend themselves to collaboration because multiple people can update the annotations without having to modify a master document. On the other hand, document control may be more important than collaboration, in which case storing the annotations within the document would be more appropriate.",
      "Example technologies",
      "While Googling for whats around the web regarding Annotations, I ran across Ian Lumbs blog. Â Hes got a number of excellent posts on the subject. Â I also found a number of different technologies listed over at the Semantic Web portal, but the list appears to be dated (several were private projects, there were a couple broken links, and a few of the projects appear to be unmaintained.) However, there were enough working projects there for me to get an idea of what people are doing in the field of Annotations. The promising projects I saw there were:",
      "Annotea\n  Annozilla\n  GATE Teamware\n  Semantic MediaWiki+\n  KIM Semantic Annotation Platform\n  Nuxeo Document Management",
      "Annotea is more of a protocol than software, but there were a number of client and server implementations listed (Annozilla was one of them). The Zope server product ZAnnot was a breeze to install on top of a fresh download of Zope, and I was able to get it working with Annozilla pretty quickly. Annozilla itself, though, needs a little TLC before I can incorporate it into a working system.",
      "First of all, Annozilla hasnt been updated in about nine months and requires FireFox 3.5 (current version is 3.6 and I reinstalled with version 3.5 just to try it out.) Secondly, the annotations themselves are free-form and cannot be constrained or reused (such as with an ontology). This is a well-known problem in the world of tagging content, where search-as-you-type tagging helps you avoid multiple tags that are almost but not exactly the same (such as the tags \"annotations\" and \"annotation\".)",
      "GATE (General Architecture for Text Engineering) is an amazing collection of projects centered around doing things with text. I found only one technology in there that was relevant to annotations, and that was regarding automatic annotations (examining text and feeding annotations into Annotea.) There were a lot of other interesting libraries in there and I hope to check them out later. Â Specifically, they announced a Teamware application would be forthcoming which would incorporate annotation technology and workflows.",
      "Likewise, the KIM Semantic Annotation Platform seemed to be oriented more toward automatic annotation rather than streamlining human annotation.",
      "Nuxeo Document Management is a Java-based product very similar to Alfresco. Â It has a document preview feature that shows you an HTML version of Word or PDF documents it stores, and also has an annotation module that lets you annotate that preview. Â [edit: Â When I originally installed it I rushed through and misread some of the documentation. Â Stefane Fermigier, the founder of Nuxeo saw this post and corrected me, but I have yet to revisit Nuxeo. Â In theÂ interimÂ I’ve edited this post to remove my misinterpretations. Â Hopefully soon I’ll be able to follow up with a more in-depth look at Nuxeo.]",
      "Lastly I gave SMW+ a try. Â Ive used Semantic MediaWiki before and I think it has great promise. Â I especially like the rich report formats that you can generate with semantic queries (like showing a SIMILE timeline as the result of a time series query.) Â There was a long list of extensions to install, but I eventually got to the point where I could copy and paste a whole RFP as a wiki page and annotate it. Â The annotation tool was AJAX-based and a little clunky, but once I was done I had a nice report of each annotation in the original document. Â It wasnt clear how to bind an ontology to it, so I have the same complaint with it as I do with Annozilla.",
      "Parting Thoughts",
      "The kind of annotation I want to do seems possible using some of the WYSIWYG editors embedded in most blogs and CMS (like kupu or TinyMCE). A colleague of mine noted that you could simply supply a custom CSS style that would be included in the menu of available styles, but refrain from adding any style changes to it. So in theory any CMS which provides document preview ala Nuxeo should be able to supply an annotation editor which wouldnt change the original. That to me is the most promising direction.",
      "If you have any experience with similar technology please let me know in the comments."
    ],
    "summary_t": "This article describes my first impressions of existing (preferably free) annotation technology and how it measures up to my expectations."
  },
  {
    "id": "010dacef40bc79e4c029c5d37ad2e9b2",
    "url_s": "https://opensourceconnections.com/blog/2010/03/31/becamp-2010-is-april-30-may-1st/",
    "title": "beCamp 2010 is April 30 & May 1st",
    "content": [
      "beCamp 2010 is almost here! April 30th and May 1st are just four weeks away!",
      "If youre a geek in or around the Charlottesville metroplex or even if youre merely tech-curious, this is the event you dont want to miss. A beCamp is Charlottesvilles version of the BarCamp unconference phenomenon, organized on the fly by attendees, for attendees. Realizing that the most energizing parts of any tech conference are the ad hoc conversations that take place in the hallways between the sessions, beCamp facilitates these types of interactions for an entire event.",
      "As of this writing, we are at 87 campers! To participate, just add your name to the wiki page!",
      "A big thank you to all our sponsors, including at this point, Hotelicopter, Google, Perrin Quarles and Associates, NRAO, and University of Virginia ITC. Interested in supporting the Cville tech community? Check out our needs at http://barcamp.org/sponsor-beCamp-2010."
    ],
    "summary_t": ""
  },
  {
    "id": "90404b80949a2707ff5747192a0ea900",
    "url_s": "https://opensourceconnections.com/blog/2010/04/21/when-talking-time-zones-bogota-eastern-time-us-canada/",
    "title": "When talking time zones: Bogota != Eastern Time (US & Canada)!",
    "content": [
      "Ive been using the timezone localization technique of asking the browser when the page loads what the browsers timezone offset from UTC is, and posting that back to the server and storing it in the session. Â However recently I noticed that with the event of Daylight Savings Time, this was no longer working, because my time would come up an hour off here in Virginia.",
      "After much faffing about, I finally figured it out. Â On the server I would ask for the set of timezones that matched the offset, and grab the first one and put that in the session:",
      "[sourcecode]\nresult = ActiveSupport::TimeZone.all.select{|t|t.utc_offset == gmtoffset}.first\nsession[:time_zone] = result.name\n[/sourcecode]",
      "The list of named time zones returned when the browser is in Charlottesville, Virginia are: Bogota, Eastern Time (US & Canada), Indiana (East), Lima, Quito.",
      "So when I use Bogota as the timezone, and ask Rails to show the time localized:",
      "[ruby]\nTime.now.in_time_zone(session[:time_zone])\n[/ruby]",
      "I get back the time without taking into account daylight savings wrong. I started trying to figure out if the browser was in a DST zone using this JavaScript code: http://www.michaelapproved.com/articles/daylight-saving-time-dst-detect/ and while it seems very promising, it still wasnt quite giving me what I want.",
      "Finally, I realized it…. By arbitrarily grabbing the first time zone in the list, I was showing time in Bogota, Columbia. But if I chose Eastern Time (US & Canada) then I do get a localized time that takes into account day light savings!",
      "So right now I have this method:",
      "[ruby]\nresult = ActiveSupport::TimeZone.all.select{|t| t.utc_offset == gmtoffset && t.name.include?(\"US\")}.first\nsession[:time_zone] = result.name\n[/ruby]",
      "Obviously this is pretty hardcoded to just work in the US, and isnt a real solution. Id love to hear other ideas! Part of me wonders if I should just display all times in UTC in HTML, and have some sort of client side JavaScript that localizes the time display?",
      "My full set of code:\nJavascript in my index.html.erb view:\n[javascript]\n// Calls the server and sets the users time.\nEvent.observe(window, load, function(e) {\nvar now = new Date();\nvar gmtoffset = TimezoneDetect();\n//use ajax to set the time zone here.\nvar set_time = new Ajax.Request(<%=url_for :controller => \"home\", :action => \"gmtoffset\"%>?gmtoffset=+gmtoffset, {\nonSuccess: function(transport) {\n//alert(\"Response\" + transport.responseText);\n}\n});\n});",
      "// http://www.michaelapproved.com/articles/daylight-saving-time-dst-detect/",
      "function TimezoneDetect(){\nvar dtDate = new Date(`1/1/ + (new Date()).getUTCFullYear());\nvar intOffset = 10000; //set initial offset high so it is adjusted on the first attempt\nvar intMonth;\nvar intHoursUtc;\nvar intHours;\nvar intDaysMultiplyBy;",
      "//go through each month to find the lowest offset to account for DST\nfor (intMonth=0;intMonth < 12;intMonth++){\n//go to the next month\ndtDate.setUTCMonth(dtDate.getUTCMonth() + 1);",
      "//To ignore daylight saving time look for the lowest offset.\n//Since, during DST, the clock moves forward, itll be a bigger number.\nif (intOffset > (dtDate.getTimezoneOffset() * (-1))){\nintOffset = (dtDate.getTimezoneOffset() * (-1));\n}\n}",
      "return intOffset;\n}",
      "[/javascript]",
      "home_controller.rb action:\n[ruby]\ndef gmtoffset\ngmtoffset = params[:gmtoffset].to_i*60 if !params[:gmtoffset].nil? # notice that the javascript version of gmtoffset is in minutes ;)",
      "result = ActiveSupport::TimeZone.all.select{|t| t.utc_offset == gmtoffset && t.name.include?(\"US\")}.first\nsession[:time_zone] = result.name",
      "render :update do |page|\npage.replace_html time_of_chat_starting, :partial=> super_short_time\npage.visual_effect :highlight, `time_of_chat_starting\nend\nend\n[/ruby]",
      "Rendered partial helper view _super_short_time.erb:\n[ruby]\n<%= super_short_time(Time.now.in_time_zone(session[:time_zone])) %>\n[/ruby]"
    ],
    "summary_t": ""
  },
  {
    "id": "06e3fde4a57ff59179ce5a44c6f1f7dd",
    "url_s": "https://opensourceconnections.com/blog/2010/04/26/arin-sime-to-present-at-agile-2010-on-range-estimation-in-scrum/",
    "title": "Arin Sime to present at Agile 2010 on Range Estimation in Scrum",
    "content": [
      "I am looking forward to presenting at Agile 2010 this year, and I was very pleased to find out that my session proposal on Range Estimation in Scrum has been accepted. Here is info about the session, and I hope to see you at the conference in August!",
      "Building a More Accurate Burndown: Using Range Estimation in Scrum",
      "Traditional Scrum burndowns are based on single point estimates of how long a task will take. However, single point estimates are inherently faulty and inaccurate, and they encourage underestimation. Learn how to incorporate range based estimation techniques into your Scrum burndown, and better communicate to your boss or clients what a project is really going to take. Arin will back up this thesis with academic and industry research, real world examples, and an engaging presentation style. Participants will leave with concrete tips & templates for using range estimates in their projects.",
      "**Process/Mechanics\n**\nThe first part of the session will focus on the pitfalls of traditional estimation techniques, in particular single point estimates. This will involve audience interaction as well as citing industry and academic research. Next the session will move into a discussion of range estimation and how that leads to better accuracy. Then I will present specific advice for how to use range estimates in common Scrum practices like Scrum poker and the burndown. Along the way I will show examples of how our company has been using this practice with clients over the last year, and some best practices that have come out of it. The techniques will be fairly simple and easy to adopt, but provide power results in estimating more accurately and better communication with managers and clients.",
      "Potential pitfalls and best practices of range estimation will also be discussed. For example, some managers are reluctant to accept range estimates, but they can be convinced when a demonstration is made to them of how it better communicates risk and allows them to make better financial decisions on the viability of a project.",
      "This topic started for me with a paper I wrote in my Masters in Management of IT program in 2009 at the University of Virginia, and now has become regular practice at OpenSource Connections. I am also pursuing further research on the topic with two professors from the University of Virginia (Professors Ryan Nelson and Mike Morris of the McIntire School of Commerce). Any preliminary findings or papers from that research will also be woven into the presentation.",
      "I have proposed this as a 60 minute talk, but it could also be made into a 90 minute Tutorial with relative ease. In that scenario, I would bring more hands on activities to encourage participants to really see the value of range estimation in their projects. I have experience with those types of sessions from helping to teach corporate education classes at Virginia Commonwealth University, and I would use that experience to make sure things are kept interesting.",
      "Itâ€™s my intention for this session to take the relatively easy-to-use but underutilized practice of range estimation, and make it so compelling that participants will want to apply it right away on their projects. This will be done through a combination of good research, real world examples, engaging presentation style, and practical tips.",
      "Learning outcomes",
      "Why single point estimates lead to underestimation\n  Why range estimation reduces natural biases\n  How range estimates better communicate a projectâ€™s impact\n  How to use range estimation in Scrum Poker\n  How to incorporate range estimates into a Scrum burndown\n  How range estimates impact project decision making",
      "For those registered on the Agile 2010 site, you can also see this session here."
    ],
    "summary_t": ""
  },
  {
    "id": "df942ea0d0e6f27c19b83ff9bf13c4b6",
    "url_s": "https://opensourceconnections.com/blog/2010/04/26/scott-stults-is-the-inaugural-osc-code-ninja/",
    "title": "Scott Stults is the inaugural OSC \"Code Ninja\"",
    "content": [
      "Last Friday OpenSource Connections held one of our regular \"hackathons\", where OSC developers get together for a day and work on a development project of our choosing, and then at the end of the day, present our work to each other. A hackathon is a fun event where we each get to explore some technology we are interested in, and see how far we can get in one day with it.",
      "This time, we decided to mix up the way we do hackathons. We added in a theme, voting on the best project, celebrity judges, a gift certificate to Best Buy, and even a trophy! The usual elements of time pressure, creativity, fun, caffeine, and a beer at the end of the day were still present.",
      "At the end of the day: Scott Stults was crowned the inaugural OSC \"Code Ninja\". Congratulations Scott! In a moment I will describe a little more about the hackathon, but first, lets all sit back and enjoy this impressive photo of Scott with his prized trophy. Scott is making a feeble attempt to strike the same pose as the karate-guy on the trophy.",
      "For the hackathon, we started the day by meeting at the Nook for breakfast. Then we headed over to a conference room at CitySpace where we set up camp for the day.",
      "The theme of the hackathon was \"time\", which meant that you could build any application you wanted, as long as there was some element of time in it. The app had to be built in the 7 or so hours of the hackathon, and judging began at 4:30pm. The applications were judged on three axes: Innovation, Business Utility/Usefulness, and Completeness. Each person voted on all the projects except for their own, giving 1-5 stars for each app on each of the three axes. The project with the highest total number of points was the winner.",
      "To add another dynamic to the voting, we invited \"celebrity judges\" to join us at 4:30 and they had votes as well. Our celebrity judges were Glenn Wasson and Brian Wheeler. Glenn is an architect at SAIC, and is perhaps most famous for being one of the founders of \"The Oracle of Bacon\" website. Brian is famous for the Charlottesville Tomorrow website.",
      "Both judges were excellent, and very gracious with their time since we kept them there until nearly 6pm on a Friday night. Thanks Glenn and Brian!",
      "We had a variety of applications built. Caleb worked with Html5 to build a dynamic graph for tracking stocks. Michael worked on extending unit testing frameworks in Visual Studio. Eric built a website where you can send your server logs to, and it will stream out audio sounds that you can listen to which indicate what types of messages are being logged (errors, info, etc). Youssef and I teamed up to build a droid app and deploy it to my phone. We had some cool ideas, but ended up scaling them down to building an Agile standup meeting timer. It made my phone vibrate, which was of course extremely cool.",
      "Despite that coolness, and despite a strong challenge from Eric, the inaugural code ninja is Scott! Scott didnt just win because he was the only one of us to prepare a powerpoint presentation about his app (that actually lost him a few votes Im guessing – yuk yuk), but because he made wikis exciting! Scott used SPARQL in Semantic Media Wiki SMW+ to show how we could track our projects at OSC and see timelines of who is working on what project.",
      "Thanks to everybody for a great time – Im looking forward to the next hackathon! I understand that Scott has removed family photos in order to place the trophy in a place of honor at his home, and so I wish the next Code Ninja the best of luck in wresting it away from him."
    ],
    "summary_t": ""
  },
  {
    "id": "921eeda540756e41c71d27199f24c802",
    "url_s": "https://opensourceconnections.com/blog/2010/05/03/b-d-d-thoughts-reloaded/",
    "title": "B.D.D. Thoughts Reloaded",
    "content": [
      "Its not exactly easy to get people to read requirement or technical specifications, much less write them. Its also hard to getÂ  developers to write tests. Its even harder to get them to write solid usable tests. B.D.D (Behavior Driven Development) which is a spin off of T.D.D (Test Driven Development) focuses on the language of testing and and quality assurance of software, in hopes to make creating tests easier as well and specifications. The concept was coined by Dan North. Since then new libraries like rspec, cumcumber, specflow, nbehave, machine spec have come about. I can agree that sometimes changing the business language can help people with making associations and understanding of processes easier. And using \"Behavior\" instead of \"test\" does help in understanding that you need to validate how the software or code is supposed to behave.",
      "Questioning The Boilerplate –  Human Readable Specifications",
      "Where people need to be more skeptical is the question of is there real business value in getting code to generate boilerplate systematic specifications? For an example, Im pulling this from a",
      "cucumber example from github.",
      "# language: en\nFeature: Addition\n  In order to avoid silly mistakes\n  As a math idiot\n  I want to be told the sum of two numbers\n\n  Scenario Outline: Add two numbers\n    Given I have entered <input_1> into the calculator\n    And I have entered <input_2> into the calculator\n    When I press <button>\n    Then the result should be <output> on the screen\n\n  Examples:\n    | input_1 | input_2 | button | output |\n    | 20      | 30      | add    | 50     |\n    | 2       | 5       | add    | 7      |\n    | 0       | 40      | add    | 40     |",
      "We live in a time surrounded by constant news, media, following four subplots instead of one. Its hard to get people concentrate for long periods of time or sit still, much less to get them to read technical or requirement specifications. Now I was to write this scenario for a requirement out as a spec, it would read like the following.",
      "The mad scientist2 with his destructo calculator enters input 1\nand input2. He manically presses said red button . The output\nmust be the sum of input 1 and input 2.",
      "(i1 + i2 = output)",
      "So why are we creating specifications that repeat words like I,As,When,Given,Then like bad boilerplate code? They might be human readable, but the questions of being understandable, easy to read, enticing to read, is a whole different story. I have to honestly question that even if a developer takes their time and makes sure the code is up to spec. The question is are the specs kept up to day (requirements change). And are lay people going to want to sit through and actually read these things? Is this a real business value? I don’t see any at this point. I see no real return on time invested to write boring systematic specs and just to have them match up in some way with code. Specifications, like any documentation worth it’s salt, needs to be able to:",
      "be engaging and appeal to more than the logical side.\n  provide the meat of information that is digestible.\n  avoid templating that read like government contracts.\n  be terse and yet expressive.",
      "We have been here before",
      "Their have been tools like this — (Fit,Finesse) — before Dan North was really doing this thing. Yet you dont see much usage from them across projects. I think the reason why cucumber has caught on, is because the rails community has been keeping momentum from going from one \"next big thing\" onto the next one. Cucumber is the new pink. But the end result is the same. Less than specular on par with boring. Hard to digest boiler plate specifications. Dont get me wrong, having specifications to work from is important. It helps to drive development and allows others to collaborate and work together. But this doesnt mean B.D.D is on the right track with its story/QA methodology. I think there is value of translating specs into code to some degree and testing against requirements. I think the value is lost when:",
      "coders who might not fully understand the business are writing specs.\n  business analysts who have no clue to code write specifications that affect how code is written.\n  specs writtenÂ  in a templated manner that is the antithesis of being terse.",
      "Should vs Must",
      "Also the domain language is for B.D.D. a bit off. If youve read technical specs, you know there is a vital difference between \"Should\" and \"Must\". Should is something software could do, but doesnt have to. Must is an absolute requirement. If you dont believe me, check out the W3 specs sometime. Yet, weve chosen \"should\" for some reason to put everywhere inside of our \"specification\" code. If were going to write specifications, we must chose words that fit.",
      "Conclusion",
      "B.D.D. isnt evil or bad, but still needs to evolve, be debated, thought on, expounded. I think the software community as a whole needs to watch out from getting caught up too much in trends for the sake of being first to do something and buying the t-shirt to say he/she was there."
    ],
    "summary_t": ""
  },
  {
    "id": "55b092b97e076a2330a98ea3ed149ee6",
    "url_s": "https://opensourceconnections.com/blog/2010/05/25/agile-cio-avoiding-heroes/",
    "title": "The Agile CIO:  Avoiding heroes",
    "content": [
      "Everybody loves employees and consultants who can get stuff done. Especially if youve made a commitment to a client or your boss, it can be very difficult to ask for an extension to that timeline. Its much easier to ask your team to put in the extra hours to get the job done, even if that means the job wont be done well. Sometimes leaders will even resort to claiming the product is \"done\", when it was never truly done at all, and needs a lot more work.",
      "Your development team will likely respond to your calls with heroic effort. And you might actually make the deadline. But if you do it regularly, your reputation as a leader may actually suffer, and your team will probably burn out unless you can pour on enough accolades and spot-bonuses.",
      "At some point, almost all IT leaders, including myself, have done it. That is unfortunately not much of a secret and its why IT teams often develop a bad reputation. The bigger secret is that a lot of developers not only regularly take on the hero role, they actually like it.",
      "This creates a dangerous situation, where IT leaders ask for heroic efforts because it is the path of least resistance. And the development teams respond with grumbles and late nights, but also with the misconception of job security and the expectation that they will receive a lot of praise when they come in on Monday bleary-eyed (but with at least part of the job done).",
      "Why is there so much danger in encouraging your developers to be heroes? Do a google search on \"cowboy coders\" or \"programmer heroes\" and youll see other blog posts that describe the whys and the pitfalls of reliance on hero development. In short, youll get inconsistent and buggy code that may not fit your architecture, wont meet the customers needs, is not very flexible, creates technical debt you will complain about for years, and will probably need to be reworked eventually.",
      "So how do you avoid heroes? As a CIO or technical lead, the tools of Agile offer many solutions:",
      "Sprint planning – restricting the work that is done to the current sprint helps discourage sudden changes of priority. Most tasks can in reality wait until the next sprint to be added, especially if the customer understands that it means the current sprint has to be put on hold or canceled.\n  Daily standups and regular customer interaction – Chronic hero programmers often like to hideaway on their work, and so they can be more easily detected when you are interacting with the customer each day. There is less opportunity for the developer to hide away and not give status updates, and the standups provide the customer more immediate feedback on everything that is being held up because they sent the hero on a distracting quest.\n  Team estimation – When the team estimates tasks for a sprint together, before they are assigned to people, that encourages open discussion and debate on how long something will take.\n  Continuous Integration – Maintaining rules about daily code check-ins and regular integration and unit testing prevents the hero from going too far away from the mainstream code base, and gives visible signs of progress. When done properly, it also helps to reduce the bugs delivered in the rushed code.",
      "In addition, in our OpenApproach implementation of Agile and Scrum, we use range estimation of tasks. Asking people to give you a single-point estimate of how long something will take encourages them to be overly optimistic. \"Oh, I can get that done in about 2 hours.\"",
      "Really? What if its not as simple as you or the developer thought? Then the only solution is to put in a heroic effort if the timeline must be met.",
      "A simple range estimate will provide a much more realistic view, and encourages a more thorough thought process: \"It should only take about 2 hours of actual coding, but Ill have to switch projects and get that site running again on my machine. Plus, sometimes those java script issues can be really though to debug, so it could take as long as 8 hours of work.\"",
      "As an Agile CIO, its your job to not only be understanding of your developers when they give you a range estimate, but encourage it. Dont settle for the heroic answer of \"Ill just put in some extra time this weekend.\" Get to the root cause of the problems, and make sure the customer or your boss is aware of the delays earlier rather than later. It will make all of your lives easier, and help you to avoid the pitfalls of programmer heroes.",
      "The Agile CIO is a series of blog posts advising IT leaders on how best to incorporate Agile techniques into their organizations. For more information about OpenSource Connections Agile process consulting services, please contact the author at [email protected]"
    ],
    "summary_t": ""
  },
  {
    "id": "9b83866e09f443b688b0fc4efcb34939",
    "url_s": "https://opensourceconnections.com/blog/2010/06/10/arin-sime-speaking-at-agilecville/",
    "title": "Arin Sime speaking at AgileCville",
    "content": [
      "Im looking forward to speaking next Tuesday night at AgileCville about range estimation in Scrum. Its been a while since Ive been able to attend AgileCville meetings regularly, so Im excited to reconnect with everyone. I hope to see you there!",
      "AgileCville\nDate: Tuesday, June 15th, 2010\nTime: 6 PM to 7:30 PM\nVenue: OpenSpace\n455 Second Street SE, Suite 100\nCharlottesville, VA 22902"
    ],
    "summary_t": ""
  },
  {
    "id": "acc5be7d429a4bd85fd04d1e78843fd9",
    "url_s": "https://opensourceconnections.com/blog/2010/06/25/the-agile-cio-virtualizing-development-environments/",
    "title": "The Agile CIO:  Virtualizing Development Environments",
    "content": [
      "Agile teams should be based on talented development staff with multi disclipinary skill sets. This allows team members to easily trade tasks and keep the iteration moving forward. As a consultant, I have seen and worked in many environments where development staff are very silod in their skill sets. In most cases they are very good at what they do, but at various times each team member can become a bottleneck because they are the only person who knows how to do a particular task or work on a certain legacy system.",
      "But multi-disclipinary skill sets are not the only key to fluidly switching people between tasks. You must also be able to change technology or development environments easily as well, and this is an area that I find almost all development environments suffer from.",
      "As an IT leader, when you bring in a team of consultants to augment your team and give their velocity a shot in the arm, you expect them to integrate quickly and be productive right away. And thats what the consultant wants too. But are you giving them the tools they need to be effective right away? Or do you expect each of them to go through the same development environment setup that you ask new hires to do?",
      "When you hire a new person, perhaps its worth the time to have them setup everything about their development environment. They learn about the nuances of your system, and a few days or a week of their time is nothing compared to the long term investment you expect from having them work for you for several years.",
      "When you bring in a consultant, its different. You may only expect to have them for a few months, and their time is more expensive then your internal staff. And they are not going to benefit much from the learning experience of setting up a dev environment.",
      "And even for your full time staff, what happens when they have to upgrade their machine a year from now, or their system dies and they have to rebuild from scratch? Do you really want to have them spend another week trying to remember how to get past that one install bug that they encountered a year or more before?",
      "One answer to this problem is to develop code using virtual machines. Instead of having your first developer start by setting up the toolsets and plugins on their machine, have them start with a clean virtual image, using a tool like VMWare Player. Keep a bare bones image around with your basic development tools, or start with one from a similar project. After the developer gets the code started and everything basically working, then they should make a copy of that image and distribute it to the rest of the development team.",
      "A backup copy of this image should also be kept on a network share or external drive so that when you add in consultants, or bring in a new hire, they can get started on code just as soon as they copy the image over to their machine. Instead of spending a week getting everything installed and picking the brain of your development staff, or Googling on obscure error messages, the only time delay is how long it takes the image to copy to their local machine (probably no more than an hour in most cases).",
      "Now you can spend their time where its valuable: Bringing them up to speed on the nuances of your code itself, not the right service pack or magic incantations to get the development tools working.",
      "You might be thinking that with cloud computing and languages like Ruby on Rails, shouldnt this be a non issue? I just pull down the code from svn or github, and Im ready to go! In my experience, while that is usually the case with relatively small projects or with projects started in the last year or two, its not always the case with lots of other code that exists in the world and that we have to work on.",
      "I was reminded of this because of a client I recently worked with is still using version 1.1 of the .NET framework, and so is required to run Microsoft Visual Studio 2003. As part of the project well probably end up doing an upgrade to more recent versions, but we still have work we have to do first. And unfortunately I discovered on my first day on the project that Windows 7 does not play well with Visual Studio 2003. Once it installed, it wouldnt connect to IIS in debug mode. After doing some googling and seeing that this is a common problem with no easy solution (I tried many of the recommended solutions to no avail), I decided it was time to just get an image of Windows XP going. The rest of the dev team is on XP, but they are not using virtual machines to do it unfortunately.",
      "So I started creating a new XP image under Virtual PC, only to have it started failing on certain files. Even though the install continued, I ended up with an image that could not talk to the outside world and was clearly half-baked.",
      "So then I started over with VMWare Player, and as I write this, my XP image with Visual Studio 2003 already installed is nearly done. 3 days later. Before I start changing code on it, Ill be setting the image aside to give to another consultant joining the team in a few weeks, so he doesnt have to go to the same level of pain. Ill also make sure the client has a copy of that image to make their life easier in the future the next time they bring us in or make a new hire.",
      "Before you attribute the delays I experienced to Microsoft, Ive seen similar challenges on Php projects and to a lesser degree on Groovy on Grails projects.",
      "Virtualization is great for reducing the size of your server room, cutting down on power bills, and becoming more green. But dont let it stop there. A little bit of time spent up front on virtualizing your development environment will save you a lot of heartache and expenses later down the road, and make your IT shop much more agile.",
      "The Agile CIO is a series of blog posts advising IT leaders on how best to incorporate Agile techniques into their organizations. For more information about OpenSource Connections Agile process consulting services, please contact the author at [email protected]"
    ],
    "summary_t": ""
  },
  {
    "id": "521e950adf6d9c8f2218e2695a83d5e0",
    "url_s": "https://opensourceconnections.com/blog/2010/07/07/be-effective-when-working-in-a-really-coolfundistracting-place/",
    "title": "Be Effective When Working In a Really Cool/Fun/Distracting Place",
    "content": [
      "Where are Eric and Youssef?",
      "This year two OSC folks are working remotely for extended periods of time. Youssef spent 6 weeks in Lebanon earlier this year, and I am spending the month of July in the mountains of Western North Carolina. We had talked about some of the rewards as well as challenges of working effectively when you are in a new/fun/exciting/[insert superlative here] place. Youssef and I came up with a few tips to make balancing work/play easier!",
      "Tip 1: Establish a Routine",
      "Youssef says: There will be no shortage of distractions, specially during the World Cup season. A trip to the mountains, a music festival or a friends unannounced (or announced) visit are all good reasons to drop what youre doing and enjoy the place you are in. The only way to both get your work done and have fun is to know when is the appropriate time for each. I found that establishing a routine worked best for me. I woke up early every morning and took advantage of the early quiet hours of the day. Started work at 8am and got plenty done early on. Then I gave myself a couple of hours of break time for unscheduled events (a friends visit, a trip to the store to get souvenirs, or even a couple of hours at the beach). Then I resumed work till the evening when I had a conference call scheduled every day at 5:45pm. After the call I was free to do whatever I wanted and that gave me plenty of time, specially in a city like Beirut that does not sleep.\n** Eric says:** I know that I sometimes struggle to get started working in the morning. And being in the office makes it simpler because everyone else is working. So having a specific schedule of when I am working helps me shift my brain out of family/vacation mode and into work mode!",
      "Tip 2: Fast Internet Connection",
      "Eric says: When we looked for cottages to rent, one of our requirements was that they have something better then dial up! We got lucky that one of our top choices was just installing a HughesNet satellite link at the cottage.\n** Youssef says:** It is embarrassing to be a on a call with a client and have that call be constantly dropped because of a bad connection. Skype requires a decent bandwidth and fast connection to work properly. You do not want a 15 minute stand up call to turn into a half an hour of you asking the people on the other end \"Can you hear me now?\" (Verizon does not operate in foreign countries!). So make sure you get yourself a decent internet connection even if you have to spend a little extra on it. And always take any install CDs or DVDs you think you might use with you. Some countries do not have the proper infrastructure and thus impose a cap on the download/upload limit you may use, which makes you think twice about how you use that limit (downloading the new Xcode from apples website is not an option).\n** Eric chimes in: **I ran into the same Skype issue, turns out Skype doesnt work well with satellite internet due to the latency issue! Fortunately, if I am upstairs on the west side of the cottage, standing on one foot, with my finger in my ear, my iPhone gets 1 bar of signal! Phew. I did notice that I needed to talk a little slower and not move about too much and folks can hear me.",
      "Tip 3: Regular Checkins",
      "Youssef says: Stand up calls in Scrum keep the developers honest. When you are working remotely, you do not want to lose touch with the work environment, specifically in a field that evolves constantly such as software development. Having a point of contact in the \"real world\" or the \"work world\" helps you stay in touch and not lose focus.\n** Eric says:** When you are in the office everyday, you pick up all the water cooler gossip. But working remotely means you lose that contact. I find that having a regular series of checkins helps stay connected. I put them on my calendar so they dont get skipped!",
      "Tip 4: Track your time!",
      "Eric says: Dont forget that you are working remotely is because you are doing something special. In my case, its taking my family for a month to the mountains. So I want to make sure that I dont miss out on the experience. Over the four weeks I am here, I am taking a week of vacation, spread out a bit. So I am tracking my hours both to make sure I dont shortchange work, but also dont shortchange the experience of being 200 feet from the Appalachian Trail!\n** Youssef says:** I definitely did not travel for 24 hours, crossed an ocean, a continent and a sea to do the exact the same things I was doing in Charlottesville. It also does not mean I should use that as an excuse to ignore my responsibilities. Tracking your time helps you balance out the experience and establishing a routine like we mentioned above goes well with this concept.",
      "Tip 5: Stay Agile",
      "Youssef says: Working remotely does not mean you should change the way you work, but flexibility is key. Working on the east coast while your client is on the west coast requires some coordination. Working in Lebanon (GMT + 2) while your coworker is on the east coast and your client is on the west coast, well that requires some bending and flexibility. The key part about dealing with this situation is an agreement with all parties involved about which approach to follow. The method I followed included three chunks of time. A chunk where I was working ahead of everyone else (during which I could be dealing with issues that came up the day before), a chunk of time where everyone was available at the same time and third chunk when I was off the clock while everyone else was still working. That allowed for ample communication and work time without sacrificing my fun time. Others might prefer to synchronize the working time, or might prefer to just deal with the delays. Balance worked best for me.\n** Eric says:** I agree with Youssef, being flexible on your approach to work can make being remote a very enjoyable and productive experience!",
      "Looking out of my window at NC Mountains"
    ],
    "summary_t": "This year two OSC folks are working remotely for extended periods of time. Youssef spent 6 weeks in Lebanon earlier this year, and I am spending the month of..."
  },
  {
    "id": "1159c8c324cbf710d9164119d7f25ae9",
    "url_s": "https://opensourceconnections.com/blog/2010/07/28/the-agile-cio-failing-fast-and-bad-stand-ups/",
    "title": "The Agile CIO: Failing Fast and Bad Stand ups",
    "content": [
      "Recently I had a chance to speak about Agile with experienced IT professionals from the 2011 class of the UVa McIntire School of Commerces MS in Management of Information Technology. This was a particular pleasure for me since I graduated from the same program earlier this year as part of the 2010 class. My talk was an introduction to agile practices, and talking about the values inherent in an agile team. You can see the slides here:",
      "http://www.slideshare.net/o19s/intro-to-agile-practices-and-values",
      "The discussion covered a lot of overview topics in Agile, in fact too many topics to fit into our one hour time slot! But true to form, the students in the class were great – they asked a lot of questions and we had a very engaging two-way conversation. I much prefer being \"agile\" in my presentations and having to skip material because the audience is engaged. Its a lot better than a command and control presentation where only the speaker is talking!",
      "Among the many great conversations were two that I want to highlight on the blog.",
      "Tip #1: Failing Fast does not mean sacrificing quality",
      "Agiles value comes in iterations",
      "One of the big values of agile development methods is the emphasis on short, iterative development cycles. This allows you to push changes to the customer more often, get their feedback, and incorporate that feedback into future releases. Iterative development allows you to get some incremental value from your work right away, instead of waiting until the \"big bang deployment\" when all features have been completed and are all released at once. Big bangs are riskier because you havent received any customer feedback yet or seen how the customer will use your software.",
      "Failing Fast means knowing what the customer really wants\nIn agile, we talk about \"Failing Fast\" because if a customer doesnt like or doesnt use a feature we are working on, we want to know that as soon as possible. That way we dont expend any unnecessary effort on developing a feature that the customer will never use (also referred to in some cases as \"gold plating\").",
      "I like the term \"Failing Fast\", but its important to understand (as one of the MSMIT students pointed out) that the failure we are willing to accept in agile is failure of the business value. In other words, if we are building something that doesnt deliver the expected value to our customers or our business, we want to know about that failure sooner rather than later, so we can adjust to it.",
      "Failure does not mean failure in the quality of the code.\nCode we deploy in agile development methods should still be well tested (preferably using Test Driven Development methods and Continuous Integration), and we should be highly confident that what we are deploying will work as designed. We just arent confident yet if the customers will like what we have designed – and that is what we want to find out as soon as possible!",
      "Tip #2: Avoiding Bad Standups",
      "",
      "Another particularly interesting discussion we had was about what makes a good \"standup.\" Many teams believe they are agile because they have standup meetings. The reality is theres a lot more to agile than daily standups, and if thats all youre doing, youre probably not even doing the standup correctly in an agile way.",
      "Basic rules to follow when conducting your standups",
      "Stand ups should be no more than 15 minutes.\n  Each developer only discusses the three items (What did I do yesterday, What am I doing today, Am I encountering any obstacles)\n  All other items of discussion should be taken off line after the stand up, and only involve interested parties, not the whole team.\n  Product owners are always invited to the stand ups, but they should be a listener only. If they have questions they can bring them up with individual developers offline.\n  As I saw posted on a discussion board recently, standups should be \"by developers and for developers.\" They should be run by developers, not project management or clients.",
      "Make sure everyone on the team values the standups\nOne team lead in the class asked how rigidly I believe you should adhere to the 15 minute length. He said that his team is spread out remotely, and so this is often their only form of communication daily, and that their meetings are more typically 30 minutes, but that it seems to be going well.",
      "My first question was \"Who thinks its going well at 30 minutes? You as the team lead? Or have you asked your team?\" I was intentionally being slightly confrontational to make sure he realized that the point of the stand up is developer communication, and to see if he had a true sense of what his developers thought of these daily meetings.",
      "The pitfalls of standups \"in name only\"\nOften times when the stand ups are run by traditional project managers, they quickly become long daily meetings, and the only person who actually likes them are the project managers or team leads. The developers are bored, get distracted and annoyed that they arent doing something more productive (like developing code!), and they tune out. Because they tune out, they arent even listening to what the other developers are saying, and they lose all benefit of the conversation. The conversation becomes a hierarchical conversation between each individual developer and the project manager running the meeting, not a conversation between developers.",
      "Those type of \"standups\" lose their value because when developers arent talking to each other or paying attention, they will not offer to help remove obstacles for each other or offer advice. It becomes more of a traditional command and control situation and de-emphasizes the collaborative nature of productive Agile teams.",
      "The particular team lead who asked the question was perhaps a bit taken aback by my response, and paused a moment to consider it. Ultimately he said that his team does also see value in the 30 minute daily standups, but I hope that my response gave him pause to consider if everyone really saw the same value in longer meetings that he does. Its always worth asking your team!",
      "Dealing with remote teams in a stand up",
      "With remote teams, its understandable that you might have longer phone calls so people can socialize a little bit (just a little though please!). But in those cases, as I recommended to the team lead, at least still focus on keeping the stand up itself to only 15 minutes. If people want to stay on the line and discuss things for another 15 or 30 minutes, thats fine.",
      "But if you get the real standup done first within your 15 minute time limit, then at least those who are not interested in socialing or further discussing that day will feel free to leave the call. By incorporating more detailed discussions and socialization into the stand up itself, you are forcing everyone to stay around for the whole call every single day. And that is definitely not going to be efficient for everyone.",
      "In conclusion…",
      "Its often said that you can understand the basic concepts of agile in a couple of days, but it takes a lifetime to master. The concepts of iterative development, failing fast, and stand ups really are pretty simple if you stick to a few simple rules. But if you dont master the underlying values of why you are doing Agile, and trying to empower your whole team (not just project management!), then you will inevitably revert back to a control & command environment and lose the benefits of whatever agile practices remain.",
      "Im pleased to be doing some follow up agile process consulting work this week for one of the students teams. If OpenSource Connections can help you improve your adoption of agile, please dont hesitate to contact us for a consultation!",
      "The Agile CIO is a series of blog posts advising IT leaders on how best to incorporate Agile techniques into their organizations. For more information about OpenSource Connections Agile process consulting services, please contact the author at [email protected]"
    ],
    "summary_t": ""
  },
  {
    "id": "d366e8ac3e0885addef0434a5f20cfdc",
    "url_s": "https://opensourceconnections.com/blog/2010/08/04/software-estimation-you-are-not-as-good-as-you-think/",
    "title": "Software Estimation:  You are not as good as you think",
    "content": [
      "I know its a bit harsh to insult you in the title of this post, but the odds are, you really stink at estimation. Even if you know youre bad at it, youre probably even worse than you think.",
      "Dont take it so personally – Im certainly not saying Im perfect at it either! But youre among friends here, so lets have an honest discussion.",
      "While you will never be perfect at estimation (it is after little more than prediction of the future), you can get better at building and communicating estimates if you remember a few key reasons why youre so bad at it. Youre still human after all, and so I guarantee you will fall into some of these traps.",
      "Reason #1: Youre doing it alone",
      "Too often, estimates are done alone. Typical scenarios include one person (such as the team lead) estimating an entire project, or a project manager going to a specific developer and asking them for an estimate. One person giving an estimate by themselves is one of the most unreliable ways to give an estimate, because there is no one to check you against common biases. And since all initial estimates tend to be very optimistic, you are almost guaranteed to set unrealistic expectations.",
      "Reason #2: Youre doing it for others",
      "When the team lead gives an estimate on behalf of their team, several problems ensue. First, you are relying solely on the judgment of one person (the team lead), and no matter how smart that person thinks they are, they are susceptible to biases when they go it alone. Second, when the rest of the team is told what the estimate for the project is, they had no voice in it, and therefore, they have no sense of ownership in that estimate. A lack of ownership means they will have no sense of accountability, and they will not go the extra mile to stick to the estimate.",
      "Reason #3: Youre being way too optimistic",
      "Very few of us give pessimistic estimates. For one reason, we want to please the customer. When they say they want something in two weeks, we make mental leaps in how we will build the software to try and get them what they want. Or we discount the likelihood of obstacles, and hope for the best case scenario.",
      "Reason #4: You think youre an expert",
      "Theres a great chapter in the book \"The Black Swan\" about the problems with predictions, and one problem described is \"the expert problem.\" Studies have shown that experts tend to underestimate their own margins of error on predictions, because they are overconfident in their expert status. As software developers, we fall into this trap regularly when we say things like \"oh yeah, that will be easy, Ive done something similar before.\" When we do remember that task last time was not as easy as we thought it would be, we often discount those obstacles we encountered as outliers. We assume that \"now that we know how to do it\", we wont encounter those obstacles again, and no other obstacles will arise. We are usually wrong.",
      "Reason #5: Youre using single point estimates",
      "Using single point estimates makes all of these previous reasons worse. Single point estimates encourage you to go with your gut and the first number you think of, or the number that the customer wants to hear. Think how ridiculous it would be someone asked you \"How long does it take to get to your house?\", and you answered \"Oh, about 25.6735 minutes\". By including all those extra decimal points, you are creating a false impression of accuracy. Single point estimates are the same idea. If I say \"it takes 25 minutes to get home\", then you will expect to be there in exactly 25 minutes. But what about traffic, or other variables? A more accurate answer is to say that it takes \"25-30 minutes, depending on traffic.\"",
      "A solution to estimation biases: Range estimation in Scrum Poker",
      "Using a range instead offers a lot of benefits. By saying \"that will take me 2-5 days\" instead of \"I can get that to you in two days\", you are communicating the uncertainty that is inherent in all software development. The size of the range you provide communicates risk. And it prepares the customer for a more realistic schedule, instead of setting you up for failure when they expect completion on the most optimistic schedule possible.",
      "Combining range estimation with the Agile practices of team estimation is doubly effective. In Agile teams we often play \"Scrum Poker.\" Ill avoid the long description of it here, but in short, each team member gets a deck of cards. When you are estimating a story or a task, each team member privately chooses a card (or range of cards) from their deck. When everyone has chosen, they all show their cards at the same time.",
      "Showing the cards at the same time combats a number of the reasons I list above, because you get a true sense of the variety of estimates across the team without introducing the biases of the team lead or architect. Those who picked the lowest or highest estimates are not discounted as outliers, but encouraged to describe to the team why they chose that number. This encourages the team to discuss the idea until they have reached a common vision for the complexity of that story or task.",
      "Agile team estimation is great, but one big problem I still have with it is the reliance on single point estimates. Thats why we use range estimation in our Scrum Poker at OpenSource Connections, and we find it to be more effective. By holding two cards instead of one, each team member is indicating a range estimate and communicating lot more about the risk and complexity that they see in that project.",
      "Next Tuesday (August 10th, from 11am-noon in room Asia5) Ill be presenting at the Agile 2010 conference on \"Building a More Accurate Burndown: Using Range Estimation in Scrum\". If youre going to be in Orlando, I hope youll stop by to hear more! You can also see my slides on slideshare.",
      "*Arin Sime is a Senior Consultant with OpenSource Connections, specializing in Agile process consulting, Solr search, and development consulting. You can follow Arins tweets at ArinSime *"
    ],
    "summary_t": ""
  },
  {
    "id": "11645ad27e3b106f525df1b09c1c4981",
    "url_s": "https://opensourceconnections.com/blog/2010/08/10/software-estimation-using-range-estimates-in-scrum/",
    "title": "Software Estimation: Using Range Estimates in Scrum",
    "content": [
      "Recently I wrote a blog post on why software developers are typically so bad at estimation. I also gave a preview of the presentation that I am giving today to the Agile2010 conference on how to combat some of these problems by building range estimation techniques into a Scrum burndown. You can see that slide deck on slideshare.",
      "To briefly recap, I pointed out that Agile team estimation and Planning Poker (or Scrum Poker) is great, but one big problem I have with it is the reliance on single point estimates in the burndown.",
      "That’s why we have built range estimation into our Scrum Poker and burndowns at OpenSource Connections, and we find it to be more effective. By holding two cards instead of one, each team member is indicating a range estimate and communicating lot more about the risk and complexity that they see in that project.",
      "So how does it work? In this blog post I’m going to describe how we do it using Google Spreadsheets.",
      "How to build a burndown using ranges",
      "Ingredients needed:",
      "Spreadsheet of any sort (I like google spreadsheets for easy sharing)\n  Scrum Poker cards\n  An engaged team!",
      "Steps:",
      "Build a normal Scrum spreadsheet with a list of tasks on it.\n  Estimate each task using Scrum poker. Have each participant hold up two cards to indicate the range of their estimate.\n  The Scrummaster should encourage debate about the estimates when they vary widely in the size of the estimates or the width (ie risk) of the estimates.\n  Once the team agrees on a range, the low and high values are entered in the spreadsheet.\n  The spreadsheet calculates an estimate which is 2/3 of the way between the low and high estimate.\n  Use the \"2/3 estimate\" as your ideal burndown to zero and to see how many tasks fit in your sprint.\n  Use the low and high estimates as a band of reasonable deviation from the ideal burndown. They should both slope to zero at the same rate as the 2/3 estimate.\n  Use the ranges to encourage more conservative adoption of tasks into a sprint, and to communicate risk and progress more clearly with customers.",
      "In the end, we come up with a burndown similar to this, which represents the low and high estimates, and is a representation of the risk of the sprint or as a band of acceptable divergence from the estimate.",
      "Make Google Spreadsheets (or Excel) do the Hard Work!",
      "The next thing to show is how we represent this in a Scrum spreadsheet, and derive the formulas to build that pretty burndown. The vertical image is an example scrum spreadsheet like we use at OpenSource Connections, and the following sections will describe the major formulas being used in that spreadsheet.",
      "This is a little tricky the first time, but then just copy the sheet from then on.",
      "Low Estimate (D6-D8) and High Estimate (E6-E8) are entered by the team as they agree on estimates for each task in sprint planning.",
      "Initial Estimate per task (F6-F8) is calculated by the spreadsheet, and is a single integer weighted 2/3 of the way from the low estimate to the high estimate:\nF6 = (D6*0.33)+(E6*0.67)",
      "2/3 Estimate (F2 and duplicated in F3) is just a sum of the initial estimates across all the tasks.\nF3 = sum(F6:F8)",
      "2/3 Estimate per day (G3-K3) is a calculated field, which decreases the estimate for each day down to zero by the end of the sprint. This provides the \"ideal burndown\" line for your chart at a 2/3 estimate.\nG3 = F3-($F3/[Days in Sprint])\nH3 = G3-($F3/[Days in Sprint]) … and so on",
      "Low Estimate (F4) is a sum of all the low estimates: F4 = sum(D6:D8)",
      "Low Estimate per day (G4-K4) is a calculated line showing the path down towards zero for our low estimates. It uses the slope of the 2/3 line.\nG4 = IF((F4-($F3/[Days in Sprint]))>0,F4-($F3/[Days in Sprint]),0) … and so on",
      "High Estimate (F5) is a sum: F5 = sum(E6:E8)",
      "And High Estimate per day (G5-K5) is calculated similar to the low estimate per day:\nG5 = F5-($F3/[Days in Sprint]) … and so on",
      "Then developers update estimates each day as usual, and this makes the \"Actual\" line in the burndown.",
      "That’s really all there is to it!",
      "Once you’ve got your spreadsheet setup, you can just copy it from sprint to sprint, and then it really doesn’t take any more time in sprint planning to use ranges instead of single points. But using ranges with a 2/3 weighting like we do will help force your team to think a little more about the risk involved in their estimates and to estimate more conservatively. This allows you to better communicate with your customer and more reliably predict what you can commit to in a sprint.",
      "*Arin Sime is a Senior Consultant with OpenSource Connections, specializing in Agile process consulting, Solr search, and development consulting. You can follow Arins tweets at ArinSime *"
    ],
    "summary_t": ""
  },
  {
    "id": "c5dcb96413d2b431ade7b7ed085a9c5a",
    "url_s": "https://opensourceconnections.com/blog/2010/09/03/agile2010-from-change-to-planning-to-just-being-truly-present/",
    "title": "Agile2010:  From Change to Planning to just being truly Present",
    "content": [
      "A couple weeks ago I went to the Agile2010 conference in Orlando, and it was a really great experience. I really enjoyed giving my talk on range estimation in Scrum, and I greatly appreciated the large turnout for the session and the great participation and questions. But the real value I got was in the other sessions I attended. I learned an awful lot during that week, and I have been slowly absorbing those lessons since returning. Im going to try and summarize a few of those lessons here. They will necessarily be brief and disjointed tidbits, but if youre interested I hope youll go find the person who gave that talk to learn more, or perhaps Ill blog in more depth about some of them later.",
      "\"Making Change Happen\" with Mary Poppendieck",
      "The key scarcity for information workers is time and energy, not as much money. Therefore if you manage your teams like volunteers and give them a challenge and a vision, they will desire leadership and not fear it. They will desire the kind of leadership that continues to provide vision and coordination, not command and control. When evaluating a team or broken process, avoid saying \"Whats not working and how do we fix it?\" Instead start with \"What is working (perhaps against all odds), and how do we clone it?\" Look for areas where the team should be failing, and yet they are succeeding despite the obstacles. They have figured something out.",
      "Agile Planning with Mike Cohn",
      "There was a lot of value in this session, but one concept I liked was \"Commitment Driven Sprint Planning.\" When you have no historical velocity for a team or a project, you should break up stories into tasks using only one story at a time. After estimating those tasks and adding them to the sprint plan, ask the developers \"Can you commit to delivering this in this sprint?\" If they can, then you pick another story, break it into tasks and estimate, and add it to the sprint. Again, ask the team if they can commit. Repeat this process until the team feels that they cant commit to getting any more work done reliably and with high quality in that sprint. The number of story points they took on is a predicted velocity and something you can use to plan the rest of the backlog with. This seems particularly useful when estimating development efforts for a proposal. Mike also talked about how to use ranges of low and high velocity from historical data to predict a range of sprints or scope that can be accomplished in the future. This validated some of the practices we have started using at OpenSource Connections and seem to be going well. One funny point that Mike made, and which is accurate, is that the best way to give an estimate is to stall. Stall until you know more.",
      "Tuesday keynote by Dave Thomas",
      "One funny (and true) point that Dave made which really stuck with me was \"dont put the ass of your laptop in somebodys face during a meeting. Its about interactions between people, not computers!\" His point was to make sure that you are focusing on communication with customers, and that communication is usually improved if you shut the lid of your laptop during meetings. Another takeaway I had from this talk was that the definition of a tangible user story should be a user story that is testable. He also emphasized the importance of using the back of the user story to record acceptance criteria. This is a best practice that I expect to do more of in the future.",
      "The Agile Manager, Johanna Rothman",
      "Strategic management sessions should be done in iterations just like sprint planning. You dont need company retreats, just plan for a half day every few weeks. Johanna also talked about the importance of decoupling individual feedback from just during annual reviews, so that you can give feedback to employees more often. She also gave a good overview of giving peer to peer feedback by following these steps: 1) Create an opening, 2) Describe the behaviors or results, 3) State the impact of those behaviors on the team, 4) Make a qrequest.",
      "Distributed Scrum, Guido Schoonheim Distributed and offshore teams are motivated by the same things that our inhouse teams are motivated by (ie, not just money, but also autonomy, subject matter mastery, etc). Its very important when building a great distributed team to create the purpose and shared vision across the whole team, not just the inhouse part of your team. Team members on both continents need to feel equal, dont make the offshore team feel like the junior partner. For stand ups, use video cameras, make sure everyone actually stands up, and pass a microphone around so everyone can hear well. Every 6-8 weeks, have a team member rotate for a couple weeks over to the other team (in both directions), so that teams can share context and build relationships.",
      "Testerfield, Dave Haeffner",
      "Dave talked about a project developed internally at the Motley Fool called Testerfield, to improve automated web testing. Hes considering open sourcing the tool, and so you should go sign up at testerfield.com to encourage him to do exactly that.",
      "Using Story Maps to Visualize your Backlog",
      "Paul Hodgetts led a very good interactive session to help practice different ways to layout stories on a backlog. Take the roadmap a user follows through your system, draw that across the top, and then place user stories under the relevant step in that process. You can use stickers on those story cards to indicate things like \"needs architect review\", \"has an external dependency\", \"requires a meeting\", etc. You can also lay them out in horizontal swim lanes underneath the roadmap to show what will be done in different phases of the project. Exactly how you map your backlog will vary with each company or project environment, but this session provided me with a lot of ideas for doing that with co-located teams. It can also be done in Google spreadsheets for distributed teams, though I suspect you lose some of the value in the process.",
      "The Curious Coach",
      "This session was really interesting, and I have been wiggling my toes ever since. Wiggling your toes in your shoes was a simple tip David Spann and Gil Broza shared about how to remind yourself to be calm and present in difficult situations or conversations. Thats just one small example, and the session highlighted many other ways to make sure that when you are coaching someone through a process change, that you maintain the values of being \"Curious, Present, and Empathetic.\" Coaching someone how to make the most of Agile processes is a different and more long term change effort, and a distinctly different skill set than standard training or teaching skills. It was very enlightening to think through those differences and practice them. The basic goal is to help someone figure out a solution to their challenges themselves, and not just immediately blurt out what you think the solution is. Thats a lot easier said than done, but the results are much different and longer lasting.",
      "Theres so much more to tell you! This is not a comprehensive list of every session I went to at Agile2010, or even a list of just the best sessions I went to. But it will have to do for now. The journey to true software craftsmanship always continues, and really has no end point. Agile2010 definitely helped me further along my journey and provided many eye opening experiences. Now its time to go apply a few more lessons!",
      "[Note: All of the photos in this blog post except for the first one came from the conference photo CD sold by Tom Poppendieck, and available at http://poppendieck.com/]  *Arin Sime is a Senior Consultant with OpenSource Connections, specializing in Agile process consulting, Solr search, and development consulting. You can follow Arins tweets at ArinSime *"
    ],
    "summary_t": ""
  },
  {
    "id": "28d9156c82d5889f14b1dc7159783d78",
    "url_s": "https://opensourceconnections.com/blog/2010/09/09/arin-sime-speaking-at-agile-richmond-on-914/",
    "title": "Arin Sime Speaking at Agile Richmond on 9/14",
    "content": [
      "Im delighted to be speaking at Agile Richmond next Tuesday evening, Sept. 14th, on Range Estimation in Scrum. This will be based on the talk I gave at Agile2010 this year. Thanks to the Agile Richmond organizers for inviting me and I look forward to meeting you there! Here are the details from the Agile Richmond email:",
      "Building a More Accurate Burndown Using Range Estimation In Scrum – With Arin Sime",
      "Join Arin Sime for a discussion on Range Estimation in Scrum. Traditional Scrum burndowns are based on single point estimates of how long a task will take. However, single point estimates are inherently faulty and inaccurate, and they encourage underestimation.",
      "Learn how to incorporate range based estimation techniques into your Scrum burndown and Agile planning, and better communicate to your boss or clients what a project is really going to take.",
      "Arin will back up this thesis with academic and industry research and real world examples of how they use range estimation with their clients. Attendees will leave with concrete tips & templates for using range estimates in their projects.",
      "Check the website for updates: www.agilerichmond.com",
      "Date & Time:\nTuesday, Sept. 14, 2010\n6pm – 8pm",
      "**Location: **\nStaff Focus\n4198 Cox Road\nGlen Allen, VA 23060",
      "*Arin Sime is a Senior Consultant with OpenSource Connections, specializing in Agile process consulting, Solr search, and development consulting. You can follow Arins tweets at ArinSime *"
    ],
    "summary_t": ""
  },
  {
    "id": "4304271278fd208e8337661366036453",
    "url_s": "https://opensourceconnections.com/blog/2010/09/30/continuous-integration-and-automation-article-is-published/",
    "title": "Continuous Integration and Automation Article is Published!",
    "content": [
      "My article on the role of Continuous Integration as helping be the bridge between the Developer and Automated Test Writer who continually engage in a Tug of War was published in the September issue of Automated Software Testing Magazine! Its been a couple of years since I wrote an article for a magazine, and this was my first time writing for a real honest-to-god paper magazine. It was a great experience, and Im hoping it helps developers and testers start to have a dialogue of how to work smoothly together."
    ],
    "summary_t": ""
  },
  {
    "id": "c581480a2f89f9f1264829e954d6565a",
    "url_s": "https://opensourceconnections.com/blog/2010/11/24/search-is-not-just-an-it-problem/",
    "title": "Search Is Not Just an IT Problem",
    "content": [
      "Editors Note: This post was originally published November 24th, 2011.",
      "I recently attended the Fall 2010 Enterprise Search Summit anticipating that it would be a battle of the search engines – why choose Solr over FAST, Endeca, Autonomy, Google Search Appliance, etc. However, in talking to people who were there and wrestling with enterprise search problems, I realized that many of them were trying to determine a search engine technology strategy without incorporating that decision into a larger overall search strategy.",
      "A computer will always do what you tell it to do, but not always what you intend for it to do. This is the case in developing a search strategy and for IT teams who are trying to execute the enterprise search technology implementation.",
      "To me, search is about the user. It is a way for a user to find the information he seeks or that he didnt know about in the first place with the most accurate results in the most rapid manner. Accuracy and relevancy is much more important than speed, as the average user wont notice the difference in milliseconds in the search results, but will most certainly notice results that dont further the inquiry.",
      "Thus, the starting point and the keystone of implementing a search strategy is in defining users, both in who they are and what they seek, and also in what action you want from that user. Is your website an e-commerce website that generates revenues from sales, is it a website that generates revenues from advertising and needs long site visits (preferably with many page visits during the users stay and subsequent returns), or is it an internal knowledge site where the user is looking to find previous use cases to prevent duplication of work? Does the user have a specific need in mind, or are is he looking to expand his knowledge about a subject? Whom will the user trust?",
      "These questions are just the beginning of the inquiry that a company must pursue to have an effective enterprise search strategy. Marketing and operations have to be involved in the planning stage. They cant just turn to the IT organization and expect magic to happen. They also have to be involved in the discussion about the user experience. The user experience is what drives behavior to the desired outcome, and a search engine user experience is different than the broader term user interface. Chances are that your user didnt land at your sites home page, so the starting point is an indication of the path of travel for your user. Provide signposts for the user that tell him what to do next, where to go, and how to find more information.",
      "A key component of extending the notion of search beyond the home pages search box is to make faceted browsing available at many points within a website experience. Preventing the user from finding more information sacrifices the potential of broadcasting (allowing the user to expand out inquiry) for the sake of narrowcasting (assuming that the user is looking for one and only one thing). Getting a user to have a \"hey, I didnt know that…\" response while on your site encourages inquiry and exploration and builds trust between the user and your site.",
      "Once the users and their experience paths are mapped out, the marketing and operations teams need to determine what the metrics of success are. Broadly speaking, the categories are simple: for marketing, its an increase in revenue and for operations, its a decrease in cost. Still, at this point, its a multivariate analysis, so identify the interim steps that lead to the end result, such as decreases in abandonment and pogo-sticking and increases in time on site or time on a page. Next, measure a baseline. Without a baseline of statistics, its impossible to determine what, if any, progress has been made subsequent to a search engine strategy implementation.",
      "Only at that point is an organization truly ready to begin analysis of a search engine technology. Evaluations of license cost, implementation cost, functional capabilities, organizational capabilities, and risk acceptance profiles all play into the determination of what search engine is correct.",
      "Even after implementation of the search engine, the work is not done. Measuring the results, identifying candidates for A/B testing, tweaking relevancy calculations, and reevaluating user personnae mean that search is neither a fire and forget function nor just an IT problem to solve.",
      "Ive had many discussions with people who tell me that theyve had problems with [insert name brand search engine here] and now are looking at Solr to improve performance. While Solr will often improve performance and relevancy, the discussion often should be reframed as \"were not sure what we expect from a search engine, and we need help in determining what we want to do and how Solr can help us to get there.\"",
      "Otherwise, enterprises simply are not unlocking the power of a true, holistic enterprise search strategy.",
      "Contact us to see how we can make your enterprise search great."
    ],
    "summary_t": ""
  },
  {
    "id": "92a462a21b85dcbc2b8b34a86c04b465",
    "url_s": "https://opensourceconnections.com/blog/2011/02/23/top-three-drivers-of-solr-adoption/",
    "title": "Top Three Drivers of Solr Adoption",
    "content": [
      "Weve been talking to our customers to understand why they are adopting Solr. Is it because its a great product from the Apache Software Foundation? Is it because Lucene has a cute naming story? Is it because of Solrs compelling feature set, ease of use, and great admin interface?",
      "Unfortunately, its none of those reasons. The top three reasons for adopting Solr are:",
      "Vendor Fatigue. Companies are tired of being tied up in complex and expensive licensing contracts, per document pricing schemes, and opaque service agreements. This Dilbert comic I saw in the paper today struck very closely:\n\nThis is why our basic services agreement is a 1 pager!\n  Flexibility. Weve worked with a number of clients whove used Google Search Appliance and Google Custom Search because they were so gosh darn simple to setup. They subsequently moved to Solr because they either ran into the limitations for indexing information or presenting a rich search user experience with those solutions. With Solr, you can integrate with any datasource, and display your search results in whatever format you need. The more \"seamless\" you want your search experience to be with your site, the better Solr looks.\n  Stability. As much as I love Lucene, its a very complex beast. And many organizations have developed their own search engines on top of Lucene that have become expensive to manage, and dont offer the features that Solr does. Were going to be busy for a long time helping companies move from pure Lucene based search to Solr!"
    ],
    "summary_t": ""
  },
  {
    "id": "2d740beb40af8be1788c13349fedd183",
    "url_s": "https://opensourceconnections.com/blog/2007/01/15/you-never-know-where-people-will-find-you/",
    "title": "You never know where people will find you",
    "content": [
      "Thanks to Pruppert, word about our iTunes deduplicate script is spreading. He posted on the Mac World Forum and had an excellent write-up on how the script differed from previous scripts.",
      "The surprise to us was how quickly Erics new post popped up on other places in the Internet. None of us know Pruppert from a random stranger; yet, he posted about us (and did an excellent job of writing about what it did) and generated a significant amount of traffic. Our websites traffic doubled since the posting, and just under 50% of the traffic has been from that posting. While we dont expect every blog post to hit like that and establish thought leadership, this shows the value of continuing to post blogs."
    ],
    "summary_t": ""
  },
  {
    "id": "f8093215921d35938ac1dd9fcb40e4bc",
    "url_s": "https://opensourceconnections.com/blog/2011/03/04/eric-pugh-to-speak-on-better-search-engine-testing-at-stpcon/",
    "title": "Eric Pugh to speak on \"Better Search Engine Testing\" at STPCon",
    "content": [
      "Im very excited to be returning to another edition of STPCon, this year in Nashville!",
      "This year I will be unveiling a new talk: Better Search Engine Testing, as well as a reprise of Turbocharge your Automated Tests with CI from STPCon 2009.",
      "Since this is the first time for my Better Search Engine Testing talk, Id love to connect with other folks in the testing community whove worked with search engines and get some feedback on my talk. Just hit me up at http://twitter.com/dep4b!",
      "Check the STPCon website for more details on this great conference: www.stpcon.com"
    ],
    "summary_t": ""
  },
  {
    "id": "d92f9a4b55aaad3691cf9d8dc9b1d284",
    "url_s": "https://opensourceconnections.com/blog/2011/03/21/scott-stults-to-speak-on-solr-security-at-the-solr-and-text-analytics-conference/",
    "title": "Scott Stults to Speak on Solr Security at the Solr and Text Analytics Conference",
    "content": [
      "Were thrilled to announce that Scott Stults was presented to speak on the topic of Multi-Level Security (MLS) with Solr at the Open Source Search Conference: Lucene and Solr in Government on June 15, 2011 in McLean, VA. He will be discussing how to use MLS to search and retrieve documents for users at various classification levels. If you have documents which are classified at multiple levels of classifications (e.g. Confidential, Secret, and Top Secret) and users with different clearances and want the right users to be able to see the right documents when they search through repositories, then this presentation is for you. If you cant make the conference, or think that one of our services may be right for your organization, please contact us!"
    ],
    "summary_t": ""
  },
  {
    "id": "332688c159e9fe066a94123246f6ea2d",
    "url_s": "https://opensourceconnections.com/blog/2011/03/28/warning-strong-opinions-can-cause-a-backdraft/",
    "title": "Warning: Strong Opinions Can Cause A Backdraft.",
    "content": [
      "In response to: ceo friday why we dont hire net programmers",
      "I’m sure there are a slew of witty responses in this rebuttal to a CEO writing this kind of article.",
      "However, I’d rather focus on the real issue at hand. It sounds like the CEO, David Barnett, took a chapter out of an idiots-guide-to-blogging article to heart and decided to vocalize something very opinionated which would his blog/company stand out among the rest.",
      "While some people still think under the pretense of \"any publicity is good publicity\", its not always that cut and dry. If done wrong, it may cause a backdraft into his companies recruiting efforts and work against one of his primary goals",
      "If you are going to write an opinionated article on developers and start-ups, it might be wise to not take lessons from David.",
      "Don’t sound like a total tool or bigot.",
      "David’s first point of failure was wording his argument in such way that it makes him sound like a bigot with no remorse till you get to the very end of the post.",
      "Even then, the end of the post sound like it was half hearted update to quell the angry masses.",
      "The title is probably the most offending line, followed by the bold face type and over-extended metaphor and is what people are really going to key in on and read.",
      "Most people don’t read full articles on a website, they scan them. They jump up and down as they read, picking up on bold fonts, headers, and bullet points, sucking in what seems like the most important data.",
      "Such cut and dry negativity towards a group people for any reason is just going to come across as cold and strike them on an emotional level rather than a logical one. Much more so if the post outlines the negative significantly more than any other content inside the same post.",
      "Thus its most likely going to elicit emotional lashing out rather than a rational dialog.",
      "Instead of targeting his criticism towards a technology he directed at people and pretty much told them they are excluded from his vision of the software \"engineer\" master race.",
      "Yes a strong opinion will cause people’s passions to stir and voice their opinion, but it should never be so slanted that it cuts just as deep as politics or religion does in a technical blog.",
      "Truth hurts, but so will a lost in sales. Instead of creating a following of fanatics, pushing the opinion too far could create enough negative backlash that someone launches a full on anti Expensify PR campaign.",
      "Don’t use half facts, smoke and mirrors, or ignorance to support your point of view.",
      "Going on tangents about Direct X vs OPEN GL or the underlying operating system factoring using a backslash when discussing the .net framework is little more than smoke and mirrors.",
      "Direct X is not a part of .Net, and any programmer worth his salt knows to use code that handles path and slashes according the current OS when targeting an application that is platform agnostic.",
      "As for the open source argument, that was true maybe 5 years ago, but the .net community has had to adapt in order compete on the volatile landscape of creating software.",
      "While the .net open source community is still in its infancy they have made strides with things like codeplex, the community promise, nuget, releasing parts of the .net framework under open source licenses.",
      "It seems like the author not only misses those points will continually choose to ignore even some of the valid points through out the comments blog that aren’t hate filled..",
      "Don’t use truisms.",
      "\"However, if you need to make a 1.7 oz burger, you simply can’t. \" Aside from the abuse of a metaphor: Don’t state something like it is obviously true, but is actually false.",
      ".Net will let you work with the bare metal and lower level should you need or want to. There is plenty of high level things that .net does for you, but that does not automatically mean your locked in to only writing code at a high level.",
      "Its not perfect or it might not be the right tool for the job. But it does now reach onto other platforms with the help of mono including the IPhone and Android (thank you Mono team), and including all the syntactic sugar of the C# language.",
      "In general its a bad idea to sell your opinion like its written in some with no margin for error based mostly on personal preference and speculation.",
      "It is going to make you look bad in the eyes of your target audience and leave you open for people to easily pick you apart. That totally defeats the point of a strongly opinionated article.",
      "Don’t abuse or stretch metaphors beyond their limit.",
      "\"Programming with .NET is like cooking in a McDonalds kitchen\". Except that this metaphor then becomes creating a 1.7 oz burger to cooking squirrel meat in a cave over fire created from banging two rocks together.",
      "If you want to create your own web server using C++ from scratch and waste a bunch of your start ups capital on non essential things then cooking squirrel meat in a cave with only two rocks and a stick is the metaphor for you.",
      "The metaphor about .net like cooking in a McDonalds kitchen is pushing it too far because it comes off as derogatory and insulting to the developer more so than the technology.",
      "You shouldn’t take a metaphor that is already stretching a bit to illustrate an exaggerated point and then continue to beat it to death throughout the rest of the post.",
      "Use the metaphor, make the point, move on.",
      "Otherwise you’re going to confuse readers and dampen any decent effect the original metaphor was intended to have to the point of obscurity.",
      "Don’t write on content that is out of your skill set as if you’ve mastered it.",
      "The author is obviously writing from a very limited point of view stating truisms as facts, throwing smoke and mirrors to back up his claim and then trying to sell everyone on the matter like his opinion or post was reasonably written in some way.",
      "It would have paid off to due some fact checking and getting some outside opinions on the matter before releasing that kind of material to the web.",
      "Especially when you had a clue that it might en-flame a whole group of people.",
      "The fact that the author failed to understand that .net is a framework instead of a programming language.",
      "That small bit is going to cost him resonating with readers since .net was cornerstone part of his opinion and he didnt fully understand what it was.",
      "The only thing that the author really touched on that is probably a real reason for startups not to .net is the fact that the open source community is still in its infancy.",
      "But obviously he didn’t follow through with line of thinking. Instead he spent most of his time insulting people while chasing tasty squirrel meat inside of McDonalds. (Where is PETA when you need them?).",
      "In Conclusion.",
      "It good to have a strong opinion that helps you separate yourself from the competition. Just don’t try support it with bad metaphors, truisms, and bigotry aimed at people and expect your target audience not to take notice.",
      "A strongly opinionated posted should never read like it was written by Billy Madison on the Industrial Revolution and puppies:",
      "Here at OSC we’re definitely not above being humbled and continuing to learn from others. Comments, Suggestions, Critique, and Corrections are always welcome below."
    ],
    "summary_t": ""
  },
  {
    "id": "5d8ee51289b832145559870047c83f63",
    "url_s": "https://opensourceconnections.com/blog/2011/04/08/michael-herndon-named-lucene-net-committer/",
    "title": "Michael Herndon Named Lucene.NET Committer",
    "content": [
      "OpenSource Connections own Michael Herndon has been added to the Lucene.NET committer list. The project website can be found here. Lucene.Net targets the C# and .NET platform for an algorithmic port of the Java Lucene search engine. Lucene.NET is an open source C# and .NET project which has recently reentered incubation into the Apache Foundation incubator. According to Eric Pugh, principal at OpenSource Connections and co-author of Solr 1.4 Enterprise Search Server, \"\"Having Michael named as a committer to the Lucene.NET search engine in the Apache Software Foundation is a high honor. It shows the commitment that OpenSource Connections has to the Apache Lucene community and open source .NET projects. We look forward to seeing the relaunch of this project to make open source search available on the .NET platform.\""
    ],
    "summary_t": ""
  },
  {
    "id": "5d5bc0e8ff8456a199efa57d2d4f39a8",
    "url_s": "https://opensourceconnections.com/blog/2011/04/27/should-i-deploy-solr-4-0/",
    "title": "Should I Deploy Solr 4.0?",
    "content": [
      "I recently had an IRC conversation about Solr 4.0. The main question that the person who was chatting with me had was \"How far out is the 4.0 release?\" The answer, as with almost any open source project, is \"when its released.\"",
      "",
      "Naturally, that answer doesnt really help get to the crux of what most IT teams who either use or are considering Solr need to figure out, which is whether 4.0 is stable enough to deploy in a live environment.",
      "",
      "Solr, even in unrelated versions, has historically been pretty stable. So, if a new version, in this case 4.0, has the functions that youre looking for – in this conversation, it was function queries like idf() or termfreq() – then unless youre comfortable with compiling a previous version of Solr and creating your own code on top of it, then youre probably going to want to go with the latest version.",
      "",
      "Of course, this approach does come with risk. I have only heard of 1 actual \"bug\" that led to incorrect/wrong results sneaking into the Solr code base in an unreleased project, and it was quickly found and fixed. But, since youre working on a code base which may change somewhat, if you are building indexes that you can not easily rebuild, for example, indexing the Internet and cant recrawl to generate the data – meaning if Solr is your \"system of record\", then be aware that over time the index file format may change because Lucene is changing under the covers and periodically there is an email that tells you that you need to rebuild your indexes. But, if you are basically taking a download of Solr 4.0 as it is today, and then only going to update a) when new killer awesome feature added or b) when 4.0 comes out, then reindexing shouldnt be a problem.",
      "",
      "The other aspect of deploying Solr 4.0 is your testing environment. If you have strong system and functional testing, then you can be fairly sure that things are working appropriately. If youre not certain about testing, check out my presentation on Better Search Engine Testing from this years Software Test and Performance Conference."
    ],
    "summary_t": ""
  },
  {
    "id": "eed91116cc46c21756b9b7ac77589645",
    "url_s": "https://opensourceconnections.com/blog/2011/05/23/jason-hull-featured-discussing-solr-and-opensource-connections-on-veterans-affairs-blog/",
    "title": "Jason Hull Featured Discussing Solr and OpenSource Connections on Veterans Affairs Blog",
    "content": [
      "Jason Hull discusses OpenSource Connections participation in the National Veteran Small Business Conference and how their move to focus primarily on Solr affects the business on the latest VAntage Blog of the U.S. Department of Veterans Affairs. You can read the article here."
    ],
    "summary_t": ""
  },
  {
    "id": "4e7df74638d2d4949088902579a43f1e",
    "url_s": "https://opensourceconnections.com/blog/2011/05/23/working-on-apaches-lucene-net/",
    "title": "Working on Apaches Lucene.Net",
    "content": [
      "I was requested to compose a blog post about how what Ive been working within the Lucene.Net project as a committer. Who in there right mind would ask me to do that? CoughcoworkerCough. They already hang signs around my desk that tells you not to feed my ego. Why should I have to write about my rock star status? I have fans for that. Despite the popular belief that Im a sexy nerd with naracistic tendencies, more awesome than Captain Awesome, I am just a down-to-earth guy like Whil Wheaton. Seriously though, this post isnt going to be about flaunting or bragging rights. It is poor taste to do something that diminishes something that really takes a team effort and community to accomplish. What this post is going to really focus on is the why or motivations behind the actions, which is sometimes more important than the actual actions themselves.",
      "The Decision to be a committer Lucene.Net was on the verge of being",
      "forked into oblivion and put into Apaches Attic. The Attic, which sounds like reference to Josh Weldons Doll House, fundamentally works the same way it does on that show. All fraked projects go there to hibernate or die. I joined Lucene.Net as a committer to prevent its complete demise and to foster growth in its developer and user community to a point that it never has the issue of stagnation again. My focus on the project will be growing the community, mostly through writing and media. If successful, Id like to be able provide materials to help other .NET opensource projects market their project and grow the overall .NET opensource community into maturity.",
      "What a committer is actually responsible for There is often a misconception that a committer to an opensource project is there to write code. Even though the committer has access to the repository, that does not mean they actually have to write the code. Project committers actually have the responsibilities of fostering the growth of the project, marketing the project, guiding its direction through transparent communication with the community, and all the boring managing stuff that most developers do not want to bother with.",
      "The bigger picture The .NET ecosystem could not afford to lose another core foss project. The underlying platform is rock solid and for a strongly typed language, C# is syntactically sugar coated and exceptionally versatile. However, Microsoft can not and should not be the ones continuing to publish what the community sees as the de facto standard or definitive project or library in all spaces within the .NET ecosystem. The overall .NET community exhibits codependency issues with Microsoft. Libraries like Nuget and Asp.NET Mvc are awesome libraries, but the community should have created these libraries on its own a long time ago. There is a great number of abandoned .NET opensource projects littered all over the web. The community has already lost its sorely needed but under appreciated NDoc tool in the past and really can not afford to keep losing vital projects. The community needs to mature. Even Lucene.Net needs to grow up. It needs to do more than port the Java code into C#. It needs to contribute to the growth of underlying logic of Lucene and even create a healthy competition with its parent project. Thus Ive made my decision to jump into the fray of working within Apaches Lucene.Net project in order to put money where my mouth is.",
      "So what have you been doing these last few months? Digging into Lucene.Net and Lucene to better understand it. It is hard to market something if you do not understand how it works and how it can be improved. Ive been reading",
      "Lucene in Action 2nd edition to see up-to-date documentation on Lucene. Ive also been reading Language Implementation Patterns to get my head back into ASTs (abstract syntax trees), grammars, parsing, tokenizing, and etc. I have also been coding a small blog engine using Asp.NET Mvc. This is what is currently powering wickedsoftware.net, a site dedicated to the effort of marketing .NET opensource projects. The first project being marketed is Lucene.Net. Last but not least, Ive been tinkering with the actual Lucene.Net API, blogging about some of the deprecations found in the API of v2.9.2. Ive also helped to get the word out about the hackathon, logo design contest graciously put together by stack overflow, and a vote to drop support of .NET 2.0 and move forward to .NET 4.0",
      "What will you be doing in the future?",
      "Creating build scripts to build the project from command line.\n  Working towards getting Lucene.Net into Apaches continuous integration setup in hudkins (jenkins)\n  Putting out a monthly post on what is going with Lucene.Net. Check out Mays update.\n  Writing, documenting, and tweeting about Lucene.Net\n  Attending the Lucene Revolution conference this week in San Fran. Dont be shy and say hi if youre attending that conference. If you have any feedback for the project hit up the",
      "mailing lists, the irc channel #lucene.net, or you can tweet me @michaelherndon on twitter."
    ],
    "summary_t": ""
  },
  {
    "id": "21374f049e675abdef0303d72fad7be4",
    "url_s": "https://opensourceconnections.com/blog/2011/05/26/scott-stults-discusses-value-of-solr-certification-exam/",
    "title": "Scott Stults Discusses Value of Solr Certification Exam",
    "content": [
      "OSCs Scott Stults and Michael Herndon are currently at Lucene Revolution and Scott was interviewed by CMSWire about the value of getting a Solr certification. You can watch his interview at the CMSWire coverage of day 1 of Lucene Revolution.",
      "OSC principal Eric Pugh took part in developing the Solr certification test with Lucid Imagination. You can read his thoughts on the value of the Solr certification exam."
    ],
    "summary_t": ""
  },
  {
    "id": "5704f0433ecaa6562621ee5311f6ac8a",
    "url_s": "https://opensourceconnections.com/blog/2007/01/26/migrating-from-simplelog-to-wordpress/",
    "title": "Migrating from Simplelog to WordPress",
    "content": [
      "After 7 months and 34 posts, weve outgrown Simplog. Were just finishing migrating all the content to our new WordPress blog, but Im sure youll see some bumps in the road as we get URLs fix, image references resolved, and all the other malarkey that moving platforms requires.",
      "While Simplelog was a great tool, and its simplicity was wonderful, weve gotten to the point where we want to be able to handle things like:",
      "Aggregating multiple feeds\n  \n  \n    Open access to many users\n  \n  \n    Properly ping Technorati on updates!\n  \n  \n    Be able to post using thick client tools.",
      "If you find something broken, please feel free to drop me a note at [email protected]"
    ],
    "summary_t": ""
  },
  {
    "id": "891ed2706e531ea57e3f72d9f1dca807",
    "url_s": "https://opensourceconnections.com/blog/2011/06/10/lessons-from-di2e-intelligence-is-about-search-not-gathering/",
    "title": "Lessons from DI2E: Intelligence is About Search, Not Gathering",
    "content": [
      "I just came back from the 2011 DI2E conference in Dallas, Texas and certainly left with a lot of impressions. I want to state up front that my impressions were very positive about the state of U.S. intelligence activities. Weve come a long way since we first started inserting Green Berets into Afghanistan. Whereas we once faced an intelligence gathering problem, we no longer do. Instead, we have trouble indexing, sorting, filtering, and analyzing the glut of intelligence that we have gathered. I did notice that Solr and Lucene are in use pretty robustly in the intelligence community, with Solr a part of the ICDL and JDL (intelligence community data layer and joint data layer respectively). Thats fantastic news, but theres a long way to go.",
      "Going to the cloud helps, but is not the panacea. One of the big issues that the intelligence community faces is the ability to index what it calls \"big data.\" Granted, it was a public conference, so they werent going to reveal all of the secrets, but I didnt hear anything which led me to believe that the data indexing issue was insurmountable. Certainly throwing enough hardware at the problem can help solve it, but whose cloud rules? So, instead of waiting for the cloud to deliver a magical elixir, the intelligence community can also look at improving indexing, caching strategies, and leveraging other systems. One common complaint that I heard was that Hadoop causes read time isues – its too slow. To answer that, look at Katta or Cassandra, or be willing to crack into Hadoops code and do modifications. The government cant just rely on COTS to solve its problems.\n  A glut of money led to inefficient development and stovepipes. Because each service got a pile of money, they each developed their own solutions. Most of them dont integrate well with other systems and solutions. They also use proprietary systems and protocols. At least the military recognizes the problem, as one speaker said that they can no longer add twenty different license costs for software that all does essentially the same thing.\n  Even with all of that money, search user experience got left behind. The military employs people, called Knowledge Managers, to teach people how to use the tools that are there. However, the tools are, at best, clunky, and at worst, a terrible user experience. As a result, as one of the speakers noted, over 90% of the tools are not used. I am certain that Netflix does not have a cadre of trainers to teach its users how to find movies. Zappos does not have its payroll stacked with people who can tell you how to find shoes on the Zappos website. Why? Because the user experience is well conceived and well executed. The military should look at innovators in industry and adapt their models. The money saved spending a little on vastly improved user experience will be more than made up for in the reduction of knowledge management requirements. Improving search and the search user experience has a HIGHLY positive ROI in this environment. It begs for attention and improvement.\n  You cannot just Google for intelligence information. Googles strength is that it has the wisdom of the masses. It has algorithms designed over the years to identify what the average person wants to see and when a website has legitimate information rather than being a spam or link farm. The intelligence community is vastly different. First, how many regular Google users are trying to do an IPB for a commander via Google? The sample size from which to draw the algorithms is incredibly small. Secondly, while analysts are interested in knowing the provenance of intelligence information, there is no spam in the intelligence community dataset. Thus, the real need is to tune for relevancy – get relevant results for the specific user set that the search identifies. When the intelligence community buys a license for a search engine which doesnt make relevancy tuning easy, intuitive, and comprehensive, it is wasting taxpayer money.\n  Acquisition hinders innovation. Because requirements and contracting have to be so specific with most government contracts, the cycle time from need to fielding is often over two years. That does not keep up with the pace of change. This is not a new jet fighter. New, category killer technology is unlikely to pop up overnight. However, in a theater where information dominance is paramount, that lack of speed is going to kill soldiers. Risk aversion and an acquisition strategy shackled by a 1970s Cold War mindset means that we dont get the best information technology available out to the soldiers who need it most in a timely fashion.",
      "As GEN Odierno stated, the intelligence community is going to need to get more bang for the buck. Defense budgets have been reduced and will continue to see declines in the intermediate future. This funding reduction means that intelligence community technology users will need to demand increased interoperability, open standards, and control. They will need to be willing to look at new, innovative, and alternative solutions, not the same old ones trotted out by the same old integrators, and they will need to demand alacrity and agility from both providers and the acquisition community, because the rate of growth of the amount of collected data will only continue to increase. The tooling and capability to search, retrieve, and do something with that information needs to catch up quickly."
    ],
    "summary_t": ""
  },
  {
    "id": "5502fee233d864da0d0440374c563126",
    "url_s": "https://opensourceconnections.com/blog/2011/06/17/multi-level-security-in-solr/",
    "title": "Multi-Level Security in Solr",
    "content": [
      "On June 15, I presented at the Basis Technologies Lucene and Solr in the Government conference on the topic of multi-level security (MLS) in Solr using ManifoldCF. This post covers the notes from my presentation.",
      "My presentation covers document security as it pertains to search:\n…across multiple repositories\n…by users of varying clearance\n…against documents of varying classification.",
      "Overall this is known as Multi-Level Security, or MLS.",
      "MLS isn’t the only way to implement Document Security. One way to implement document security is to only permit equal parties access to the system itself. The information flow in to and out of the system is screened by a separate process, so the system itself doesnt have to screen it at all. This type of system often fits into a hierarchical network where users may view information at an equal or lower clearance level, and any information created is only available to an equal or higher clearance level. An example of system high security is where separate indices are created for each clearance level. Users can only see the indexed data from an approved clearance level.",
      "Multi-Level Security, or MLS, is the capability to recognize differing access permissions between otherwise equally trusted parties. It encompasses the notion of \"compartmentalization\", where I am given access to the information I need to do my job, and you are given access to different information because you do a different job. This is possible within the same system because our differences are recognized and used to grant appropriate access.",
      "This idea of compartmentalization was famously popularized during the Manhattan Project. The people who were enriching the plutonium knew all about the state-of-the-art of enriching plutonium, but knew nothing about bomb designs.",
      "MLS is hard to do right and I’ll explain the reasons why a little later. In fact most of the systems I’ve worked on started with system-high security, and when the project gained enough traction or funding, MLS was added.",
      "The difference between MLS and \"system-high\" is that in the case of system-high, the system itself cannot be relied on to appropriately keep track of and enforce access restrictions beyond a basic level. Additional processes must be introduced to filter information. This introduces a time and resource cost that may reduce the value of the information itself. Moreover, the value of the system is reduced because fewer people are allowed to use it.",
      "Next, well discuss how Document Security relates to Search. The problem of appropriate access to information is particularly important to Search applications because the search engine needs access to all of the information in a repository in order to index it, and it must only present search results filtered by the users clearance. In order to do this filtering in a timely manner, the security information of the documents needs to be indexed and applied to the search algorithm.",
      "If security is \"bolted on\" as an afterthought, the order that documents are listed and the estimation of total relevant documents will also have to be recalculated, and the overall feature set of the search engine may have to be limited or re-implemented to prevent \"leakage\". By \"leakage\" I mean in general any information the search engine inadvertently provides that is outside the user’s clearance.",
      "An example of \"bolted-on\" security would be, if our search engine provided us an unfiltered result set, we could then post-process each page of results before they are sent to the user. Now, in order to avoid blank or short pages we’ll need to re-page the results.",
      "An example of leakage would be, if I typed in \"Scott Stults ATF raid\" and got back 15 hits, I might want to tone down my Fourth of July plans!",
      "Another well-known technique for teasing out information from a search engine involves exploiting the little document excerpts you get below the document titles in search results pages.",
      "Also, even if highly relevant and sophisticated search capabilities can be applied to one document repository, that information is much more useful when combined with information in other repositories. This is called Federated Search. Those repositories may have their own set of classifications which need to be honored by any system providing access to them.",
      "For example, until recently (and it may still be the case with some systems) one Federal Agency and Another had separate clearance labels, and a clearance with one meant nothing to the other. If you had clearance with both and were performing a federated search across both sets of repositories, your clearance with the first would have to be applied to the first set of repositories, and your clearance with the second would need to be applied to those repositories. And then you’d need to blend the two result sets together so that you’re looking at the most relevant results from either.",
      "That’s a difficult problem, to be sure. However, if you are able to technically implement that, you will gain a much more complete picture of all of the information available regarding your query.",
      "This is the challenge addressed by the Manifold Connector Framework, known as ManifoldCF. ManifoldCF is an Apache Software Foundation \"Incubator\" project. It is composed of a set of connectors to repositories, authorities, and search engines. These connectors are joined together during indexing so that the security tokens (the classification of the documents) are indexed. Then when a user performs a search, the users access tokens (or clearances) are applied as search criteria.",
      "Included in repository connectors that MCF has right now are the file system, RSS feed, web, Windows Share/DFS, database, Documentum, and SharePoint. By far the ones who get the most attention on the MCF mailing lists are SharePoint and Documentum. (Personally, I will probably see more use out of the generic database connector since that’s where the documents are stored in the systems I usually work with.)",
      "The purpose of a repository connector is to \"crawl\" each document in the repository and send it to the output connector. It’s also responsible for doing incremental updated crawls over new or changed documents.",
      "Active Directory and Documentum are some of the authority connectors. There aren’t a lot right now. They are responsible for supplying the authorization tokens (or clearances) for each person when they perform a search.",
      "And lastly, there are the search engines, or output connectors, that MCF currently knows about: Solr and MetaCarta GTS. The output connector knows how to feed the documents it receives from the repository connector to its target search engine. (and of course Solr is everyone’s favorite!)",
      "Before we dig into how security enforced by ManifoldCF, let me first give you a quick overview of what MLS looks like logically.\nFormally, MLS fits a Boolean logic construct called \"implication\": x–>y = -(x /\\ -y)",
      "The thing on the left reads as \"If x then y\", and the thing on the right-hand side of the equation is how that gets translated into Boolean logic. When that logic is interpreted as a security algorithm it enforces the notion that any token in x must also be present in y. Any \"extra\" tokens present in y do not affect how access is granted. In English it reads \"do not allow any document whose tokens are in x but not in y.\"",
      "This logic is hard for the Boolean logic normally implemented in search engines because unary NOTs (the dog-leg symbol right after the equals sign) are hard to dereference. (Imagine asking Google for all documents that DONT contain the word \"security\".)\nIn fact, Lucene does not allow unary NOTs, but Solr does. Furthermore, sometimes its more efficient to specify who isnt allowed access, rather than constructing an inverted set of every single security token.  In fact if you do construct such an inverted set so that you can index it, you will need to reconstruct it and re-index your whole repository whenever a new token is added.\nBy design, these two notions of authorization grants and denials together fit the way our federal government applies security markings to documents.",
      "So now we turn our attention back to ManifoldCF and Solr. I described the logical underpinnings of MLS, and this is exactly how Manifold interacts with Solr to provide document security.",
      "subUnprotectedClause.add(new FilterClause(new WildcardFilter(new Term(allowField,\"*\")),BooleanClause.Occur.MUST_NOT));\n\n\n  \n  subUnprotectedClause.add(new FilterClause(new WildcardFilter(new Term(denyField,\"*\")),BooleanClause.Occur.MUST_NOT));\n\n\n  \n  orFilter.add(new FilterClause(subUnprotectedClause,BooleanClause.Occur.SHOULD));\n\n\n  \n  int i = 0;\n\n\n  \n  while (i < userAccessTokens.size())\n\n\n  \n  {\n\n\n  \n  String accessToken = userAccessTokens.get(i++);\n\n\n  \n  TermsFilter tf = new TermsFilter();\n\n\n  \n  tf.addTerm(new Term(allowField,accessToken));\n\n\n  \n  orFilter.add(new FilterClause(tf,BooleanClause.Occur.SHOULD));\n\n\n  \n  tf = new TermsFilter();\n\n\n  \n  tf.addTerm(new Term(denyField,accessToken));\n\n\n  \n  bf.add(new FilterClause(tf,BooleanClause.Occur.MUST_NOT));\n\n\n  \n  }\n\n\n  \n  bf.add(new FilterClause(orFilter,BooleanClause.Occur.MUST));\n\n\n  \n  return bf;",
      "This is an excerpt from ManifoldCFSecurityFilter.java. (Please don’t strain your eyes!) Its how you would write the logical implication I just showed you in Java. To use this in Solr, you simply put the compiled Java file where Solr can find it and include this filter (or \"Search Component\" in Solr parlance) in the Solr configuration file.",
      "Elsewhere in this file is a call to ManifoldCF to retrieve the security tokens for a particular authenticated user, and we configure that within ManifoldCF when we set up the indexing Job. So for any particular \"hit\" within Solrs index Manifold has mapped that document back to the right authority, the correct authorization tokens are used in the filter, and the correct filter is applied to the result set.",
      "Next, let’s briefly cover about some of the other things you should consider when implementing MLS with search.\nThe process of filtering results based on the appropriate user authorization tokens is simple enough to be very efficient at query time. However, that process quickly becomes unwieldy when multiple authorities have to be consulted. The filter has to use each documents provenance to determine the appropriate authority, and each authority will have a set of authorization tokens to grant the user.",
      "To be sure, having to incorporate multiple user authorities is a problem only the largest organizations may face, but it isnt a fictional problem. Its something joint task forces face all the time. So in order for documents to have their full value there must be full agreement on what tokens are used and what they mean.",
      "Another consideration to bear in mind is that everything Ive described to this point falls under the category of \"early binding.\" The access restriction that is enforced only applies to how the documents were classified when they were last indexed. This isnt much of a problem with older documents that have \"settled\" into their classification, but for new information pertaining to current events, early binding may not offer enough assurance.",
      "For example, not much is happening in Panama these days but the US has conducted military operations there so there is probably some classified material pertaining to that, and those classifications aren’t likely to change. On the other hand, documents pertaining to missions last week may get an immediate bump in classification if they suddenly pertain to missions happening right now.",
      "Whenever a document’s classification changes it must be removed from the index and then re-indexed with the new classification. An additional way to mitigate the problem of frequently changing classification is to implement \"late binding\", or re-request the documents classification immediately before presenting it to the user. But this additional assurance comes at the cost of speed. In reality a combination of all of the above would help optimize the need for quick access and a high-level of assurance.",
      "Solr by itself is a wonderfully flexible platform, and as I pointed out before, its query filters allow for MLS to be implemented in a fairly simple environment. Manifold, however, greatly simplifies the process of indexing, even across multiple repositories. And in the single-repository scenario, the standardized way in which Manifold specifies and schedules indexing jobs will definitely be beneficial.",
      "MLS is an important capability necessary to realize the full value of information stored in a document repository while also maintaining appropriate access control. When used in conjunction with Federated Search, that value is amplified by the information stored in neighboring repositories. Even if your repository isn’t federated now, Manifold and Solr significantly lower the barrier to achieving federation later at very little incremental implementation cost.",
      "And, at the very least, ManifoldCF offers to be a consistent and reliable tool for regularly indexing documents.",
      "Links:",
      "Multi-Level Security Using Solr",
      "ManifoldCF"
    ],
    "summary_t": ""
  },
  {
    "id": "74e198ed227f9fca71d7c54a03f07ab0",
    "url_s": "https://opensourceconnections.com/blog/2011/07/12/using-the-3-rs-of-site-search-to-improve-search-roi/",
    "title": "Using the 3 Rs of Site Search to Improve Search ROI",
    "content": [
      "Oftentimes, we see e-commerce marketers who spend a lot of time on the design of their website, make it look pretty, have rounded corners, and follow all of the latest Web 2.0 standards only to add in a search box with no forethought, as if it was a waste of precious online real estate. What marketers fail to realize is that site search is not a cost center, but, rather a potential profit center. Just as thought, revisions, time, and testing go into website design and functionality, so, too should the same effort be put into making site search a prime feature of an e-commerce website to increase its ROI. They fail to adhere to what we call the 3 Rs of site search: results, relevancy, and revenue.",
      "Results: When a user types in a term in a search box, they are looking for something that the information scent within the native navigation of the website has not provided. However, for many terms, the user may have done something simple such as misspell the word or put in a synonym, and when the site returns a web page informing the user that there are zero results, the user leaves, and the opportunity to make a sale has departed with the user. Solr provides an easily managed method of dealing with misspellings and synonyms, although this is only the first step of many to take.",
      "Here is an example of a zero result search. Instead of having misspellings of Sherando, Recreation.gov provides zero results, leading to a probable abandon.",
      "Relevancy: The flip side of zero results is far too many results with little to no relevancy for what the user seeks. By failing to provide relevant results to the user, the e-commerce marketer has grown the size of the haystack that the user must sift through to find the needle. In an age where Google taught the user about the search box and Solr and Lucene have gone a long way in commoditizing search, the user is not going to spend time going through pages and pages of results to find a specific one. The right result has to be in the top three or you risk losing that customer.",
      "Here is a search for social networking on FedBizOpps, www.fbo.gov. It's very unclear how IT equipment and ER safe rooms relate to the search term social networking.",
      "Revenue: Because the typical e-commerce marketer doesn’t view search as a profit center, he or she is not measuring the revenue which comes from the channel. Tracking user behavior and history through the buying funnel can yield a great deal of insight about the expected value of a user along each stage of the funnel as well as the expected value of search phrases within each stage of the funnel. Measuring the expected value can also provide a rich testing agenda for testing alternative scoring and boosting models – Solr can be tuned to boost certain results within a profitability scoring model – as well as cross-sell and upsell opportunities. Once the first two Rs have been accounted for, which is the basic blocking and tackling of well-implemented search, then the aggressive and creative marketer can dramatically improve the return on investment of site search.",
      "If you don’t know where to start, talk to us about how we can leverage our analytical tools and Solr and Lucene expertise to make your e-commerce site search a profit center and increase the ROI of search for your company."
    ],
    "summary_t": ""
  },
  {
    "id": "9badfa5c8ef5340c283a980371dce30f",
    "url_s": "https://opensourceconnections.com/blog/2011/07/15/inaugural-charlottesville-solrlucene-meetup/",
    "title": "Inaugural Charlottesville Solr/Lucene Meetup, August 15, 2011",
    "content": [
      "On August 15, OpenSource Connections, along with Lucid Imagination and SAIC, are hosting the inaugural Charlottesville Solr/Lucene Meetup at SAICs office in the UVa Research Park. It will be held in the 3rd Floor Conference Room, 1001 Research Park Blvd., Charlottesville VA 22911.",
      "We have some great presentations lined up, and want you to participate as well. The speaker lineup includes:",
      "•         What’s New in Solr 3.2/4.0 – Erik Hatcher, Lucid Imagination",
      "•         Multi-Level Security in Solr – Scott Stults, OpenSource Connections",
      "•         Solr Testing – Eric Pugh, OpenSource Connections",
      "We will be providing pizza and drinks for attendees as well.",
      "You can sign up to attend here.",
      "We look forward to seeing you there!"
    ],
    "summary_t": ""
  },
  {
    "id": "efe2c0a4f491c1cf36cdf597c533a62a",
    "url_s": "https://opensourceconnections.com/blog/2011/07/27/search-engine-testing-and-apache-lucene-eurocon-oh-my/",
    "title": "Search Engine Testing and Apache Lucene Eurocon Oh My!",
    "content": [
      "Eric Pugh got notified today that hes been accepted to speak on \"Better Search Engine Testing\" at the 2011 Apache Lucene Eurocon.",
      "So, what, you may ask, does search engine testing entail?",
      "Great search implementation isn’t just about installing Solr and making it run a millisecond faster. It’s about setting up the conditions to ensure that the results which the search engine returns off of a query are the results which the search engine should return off of a query. How do you know what search results the engine should return? This is where your customer has to be integrally involved. Testing search results has similarities and differences to other areas of enterprise testing. Some areas of enterprise testing don’t require customer input, for example, how much concurrent load can a site handle before it fails? Others, though, do, such as what is the expected result when I click on a given link. In those instances, once client input is provided, then testing toolkits such as Selenium exist to automate testing. A similar process occurs with search testing. Content owners will have the best idea of what results should come up on a given query, and once you have that list, then you can automate the tests to measure adherence of the actual search engine results pages with the expected results.",
      "Testing, though, is never a complete process. Aside from the most static of sites, content changes. Usually, changing content reflects changing business priorities, and as those priorities and the resulting content change, so too should the test results. Additionally, over time, important queries may change. What was important a year ago may no longer be as critical or as profitable. As a result, content owners should constantly revisit both the list of important queries as well as the results that those queries should return to get a feel for how the search engine is performing against expectations. It may require tuning, but only through constant testing will the content owner know that tuning is required.",
      "What about the long tail? some will ask. To answer the question of the long tail, the customer must first answer the question of where the money comes from. Does it truly come from the long tail, as the case with eBay or Amazon, or does a bulk of profit come from a few queries? Remember, you can tune for the long tail, but it cannot come at the expense of other, more profitable queries. Just as a mathematician can derive a formula to match any curve but often cannot predict the next point, it is possible to overtune a search engine to match the long tail but miss the next big revenue opportunity.",
      "What do you think? Am I close? Would you sign up to hear more about the session? Talk to us and let us know if this is what you see with your search engine implementation."
    ],
    "summary_t": ""
  },
  {
    "id": "5a1680ab2cfe528413ab84f42c283698",
    "url_s": "https://opensourceconnections.com/blog/2011/09/19/netflix-qwikster-and-the-potential-failure-of-search-user-experience/",
    "title": "Netflix, Qwikster, and the Potential Failure of Search User Experience",
    "content": [
      "Yesterday, Netflix announced the separation of its DVD-by-mail division into a new entity called Qwikster. The new company will have a separate website and focus entirely on mail delivery of DVD and video game rentals. It also will, in a few weeks, have a new website.",
      "In splitting the previously Siamese twins, Netflix also decided to make its users create new user accounts and undergo separate billing for its two companies. From a business perspective, this makes sense, as it will allow each company to focus on its specialties and to have a separate P&L statement.",
      "However, as pointed out in the comments to CEO Reed Hastingss blog post (search for the comment right before Hastingss reply of \"ouch\"), the team didnt think of important potential scenarios in the new dual entity system. Netflix is a Solr user, but here is a use case that points out that mere implementation of Solr does not a great search experience make. The poster queries about a simple use case: what if a user of one system searches for a movie and its not available on that system. Will the search interface allow the user to see if the movie is available on the other system? Thus, lets say that I search for The Amazing Race on Netflix. Its not available by streaming, but it is available by DVD. However, as Hastings sheepishly points out, in this case, \"Youd have to search the second place if we didnt have it in the first place.\"",
      "Oops. Cue user discontent.",
      "This is not the first time that enterprise search designers have failed to take into account pretty key use cases of search when rolling out a search engine. I see intelligence analysts having to struggle with poor search user experience all the time, unfortunately.",
      "How should enterprise architects work to mitigate this scenario? Conducting personae and scenario analyses. Identify the typical personae who will be using the website and the potential scenarios in which they will be using search. Search is more than just a white box with some text that you enter, click on search, and magically get results. By thinking through the use cases, then marketers will have a better set of requirements to pass along to the IT team responsible for implementing search.",
      "The IT staff of Netflix is technically correct. They have a search engine that works. What they might not have, as Hastings had to publicly admit, is a search engine that works well.",
      "Do you want to prevent this from happening to your search engine? Take a look at our Adopting Solr Workshop and other Solr services."
    ],
    "summary_t": ""
  },
  {
    "id": "705f70a602c005036cea4eeb66bba50b",
    "url_s": "https://opensourceconnections.com/blog/2011/09/21/opensource-connections-adds-site-search-analytics-service/",
    "title": "OpenSource Connections Adds Site Search Analytics Service",
    "content": [
      "At OpenSource Connections, we strongly believe that search is not simply a matter of installing a search engine, putting in a search box, and forgetting about it. Yet, thats what we find most site owners do, and when they do that, they leave the door open to customer dissatisfaction and lost revenue opportunities.",
      "Thats why were launching our Site Search Analytics (SSA) service. As Lord Kelvin said, \"If you can not measure it, you can not improve it.\" Making a site search implementation great means knowing the relevant facts about what your users are doing with site search, what they want to do with it, and what you want them to do. Without knowing the proper metrics, how to evaluate them, and how they relate to your specific implementation, your site search is likely just a white box with a search button that you hope gets used.",
      "Take charge of your site search and find ways to maximize revenues and increase user satisfaction with your website. Know where you need to improve, how to get there, and how to measure success. Create the definitive data to provide to your IT staff to show what needs to change. Show your content managers where their content falls short and how to improve it to provide meaningful information to your customers.",
      "It all starts with the quantitative evaluation of where your site search stands and where you want it to go. With our Site Search Analytics practice, you can do just that."
    ],
    "summary_t": ""
  },
  {
    "id": "47b0a24ab3c2aed1f004819914686b66",
    "url_s": "https://opensourceconnections.com/blog/2011/09/28/you-dont-know-zipf-about-your-landing-pages/",
    "title": "You Dont Know Zipf About Your Landing Pages",
    "content": [
      "Unless you have a very well-known brand where people type your URL to visit your site, then chances are that most of your traffic comes from an external search engine such as Google, Bing, or Yahoo. A quick analysis can tell you that, relative to the total number of keywords visitors use to get to your site, a small percentage of them are the ones which are truly driving your traffic. This distribution is called a Zipf distribution: a fat head and a long tail. The long tail phenomenon is brilliantly described by Chris Anderson in his book The Long Tail.",
      "However, knowing whence your traffic came is just the beginning of the analytical problem. Do the same keywords tend to land on the same pages? Are these the pages that you want them to land on? What happens once they land there? Are they leaving, clicking, buying, or searching?",
      "An interesting aspect to look at is what happens if the user then searches for a term on your site after landing from an external search engine. We find that a couple of scenarios arise most often.",
      "The Zipf Distributions Match. If this is the case, then you have the general scent of information that a user seeks, but the page where the user landed isnt scratching the itch and providing the right information. Some actions you can take include:\n    \n      Improve the content of your most hit landing pages. Evaluate where your users go subsequently and see if you can cross-purpose the information in a meaningful way.\n      Add links to other interesting content on the landing page. Make informational navigation easy by providing more breadcrumbs for your user to follow to get to other content by providing the links.\n    \n  \n  The Zipf distributions dont match. If this is the case, then either your pages dont provide the type of information that your user thought that he was getting, or the information has spurred other queries of discovery which will lead the user down a path that wasnt originally anticipated. Here are a couple of things you can do:\n    \n      Clean up metadata and navigational clues. Make it crystal clear to both search engines and to the user what the page is meant to accomplish.\n      Provide other exploratory links to encourage navigational browsing. Give links that arent directly related to the page but may broaden the scope of search through discovery. Allow users to stumble upon (to borrow a website meme) new ideas and content through links.",
      "Taking a look at Google Analytics to see what search terms are used to drive traffic is a good starting point, but its not the terminus of your analysis. You also need to look at what happens to the user after landing on a page, and by taking advantage of site search analytics, you can take advantage of additional information that your user is providing about what he seeks from your site.",
      "Do you want to understand how to measure your user behavior and use it to increase revenues? The OpenSource Connections Site Search Analytics service can help you make your website a profit center, not a cost center."
    ],
    "summary_t": ""
  },
  {
    "id": "23d1669d26b45cc46c21e82b520eaaf0",
    "url_s": "https://opensourceconnections.com/blog/2011/11/10/apache-solr-3-enterprise-search-server-published/",
    "title": "Apache Solr 3 Enterprise Search Server Published",
    "content": [
      "We are pleased to announce that Eric Pugh and David Smiley have published the second Solr book in publication, Apache Solr 3 Enterprise Search Server. It is an update to the blockbuster Solr 1.4 Enterprise Search Server, and is available at Packt Publishing.",
      "\"This is an important update and covers all of the features that the Solr 3 series of releases contain,\" says co-author Eric Pugh.",
      "The physical print book will be available starting on November 14, 2011, according to Sarah Cullington, editor at Packt Publishing.",
      "5% of all book sales are contributed as donations to the Apache Foundation."
    ],
    "summary_t": ""
  },
  {
    "id": "12633087db07703be6210d4e2b69ac10",
    "url_s": "https://opensourceconnections.com/blog/2011/12/23/indexing-chinese-in-solr/",
    "title": "Indexing Chinese in Solr",
    "content": [
      "Recently, we had a project where we helped a client index a corpus of Chinese language documents in Solr. We have asked Dan Funk, a committer to Project Blacklight to provide a guest blog post for us on the details of how to approach indexing Chinese, particularly when you are a non-speaker.",
      "Take it away, Dan!",
      "Indexing Chinese in Solr",
      "Prologue (Including thanks, and some vital orientation)",
      "Before I start, Id like to lay some thanks on a few people who helped me muddle through indexing a language I cant speak, and having me come off looking like a pro.  Wiley Kestner ([@prairie_dogg][1]) sat for hours giving me tips and pointers about Chinese. Christopher Ball helped me quickly put an excellent and professional face on my work by using the Blacklight project. And Eric Pugh (@dep4b) provided some much needed mentoring – helping me see a way forward in what I initially believed was an intractable problem.",
      "If you dont read Chinese or have not worked with it before, here are few things you should know:",
      "Chinese words are frequently made up of more than one character, and words are not separated by spaces.  (read this as \"Tokenization is a big problem.\")\n  Spoken Chinese is completely different from written Chinese, so dont stress about the multitude of dialects when you are indexing.\n  There are two common types of written Chinese: Standardized and Traditional.   Since Traditional can be converted to Standardized fairly easily, the focus of this document is on Standardized.  Traditional text has many more characters and thus the potential for deeper subtler meanings.\n  Though traditionally written from top to bottom, right to left, it is far more common to see Chinese written from left to right – particularly on the web.\n  Dont depend on your documents being in UTF – you are far more likely to encounter GB2312 encoding.\n  A great method for testing relevancy in a language you dont know is to use a Judgment List, please see Eric Pughs presentation here for more information.",
      "My Best Advice:",
      "Ok, here are two most important pieces of advice I can give you:\n  \n\n  \n    #1:  Separate your Chinese text into its own field(s).  That is to say, dont try and index multiple languages in the same field.  If your Lucene/Solr field structure is complicated, add a second core with duplicate field names.   Why? A. You set yourself up for handling additional languages fluidly and effectively. B.  You can use the best indexer available for each language (see advice #2) C. You improve overall performance because the indexes are smaller and tighter. D. You remove confusing, and likely false, results in a language the end user does not understand.\n  \n\n  \n    #2: Use the CJK or Paoding analyzers for your Chinese Text.  There is some great documentation out there for CJK, but if you would like to give Paoding a shot, here are some directions to help get you up and running:",
      "1. Dont use the binary distribution.  It wont work with the latest versions of Solr.  Instead, grab the source:",
      "[email protected]:~$ cd code",
      "[email protected]:~/code $ svn co  http://paoding.googlecode.com/svn/trunk/ paoding-analysis",
      "2. Compile it with Ant.",
      "[email protected]:~/code/paoding-analysis $  cd paoding-analysis",
      "[email protected]:  ant",
      "…",
      "Building jar: paoding-analysis.jar",
      "3. Build a modified Solr war file.",
      "The Paoding analyzer, while brilliant at analyzing Chinese text,  was not originally built to work well in a web deployed environment, and depends heavily on file paths to get to its built in dictionaries.  To correct for this, you will need to inject the analyzer and its configuration files into your solr war file.  I tested this approach with apache-solr-3.4 doing the following:",
      "[email protected]:~$ mkdir temp",
      "[email protected]:~$ cd temp",
      "[email protected]:~/tmp$ unzip /usr/local/apache-solr-3.4.0/dist/apache-solr-3.4.0.war",
      "[email protected]:~/tmp$ cp ~/code/paoding-analysis/paoding-analysis.jar WEB-INF/lib/",
      "[email protected]:~/tmp$ cp ~/code/paoding-analysis/classes/*.properties WEB-INF/",
      "[email protected]:~/tmp$ zip -r * apache-solr-3.4.0-paoding.war",
      "4. Update your Solr configuration and add support for a paoding string.",
      "</p>",
      "",
      "</fieldType>",
      "5. Copy over Paodings dictionary files into your solr home directory.",
      "[email protected]:~/solr_home$ cp ~/code/paoding-analysis/dic my_solr_home",
      "6. Set an environment variable to let the Paoding Analyzer know where to find the dictionary files:",
      "[email protected]:~/solr_home$  java -DPAODING_DIC_HOME=./dic -jar start.jar",
      "我的 my;",
      "高的 high, tall;",
      "是的 thats it, thats right;",
      "是…的 one who…;",
      "目的 goal, true, real;",
      "的确 certainly",
      "</col> </col> </col> \n      \n      \n        Method\n      \n\n      \n        Pros\n      \n\n      \n        Cons\n      \n    \n\n    \n      \n        Default Solr setup\n      \n\n      \n        No new configuration required, and roughly supports multiple languages.\n      \n\n      \n        Tokenized on spaces – but will shift to character tokenization for Chinese text.  See previous section for why this is problematic.\n      \n    \n\n    \n      \n        CJK\n      \n\n      \n        Thoughtfully parses Chinese characters – understands that character groups alter meaning. Ships with and is part of Solr’s default configuration.\n      \n\n      \n        Does not use a dictionary, depends largely on an n-gram based algorithm that creates all possible groupings of pairs of symbols in the text.\n      \n    \n\n    \n      \n        Smart Chinese\n      \n\n      \n        Uses a dictionary to pull out characters.  Ships with solr as an add-on package.\n      \n\n      \n        The dictionary is minimal and handles general cases well, but many nuances of the language are lost.  It requires a custom Solr configuration.\n      \n    \n\n    \n      \n        Paoding\n      \n\n      \n        Uses a large set of dictionaries, and provides exceptionally good search results across a multitude of contexts.\n      \n\n      \n        Can be very difficult to configure and setup – almost all documentation is written in Chinese.  Does not ship with Solr, and must be built from source to work correctly with the latest stable Solr versions.",
      "Example 1: 爬蟲",
      "</col> </col> </col> </col> \n      \n      \n        Method\n      \n\n      \n        T1\n      \n\n      \n        T2\n      \n\n      \n        Hits\n      \n    \n\n    \n      \n        Default Solr setup\n      \n\n      \n        爬\n      \n\n      \n        蟲\n      \n\n      \n        2 hits (doc 8 and doc 3) * 爬蟲類:\"reptile\" – a good hit. * 爬岩鳅: \"Beaufortia loach\" – bad hit. – Even more problematic, is that your highlighting will identify these as two seperate hits, even when it gets it right.\n      \n    \n\n    \n      \n        CJK\n      \n\n      \n        爬蟲\n      \n\n      \n      \n\n      \n        1 hit (doc 8 ) It gets the right document.  But this is because CJK always groups by 2, we will see it fall short on the next example.\n      \n    \n\n    \n      \n        Smart Chinese\n      \n\n      \n        爬\n      \n\n      \n        蟲\n      \n\n      \n        2 hits (doc 8 and doc 3)\n      \n    \n\n    \n      \n        Paoding\n      \n\n      \n        爬蟲\n      \n\n      \n      \n\n      \n        1 hit (doc 8 )",
      "ICU Project",
      "Example 2: 胡志明",
      "; 明白 míngbai clear, understand\n\nAnd if you randomly pair (in the case of CJK), you just get sounds, common pairings used in names.\n\n\n  </col> </col> </col> </col> </col> \n      \n      \n        Method\n      \n\n      \n        T1\n      \n\n      \n        T2\n      \n\n      \n        T3\n      \n\n      \n        Hits\n      \n    \n\n    \n      \n        Default Solr setup\n      \n\n      \n        胡\n      \n\n      \n        志\n      \n\n      \n        明\n      \n\n      \n        6 Hits: 胡志明 (Hồ Chí Minh) 胡 志 (Ho Chi) 明目  (eyesight) 眼目 (eyes) 杂志中的 (magazines) 起明显 (the aparent)\n      \n    \n\n    \n      \n        CJK\n      \n\n      \n        胡志\n      \n\n      \n        志明\n      \n\n      \n      \n\n      \n        2 Hits 胡志明 (Hồ Chí Minh) 胡志  (Ho Chi)\n      \n    \n\n    \n      \n        Smart Chinese\n      \n\n      \n        胡\n      \n\n      \n        志\n      \n\n      \n        明\n      \n\n      \n        4 Hits: 胡志明 (Hồ Chí Minh) 胡 志 (Ho Chi) 明目  (eyesight) 眼目 (eyes)\n      \n    \n\n    \n      \n        Paoding\n      \n\n      \n        胡志明\n      \n\n      \n      \n\n      \n      \n\n      \n        胡志明 (Hồ Chí Minh)\n      \n    \n  \n\n\n\n  In Conclusion\n\n\nIt is possible for you to index Chinese, even if you dont speak it.  The largest problem you will face is in correctly parsing the text, but there are several effective tools that help solve the problem.  I would strongly discourage you from indexing Chinese content with Solrs default settings.  You will not get good results.  If you need to quickly add support for Chinese to an existing project, I highly recommend using the CJK analyzer.  However, if you have a discerning audience, a specialized area, or the need to enhance the quality of your results over time (by expanding on the included dictionaries) then Paoding is an excellent choice.\n\n\n  Resources and References\n\n\nhttp://www.zein.se/patrick/3000char.html** – **The most common Chinese characters in order of frequency\nhttp://translate.google.com/** – **A fantastic way to quickly translate a few characters or a whole page of text.\nhttp://site.icu-project.org/** – **Provides an API for converting from Traditional to Standardized Chinese Characters.\n\n [1]: http://www.twitter.com/prairie_dogg",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "40701e4dad679df7344db2c0fb17e620",
    "url_s": "https://opensourceconnections.com/blog/2007/02/05/using-mod_auth_mysql-as-a-single-sign-on-tool/",
    "title": "Using mod_auth_mysql as a single sign-on tool",
    "content": [
      "I recently migrated our Subversion repositories and trac sites from one server to another. This gave me the chance to reorganize our layout and authentication mechanism. I chose to replace our existing .htaccess files with mod_auth_mysql because it fit two key criteria:",
      "Its easy to administer – Logging into the server and running htpasswd every time I need to add a user or change a password is a PITA\n  Most of the applications we use offer MySQL authentication functions, so I can grant access to several things at once.",
      "The docs on how to set it up were fairly clear, with one exception: I would have liked to have seen an example of how to use multiple tables for group membership. After a little more Googling I ran across such an example, taken from  Integration of Jira User Management with Apache.\nCommon database normalization techniques suggest that users and passwords go into one table, groups in another, and a third table should be used to join the two by IDs. This is what I was after, and thats what Jiras integration doc showed me. It was also confirmation that I had chosen an appropriate central authentication mechanism.",
      "Eric was nice enough to whip up a Rails CRUD utility to manage the tables (which will be posted soon) and from there it was a simple matter to instruct Apache to use those tables whenever it needed authentication. By the way, a great way to do that is with a separate file such as \"mysql-auth.inc\" and just use the Apache Include directive to include that file wherever you need authentication."
    ],
    "summary_t": ""
  },
  {
    "id": "6dc49a32197cc32c2069541ef284eea7",
    "url_s": "https://opensourceconnections.com/blog/2012/02/06/why-use-a-javascript-interface-for-solr/",
    "title": "Why use a Javascript interface for Solr?",
    "content": [
      "Richer presentation.  Deeper data in search results.",
      "Search UIs are getting richer and more interesting by the day.  Google page snapshots, animated disclosures of richer data on mouse over, and endless pagination are great examples. Often this type of functionality has a dark side.  As each of these features is layered on and integrated with our HTML, it can begin to feel like death by a thousand paper cuts.",
      "That’s where building a pure Javascript interface to search can start to help clean up and organize our code.  At some point we manipulate the DOM so much we may as well have just created it in Javascript in the first place.",
      "Once we have separated presentation from data we can begin to apply tools like design patterns (Visitor comes to mind) that make our code easier to read and modify.\n** **",
      "Pass the buck on rendering",
      "*",
      "Cloud meet Ocean.",
      "On demand pages are expensive to render, and search is solidly in this category.  Popular searches can be cached at the HTML level, but this is only only effective for very simple search behaviors and comes with its own set of problems.  Cache expiry for content and what to do with Solr caching are two of these complications.",
      "CSS is to HTML what Javascript is to Search.  By giving the browser the ability to nicely render our data though Javascript we can refocus our servers on just serving the bits that change.  Turning Solr results to JSON or XML is often much faster than rendering the same data as HTML.  As a bonus, our presentation layer can be cached on the client giving us lower overhead on subsequent requests.",
      "Even more server capacity can be reclaimed by letting the app server be a proxy for JSON results right out of Solr.  This is an excellent way to get the most out of Solr’s own query caching.",
      "Need more speed?  Expiry headers and proxy caches like Varnish or Squid can still be used to further speed up the resulting JSON or XML requests.",
      "Decoupling for Scaling",
      "*\nBetter yet, what if we could scale search independently of content?  Using techniques like Cross Origin Resource Sharing (CORS) or techniques like JSONP we can remove our app servers from the stack and go directly to the Solr servers.",
      "Decoupling gives us much more freedom to make good scaling choices in server configuration.  We can optimize search servers around serving small JSON documents and our app servers around supporting more traditional web content.",
      "",
      "stacked vs decoupled servers",
      "",
      "Its also profoundly great for testing!  Expectations for and of the search servers can be verified independent of the app server.",
      "Gotchas",
      "Security",
      "Im assuming you dont want marketers injecting their sales pitches into your search results.",
      "Solr doesnt really have a provision for users and privileges which might have horrified you earlier when I implied you could make your Solr server public-facing.  Fortunately this is easily managed.",
      "Configure access to Solr endpoints on the web server.  Some things to keep in mind:",
      "Lock down any /update or /admin requests at this level\n  \n  \n    If you don’t plan on blocking /update and /admin here, move them to a different endpoint than the traditional Solr URI scheme and restrict access to an IP range or require authentication\n  \n  \n    Make sure to use invariants in the Solr server configuration if there are filter rules that must happen\n  \n  \n    Don’t put your security logic in the client side Javascript, reverse engineering and spoofing is possible even with compressed or obfuscated Javascript\n  \n  \n    Worried about someone requesting 1 million rows of data?  Look into Apache’s mod_header or write a simple proxy app.  This also works for preventing deep dives (start = 50000000)",
      "SEO and Accessibility",
      "You dont have to worry about SEO on search pages.",
      "Dont believe me? Good.  Who knows how Google and friends ultimately decide which content matches a query?  If your SEO-practor says results pages matter then you will need a \"Plan B.\"",
      "Use an AJAX request to set a cookie that lets your server know that the client supports Javascript and AJAX.  From there you can decide to send them to an old-style search results page if needed.  As long as the results are roughly the same Google has said this should be okay (and you dont base this logic on browser identification).",
      "And while you are at it, this page can be really helpful as a fallback to unconventional browsers like visitors with disabilities.",
      "Getting Started",
      "Check out the ajax-solr project if youd like to start working with some of these methods.",
      "More?",
      "Let us know in the comments if youd like to see more of our approach to Javascript and Solr.",
      "",
      ""
    ],
    "summary_t": "Richer presentation.  Deeper data in search results"
  },
  {
    "id": "1a1bb172e466a9a8225751da8d328271",
    "url_s": "https://opensourceconnections.com/blog/2012/02/09/patent-troll-shows-need-to-improve-search-to-disprove/",
    "title": "Patent Troll Shows Need To Improve Search To Disprove",
    "content": [
      "As the \"father of the Internet,\" Tim Berners-Lee, got onto a plane to head to the burgeoning tech center of Tyler, Texas (all sarcasm intended), he must have wondered how he found himself in this position. Berners-Lee is testifying at a court case of a patent holder who is suing Google, Amazon, and others regarding the viability of a patent claiming intellectual property ownership over the \"interactive web.\"",
      "At issue is whether or not the patent should have been issued in the first place. At its core, a patent claim is one which is made in an item or process which is new, novel, and never been done before. Yes, the redundancy is intentional. It is up to the patent examiner to look at previous patents to determine whether or not what is being claimed for a patent actually existed prior to the patent claim being made.",
      "If you think about the typical tools of the search trade, you think of, first and foremost, the search box. People type in terms in a search box and results come out. Depending on the effectiveness of the underlying search engine and its relevancy algorithm, most of those results are at least somewhat related to what the search term was.",
      "What happens, then, when youre trying to disprove that something exists? How do you go about that search? Do you type in the term and hope for zero results? Do you go through every list of synonyms that you can think of to try to eliminate its existence? In logic, proving something is a much simpler task than disproving something. The inability to prove the existence of a deity compared to the inability to disprove a deitys existence is the fodder of many religious debates. Proving existence is accomplished through induction. To disprove something only requires proof of the existence of one value for which the statement is not true (e.g. my dogs bone does not exist; oh, wait, theres his bone…statement disproved).",
      "In this instance, patent examiners try to disprove by finding prior art. However, theres not really a setting in a search engine to make a disproving search versus a finding search. If there was, then many patent trolls would be uttering a long bellow of anguish and going back under the bridges where they were hiding.",
      "In a forthcoming article, we will show how you can use Solr to use search to disprove the existence of something.",
      "In the meantime, we can hope that the courts come to their senses and dont let the trolls win."
    ],
    "summary_t": ""
  },
  {
    "id": "dfabee8bc582e184ed6301fee3731133",
    "url_s": "https://opensourceconnections.com/blog/2012/02/13/identifying-anomalous-driving-patterns-using-hadoop-and-cassandra/",
    "title": "Identifying Anomalous Driving Patterns Using Hadoop and Cassandra",
    "content": [
      "Overview",
      "In order to identify unusual behaviors of tracked targets in a large and ever-expanding data set of raw geospatial track data, we built a analytics application based on Cassandra, Hadoop, and proprietary geospatial algorithms.  These algorithms consumed track data and generated summary data, both of which were stored in Cassandra.  The processing engine for the data was Hadoop.",
      "Background",
      "The \"NoSQL\" database Cassandra has established itself as a leader in raw I/O capability. Its eventual consistency and distributed hash algorithms allow it to efficiently migrate data through a network graph. The result is a streamlined path from where data is ingested to where it is used.",
      "Hadoop is fast-becoming the industry standard for distributed processing. Its ability to individually scale parts of an algorithm as well as a whole job help developers eliminate bottlenecks in the data flow and utilize all of the hardware at their disposal.",
      "Together, Hadoop and Cassandra provide a highly scalable framework on which to build analytical applications that operate over massive amounts of data.",
      "Solution",
      "OSC developed a custom map-reduce job that fed all of the existing data through various web services and stored the resulting tracks with the originals. Each I/O record of the job aligned with its Hadoop processing node and instances of third-party web services to evenly divide the workload and ensure minimal network traffic. After processing the historical data, the client was then able to reuse the same map-reduce process to quickly ingest new records in both bulk and ad hoc form. The new meta-data resulted in an order of magnitude increase in track data points made available for analysis.",
      "The system was tested by using public data of taxi cabs driving in San Francisco."
    ],
    "summary_t": ""
  },
  {
    "id": "5e83d55b477a330c9a85141eba3d9ea9",
    "url_s": "https://opensourceconnections.com/blog/2012/02/17/talking-solr-in-las-vegas-february-22-2012-700-pm/",
    "title": "Talking Solr in Las Vegas, February 22, 2012 7:00 PM",
    "content": [
      "Do you like Solr? Live in Las Vegas? Are you going to be in Vegas on Wednesday, February 22, 2012? Like free dinner?",
      "If this sounds like you, then join Eric Pugh and John Berryman at Mundo, 495 S. Grand Central Pkwy. A-116, Las Vegas, NV 89106 at 7:00 PM on Wednesday, February 22, 2012.",
      "Since were going to be providing dinner, we need to make reservations; therefore, we need to know that youre coming. Wed hate to eat food in front of you. If youre interested, please send us an email at talktous at opensourceconnections dot com, and well add you to the list."
    ],
    "summary_t": ""
  },
  {
    "id": "29080652aed63b039762ffb36d3d4c7a",
    "url_s": "https://opensourceconnections.com/blog/2012/02/19/an-introduction-to-results-utility-positioning/",
    "title": "An Introduction to Results Utility Positioning",
    "content": [
      "When a user enters in a search query and is taken to a search engine results page, then the probability of clicking on a result is a function of three items:",
      "The density of the searched for terms found in the search snippet. Ideally, these are somehow highlighted – a different color, italics, bolding – to show the user that the search term is indeed found in the target link.\n  The actual ranking in the search result. The higher the result, the more likely the user is to click on  that result.\n  The search engine result as compared to what the content manager thinks should be in that position.",
      "This article will cover the first two of those aspects: the keyword density and the relative placement.",
      "First off, lets understand why this is an important concept for content managers and IT-based search engine managers within the enterprise. The algorithm of a search engine will determine relevancy of results within the index of the corpus of documents and materials within a website. This relevancy is usually based on keyword and concept frequency; however, it is possible, particularly within Solr, to tune a search engine so that relevancy is based on other factors, such as recent sales or recent clicks on results. While, for example, a marketing manager may want to have the product with the highest profit appear at the top of a search engine results page (SERP), this can be disastrous to the overall profitability and sales of an e-commerce enterprise.",
      "Lets examine why this is the case. Imagine if a user searches for the term \"red ball.\" A red ball itself sells for $1.00 and has a $0.10 profit. If a user sees red ball as the top result, he will eventually buy the red ball 50% of the time. In this case, if the red ball comes up first, then the expected value of the search for the term \"red ball\" is $0.05. However, if a marketing manager decides that blue balls, which have a sales price of $2.00 but a profit of $0.50, should be first. Given past clickthrough rates (CTR), we know that if the user searches for \"red ball\" but sees the blue ball first, there is a 1% chance of a purchase. This leads to an expected value of $0.005. We also know that if the red ball is second, there is a 20% chance of a purchase, which gives us an expected value of the second position of $0.01. Combine the two, and you have an expected value of $0.015.",
      "This is a very simplistic analysis, but is the framework to understand why putting less relevant but more profitable items above the more relevant but less profitable product. Someone searching for \"red ball\" often wants a red ball, and, on a rare occasion, can be convinced of a cross sell of the value of a blue ball. However, the place to suggest alternative items may be on a shopping cart page or a checkout page (both of which are easily accomplished with Solr) rather than manually forcing a different result set.",
      "To evaluate the measurable quality of a SERP in a quantifiable measure, we must evaluate both the relative position of a result and the keyword density of the search term within the result snippet.",
      "First, we look at the probability of a click result given the relative ranking of a result within the search engine results page. All things being equal, we know that the top position gets more clicks than any other positions. Short of having a test and control set to measure the delta created by altering a search relevancy algorithm, we can use existing click probability data to provide a proxy for a relative scoring, setting the top result to the highest score and normalizing other positions compared to the top position.",
      "Then, we look at the keyword density of the SERP result snippet to determine how much signposting we are providing the user to signal that the result has appropriate relevancy for the search term. We use a discounted cumulative gain function of the keyword density to create a scoring model for the density, with a top score ceiling. We believe that there is a rapidly declining utility of incremental density beyond a certain set point and do not score additional value for density above that value.",
      "We then combine the two values and provide a normalized score from 0 (low) to 100 (high). We believe that a score of 70 or higher is excellent and provides appropriate content-based signposting for users in their search terms.",
      "Ideally, you should be looking at your top 100 search terms and getting a RUP score for each of them. Low scores will point to either the need to adjust the relevancy algorithm within your search engine or to guide your content managers to adjust the content of their posts to provide a higher keyword density for the top search terms. In either instance, IT or content management teams will have a quantifiable metric to point to in order to drive change and also to measure the success of subsequent changes.",
      "If you need metrics on the viability of your search engine results pages, please contact us or e-mail us at talktous at opensourceconnections dot com."
    ],
    "summary_t": ""
  },
  {
    "id": "2d45978ade32280810d8ae8473d0a867",
    "url_s": "https://opensourceconnections.com/blog/2012/02/19/osc-launches-http-www-smallbizcontracting-com/",
    "title": "OSC Launches http://www.smallbizcontracting.com",
    "content": [
      "At OpenSource Connections, we focus on solving the search problems that others face. One of the problems that we have noticed is that tools for U.S. government acquisition officers are poor at best. The SBAs Dynamic Small Business Search (DSBS) and Central Contractor Registry (CCR) only provide the scantest of information to an acquisition officer who wishes to determine which companies are capable of performing required work. Resultantly, they turn to Google or Bing to search for companies and often have to do cross-referencing, which leads to human error. Worst of all, there is often a lack of subject matter expertise that allows the acquisition community to accurately identify when companies can perform the work, leading to false negatives and leaving the companies who could perform the work out in the cold. OSC views this predominantly as a search problem; there are no effective search tools which enable the contracting officer, in one place, to do market research and to find companies to perform work. Because of the lack of effective search tools, market research often fails or relies on a many-to-one relationship rather than a one-to-many relationship. In terms of relative effort, it is easier for a contracting officer to, through the use of a tool, find providers than it is for multiple providers to write deep responses to RFIs. Because of this problem, we have launched the SmallBizContracting website at http://www.smallbizcontracting.com. In its current state, it is much more of a proof of concept website than something which is polished and hardened. However, we wanted to push out what wed been working on in our BHAG time to the public so that we could start to get comments on it. Briefly, we merged data from CCR, the Center for Veterans Enterprise (CVE) database, and individual company websites to get a richer, fuller picture of corporate capabilities than any existing government tool provides. We simply have merged into one tool the capabilities of CCR, CVE, DSBS, and Google. Please leave a comment here, e-mail us at talktous at opensourceconnections dot com or contact us to let us know what you think. In the upcoming weeks, well provide our take on what deeper data analysis tools such as Accumulo and Mahout have to tell us about the publicly available information we have collected about tens of thousands of companies."
    ],
    "summary_t": ""
  },
  {
    "id": "afcd7b61ed84ef40e9be0918379906ac",
    "url_s": "https://opensourceconnections.com/blog/2012/02/29/aws-ephemeral-storage/",
    "title": "AWS Ephemeral Storage",
    "content": [
      "You may not know this, but each EC2 instance has available to it lots of \"local\" storage. For example, the Large instances that Ive been using have two 420 GiB volumes available to them. HOWEVER, when you launch the typical \"EBS-backed\" AMIs from the management console these volumes wont be available to you.",
      "There are up- and downsides to both EBS and Ephemeral volumes, but for me the big win for Ephemeral is that its I/O speed is at least twice as fast as EBS and I dont have to allocate the extra space I usually need. The catch is you have to use the command-line tools to start the instances. Heres the link to the original blog post that clued me in:  http://www.webadminblog.com/index.php/2010/03/23/amazon-ec2-ebs-instances-and-ephemeral-storage/"
    ],
    "summary_t": ""
  },
  {
    "id": "65828417d5834ac0756e46ef1d774d7d",
    "url_s": "https://opensourceconnections.com/blog/2012/03/02/himss-healthit-wrapup/",
    "title": "HIMSS HealthIT Wrapup",
    "content": [
      "After a couple of exhausting days at HIMSS last week, where I was overwhelmed both by being a first time visitor to the glitz of Las Vegas and the sheer size of the HIMSS exhibit hall, I’ve come to the conclusion that HealthIT is currently running on two separate parallel tracks.",
      "The first track is the effort to integrate data. I first became aware of the mishmash of XML and EDI based standards back in 2008 when popHealth was launched. I remember seeing a printout of a HL7 formatted message where the machine readable XML encoded data was done in green, and the older proprietary formats, wrapped in some XML tags, was done in red. Most of the document was red, very little was in green signifying that the data was interoperable between multiple EHR systems.",
      "Fast forward to the 2012 HIMSS conference and it turns out we as an industry are still trying to solve this problem. I met a number of vendors such as PilotFish and Meddius who offered products for transferring documents between flavors of HL7 and into and out of proprietary formats. The sessions by the Veterans Administration and Department of Defence on merging their health records into a \"Integrated Electronic Healthrecord\" or iEHR was some of the most interesting sessions, and demonstrated both the challenges of integrating EHR’s and the the benefits of having a single holistic view of a patient, regardless of where their health records exist. GovHealthIT has a great background article on this effort.",
      "However, I feel like, despite all the money, and the vast improvement to patient outcomes that will come from integrating various EHR’s, we are still just ensuring that if the patient needs a leg amputated, we know which leg is the right one! All this massive IT \"plumbing\" is just introducing efficiencies in shuffling data around and therefore reducing errors. The second track that I saw at HIMSS is the one where we actually try and change patient outcomes. Through technology, we ensure that the patient doesn’t even need to have a leg to be amputated because we know which treatments they will respond best to. Companies like Apixio are assuming that this IT plumbing has been solved, and the real value in changing patient outcomes is looking at all the data available about a patient, both structured and unstructured, and coming up with better treatment options. If you are looking at one type of treatment, can you compare the efficacy against the larger populate of similar folks to validate it is the right one?",
      "This is the really exciting area to work in. Using the principles of big data, cloud computing, and data anlytics to make sense of the massive amounts of uncoded data available to clinicians so they can make better decisions. Hopefully at HIMSS 2013 we’ll see more sessions on \"How we improved patient outcomes via data analytics\" and fewer on \"how we tried to merge disparate data together\". I think the vast increase in information has driven this major change in medicine:",
      "",
      "As technologies such as entity extraction, natural language processing, and other text mining tools progress, we may end up seeing the tension between those who want all the data to be highly structured, and those who say \"give me your unstructured data, I’ll figure out what the structure is\" disappear. Images become OCR text. Raw text becomes entity extracted structured data. And patient outcomes improve."
    ],
    "summary_t": ""
  },
  {
    "id": "c00a54763357b0ae75e414375d5d5488",
    "url_s": "https://opensourceconnections.com/blog/2007/02/13/how-to-remove-plugins-installed-as-svnexternals/",
    "title": "How to remove plugins installed as svn:externals",
    "content": [
      "I love how easy it is to keep up with the latest and greatest of a plugin in Rails: script/plugin install -x https://some.subversion.server/plugin.",
      "Unfortunantly, if you then want to remove it, you need to then go and muck around with subversion externals in /vendor/plugins. It took me much hunting around and playing with svn proplist and svn propget to figure it out. It turns out to be as simple as just",
      "cd ./vendor/plugins and then svn propedit svn:externals . --editor-cmd vi",
      "and removing the line. Then just commit the changes plugins directory!",
      "The canonical information on properties is available from the Redbook"
    ],
    "summary_t": ""
  },
  {
    "id": "748d0265c6105272e7c1a63a39bbc468",
    "url_s": "https://opensourceconnections.com/blog/2012/03/06/the-failure-of-search-at-fedbizopps/",
    "title": "The Failure of Search at FedBizOpps",
    "content": [
      "The United States government procures most of its goods and services through competitive bidding. The notices of its requirements are posted on a centralized website called FedBizOpps, www.fbo.gov. Contracting officers and liaisons of government agencies to private sector providers recite from rote the message that everyone should search for opportunities on FedBizOpps. Yet, the system is an abject failure for providing adequate search, requiring private sector providers to needlessly scour irrelevant and numerous postings to find information which may be pertinent to their abilities. Consequently, the right opportunities are not provided to the right providers, which means that competition is not optimal, the government doesnt always get the best providers, and it often winds up paying more for less quality.",
      "Lets take several examples of the inadequacy of search and examine them in detail.",
      "* No document searching",
      "",
      "",
      "Here is a posting of Solicitation Number FA8771-12-R-1013 for the Air Force. The Air Force needs a search engine. From this page, all you can tell is that the Air Force needs a search engine. What if you provided an image search engine? Does this requirement pertain to you? Looking at the attached document reveals that the Air Force is looking for a full text search engine to index millions of documents and provide in-document searching (which, ironically, FBO does not provide). One specific requirement is Boolean search. What if you were a provider who had a great Boolean search and wanted to find requirements of the government that had the term Boolean search?",
      "We would have to search FBO for the term. Below is the search interface provided on the home page, with no links to an advanced search capability.",
      "",
      "Typing in \"Boolean Search\" in the keyword box leads to the following result page:",
      "",
      "However, we know that Boolean Search is in the RFI; its highlighted here.",
      "",
      "Therefore, the search functionality of FBO does not include the attached documents, which is often how contracting officers actually post their requirements. Rarely are full requirements posted on FedBizOpps; instead, theychosen fac are placed in attachments or on links to other websites which FedBizOpps does not index.",
      "* Does not account for misspellings",
      "",
      "FedBizOpps does not account for misspellings, either on the part of the searcher or the part of the content provider. It asumes perfect spelling on both parties parts, which is a bad assumption.",
      "Lets take the example of a couple of commonly misspelled words in the English language.",
      "First, lets search for the term \"calender.\" A system with an appropriate misspelling cross-reference would understand that the term provided is the misspelling of \"calendar\" and adjust appropriately. Instead, the user gets a paucity of results.",
      "",
      "The problem is worse if the content provider misspelled calendar as calender and the searcher was looking for the properly spelled term. It wouldnt show up.",
      "Another example of a commonly misspelled word is jewelry. Its often misspelled as jewelery. Searching for the properly spelled term yields 12 results.",
      "",
      "Misspell the term, and there is no love from FedBizOpps.",
      "",
      "* No facets, poorly chosen facets on advanced search page",
      "",
      "A well-designed search system should allow a user to filter out results directly from the search page rather than needing to flip back and forth between search query pages and search results pages. For example, the term \"ruby\" is ambiguous. Is it the programming language, the mineral, the color, or something else? Allowing for faceting in the SERP would, with a couple of clicks, allow the searcher to narrow down on the desired type of ruby. Instead, the searcher is required to look through a panoply of results and required to flip back and forth with the advanced search page, hopefully not losing the context along the way.",
      "",
      "The advanced search page has many of the facets that someone searching might require, but they also require significant domain specific knowledge, which raises the bar for someone who wants to provide goods and services to the government. The governments goal should be to make it easier to provide goods and services, thereby reducing costs, not to make it more difficult to do so.",
      "",
      "How about collapsing some of these \"facets\"?",
      "",
      "Oftentimes, the information provided to FBO is simply incorrect. Take the following notification I received recently. Apparently, someone in the government thinks that a pallet rack pad is a piece of technology.",
      "",
      "Chances are if I provided pallet rack pads, Id not be searching in classification code D.",
      "* No context in search results",
      "",
      "Since the SERP does not provide the context around which the search terms appear, the only to determine if there is applicability is to read the title and hope that the title gives appropriate contextual clues. Oftentimes, the title of the post is insufficient to relate any useful information. See the example below.",
      "",
      "Youd never be able to guess from the title that this is a RFI for information technology support services. Youd also never know from the information available to the FBO system that it was for website maintenance in a .NET 4.0 system.",
      "It is clear from even a cursory examination that the FedBizOpps search system was designed several years ago, and does not account for the use cases of current users and that does not account for even the most common edge cases. Because of the poor search user experience on FedBizOpps, it is difficult for providers to find opportunities where they can provide goods and services to the government, and the federal government misses out on getting as broad of a pool of potential vendors as possible because oftentimes the message misses the target.",
      "FedBizOpps is long overdue for a radical overhaul and redesign. Whether it will or not is a doubtful question.",
      "How else can FedBizOpps improve its search? Join the discussion and comment below!"
    ],
    "summary_t": ""
  },
  {
    "id": "5202d0ca269ef8a1215b2115445ea1b9",
    "url_s": "https://opensourceconnections.com/blog/2012/04/20/an-introduction-to-results-position-analysis/",
    "title": "An Introduction to Results Position Analysis",
    "content": [
      "In a previous post, we looked at the first two components of the success of search: the density of highlighted search terms and the placement within the search engine results page. However, there are times when the science of a search engine does not match the art of marketing.",
      "It is the role of a web manager and of marketing in general to understand the voice of the customer. They place themselves in the role of a customer and try to understand what a customer is trying to accomplish when performing a task. Additionally, the marketing and web management roles should have an understanding of the profitability of certain products versus other products and how generating sales in one versus the other will affect the bottom line of the company.",
      "Oftentimes, a marketer will have a list of what results he or she will want to see given a search. It is possible to evaluate how the SERP compares to the marketers ideal list using a similar approach to the Results Utility Positioning calculation that we discussed previously. Conceptually, this is similar to the judgment list we discussed when covering how to index Chinese in Solr.",
      "The Results Position Analysis (RPA) score is a factor of two attributes related to the SERP and the marketers judgment list. The first one has to do with actual matching. Does a result in the judgment list actually appear somewhere on the SERP? If so, this is positive. The second attribute has to do with the distance from the result on the SERP compared to the location in the judgment list. A result in the SERP which is tenth on the page doesnt have much value when the judgment list has it first, and, similarly, a result that is first on the SERP doesnt have much value if the judgment list has it showing up tenth.",
      "We normalize results to a 0 – 100 score and add it to the RUP score to come up with a cumulative score. The RPA and the RUP should, to some extent, inform each other. Perhaps the marketer has an unrealistic ideal of what should be coming up. If someone searches for red shoes, blue elephants will never sell. On the other hand, if the marketers judgment list is fairly reasonable, a low RUP score should indicate failures of content or in the search algorithm to cause the quality of the SERP to match the judgment list.",
      "Combined, RUP and RPA can be a very powerful measurement tool to improve the profitability of search within an e-commerce website. Site search is often ignored, particularly because there arent many known metrics to evaluate both the qualitative (RPA) and quantitative (RPA) qualities of the search engine results page.",
      "If you need metrics on the viability of your search engine results pages, please contact us or e-mail us at talktous at opensourceconnections dot com."
    ],
    "summary_t": ""
  },
  {
    "id": "c5363758a31d1c046abee339fba28206",
    "url_s": "https://opensourceconnections.com/blog/2012/06/01/lucenerevolution-2012-recap/",
    "title": "LuceneRevolution 2012 Recap",
    "content": [
      "I went to LuceneRevolution to test out my assertion that",
      "Search is the dominant metaphor for working with Big Data",
      "and based on the conversations that I had, that assertion holds water.",
      "As Grant Ingersoll pointed out in his keynote, the basic plumbing required for Big Data: storage, distributed processing, cheap price tag, have been met. What we are missing is the actual ability to make decisions based on the information contained in our Big Data sets. We are still caught up in the navel-gazing activity of \"how much raw data have I collected\", and arent focusing on \"Should I make decision X or Y based on my data.\" There is a huge gap between those who write MapReduce jobs, and those who need access to the results of those jobs. Processed results arent enough, and we shouldnt need to file the equivalent of a FOIA request with our IT department to gain access to the raw data. Search-based applications, also known as Search, Discovery, and Analytics (SDA) fill the gap between the developers and data scientists working with the raw data and the business users attempting to make data-driven discussions.",
      "Search engines were the original \"Big Data\" ten years ago. Then the rise of Google led to the search market bifurcating into efforts related to internal Enterprise search, and e-commerce search. The importance of Search seemed to dwindle, witness the declining attendance count at conferences like Enterprise Search Summit. But with the accelerating growth in data, aka \"Big Data,\" search in the last few years has moved from a basic input box to the feature that can make or break your application.",
      "Other thoughts:",
      "Met a number of ex-Endeca folks. Im hoping that the Lucene community takes advantage of these people who’ve done cool things with other search engines like Endeca, and bring some of their great ideas into Lucene and Solr. New blood is good.\n  This continues to be the \"Year of Big Data\". Im looking forward to tighter integration between the search and the big data communities.\nLots of folks are building custom QueryParsers to solve specific problems. Be interesting to see how much of this becomes generalized and open sourced.\n  Microsoft seems to have dropped their knee-jerk reaction against Java, and is working to make it easy to run the Big Data ecosystem of projects on their cloud platform Azure.\n  People are anxious to use Lucene 4. A strength of the Solr open source project is the incredible level of unit testing that is there. Go ahead and use it! If your IT manager doesn’t like to use unreleased code, tell’em to come <a href=\"mailto:[email protected]\">talk to me</a>!\n  ElasticSearch continues to have some great mind share, but suffers from the much smaller committer community of 1! The competition is keeping Solr honest.\n  Mark Miller gave a big pitch for the RandomizedTesting which is an extraction of Solr/Lucenes awesome unit testing framework into something generic. Anything that makes testing complex systems simpler is good.",
      "It was a great conference, very thought provoking, great people and conversations. LuceneRevolution 2012 continues to set the bar for hard core technical conferences. Attendance is a no-brainer if you are working with Lucene!"
    ],
    "summary_t": ""
  },
  {
    "id": "303d8aa72dacc6d327c2872adea6a9bb",
    "url_s": "https://opensourceconnections.com/blog/2012/06/06/indexing-big-data-on-amazon-aws-screencast/",
    "title": "Indexing Big Data on Amazon AWS: The Screencast",
    "content": [
      "My colleague Scott Stults talks about our experiences in using Amazon EC2 to perform distributed indexing into Solr as well as reprocessing a million TIFF images. At one point we had over 350 EC2 Large instances running!",
      "Enjoy!",
      "Indexing Big Data on Amazon AWS from Lucene Revolution on Vimeo."
    ],
    "summary_t": ""
  },
  {
    "id": "5a81d30f9d94d94e97d9c287ed6f0f83",
    "url_s": "https://opensourceconnections.com/blog/2012/07/05/interning-and-solr/",
    "title": "Interning and Solr",
    "content": [
      "Welcome to the first blog post (of many, hopefully) by OpenSource Connections’ 2012 summer interns. After our first few weeks as interns at OSC, we have begun to get the hang of search in general and Solr in particular. I am a rising fourth year at UVa majoring in Computer Science and Math with a Minor in East Asian studies. I have experience as a Teaching and Research Assistant with the University but this is my first internship with a software company. It is my hope that this summer I will get plenty of experience with cloud computing and big data.",
      "When I tell people that I am working for a company that specializes in search, the usual response is, \"Oh, like Google?\" Until last week I could only half-heartedly agree and mention it was called Solr, mention that it was used by Netflix, Zappos, Reddit, and others. Now I can confidently say that Solr is a whole lot more than just a black box that is \"kind of like Google.\"",
      "So what do you tell someone who wants to understand what it means when a person says he works on search? Like most kinds of software development, search can be broken up into back end and front end/functionality and feel. Many people think that search is all about the perfect algorithm, factoring in as many variables, like page views and links, to find your perfect result. But creating a good interface to help someone use your search in the way you intend is just as important.",
      "Since Solr is a well-established search engine, the algorithms have already been established and developed by the open source community. For most, the back end of a Solr search page consists of adjusting pre-established parameters to match with existing data. There is no struggling to invent a new algorithm; it has been done and it works well. There is a lot more thinking about data. How can data be organized efficiently and logically? Can your existing data be transmitted easily into Solr?",
      "Though Solr is written in Java, there is no need to write in it. A web developer who has never programmed in Java need not worry. All the configuration for Solr can be managed through a series of xml files. Modify these, start the Solr server, import the data, and search away!",
      "Probably the most important piece of the front end is the search engine results page or SERP. By default, the response to a query to Solr is an xml response document – definitely not the most pleasing thing to look at, but it contains all of your desired information. There are apis to return json, python, php and others that usual web developers should be comfortable with.",
      "For our first project, http://vacode.org/ (and the State Decoded project), we have chosen to use AjaxSolr which uses the little loophole of jsonp. It has a nice base of widgets and abstract types to build a beautiful SERP. Additionally, there are nice examples of the important parts of the SERP: easy pagination, visible tag clouds and filters, and of course the list of results. For getting things done quickly, it is definitely a great tool for creating the front end of your dreams.",
      "When we started, the search engine in use was Sphinx, and the search page was very bare bones.",
      "",
      "Throughout the process we learned how to stand up our own Solr and modify the configuration to import data from a database as well as some interesting web development. We added plenty of cool features: filtering by title and section, tag cloud, and page suggestions. We also added the ability to search and filter through different kinds of documents that were being stored (e.g., court decisions, comments, etc.). Hopefully you will see a version of our search go into production soon, but in the meantime, here is a sneak peek!",
      "",
      "It seems many web developers are daunted by deploying and maintaining their own search engine, but I would encourage them to give Solr a try and listen in as the OSC intern team learns the ropes.",
      "Until next time,",
      "Joseph"
    ],
    "summary_t": ""
  },
  {
    "id": "36e65ab39fe652d3f66799c5a2faecff",
    "url_s": "https://opensourceconnections.com/blog/2012/08/14/codefloat-2012-by-the-numbers/",
    "title": "CodeFloat 2012 \"by the numbers\"",
    "content": [
      "",
      "For the past year Ive been talking with Christopher Ball and Erik Hatcher about how frustrating it is to have to carve time out of our regular day jobs to work on Solr and Lucene. Thanks to their enthusiasm for the idea of spending a couple of days hacking on Solr and Lucene, last weekend OSC hosted folks who spent two days of writing code and learning from each other. And since hacking all day Friday and Saturday burns you out, some of us went tubing on Sunday!",
      "\"by the numbers\" what happened:",
      "Attendees: 12, Eric, Scott, Matt, John, Kasey, David, Joesph, Erik, Anthony, Jessica, Christopher, Jake",
      "Visitors who stopped in for some search chat: 2",
      "JIRAs Closed: 4 – SOLR-358, SOLR-3051, SOLR-1486, SOLR-3292",
      "Commits Made: 3 – SOLR-1280, LUCENE-4265, LUCENE-4266, SOLR-3648",
      "Wiki Pages Updated: 2",
      "Motto for the weekend: \"Its literally going to take at most an hour. Make that 18 minutes.\" – David Dodge",
      "We had Show and Tell each day, and here is what we saw:",
      "A \"single page\" search app using EmberJS from Matt Overstreet that demonstrated the awesome data binding aspects of EmberJS. Instant Search results were as simple as binding the results pane to the query box and issuing queries to Solr.\n  Erik Hatcher put together a demo of the ScriptUpdateProcessor that lets you use a scripting language (JavaScript, Ruby, Python) to manipulate incoming documents. The ability to rapidly prototype ideas through this tool is exciting. More information at http://wiki.apache.org/solr/ScriptUpdateProcessor.\n  Anthony Burton walked us through some challenges he was having in using XPath with DataImportHandler to index xml documents. We all debugged the issues as a group, with David Dodge having the key insight into the pattern of XPath that was required to make it all work. Lots of discussion on the pros and cons of DIH!\n  I attempted to demo using Apache UIMA with Solr. Jessica Bonnie and I hacked on the example app documented in the wiki at http://wiki.apache.org/solr/SolrUIMA, and at the end of it I had the WhitespaceTokenizer working through the UIMA framework. However, no luck in getting the more interesting AlchemyAPI and OpenCalais integrations to work.\n  John used node.js to index the public data from StackOverflow directly into Solr via the JSON updater in Solr.",
      "And then we floated!",
      ""
    ],
    "summary_t": "For the past year Ive been talking with Christopher Ball and Erik Hatcher about how frustrating it is to have to carve time out of our regular day jobs to wo..."
  },
  {
    "id": "fb6c81f2668a3aeac6c92e717ac1415b",
    "url_s": "https://opensourceconnections.com/blog/2012/08/28/fresh-shiny-moo-business-cards/",
    "title": "Fresh shiny Moo business cards!",
    "content": [
      "We love Moo.com for business cards.  While expensive per card, they let you order in small batches, as few as 50 cards, which lets you experiment with job titles and designs to see what works best.  We get the Classic card stock, which is a nice heavy paper that is glossy and feels expensive to the touch.  Ours come with the always hip rounded corners.  Folks are constantly commenting on how nice they are.",
      "About the only thing I wish was that it was easier to write notes on the cards, they tend to \"shed\" the ink.",
      "Look closely at the cards below and you can see we each came up with the job title that most describes what we do (or aspire to!)",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "f514d41d89863d4ec69a855bef6b6401",
    "url_s": "https://opensourceconnections.com/blog/2012/09/21/our-own-matt-overstreet-introducing-aneesh-chopra-at-rva-startup-weekend/",
    "title": "Our own Matt Overstreet introducing Aneesh Chopra at RVA Startup Weekend!",
    "content": [
      "Matt Overstreet was the Master of Ceremonies for the Richmond Startup Weekend earlier this month. And the guest speaker was Aneesh Chopra, first CTO of the United States.",
      "",
      "Matt Overstreet introducing Aneesh Chopra",
      "Our friends at GroundWorkDesign, who build virtual experiences, recorded this."
    ],
    "summary_t": ""
  },
  {
    "id": "29e601d3a2d7b87b93cfe7c9eb90ad21",
    "url_s": "https://opensourceconnections.com/blog/2012/09/26/thinking-about-solr-4-book-update/",
    "title": "Thinking about Solr 4 Book Update",
    "content": [
      "Im just starting to think about updating the book Solr Enterprise Search Server that David Smiley and I co-authored for Solr 4 now that it is very close to being launched!",
      "David and I did a big rewrite of the book from Solr 1.4 to Solr 3.x, trying to make it a more coherent story. But for Solr 4, we are thinking of doing a simpler update. So what would you like to see added (or removed??). SolrCloud of course. The new NRT search features. The new join syntax. What about the Lucene codecs? Too low level? Maybe some of the examples should be pruned out to make the book more focused as well.",
      "Were thinking about bringing in another co-author to help with the writing as well, anyone want to see their name in lights? Writing a book, while at times thankless, is also the quickest way to build your personal brand!"
    ],
    "summary_t": ""
  },
  {
    "id": "5336f3090e305a2453fa6509806d753d",
    "url_s": "https://opensourceconnections.com/blog/2012/09/28/packt-celebrates-1000th-book-release-get-a-free-ebook/",
    "title": "Packt celebrates 1000th book release (get a free ebook!)",
    "content": [
      "Our publisher for Solr Enterprise Search Server, Packt Publishing, has just announced that they have published their 1000 book. Packt has been great at fostering new writers like David Smiley and myself, and their direct financial support of open source communities has been great. Weve funneled thousands of dollars to the Apache Software Foundation just on the Solr book royalties.",
      "To mark the occasion, they are opening their library to the public!",
      "Packt is certain that in its 1000 titles there is at least one book that everyone in IT will find useful right away, and are inviting anyone to choose and download any one of its eBooks for free over its celebration weekend of 28-30th Sep 2012. Packt is also opening its online library for a week for free to give customers an easy to way to research their choice of free eBook.",
      "To access the library follow the link http://bit.ly/RXnAMc."
    ],
    "summary_t": ""
  },
  {
    "id": "af0f5ea8970148d759eeb7c591ced1dd",
    "url_s": "https://opensourceconnections.com/blog/2007/02/16/ajaxscaffold-and-file-uploads/",
    "title": "AjaxScaffold and File Uploads",
    "content": [
      "Weve had a little Expenses Rails app for tracking charges and funneling them back to Accounting. One of the key bits is that we can upload a picture of the receipt to each expense. Its remarkable how good resolution camera phones have these days, and its a lot better then walking around with a pocketful of receipts while on a business trip!",
      "So we used the basic Rails scaffolding, but I am a big fan of Richard Whites Ajax Scaffold. And since it is now a Rails Plugin, I wanted to give it a try. I dropped it into the app, and everything worked fine. That is until I tried to upload my image of a receipt. Bam… I started thinking this was a weakness of Ajax Scaffold, but after a bit of digging around it turns out binary data cant be sent via AJAX calls! Ugh…",
      "Well, after playing around with various options for a while, i decided that I wasnt going to use something weird like a Flash based uploader or what not. Instead, when adding a new expense, I do a regular post via form_tag that takes you back to the list page, and when editing a receipt, where image changing isnt supported anyway, I use the ajax goodness via form_remote_tag.",
      "I can tell which method is being invoked on _new_edit.rhtml partial by doing: < % if params[:action] == 'edit' %> , and use the appropriate form tag. The only issue was that to keep the other Ajax calls working properly, the form_tag id had to use the full :id => element_form_id(@options) id. The only real downside is that when creating a new expense, if there is an error, you are popped out of the table view, into a page with just the new expense partial, but you can still save your data.",
      "At any rate, Ajax Scaffold as plugin has grown more mature then ever. I really appreciate how easy it is to do things like change sorting order, or add your own filters to the data. And the fact that all the raw Ajax Scaffold template stuff stays in /views/ajax_scaffold/, and allows you to override individual .rthtml templates by adding them to your /views/model/ directory is just great. It also become a lot cleaner and easier to understand the internal workings as well. I am very much looking forward to Ajax Scaffold 4.0!"
    ],
    "summary_t": ""
  },
  {
    "id": "f0e3afcf13724160ae2aa19aac1413c6",
    "url_s": "https://opensourceconnections.com/blog/2012/10/01/switching-jobs-damn-the-torpedoes/",
    "title": "Switching Jobs – Damn The Torpedoes",
    "content": [
      "Hi! Im Doug Turnbull, the new guy. Im really excited to be joining Open Source Connections. Ive been seeing first hand how capable my new colleagues are and am really excited by many of the things theyve been cooking up.",
      "Technologically, Im going to be learning A LOT in the next several months. I have a ton to absorb on Solr, Lucene, Hadoop, Ruby, and Rails among other things. My goal is to share my thoughts while I come up to speed and absorb all wisdom of my colleagues. Share the mistakes and lessons I learn. Share the random thoughts and ideas that seem interesting.",
      "Before I start that journey though, I thought Id share something about my background and my decision to join OSC. I need to prove I bring my own geek cred to the table, right?",
      "Well, since I was a kid, Ive been mesmerized by computers. Back then it was all AppleBasic on my Apple IIe and later QBasic/TurboC/Turbo Pascal in DOS. Even when it was just console IO and using extended ASCII to make little dungeons to play in, I was enthralled by what I could do. I still like seeing that bit of text come out on the console, getting my little power thrill from the simple act of saying \"Hello World\".",
      "Professionally, Ive had a variety of experiences. Most recently as a C++ and python dev doing network analysis for Digital Receiver Technology on their software-defined radio platform. That job had a lot of requirements for maximizing processing throughput. I spent a bunch of my time focussed on performance problems, writing a little tool call VisPerf to help me analyze and improve the performance of C++ applications. I learned a lot of what to do and what not to do to make C++ code fast. I fine tuned data-structures to make them more cache friendly. I sped up a lot of slow string formatting code. I reduced the number of times memory was allocated from the heap in the processing pipeline. Lots of deep knowledge in what is slow and what is fast in C++, specifically in code compiled by Microsoft Visual C++ 9.0 SP1.",
      "At DRT Ive also been a test-driven development evangelist. Theres something cool about untangling a giant mess of legacy code and taming a meaty part with a suite of tests. Theres also something cool about finding bugs, writing the test to recreate the bug, and proving youve fixed the bug – now and forever. The coolest part though is how TDD impacts the culture. Sitting, pair programming with my buddy, writing the test before touching production code and asking ourselves, well what should this do? Its a pretty radical change in perspective, and I love radical changes in perspective. Hopefully I can carry my TDD experience forward wherever appropriate.",
      "More specific to Charlottesville, concurrently with my work at DRT, Ive spent a lot of time as the primary web developer for Charlottesvilles Outdoor Adventure Social Clubs members-only web app. In that role I did my best to be a full-stack LAMP developer (Linux, Apache, MySql, and Php). I was also the main everything from sys-admin to javascript developer to designer of new features for about seven years. I worked on linking up the clubs SmugMug content with the clubs site, and allowing members to associate Facebook accounts with club accounts and lots of other neat features.",
      "I chose to work for the club because I wanted something completely out of left field to work on. I love those kind of things. Painful at first, but in the end Im better for it. Ive found a more efficient way to do something, and for me faster ways to implement something cool is always a plus. The larger the library I can pull from the quicker I can go from idea to cool thing on a computer.",
      "Which gets to why Im starting up at OSC.",
      "At DRT Ive become a pretty big domain expert. See, for you youngins who might not realize this, a bad thing can happen if you consistently do a really good job at a project. You become the go-to guy/gal for that project. Emergency bug fixes, meetings, customer consultations, it feels natural for others to seek you out on your project. For a long time (months, years, decades even) this is professionally fulfilling. It feels good to keep learning about your project and all things related to it. The project becomes your baby. You get praise for your work on it. You get paid more cause your the \"guy\"/\"gal\" for that project.",
      "Eventually though, after many fulfilling months/years/decades, you get to a point where the technological work around the project gets repetitive. You find yourself answering the same questions again and again despite your best efforts at education and cross-training. You solve very similar problems over and over. You have a few people who might know something, or that youve tried to bring up to speed, but invariably the company only trusts you to do the heavy lifting. You ask to try something different, but invariable you cant shake your original project. Starting on your other project, you still get the same calls, go to the same meetings, and find yourself still preoccupied with things related to your original project.",
      "Many software engineers find themselves in this kind of conundrum. Some employers are helpful, but sadly, many are not. So what’s an engineer to do? Well you pretty much have three career options from this point.",
      "The first option is you can become a highly paid expert (read consultant) on your project. As long as its a thing, you can rake in the dough from your current employer or whomever. Maybe its a little boring and repetitive. Maybe theres not much else to learn, but it pays the bills and leaves your bank account in a pretty healthy state. That is as long as that project is a going concern, which can be a big gamble. It could become obsolete or unimportant by something else. You might end up regretting steering your career in a dead end. Or you might end up Daddy Warbucks laughing at us from on top of a giant mountain of consultant cash.",
      "The second option is to pivot. Make a radical change to some part of your experience, but keep another part the same. For me, maybe that would be continuing with the same technology as my current project, but not in C++/Python. Or stick with C++/Python, but with a different problem domain. A rational, halfway solution, this method keeps you in a reasonable compensation situation, allowing you to emphasize one piece of your expertise while not emphasizing another. A good option, in a way hedging your bets. Sticking with one skillset you know and feel secure in while taking on another that may be a bit risky.",
      "Then theres the third option. The crazy (but fun) option. I call this option \"Damn the torpedoes!\" Try something radically different and don’t worry about the consequences. Something pretty much the opposite of everything youve done up to that point. In this scenario you’re gambling more on your intangibles than on whether any specific technology or skill is going to last into the future. You’re saying that you have the attitude and ability to transcend specific skills.",
      "Selling an employer on the 3rd option after gaining some seniority isn’t easy. Its easy to measure specific pieces of knowledge. It’s harder to measure intangibles like attitude and ability to grow. You say I promise I enjoy learning and breaking out of my comfort zone – but how do you really know? I promise I’m not going to gravitate back to my comfort zone and open every can with a knife cause I can’t be bothered to learn about can-openers. Are you certain?",
      "Well, Ive pretty much made the decision to figure out a way DAMN THE TORPEDOS! If I can become the prime contributor at the Outdoor club starting from nothing and having enjoyed the experience, whats stopping me! The time is ripe for change, for new ideas. For identifying and destroying my comfort zone. I want to find better, newer ways to do things. Radical ways to do things! I want to be evaluated on my intangibles, not whether my brain holds the specific facts of what a pointer is and what it means for it to be NULL and what it means for it to be dangling. The latter is trivia.",
      "Lets do this! May the neural pathways built in by past experiences be torn down in a terrifying maelstrom so that they can be rebuilt in new but temporary glory!",
      "Of course, as I said, the 3rd option is always easier said than done. Luckily I’ve found OSC which understands the kinds of intangibles that really counts.  I’m happy to be given the chance. Can I prove to them that I have them? Can I become a pro at this stuff? Can I then become good enough so that I can teach you through this blog?",
      "Well as I write about things big and small, you (and they) can be the judge as to whether or not I’ve succeeded at any of that. Happy trails DRT, its been a good and very educational run, but now its time to damn the torpedoes, charge straight in, and see how much I can turn my mind inside out in the process!",
      "Wish me luck!"
    ],
    "summary_t": ""
  },
  {
    "id": "984a9a36d3be921c637255761798d2e8",
    "url_s": "https://opensourceconnections.com/blog/2012/10/04/big-search-for-big-data-the-screencast/",
    "title": "Big Search for Big Data: The Screencast",
    "content": [
      "Better late then never, here is the screencast of my presentation Big Search for Big Data that I gave at LuceneRevolution. Its a story of the mistakes and victories we had in a major search project we did for a Federal customer.",
      "Big Search with Big Data Principles from Lucene Revolution on Vimeo."
    ],
    "summary_t": ""
  },
  {
    "id": "aafb9c5a774e35fb5c2b8a5d9787aa95",
    "url_s": "https://opensourceconnections.com/blog/2012/10/06/ubuntu-12-04-guest-performance-in-full-screen/",
    "title": "Making Ubuntu 12.04 not crawl in Virtualbox",
    "content": [
      "So Ive got a new hooptie laptop (a pimped out Sager NP9170). Since Im pretty much changing everything about my career, I thought Id stick to one thing staying the same – Ill stay with a Windows PC (specifically Win 7 x64 sp1). Traditionally when I need to do Linux dev in this kind of environment, Ive loaded up Ubuntu in VirtualBox. It typically works really well. The number of glitches I have to work through tend to be really few compared to a direct Linux install. Mostly cause Im not trying to force Linux to run on a bunch of hardware its probably missing support for. I have all the power of Linux and all the device support of Windows, woohoo! And I can easily do Windows dev at the same time, double woohoo! Anywho, a few things have changed since the last time I got this setup working:",
      "Ubuntus fancy Unity interface. Pretty heavy 3D desktop environment that tends to lurch unless you have the right hardware and right Virtualbox settings\n  Nvidia Optimus. Optimus is a video card power saving technology that Ill describe below The intersection of these two new things caused my Ubuntu guest to lurch pretty badly after the initial install. The first thing that occured to me was to give the guest enough RAM, processors, and video RAM with 3D acceleration to quench its appetite. Luckily Ive got quite a bit installed on this new laptop, so the host has plenty to be generous with. In my configuration, I made the following change.",
      "Allocated 2 logical cores to the Ubuntu guest\n  Allocated 4 GB of RAM to the Ubuntu guest\n  Maxed out guest’s video memory to 128 MB\n  Enabled 3D acceleration That helped a little, but it still wasnt quite enough. I also did an update to Ubuntu to bring in all the updates since the 12.04 ISO was released. This plus installing the",
      "guest additions got things working pretty well in windowed mode. (It turns out 3D acceleration for the guest is meaningless without the guest additions). Unfortunately, Unity wasnt happy when I went into full-screen mode. It was terribly slow for some reason. I tried a lot of things to figure out what was going on. I updated my Nvidia drivers. I installed a fresh Ubuntu virtual machine and played with the settings. I attempted to search for people with similar problems. My google-fu failed me. None of these things helped improve the performance of my VirtualBox. Eventually, I stumbled on something suspicious.",
      "",
      "On a whim I wanted to see if anything was awry in the host’s video card settings. Right-clicking on the desktop and selecting \"screen resolution\" and then \"Advanced Settings\", I noticed right away that the the graphics adapter was listed as \"Intel HD Graphics 4000″. Not the beefy Nvidia 670M I have installed. I thought a return of my laptop to Sager was definitely going to happen. As far as I could tell, for some reason the Nvidia that had come with my Sager was not operating. I attempted to delete the Nvidia from device manager and rebooted, hoping it would \"reinstall\" correctly. I attempted to delete the integrated graphics card, which was pretty stupid. Thankfully Windows booted in CPU driven graphics mode and saved my butt.",
      "After some research, I was lucky enough to stumble on this SuperUser question with a screenshot that looked eerily familiar. What I learned was that Nvidias souped up laptop graphics card had something called \"Optimus\" built in. Optimus auto-selects between the low-power integrated, Intel graphics card and the fancy-schmancy super accelerated but power-hogging Nvidia card.",
      "I followed this",
      "video to tweak Optimuss settings on a per-application basis. The trick is to set the graphics card to be used for VirtualBox. You can also set what graphics processor to use in the Nvidia control panel under \"3D Settings\" -> \"Manage 3D settings\". Select the \"Program Settings\" tab andn hit the \"Add\" button to add the VirtualBox executable. Then under bullet 2, select the High-performance NVIDIA processor for VirtualBox.  Once I did this, I now get great performance with VirtualBox in full screen. To sum up, to get ideal guest OS performance I followed all these steps: Via VirtualBox Manager:",
      "Allocated 2 logical cores to the Ubuntu guest\n  Allocated 4 GB of RAM to the Ubuntu guest\n  Maxed out video memory to 128 MB\n  Enabled 3D acceleration In Ubuntu Guest:\n  Updated all guest software via Ubuntu S/W Center\n  \n    Installed Guest Additions on ISO provided with VirtualBox On Host:\n  \n  Installed NVIDIA drivers on Host\n  Via NVIDIA Control Panel set VirtualBoxs graphic processor to NVIDIA instead of the integrated graphics Voila! You have the latest Ubuntu Unity hawtness working flawlessly as a guest in Windows!"
    ],
    "summary_t": ""
  },
  {
    "id": "78c95deb42ac3888e62675dc0dc48b19",
    "url_s": "https://opensourceconnections.com/blog/2012/10/23/real-time-doctest-checking-in-vim/",
    "title": "Real-time Doctest Checking In Vim",
    "content": [
      "Vim is a terrific editor.",
      "The one thing that kills me though is \"the cycle\"–the cycle of going from Vim to the console to try out what I just wrote, then back to Vim back to fix what i just wrote, then back to the console to try again. Most annoyingly this cycle is usually for little things that many modern IDEs can catch in real-time like every programming languages version of the proverbial forgotten semicolon. In Python, for example, I often find myself forgetting colons after blocks, or somehow screwing up indentation.",
      "Anything I can do to catch the stupid stuff early and avoid wasting precious minutes in fix-try mode is invaluable. This is where the Vim plugin Syntastic really shines. Syntastic runs any kind of lint or compiler against your code when vim writes the file. It then gives you a nice bright arrow on the left saying HEY STUPID. Which is great. You see instantly where you goofed.",
      "",
      "For Python, the lint/error checking tool that syntastic uses by default is Flake8. Flake8 combines error and Python style checking (against PEP8) into one tool. It tells you about the errors you definitely need to fix (missing colons and what-not) but also nags you about keeping your code consistently styled based on PEP8. Together, Flake8, Syntastic, and Vim give you a pretty amazing set of tools for doing rapid Python development. My mistakes are caught instantly and fixed right away and I spend less time in the fix-try cycle between Vim and the console.",
      "Theres one ingredient though that seems to be screaming to be added to this killer combo – integration of Python doctests. If youre unfamiliar, doctests provide a very low barrier to entry method of developing basic tests around a Python class or function. In Python its pretty typical to develop a class and head to the interactive shell to try out your new creation. By playing around with the most obvious cases in the Python shell, you can see very quickly whether or not your code basically works. Doctests capture your experience in the shell by building some basic tests as usage examples in the class or functions docstring:",
      "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n      \n\n      \n        class Adder(object):\n    \"\"\"\n    Doctests examples, showing usage as if \n    Adder were used in the console:\n    >>> a=Adder(2,3)\n    >>> a.add()\n    5\n    >>> b=Adder(3,3)\n    >>> b.add()\n    5\n    \"\"\"\n\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def add(self):\n        print self.x + self.y",
      "In the example above, theres 2 doctests for the Adder class. Everything after »> is treated as if its entered into the Python shell. Resulting output (returned or in this case printed) in the Python shellwould get output on the next line. The first doctest (adding 2 and 3) will pass. The latter, however, should fail (3+3 != 6).",
      "Doctests fit nicely with rapid prototyping and experimentation that Python is often ideal for. They cant and shouldnt replace hundreds of detailed unittests, but they are great when you want to get something working reasonably well fast.",
      "What would make them even more powerful is if we could integrate them with the Vim/Syntastic/Flake8 combo. Ideally we could seamlessly integrate doctest failures with the other Syntastic errors so test failures / syntax errors / style issues get all treated equally in the Syntastic/Vim interface. In fact thats exactly what Ive done! In my experimental Flake8 fork Ive added a fairly rudimentary first iteration prototype of running doctests and showing their failures by outputing them in Flake8:",
      "",
      "Ive done this through using Pythons doctest module. Right now the code is fairly preliminary, using the doctest unittest API to run tests and then parse the output with some regexs. If youre curious how to work with this API, you can check out this file which has the meat of the doctest running for a specified path. As this code depends on parsing output, Im not certain how well it will work outside of the Python 2.7 line that Im using. If I have time and this continues to seem useful, Ill be switching to the advanced doctest API  which should be more portable.",
      "Ive installed this first iteration locally as a fun thing to try out. Of course it seems like an awesome idea, but Im curious to see how well it works in practice. Is the seamless integration with other Syntastic errors appropriate? Or do we need a more test-specific Vim plugin? Do testing errors need to be more verbose? Or should one really switch to a more debug-centric outside of an editor mode to get the deep information about a test failure? Would this kind of approach be appropriate for more in-depth traditional unit test frameworks? Will the tests take such a long time to run that vims write performance will plummet?",
      "If youre curious, I encourage you to install Syntastic then try out my Flake8 fork (install via distutils) and let me know what you think (and more importantly what bugs you encounter). You may find you prefer it over the vanilla version if you use a lot of doctests. You may find Im nuts. Either way Id love to hear from you on what you think."
    ],
    "summary_t": ""
  },
  {
    "id": "b844cedcbae60f346a9c1468bf1a1885",
    "url_s": "https://opensourceconnections.com/blog/2012/11/05/backing-up-virtual-machines-in-virtualbox/",
    "title": "Backing up Virtual Machines in VirtualBox",
    "content": [
      "And now to the fun subject of virtual machine images!",
      "My computer blue screened the other day. So I got fairly paranoid. This is the second time in a month this has happened. I needed to make sure my backup strategy was solid. The most important thing I want to backup is my Ubuntu VirtualBox image. Here at work we use CrashPlan which seems to do a decent enough job automatically backing up the directories I tell it to. However, the dumb me was just pointing CrashPlan at the directory with my virtual machine images and hoping for the best. This is, of course, problematic as CrashPlan is most likely running its backup while Im actively working in my virtual machine. How can I have any guarantee that that image is in any kind of consistent state?",
      "Backing up with a versioning system",
      "I tried all kinds of hair-brained schemes to backup my virtualbox image in something resembling automatic. I really wanted some kind of diff-based approach to backing up the large file. One thought I had was that before launching the virtual machine in a batch file or script Id commit the vdi it to a version control system.  Then Id point CrashPlan at the associated repository which would be backed up in the background. I dont reccomend this. Git turns out to have a sensible maximum file limit, which was a pretty big \"hey idiot… what are you doing\" warning. I found and tried Boar, a versioning system which attempts to work well for binary files. Unfortunately even its diffing ability in this situation was lacking. After 2 commits of a 20 GB .vdi file, the repository had grown to 40 GB+. This solution wasnt very space efficient. Moreover committing such a large file is tediously slow. Everytime I launched my virtual machine Id have to wait for this boring 5-10 minute process.",
      "Backing up using VirtualBox snapshots",
      "The canonical solution turns out to involve a VirtualBox feature known as snapshots which I had before now known nothing about.",
      "From a users perspective, a snapshot is a restore point. If I create a snapshot, I can go back to that point in time. The best part is I can take a snapshot of a system even while its running. In the simplest use-case you have a linear progression in time of various snapshots. You can restore your virtual machine to any snapshot in the history. You can also do crazy things like go back to a snapshot and create a branch from that snapshot – taking your virtual machine in multiple experimental directions from a single restore point.",
      "How snapshots actually work makes it extremely powerful for backups. A snapshot turns out to be the diffing system I was looking for. When you take a snapshot of a virtual machine, in the default \"normal\" mode, the associated parent (either the virtual machine image or another snapshot) is frozen and no longer written to. Instead, all writes go into the file associated with new snapshot. This file is in essence a kind-of commit log against the underlying virtual file system of everything that has happened after the snapshot in time. Its a diff of stuff thats changed since the snapshot took place. Restoring to the snapshot point is as simple as throwing away the snapshot-file – the commit log – and unfreezing the snapshots ancestor.",
      "Deleting a snapshot is not removing all the changes in that commit log. Instead its instead folding that snapshot into its ancestor (back to the vdi or another diff file). Its actually committing the diff.",
      "Which leads me to understanding why this can work as a backup strategy.",
      "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n      \n\n      \n        #!/bin/bash\n\nVBOXMANAGE=\"/usr/bin/VBoxManage -q\"\n\nif [ $# != 1 ]\nthen\n    echo \"Usage: $0 VBoxName\"\n    exit\nfi\n \necho \"Renaming old snapshot...\"\n$VBOXMANAGE snapshot \"$1\" edit previous --name deleteme\necho \"Renaming current snapshot...\"\n$VBOXMANAGE snapshot \"$1\" edit current --name previous\necho \"Taking new snapshot...\"\n$VBOXMANAGE snapshot \"$1\" take current\necho \"Deleting old snapshot...\"\n$VBOXMANAGE snapshot \"$1\" delete deleteme",
      "You can backup a running VirtualBox virtual machine by maintaining a cascade of snapshots. One snapshot, knows as \"current\"  is the most recent snapshot. Restoring to it restores to the last backup. The diff file associated with it (holding all the stuff that has happened AFTER the snapshot) reflects all the non-backed up changes and is the where VirtualBox is actively keeping the guest OSs writes. This diff is a kind of \"commit log\" of all the changes that are going to the virtual disk. The \"previous\" snapshot is the restore point before current. In this script, previouss commit log is the old current. The commit log/diff associated with \"previous\" reflects the changes between the previous/current snapshots. Finally this script also has a \"deleteMe\" – the old previous. On every run, deleteMe, is folded back into the main vdi file by telling VBoxManage to delete the snapshot (deleting a snapshot doesnt remove the associated data, it just folds in the data and forgets the restore point).",
      "This strategy lets us keep 2 restore points (in case an accidental backup backs up an unstable image). Its a great strategy, but….",
      "Snapshots – Tread Carefully",
      "Sadly, for me personally, live VirtualBox snapshots havent been a terribly robust backup strategy. Ive unfortunately seen several snapshots fail. Or, worse, had VirtualBox crash while a snapshot was taking place. Luckily I havent lost a lot of data, as Ive been diligent about pushing my code to github. When a snapshot has failed, Ive had to edit my virtual machines vbox.xml file. A file that clearly states \"DO NOT EDIT\" at the top. Its easy to fall into the lull of thinking that this seems like something that should either succeed or fail atomically like comiting to a versioning system. It hasnt been my experience that this is the case.",
      "Heres a gallery of horrors of some of the errors Ive seen. First theres the \"A differencing image of snapshot could not be found\" where somehow a snapshot image file gets lost",
      "",
      "Ive also encountered this error – \"Hard Disk XXX cannot be directly attached to the virtual machine because it has 1 differencing child hard disks\". Ive had errors taking snapshots, including having the snapshot process hang with a live VM. Sadly I cant say I trust live snapshots right now.",
      "Ive reverted to a simpler, non-live backup strategy that only takes snapshots immediately before starting up my virtual machine and wont take a snapshot while VirtualBox.exe is running (asking you to close VirtualBox before continuing). This is a combination of the script above reworked into Python on Windows and this Python ActiveState recipe. Ive replaced the VirtualBox icon pinned to the taskbar with a batch file that runs my script, and use the default VirtualBox icon on the task bar.",
      "I also only ever have one snapshot currently running – \"current\". Before launching VirtualBox, current gets compacted into the main vdi image via a snapshot delete. I then take a new \"current\" snapshot which VirtualBox uses. For me, this seems to be the best solution thus far. I have one differencing image active at one time. The main vdi gets updated right before the VM launches. Therefore when crashplan backs up that folder, it should be backing up a stable vdi thats not constantly changing and getting unstable. This seems to be the best solution for me for maintaining a trustworthy backup strategy without weird errors that crash my live VM.",
      "Anyway, Id definitely be curious to here about your experiences backing up VMs!"
    ],
    "summary_t": ""
  },
  {
    "id": "6fddcac670378bafe4003cab7aafc58e",
    "url_s": "https://opensourceconnections.com/blog/2013/01/11/code-quality-is-a-dial-too/",
    "title": "Code quality is a dial too",
    "content": [
      "This article is a preview of my January 22nd talk at Charlottesvilles Neon Guild where Ill be talking about my experiences with rapid prototyping. If youre local, Id love to see you at the meeting and hear your thoughts!",
      "",
      "Theres a time to stop and be finicky about code. A time to chose the best idioms and design patterns; think deeply about how classes relate to one-another. This is the code we know we want to keep. Its tied to a proven product and we need to act with care.",
      "Theres also a time when code quality doesnt matter. This happens when we prototype. When we explore the problem space, its likely that large parts of the initial vision will be thrown out or reworked. Its not worth perfecting code that will get discarded or replaced.",
      "Its important to keep this difference in mind because often we Software Engineers fill the role of holy defenders of software quality. In our day jobs, we work on mature products – pieces of software that need to be carefully tended like a garden. Theres a general mild defensiveness(paranoia?) against changes that may hurt software quality. Theres always a \"them\" that want to get something big/crazy done within a crazy aggressive schedule that may threaten our carefully tended garden.",
      "While this attitude helps us in our day job, it doesnt help us when slinging around new ideas.",
      "Instead, the challenge is to work directly against our nature. We need to write dumb, naive code – the dumbest most naive and horribly basic code that shows off our idea. Let it crash and perform poorly and flounder. Ignore those exceptions that are certainly going to happen in certain cases. That only gives you an excuse to remind everyone that what youre doing is a prototype and not a product.",
      "Because, without a quick, direct path to feedback you might be taking a long road to nowhere. Remember the goal is to fail early and often. Your goal isnt to build a robust application, its to build the simplest end-to-end expression of your idea that you can. You want to show what youre thinking at the earliest opportunity to get feedback. Finding the deficiencies in your grand idea with your time is far more crucial to your success than perfecting the implementation.",
      "So dont dig holes that youll only have to fill in again. Spend some time and reflect on where you are keeping your code quality dial. Are you working on proven, carefully tended code or are you trying to test new ideas? Or maybe youre somewhere in between? Wherever you are tune your code-ometer accordingly, being careful to invest no more and no less than you need to in the code youre writing."
    ],
    "summary_t": ""
  },
  {
    "id": "2ec546da027b92df94836aa6256df275",
    "url_s": "https://opensourceconnections.com/blog/2013/01/17/escaping-solr-query-characters-in-python/",
    "title": "Escaping Solr Query Characters in Python",
    "content": [
      "I’ve been working in some Python Solr client code. One area where bugs have cropped up is in query terms that need to be escaped before passing to Solr. For example, how do you send Solr an argument term with a \":\" in it? Or a \"(\"?",
      "It turns out that generally you just put a \\ in front of the character you want to escape. So to search for \":\" in the \"funnychars\" field, you would send q=funnychars:\\:.",
      "Php programmer Mats Lindh has solved this pretty well, using str_replace. str_replace is a convenient, general-purpose string replacement function that lets you do batch string replacement. For example you can do:",
      "$matches = array(\"Mary\", \"lamb\", \"fleece\");\n$replacements = array(\"Harry\", \"dog\", \"fur\");\nstr_replace($matches, $replacements,\n            \"Mary had a little lamb, its fleece was white as snow\");",
      "Python doesn’t quite have str_replace. There is translate which does single character to single character batch replacement. That can’t be used for escaping because the destination values are strings(ie \":\"), not single characters. There’s a general purpose \"str_replace\" drop-in replacement at this Stackoverflow question:",
      "edits = [(\"Mary\", \"Harry\"), (\"lamb\", \"dog\"), (\"fleece\", \"fur\")] # etc.\nfor search, replace in edits:\n  s = s.replace(search, replace)",
      "You’ll notice that this algorithm requires multiple passes through the string for search/replacement. This is because that earlier search/replaces may impact later search/replaces. For example, what if edits was this:",
      "edits = [(\"Mary\", \"Harry\"), (\"Harry\", \"Larry\"), (\"Larry\", \"Barry\")]",
      "First our str_replace will replace Mary with Harry in pass 1, then Harry with Larry in pass 2, etc.",
      "It turns out that escaping characters is a narrower string replacement case that can be done more efficiently without too much complication. The only character that one needs to worry about impacting other rules is escaping the \\, as the other rules insert \\ characters, we wouldn’t want them double escaped.",
      "Aside from this caveat, all the escaping rules can be processed from a single pass through the string which my solution below does, performing a heck of a lot faster:",
      "# These rules all independent, order of\n# escaping doesn't matter\nescapeRules = {'+': r'\\+',\n               '-': r'\\-',\n               '&': r'\\&',\n               '|': r'\\|',\n               '!': r'\\!',\n               '(': r'\\(',\n               ')': r'\\)',\n               '{': r'\\{',\n               '}': r'\\}',\n               '[': r'\\[',\n               ']': r'\\]',\n               '^': r'\\^',\n               '~': r'\\~',\n               '*': r'\\*',\n               '?': r'\\?',\n               ':': r'\\:',\n               '\"': r'\\\"',\n               ';': r'\\;',\n               ' ': r'\\ '}\n\ndef escapedSeq(term):\n    \"\"\" Yield the next string based on the\n        next character (either this char\n        or escaped version \"\"\"\n    for char in term:\n        if char in escapeRules.keys():\n            yield escapeRules[char]\n        else:\n            yield char\n\ndef escapeSolrArg(term):\n    \"\"\" Apply escaping to the passed in query terms\n        escaping special characters like : , etc\"\"\"\n    term = term.replace('\\\\', r'\\\\')   # escape \\ first\n    return \"\".join([nextStr for nextStr in escapedSeq(term)])",
      "Aside from being a good general solution to this problem, in some basic benchmarks, this turns out to be about 5 orders of magnitude faster than doing it the naive way! Pretty cool, but you’ll probably rarely notice the difference. Nevertheless it could matter in specialized cases if you are automatically constructing and batching large/gnarly queries that require a lot of work to escape.",
      "Anyway, enjoy! I’d love to hear what you think!"
    ],
    "summary_t": ""
  },
  {
    "id": "a68ec8be72ef61d9c22927077fd224af",
    "url_s": "https://opensourceconnections.com/blog/2013/01/24/adventures-in-puppetry/",
    "title": "Adventures in Puppetry!",
    "content": [
      "My first attempt at using Puppet to automate a server build was not as easy as Id hoped. The goal seemed simple enough: Build a RedHat server running Solr 4. What I didnt know was that this particular path is not as well-traveled as, say, running WordPress on Ubuntu. Youll notice I wrote \"running Solr 4″ rather than Tomcat or Jetty. When I began this quest I didnt care which container held Solr. After a quick discussion with Eric Pugh I found out that, unofficially at least, the core Lucene and Solr developers recommend Jetty 6 for running SolrCloud. The reasoning was easy enough to understand: Jetty 6 is packaged with Solr and thats what gets tested the most. Since SolrCloud is a very new feature I was inclined to go with whats tested. So from that point on I focused on installing Jetty 6. Getting Puppet installed was easy. We use Amazon Web Services a lot, so the first thing I did was log into AWS and instantiate a RedHat Enterprise Linux 6 (RHEL6) virtual machine. Thats the RHEL version that shows up in the quick launch screen, so thats what I picked. Latest and greatest, right? Heres the Puppet install distilled:",
      "sudo rpm -ivh \\\n\nhttp://yum.puppetlabs.com/el/5/products/i386/puppetlabs-release-5-6.noarch.rpm\n\nsudo vi /etc/yum.repos.d/puppetlabs.repo\nsudo yum update\nsudo yum install puppet\nsudo puppet resource cron puppet-apply ensure=present user=root minute=1 \\\ncommand='/usr/bin/puppet apply $(puppet --configprint manifest)'",
      "Well that was easy! In fact, I could have gotten that far by using one of the sample Cloud Front templates AWS provides. My next step was to see if PuppetForge had a Jetty 6 module: Nope! But I did see that there was a Solr module, but it was only working on Ubuntu. To see why, I went to the projects GitHub repository and browsed the init.pp to see if I could find anything Ubuntu-specific. The only thing I found was a reference to a package called \"solr-jetty\". That meant that there was an Ubuntu package called solr-jetty available, but not one for RHEL6. To make some forward progress I decided to try this out on Ubuntu so at least our development environments could be automated. What I found out was that the solr-jetty package uses jsvc to launch Jetty, and jsvc is a binary executable thats dynamically linked to a shared library called \"libcap.so\". My Ubuntu VM only had something called \"libcap-ng.so\", and no, it wasnt compatible. Fine. I didnt want to run on Ubuntu anyway. From there I spent some time searching for and checking out the various Jetty RPMs available for RHEL6, but they only included the APIs. What I really wanted was an RPM that did operational things like create a Jetty user and init.d service. The Red Hat Yum facilities dont have it, but I found a package on JPackage that fit the bill: Plain ol jetty6 RPM. But JPackage doesnt work on RHEL6! This might have been a temporary bug in their site mirrors, but it kept me from getting what I wanted. Fine. Ill run RHEL5. And then something magical happened: Things started working! RHEL5 doesnt include wget, so I tried installing it through puppet:",
      "puppet module search wget\npuppet module install maestrodev-wget\npuppet apply -e 'include wget'",
      "And it worked! Next I hooked up JPackage:",
      "wget http://www.jpackage.org/jpackage50.repo\nvi jpackage50.repo",
      "I had to enable the distro-specific repository, but when I automate all this Ill just install a pre-edited jpackage50.repo as part of the manifest. The Jetty Puppet modules I found were only available on GitHub and not PuppetForge. I briefly tried using one of the git modules available on PuppetForge. I chose one that seemed the simplest and installed it to my locally available modules:",
      "puppet module search git\npuppet module install puppetlabs-git\npuppet apply -e 'include git'",
      "This didnt work, but that was probably due to my newbie approach in the last line rather than putting something in site.pp. I decided to uninstall that module and just pull down the zip file I needed manually:",
      "puppet module uninstall puppetlabs-git\nwget https://github.com/puzzle/puppet-jetty/archive/master.zip\nmv master master.zip\nunzip master.zip\nmv puppet-jetty-master /etc/puppet/modules/jetty\npuppet apply -e 'include jetty'",
      "This spat out a bunch of errors related to the particular Java package RHEL5 came with: 1.4.2-gcj. Wow: I didnt realize RHEL5 was that old! Now, the GCJ Java package is a dependency for a few other packages so Puppet couldnt uninstall it without some help. It didnt look like I needed those other packages, so rather than risk running into dependency hell by having Puppet uninstall them and then repeating for every other dependency, I decided to use Yum to uninstall:",
      "yum erase java-1.4.2-gcj-compat\npuppet resource package java-1.4.2-gcj-compat ensure=\"absent\"",
      "Im not sure if the last line did anything to change the state of that Java package in Puppets internals, but it did show me that Puppet believed Java was absent. Finally:",
      "puppet apply -e 'include jetty'",
      "Puppies and rainbows! It worked! Then the icing on the cake:",
      "puppet resource service jetty6 enable=\"true\" ensure=\"running\"",
      "I poked around the running processes, etc and var/log folders, just to see if it looked like Jetty was installed properly. Then I hit localhost:8080 with curl and got a nice 404 error to stdout and a log entry in the request log. So the road was rocky and filled with dead-ends, but it worked. I saw a few comments along the way from people who were hosting their own Yum repository for deployments. At the time I didnt want to add that complexity to our environment, but now it looks like the best way to proceed if we want to run Jetty on RHEL6. As for running it on Ubuntu, well, thats a challenge I can safely pass on for now.",
      "1/25/2013 Edit: It looks like theres an even easier way to install JPackage with Puppet:",
      "puppet module install yguenane-jpackage\npuppet apply -e \"include jpackage\"\npuppet resource yumrepo jpackage-rhel enabled=\"1\"",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "05fc00ef510f9a415f097bf434187e86",
    "url_s": "https://opensourceconnections.com/blog/2013/01/31/charlottesvilles-big-data-meetup-what-is-a-data-scientist/",
    "title": "Charlottesvilles Big Data Meetup – What is a Data Scientist?",
    "content": [
      "Scott and I ventured out of the office yesterday evening to check out a new group starting up– Charlottesvilles Big Data Group. The most exciting thing about the group was how diverse it was. Folks from all over the spectrum from hardcore science, to Bioinformatics, to entreprenuers trying to build an awesome product, to folks like Scott and I focused on customer solutions.",
      "The group engaged in a fascinating discussion on what exactly a \"Data Scientist\" does. Data science being a crucial facet to the Big Data world, it was an interesting discussion about the roles and tools employed by the data scientist as well as the ways engineers supported data scientists in their work.",
      "What is a Data Scientist?",
      "Something that really stood out from this discussion is how a data scientist really contrasts from the traditional \"experimental\" scientist. The traditional scientist builds an experimental design based on a hypothesis. An experimental design is established whereby all the independent variables – those that can influence the outcome – can be controlled. So we have a closely watched, sterile lab where every variable that might influence a result from humidity to temperature to smell are extremely tightly controlled. By changing just one or two of these variables while holding all other variables steady, we can observe a resulting, \"dependent\", variable and make a claim that there is a very strong likelihood and magnitude of a causal relationship between the independent variable(s) we tweaked and the resulting dependent variable we observed. So, for example, we could say that for every 10% of humidity increase, with all other variables steady, 5% more rats fail to solve a maze.",
      "How does a data scientist differ? Well its still hypothesis driven research. The difference is in the nature of the experimental design. Instead of running an experiment in a sterile lab where we are carefully controlling everything from the humidity to the temperature to the expression on the experimenters face, we instead have a giant mass of data potentially from uncontrolled conditions. The experiment becomes a matter of combing through massive amounts of data after the fact. For example, finding enough cases where rats ran through the maze at a given temperature, light level, and all other variables constant, except for a varying humidity. Because we simply have massive and massive amounts of data, there are enough times of all things being equal except humidity, that we can go back and make a statistically significant assertion about what rats do when the humidity changes.",
      "This is particularly useful when there is simply no way to control all the independent variables, and notions of causality are weaker. For example, tracking children through education programs. There is simply no way to setup an experimental design where we force one set of children to undergo one set of circumstances and force another set of children to go through another. Moreover, we cant ethically create an experiment that ensured each child had the exact same socioeconomic background, home life, cultural background, exercise, environment, and all the other dozens of factors that might influence their education outcome. So our only option is to collect tons and tons of data about kids, and see what shakes out. There may be enough times when certain variables are held steady except for one that a definite outcome could be measured.",
      "Exploring The Data Scientists Toolbox",
      "All this talk about experimental design being fairly abstract, its important to keep in mind that dealing with, warehousing, and processing massive amounts of data efficiently to answer the data scientists questions is in itself a hard problem. Through the Charlottesville Big Data group, Scott and I are very excited to learn more about how the diverse groups in the Big Data world deal with Big Data at scale and how that contrasts with some of the customer facing Big Data/distributed search problems weve encountered.",
      "At the core of the problem is knowing what data structures are the best tool for the job. At OpenSource Connections, we have a pretty broad understanding of the strengths and weaknesses of various solutions such as distributed search, NoSQL databases, relational databases, and plain flat-file logs in Hadoop using Map Reduce. Matching up hard data problems with the right solution using the right data structure requires crucial collaboration between everyone. The discussions around what tools such a diverse group have used to solve their problems was and will be a very powerful component of the group. For example, what are the similarities and differences between natural language processing and search at scale vs collecting raw numeric statistics? Are there things that the two groups could learn from each other when it comes to how data is stored and processed?",
      "Another crucial tool in the toolbox is data visualization. Were excited to contrast our experience in user-focused, discovery oriented UI design with the visualizations scientists use to explore their data sets and reach interesting conclusions. One potential fascinating area that came out of our discussions was how does one visualize both the complete data set while allowing for exploration of orders of magnitude smaller features of the data? This is of extreme interest when it comes to visualizing DNA or exploring features of astronomy. The issues of scale are massive and difficult for the human mind to comprehend spatially. Are there lessons about how this problem was solved that can inform others doing Big Data visualization?",
      "Next Time – Tools & Workflow",
      "Were extremely thrilled to continue exploring these issues. At the next meetup, were hoping to explore how everyones workflow differs. What does the pipeline of a Bioinformatics person look like? How does it differ from the pipeline we use when analyzing text before putting it into Lucene? Also of crucial importance, what tools are used along that pipeline? Are any of the tools favored by one domain potentially applicable to another domain?",
      "In short, its a very good time to be a data scientist or big data software engineer in Charlottesville, we hope youll join us for the next meetup!"
    ],
    "summary_t": ""
  },
  {
    "id": "a823bb6a385bff98b2dd45aee1032d25",
    "url_s": "https://opensourceconnections.com/blog/2007/02/23/continous-integration-tools-market-is-splitting/",
    "title": "Continous Integration Tools Market is splitting…",
    "content": [
      "In the beginning there was CruiseControl, the original Continous Integration tool. Most people, including myself, cut there teeth on CruiseControl. Then, over time, a couple of opensource projects started spawning commercial tools, like AntHill and AntHillPro, and I thought all the new innovation in CI tooling was going to be by commercial companies.",
      "Well, I am glad to see that isnt the case. At the heavy/powerful, and light and simple end of the spectrum there are two new tools to add to the evaluation lists: Hudson and CruiseControl.rb. Neither of them is currently listed on the Continuous Integration Server Feature Matrix page, but they both offer new approaches and features to CI.",
      "Hudson is Java based, and provides similar functionality to CruiseControl and Continuum. It is better then CruiseControl in that it allows you to configure new jobs via the web interface, and has a reasonable well thought out plugin architecture. It looks like it has a leg up on Continuum because it seems more polished, and designed to manage larger numbers of projects.",
      "The key things that I like about Hudson is that it supports distributed builds, and scheduling builds outside of Hudson, and just reporting results.",
      "The distributed builds are based on deploying a \"slave agent\" to each slave builder. You can specify for your projects if they need to be built on one specific server, or if they can be built on any server. It also tries to move long running builds to the slaves versus the master project. You will have the usual challenges of deploying all the tools to each slave builder however. But if you are building many heterogenous projects, like C code, PHP, Java, Ruby, then you can simplify the setup task by having seperate server for each type of environment. For one client, we have seperate Continuum instance building a single .NET project on Windows, and this would have been a great place to have a slave builder instead.",
      "The external builds stuff is really exciting, and an original idea. If you already have a build setup, like maybe doing builds on Linux and Solaris for example, and dont want to throw it away, then you can just tweak Hudson to read in the result of your external build, so you can get the reporting and notification abilities of Hudson! This is a great way to slowly introduce CI into an organization that has preexisting processes without causing a lot of disruption.",
      "CruiseControl.rb, while sharing its name with CruiseControl and CruiseControl.NET looks like a completely redone implementation in Ruby. Instead of just being a rewrite of CruiseControl, like CC.NET is, it has been written in a thoroughly \"Thinking in Ruby\" fashion. Out goes the complex XML config file, in comes a very simple Ruby script for configuration. Still there are two seperate processes, a build agent and a dashboard gui (a simple Rails app), but both are started up together. It definitly passes the 15 Minute rule, as I was able to deploy it and have it running in no time. You can see it online at http://labs.opensourceconnections.com:3333, where I have dropped two projects, Horseshow, a Rails project, and Boozer, a user auth plugin for Rails…",
      "This looks like a great choice for projects who are already using Ruby, and dont need/want the complexity/power of a full blown CI tools like Continuum or Hudson. Where is looks like it falls down is if you have tens of projects that you want to manage. Or lots of different SCM systems. You also have to log on to the server and use a command line cruise tool to add projects. And edit a project config ruby file to specify who the recipients are. But, for a small team where sharing out access is fine, or for just a couple projects, it looks like just the ticket. For Ruby projects at least, its the new King of the Hill."
    ],
    "summary_t": ""
  },
  {
    "id": "7b85448c049c8815ba2a319788a5d3d8",
    "url_s": "https://opensourceconnections.com/blog/2013/02/01/2-day-solr-training-coming-to-charlottesville-in-march/",
    "title": "2-Day Solr Training Coming to Charlottesville in March!",
    "content": [
      "Course Overview",
      "OpenSource Connections, in partnership with LucidWorks, is pleased to present Solr Unleashed, a 2-day Solr training course right here in Charlottesville. If youre new to Solr, then this course will put you well on your way toward building a best-practices search application customized to your particular needs. And even if you have plenty of Solr experience, this course will introduce interesting, new, and useful aspects of Solr that you need to be aware of. Here, take a look at the course outline and see what were talking about.",
      "Introducing your Trainers",
      "John Berryman",
      "Coming from a background of Aerospace Engineering, John soon discovered that his true interest lie in software and technology. In early 2011, John began work with Opensource Connections where he consults large enterprises about full-text search and Big Data applications. Highlights to this point have included prototyping the future of search with the US Patent and Trademark Office, implementing the search syntax used by patent examiners, and building a Solr search relevancy tuning framework called SolrPanl.",
      "",
      "Matt Overstreet",
      "Usability is Matt Overstreet’s mission. He has worked with Federal, Fortune 500, and small businesses to help collect, mine and interact with data. He solves problems by synthesizing his experiences drawn from a liberal arts and technical background.",
      "",
      "When and Where",
      "Solr Unleashed will take place on Thursday and Friday, March 7-8, 2013 at the OpenSource Connections office conveniently located just southeast of the Downtown Mall. Lunch will be provided by Hotcakes, snacks by SpudNuts, both popular Charlottesville institutions.",
      "",
      "Heres what Verizon had to say about the course:",
      "\"It was a great course and [the trainers] were extremely helpful answering our questions.\" – Luis A.",
      "And heres what BestBuy had to say about the course:",
      "\"I found the training course to be an informative and comprehensive dive into the SOLR platform, it touched on many key points and important features of SOLR. I found the course to be a perfect introduction for those new to the SOLR or intermediate users who might want to delve deeper into the platform.\" – Kurt B.",
      "If you are interested in learning more, then please contact us.",
      "Once you are ready, sign up!"
    ],
    "summary_t": ""
  },
  {
    "id": "410453f6ce7a6da6647165a992b75a35",
    "url_s": "https://opensourceconnections.com/blog/2013/02/02/the-hello-world-of-mapreduce-scores-a-goal-for-science/",
    "title": "The Hello World of MapReduce Scores a Goal for Science!",
    "content": [
      "Just about the first thing youre going to do when you learn MapReduce is do the word frequency job. That just reads in a bunch of text and reports on how many times each word is used. The mapper spits out a map of \"word\" and \"1\", and the reducer adds each number associated with each word.",
      "That seemed like a toy problem when I first did it. What I didnt realize at the time was that some words and phrases can be used to determine the date a particular text was written. For example, the frequency of the word \"gnarly\" increased significantly around 1982 when Fast Times at Ridgemont High hit the theaters.",
      "Now it seems that historians are using this technique to date texts so that they can properly order events. A great overview of this process is given at The Physics arXive. From the article:",
      "For example, Tilahun and co say that the phrase \"amicorum meorum vivorum et mortuorum\", which means \"of my friends living and dead\", was popular between the years 1150 and 1240 but not at other times. And the phrase \"Francis et Anglicis\", which is a form of address meaning \"to French and English\", was phased out when England lost Normandy to the French in 1204.",
      "The other thing a word and phrase frequency map does is form a sort of high-level fingerprint for a collection of text. In addition to classifying text according to date, this fingerprint can also be used to gauge the likelihood a certain author wrote a particular document.",
      "All from the \"Hello World\" of MapReduce. Dude!"
    ],
    "summary_t": ""
  },
  {
    "id": "831a689983f70ec5c3ef4fb728969e95",
    "url_s": "https://opensourceconnections.com/blog/2013/02/06/opensource-connections-is-proud-to-host-beswarm-4/",
    "title": "OpenSource Connections is proud to host beSwarm 4",
    "content": [
      "Opensource Connections is proud to host beSwarm",
      "What",
      "beSwarm is a gathering of tech enthusiasts from all over central Virginia to discuss, share, and learn about anything that interests us. The idea of the event is that participants \"vote with their feet,\" moving around the room and forming swarms around interesting discussions and presentations.",
      "When",
      "Saturday March 2nd, 2013 from 10:00AM to 5:00PM",
      "Where",
      "Opensource Connections International Headquarters (here’s the MAP) Parking available on the street.",
      "Schedule",
      "10:00 Donuts made out of potatoes (that is, SpudNuts)\n  10:00 – 3:30 Show and tell, hacking, general shenanigans\n  12:30 Lunch (Bagby’s sandwiches)\n  3:30 – 4:30 Lightning Talks\n  4:30 – 5:00 Community Issues (Who’s doing the next beSwarm, etc.)\n  All day: Liquid stimulants and depressants. And water: carbonated, sugared, and plain.",
      "Cost",
      "Free!",
      "Please sign up here"
    ],
    "summary_t": ""
  },
  {
    "id": "e8789ba01e9ced10c96affe40d3a8689",
    "url_s": "https://opensourceconnections.com/blog/2013/02/11/convenient-solr-feature-facet-over-the-same-field-multiple-times-with-different-filters/",
    "title": "Convenient Solr Feature: Facet over the same field multiple times with different filters",
    "content": [
      "Did you know that you can facet over the same field more than once? Did you know that you can rename the facet field? Did you know that you have display the facet with different filters?",
      "Lets say I want to look at the number of StackOverflow answers for recent questions as compared to the number of answers across all questions ever asked. I did this be creating a filter query tagged with \"moreRecent.\" And then I faceted over the AnswerCount field twice, once with the alias of RecentAnswerCount and once with the alias of TotalAnswerCount. The later facet excluded the \"moreRecent\" filter query.",
      "Heres the query:",
      "http://localhost:8983/solr/collection1/select\n?q=*:*\n&facet=on\n&fq={!tag=moreRecent}CreationDate:[2011-08-05T19:34:49.473Z TO NOW]\n&facet.field={!key=RecentAnswerCount}AnswerCount\n&facet.field={!key=TotalAnswerCount ex=moreRecent}AnswerCount",
      "And here is the response:",
      "<lst name=\"facet_fields\">\n        </lst><lst name=\"RecentAnswerCount\">\n            <int name=\"2\">2543</int>\n            <int name=\"1\">2059</int>\n            <int name=\"3\">1831</int>\n            <int name=\"4\">1015</int>\n            <int name=\"5\">514</int>\n            <int name=\"6\">222</int>\n            <int name=\"7\">114</int>\n            <int name=\"0\">95</int>\n            <int name=\"8\">77</int>\n            <int name=\"9\">44</int>\n            <int name=\"10\">24</int>\n            <int name=\"11\">12</int>\n            <int name=\"12\">9</int>\n            <int name=\"14\">8</int>\n            <int name=\"13\">6</int>\n            <int name=\"15\">4</int>\n            <int name=\"16\">4</int>\n            <int name=\"18\">3</int>\n            <int name=\"17\">2</int>\n            <int name=\"19\">2</int>\n            <int name=\"21\">2</int>\n            <int name=\"22\">2</int>\n            <int name=\"20\">1</int>\n            <int name=\"23\">1</int>\n            <int name=\"25\">1</int>\n            <int name=\"26\">1</int>\n            <int name=\"28\">1</int>\n            <int name=\"24\">0</int>\n            <int name=\"42\">0</int>\n            <int name=\"116\">0</int>\n        </lst>\n        <lst name=\"TotalAnswerCount\">\n            <int name=\"2\">4862</int>\n            <int name=\"3\">3729</int>\n            <int name=\"1\">3582</int>\n            <int name=\"4\">2116</int>\n            <int name=\"5\">1172</int>\n            <int name=\"6\">560</int>\n            <int name=\"7\">297</int>\n            <int name=\"8\">183</int>\n            <int name=\"0\">173</int>\n            <int name=\"9\">109</int>\n            <int name=\"10\">62</int>\n            <int name=\"11\">38</int>\n            <int name=\"12\">26</int>\n            <int name=\"14\">19</int>\n            <int name=\"13\">16</int>\n            <int name=\"15\">12</int>\n            <int name=\"16\">7</int>\n            <int name=\"18\">6</int>\n            <int name=\"19\">5</int>\n            <int name=\"21\">5</int>\n            <int name=\"17\">3</int>\n            <int name=\"22\">3</int>\n            <int name=\"20\">2</int>\n            <int name=\"23\">1</int>\n            <int name=\"24\">1</int>\n            <int name=\"25\">1</int>\n            <int name=\"26\">1</int>\n            <int name=\"28\">1</int>\n            <int name=\"42\">1</int>\n            <int name=\"116\">1</int>\n        </lst>"
    ],
    "summary_t": ""
  },
  {
    "id": "6293df33691d59f14a1a3beff195079e",
    "url_s": "https://opensourceconnections.com/blog/2013/02/12/using-solr-join-to-find-the-best-time-to-ask-questions-on-stackoverflow/",
    "title": "Using Solr Join to find the best time to ask questions on StackOverflow",
    "content": [
      "So lets say that you have an important tech question that simply must be answered:",
      "\"What’s the difference between JavaScript and Java?\"",
      "Normally you would post it on StackOverflow and add a hefty bounty to get it answered fast. But, youve posted a bounty on the past 10 questions and now your Stack Overflow reputation is 4.",
      "Dont fret, perhaps if you just time your question correctly you can catch all those Java/JavaScript programmers right when theyre answering important questions like yours. And how do you figure out just when that magic time is? Simple, you index the entire StackOverflow data dump into Solr and treat Solr as a StackOverflow analytics engine. (Hey, you may not know the difference between Java and JavaScript, but youre nobodys fool when it comes to Solr!)",
      "So heres what this looks like. The post.xml file in the StackOverflow data dump contains all the questions and answers on the site. Posts contain the following fields:",
      "Id – Unique id for a question or answer.\n  ParentId – If this post is an answer, ParentId refers to the corresponding question.\n  PostTypeId – 1 for a question, 2 for an answer.\n  CreationDate – In Greenwich Mean Time.\n  Body – The contents of the post.\n  Title – You guessed it.\n  Tags – A list of topics for this question.",
      "In order to slice and dice the data to find the best time of year, day of week, or time of day to answer a question its a good idea to break up the CreationDate into a set of related fields:",
      "CreationMonth – 1 through 12.\n  CreationHour – 0 through 23.\n  CreationMinute – 0 through 60.\n  CreationDayOfWeek – 0 (Monday) through 6.\n  CreationDayOfYear – 1 through 365.",
      "Now all you have to do to find out that golden time for asking a question is to find the times when most people are answering questions about Java AND Javascript.",
      "http://localhost:8983/solr/collection1/select\n?q=Tags:(java AND javascript)\n&fq=PostTypeId:2\n&facet=on\n&facet.field=CreationDayOfYear\n&f.CreationDayOfYear.facet.limit=365\n&facet.field=CreationDayOfWeek\n&facet.field=CreationHour\n&facet.sort=index",
      "In words, the query q is for all questions tagged with both java and javascript. These results are filtered fq so that only answers are returned. The remainder of the parameters turn on sorted facet lists for times of the year, week, and day. So, as soon as you get query Solr, youll know the best times of the year, week, and day to ask your questions. You press enter and SNAP no results! What gives?!",
      "After a little research it turns out that only questions (PostTypeId=1) have the Tags field – so obviously you can not get a count of the answers tagged with Java AND JavaScript. So are you sunk? Is there no way to find out when the Java/JavaScript questions are getting all the attention? Are you going to have to do some crazy MapReduce indexing job to associate answers with their corresponding tags? It turns out no!",
      "Solr Join to the Rescue",
      "Thats right, Solrs Join functionality is a perfect fit for this particular problem. Lets take a look at how this would work:",
      "http://localhost:8983/solr/collection1/select\nq={!join from=Id to=ParentId}Tags:(java AND javascript)\n&facet=on\n&fq=PostTypeId:2\n&facet.field=CreationDayOfYear\n&f.CreationDayOfYear.facet.limit=365\n&facet.field=CreationDayOfWeek\n&facet.field=CreationDayOHour\n&facet.sort=index",
      "As you can see, the only difference here is strange notation at the front of the q parameter.",
      "{!join from=Id to=ParentId}",
      "That is Solrs local parameter notation, and heres what its telling Solr to do: First you have join this is actually syntactic sugar for the first parameter only. Its the same thing as saying type=join. This means that instead of using the lucene or dismax query mode, we will be using the join query mode. Next we have from=Id. To put this in SQL terms, this means that we will be be using Id as the primary key. Finally we have to=ParentId which, as you might have guessed, implies that ParentId will be used as the foreign key.",
      "When we issue the query, Solr first retrieves a list of documents matching the query Tags:(java AND javascript). Then, for every document in that result set, Solr retrieves the set of documents that have a ParentId corresponding to the Ids in the original set.",
      "In SQL world, this query would look like this:",
      "SELECT *\nFROM collection1\nWHERE ParentId IN (SELECT Id FROM collection1 where Tag = \"(Java and Javascript\")",
      "And now as soon as you you issue the query, you get the following Solr response:",
      "<lst name=\"CreationDayOfYear\">\n    <int name=\"1\">2</int>\n    <int name=\"2\">4</int>\n    <int name=\"3\">12</int>\n    <int name=\"4\">5</int>\n    <int name=\"5\">10</int>\n    <int name=\"6\">8</int>\n    <int name=\"7\">2</int>\n    <!--snip-->\n    <int name=\"362\">9</int>\n    <int name=\"363\">3</int>\n    <int name=\"364\">4</int>\n    <int name=\"365\">5</int>\n</lst>\n<lst name=\"CreationHour\">\n    <int name=\"1\">63</int>\n    <int name=\"2\">44</int>\n    <int name=\"3\">122</int>\n    <int name=\"4\">65</int>\n    <int name=\"5\">120</int>\n    <int name=\"6\">48</int>\n    <int name=\"7\">62</int>\n    <!--snip-->\n    <int name=\"21\">29</int>\n    <int name=\"22\">63</int>\n    <int name=\"23\">434</int>\n</lst>\n<lst name=\"CreationDayOfWeek\">\n    <int name=\"0\">371</int>\n    <int name=\"1\">390</int>\n    <int name=\"2\">383</int>\n    <int name=\"3\">422</int>\n    <int name=\"4\">369</int>\n    <int name=\"5\">266</int>\n    <int name=\"6\">272</int>\n</lst>",
      "You can imagine how this data could easily be used to build visualizations of the best times to query Stack Overflow for your particular topic. And actually, we are in the process of building such a visualization capability right now. See Patricias new post for an example.",
      "Also, if youre interested in playing with this yourself, check the repo on GitHub.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "a9cf2a1c281721ebc58bf404f3ea2a09",
    "url_s": "https://opensourceconnections.com/blog/2013/02/14/building-suggest-as-you-type-with-carrot2-clustering/",
    "title": "Building Suggest-As-You-Type with Carrot2 Clustering",
    "content": [
      "The first interaction that a customer has with your e-commerce web site is with the search box itself. So it is of utmost importance to make the user experience here is as clean and positive as possible. And one good way of doing this is by providing useful suggestions to the user as they type. This behavior is called Suggest-As-You-Type.",
      "I recently discover a clever new approach to Suggest-As-You-Type which makes use of Carrot2 Clustering. The basic idea is to populate a list of suggestions in an independent Suggest-As-You-Type Solr core.",
      "Lets walk through the basic idea using an example. Pretend that our online store – a grocery store – has several departments each with their own sub departments. The goal for our Suggest-As-You-Type is to provide reasonable recommendations about which department the customer should visit based upon their current search string. However, a common problem in e-commerce shops is the fact that products may be sourced from several venders, and the information that the venders provide about these products may be incomplete, inconsistent, or just wrong. So how can the Suggest-As-You-Type know that a search for \"brocoli\" belongs in the \"Produce/Vegetable\" department? And how can Suggest-As-You-Type know that a search for \"India Pale Ale\" belongs in the \"Adult Beverage/Beer\" department? As I found out, Carrot2 Clustering is how!",
      "Heres roughly how it could work. Presumably, we already have one Solr that is serving up product searches. Lets restart that Solr and enable the clustering component:",
      "java -Dsolr.clustering.enabled=true -jar start.jar",
      "Now we can make queries with the clustering request handler that will look something like this:",
      "http://localhost:8983/solr/clustering\n    ?q=*:*\n    &rows=1000\n    &carrot.title=ProductName\n    &carrot.snippet=ProductDescription",
      "And the response will in turn look like this:",
      "<arr name=\"clusters\">\n  <lst>\n    <arr name=\"labels\">\n      <str>Delicious</str>\n    </arr>\n    <double name=\"score\">3.1654221261111397</double>\n    <arr name=\"docs\">\n      <str>474523</str>\n      <str>234263</str>\n      <!-- snip -->\n      <str>553285</str>\n    </arr>\n  </lst>\n  <lst>\n    <arr name=\"labels\">\n      <str>On Sale</str>\n    </arr>\n    <!-- snip -->\n  </lst>\n  <lst>\n    <arr name=\"labels\">\n      <str>Frozen</str>\n    </arr>\n    <!-- snip -->\n  </lst>\n  <lst>\n    <arr name=\"labels\">\n      <str>Milk</str>\n    </arr>\n    <!-- snip -->\n  </lst>\n  <!-- snip -->\n</arr>",
      "As you can see, the clustering query is returning a set of tagged clusters, but the tags themselves, Milk, Frozen, On Sale, Delicious, etc. are quite scattered and are not very helpful. Lets tighten the query up a bit by only looking in the Produce/Fruit department:",
      "http://localhost:8983/solr/clustering\n    ?q=+Department:Produce +SubDepartment:Fruit\n    &rows=1000\n    &carrot.title=ProductName\n    &carrot.snippet=ProductDescription",
      "The cluster tags here will be much more fruit oriented: Apple, Orange, Pear, Red Delicious, Juicy, Citrus, etc. The tags we see here are perfect for building our Suggest-As-You-Type Solr core because when the user types any of these terms, they will likely be thinking about fruit.",
      "So lets do just that; lets build our suggest core. First we need to define the appropriate fields (in schema.xml):",
      "<field name=\"DisplayText\" type=\"string\" stored=\"true\"></field>\n<field name=\"TagWords\" type=\"ignored\" multiValued=\"true\"></field>\n<field name=\"Text\" type=\"text_general_edge_ngrammed\" indexed=\"true\" stored=\"true\" multiValued=\"true\"></field>\n\n<!-- snip -->\n\n<copyfield source=\"DisplayText\" dest=\"Text\"></copyfield>\n<copyfield source=\"TagWords\" dest=\"Text\"></copyfield>",
      "Here we store the DisplayText so that it can be displayed later. But the TagWords can be ignored because that field is only used to refer to the field that we dump into the Text field. The Text field, then, is of type text_general_edge_ngrammed. (So, the same as text_general, but then edge n-gram for faster partial-word matches. We can get a lot more clever here, but text analysis is not the focus of this post.)",
      "Now that the schema is set up, all we need are documents! We need one document for every possible Department/SubDepartment present in our grocery store. The department name goes into the DisplayText field, and, as you might have guessed, for the TagWords field we run a side job that collects the Carrot2 cluster names for each of these departments. (Thats the key part. Read it again.)",
      "Once the indexing of the Suggest At You Type core is complete, we can then take the partial searches of our customers and help direct them to the department that they are seeking. If they are looking for \"crispy …\" we will direct them to Produce/Fruit, and Produce/Vegetable, and Snacks/Chips. But we do not direct them them to Home Supplies/Cleaners! If they are looking for \"microwa…\" then we direct them to Frozen Foods, but not Adult Beverages.",
      "Now this does beg a question, and this is the question that I asked my e-commerce client: Just because we can understand which department the searcher belongs in, is it really helpful to direct them to those departments? Think about that for a second… maybe not! For instance, if a user knows that he really, really wants the \"new york chocolate cheese cake with strawberries on top\", then by suggesting that they visit the the Bakery/Cakes you are actually asking the customer to generalize their search. They came to your search box intent upon buying a New York chocolate cheese cake with strawberries on top, and you said \"Nah, why dont you just look at all our cakes. Maybe youll find something there.\"",
      "So… I think that this usage of clustering is a great example of the power of Carrot2, and I have been impressed with semantic clarity of the tags that Carrot2 provides to the clusters that it finds. However, building a good user experience for Suggest-As-You-Type is not as easy as it might first seem because there are so many different types of customers. For customers that come into your e-commerce site to simply look around, then its probably a good idea to suggest departments that they might be interested in. But for more serious customers, you might want to provide suggestions based upon previous customer searches. And for those customers that know exactly what they want, then placing products directly in the Suggest-As-You-Type response is a good idea. Ideally, a good Suggest-As-You-Type user experience would include all three of these aspects.",
      "Update",
      "Weve recently furthered our understanding of Suggest-As-You-Type. Check out our new post here. And check out this related post about Search-As-You-Type",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "c83fd04abf29fb2ffde03e48e964a5c3",
    "url_s": "https://opensourceconnections.com/blog/2013/02/18/indexing-stackoverflow-in-solr/",
    "title": "Indexing StackOverflow in Solr",
    "content": [
      "One thing I really like about Solr is that its super easy to get started. You just download solr, fire it up, and then after following the 10 minute tutorial youll have a basic understand of indexing, updating, searching, faceting, filtering, and generally using Solr. But, youll soon get bored of playing with the 50 or so demo documents. So, quit insulting Solr with this puny, measly, wimpy dataset; Index something of significance and watch what Solr can do.",
      "One of the most approachable large datasets is the StackExchange data set which most notably includes all of StackOverflow, but also contains many of the other StackExchange sites (Cooking, English Grammar, Bicycles, Games, etc.) So if StackOverflow is not your cup of tea, theres bound to be a data set in there that jives more with your interests.",
      "Once youve pulled down the data set, then youre just moments away from having your own SolrExchange index. Simply unzip the dataset that youre interested in (7-zip format zip files), pull down this git repo which walks you through indexing the data, and finally, just follow the instructions in the README.md.",
      "If youre interested in how it works, basically weve modded the schema (solr_home/collection1/conf/schema.xml) to incorporate the fields from StackExchanges post files. We use extractDocument.py to convert from StackExchange format to Solrs xml format. (Its a simple file, go ahead and take a look.) Finally we index by simply posting the output to Solrs update endpoint.",
      "So, is this a mature indexing platform for StackExchange? Its getting there! With a recent change weve added the capability to post to Solr in batches. (Earlier the script was posting the entire file to Solr, and thus running out of memory.) Besides the indexing script, weve also included a rudimentary visualization component (which will likely be expanded greatly in the future). Finally, we realized that its painful to download the entire StackExchange data dump just so that you can start playing with Solr. Therefore weve included all the posts for the SciFi Stack Exchange so that you can beging experimenting immediately.",
      "Want an interesting first project with your new Solr data?",
      "Use it to find the best time of day or day of week to ask a particular question on Stack Overflow.\n  Create a k-NN document classifier.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "e0bdcf73b778f7b19699554f8d5ae650",
    "url_s": "https://opensourceconnections.com/blog/2013/02/21/lucene-4-finite-state-automaton-in-10-minutes-intro-tutorial/",
    "title": "Lucene 4 Finite State Automata in 10 minutes (Intro & Tutorial)",
    "content": [
      "Sample Automaton",
      "This article is intended to help you bootstrap your ability to work with Finite State Automata (note automata == plural of automaton). Automata are a unique data structure, requiring a bit of theory to process and understand. Hopefully whats below can give you a foundation for playing with these fun and useful Lucene data structures!",
      "Motivation, Why Automata?",
      "When working in search, a big part of the job is making sense of loosely-structured text. For example, suppose we have a list of about 1000 valid first names and 100,000 last names. Before ingesting data into a search application, we need to extract first and last names from free-form text.",
      "Unfortunately the data sometimes has full names in the format \"LastName, FirstName\" like \"Turnbull, Doug\". In other places, however, full names are listed \"FirstName LastName\" like \"Doug Turnbull\". Add a few extra representations, and to make sense out of what strings represent valid names becomes a chore.",
      "This becomes especially troublesome when were depending on these as natural identifiers for looking up or joining across multiple data sets. Each data set might textually represent the natural identifier in subtly different ways. We want to capture the representations across multiple data sets to ensure our join works properly.",
      "So… Whats a text jockey to do when faced with such annoying inconsistencies?",
      "You might initially think \"regular expression\". Sadly, a normal regular expression cant help in this case. Just trying to write a regular expression that allows a controlled vocabulary of 100k valid last names but nothing else is non-trivial. Not to mention the task of actually using such a regular expression.",
      "But there is one tool that looks promising for solving this problem. Lucene 4.0s new Automaton API. Lets explore what this API has to offer by first reminding ourselves about a bit of CS theory.",
      "Theory – Finite State Automata and You!",
      "Lucenes Automaton API provides a way to create and use Finite State Automata(FSAs). For those who dont remember their senior level computer science, FSA is a computational model whereby input symbols are read by the computer – the automaton – to drive a state machine. The state machine is simply a graph with nodes and labeled, directed edges. Each node in the graph is a state, and each edge is a transition labeled with a potential input symbol. By matching the current nodes edges to the current input symbol, the automaton follows edges to the next state. The next input symbol is read, and based on the transitions of the new state, we transition to yet another state, so-on and so-forth.",
      "More importantly here, FSAs are a way of specifying a Regular Language. If we think of all the input symbols as elements of a language, we can use an FSA to specify a language and determine if a given input string is valid for the language. We do this by processing the input string, following the transitions of an FSA until we reach a terminus node. If we exhaust the input symbols at such a node, then the string of symbols can be said to match the language. Otherwise, we can say the input string does not match the language.",
      "So for example, in the figure below. The string \"nice\" that is the sequence of symbols \"n\", \"i\", \"c\", \"e\" are accepted as a member of the language. All other strings are rejected:",
      "An FSA specifying a language that accepts the word \"nice\"",
      "A regular language could be just a set of valid strings. Or, it could be something a bit fuzzier like a Levenshtein distance or regular expression which as it turns out can be represented in a regular language. But even more powerfully, it can be a concatenation, union, or intersection of all of these.",
      "In Practice – An Automaton as a data structure",
      "In practice, Lucene automata are useful as as a data structure that bridges between a traditional Set<> and hand-written regular expression. When compared to a HashSet or TreeSet the memory representation (can be) much, much smaller, with very fast lookups. Moreover, Automata give you fuzzier features like regular expression matching.",
      "Its not a general purpose set replacement, however. For an automaton, the set is all the strings of symbols that match the language. However, due to all the fuzzy matching potentially using \"regular expressions\" enumerating all the input strings that match the language, that is enumerating the members of the set, is non-trivial and might never terminate. Think about it this way, traversal involves manually traversing a graph, so every node, whether a terminus or not, must be visited. This traversal might never terminate because you could have a * regular expression. So enumerating the strings that match the language will involve infinitely repeating the pattern before the *.",
      "Another factor to consider is that while lookup time and memory usage are much, much smaller when compared to a Set<>, indexing can be very time consuming. For the use case of creating an automaton up front to specify a relatively static language once this isnt a big deal. But for data structures that change frequently, automatons shouldnt be the first choice.",
      "When compared to a regular expression, the Automaton lends itself to being able to hold more complex regular languages than the ones you could specify in a traditional regular expression. For example, it would be difficult to specify a regular expression that had 500,000 potential first names followed by 1,000 last names unioned with 1,000 last names, a comma, followed by one of 500,000 first names. Thats Perl thats not even write only. Automatons give you the ability to effectively write an extremely rich and large a \"regular expression\" in readable code in a way that wont make generating/parsing such a regular expression a giant chore.",
      "Next, lets see how we actually build a useful automaton",
      "Teh Codez – Building an Automaton",
      "(Note all code below can be found at this github repo)",
      "To show off the API, lets take the example we discussed at the outset. Lets say our language allows two forms of full names \"FirstName LastName\" and \"LastName, FirstName\". For simplicity, lets validate only a set of names. First Names: {\"Doug\", \"John\"} and last names {\"Berryman\", \"Turnbull\"}. So in our world all the valid names are \"Doug Berryman\", \"Doug Turnbull\", \"John Berryman\", and \"John Turnbull\". Forms where last name comes first followed by a comma, followed by the first name (ie \"Turnbull, Doug\") are also considered valid.",
      "So how do we use the Lucene API to build at Automaton to validate this syntax?",
      "There are two key classes that youll use over-and-over for building meaningful automata. First the BasicAutomata class provides static methods for constructing automata out of simple building blocks (in our case individual strings). Second the BasicOperations class provides utility methods for combining Automata into unions, intersections, or concatenations of other automata.",
      "Outside of these two central classes, we can also fold in additional automata from other classes. For example, regular expressions via the RegExp class.",
      "Ok, now lets actually start putting together some code. Lets first look at the form \"FirstName LastName\". We want to specify a language that takes any of our first names {\"John\", \"Doug\"}, followed by some number of whitespace characters, then followed by any of our last names {\"Turnbull\", \"Berryman\"}.",
      "Our first piece of code forms the foundation for all of our other automata. One of the basic automata we need to build is simply one built from a set of valid strings. For example, an automaton for last names that says \"Turnbull\" is valid, \"Berryman\" is valid, but \"Pugh\" is not.",
      "As this task is going to be a pretty common, occurrence, well be using this utility function that builds an automaton for accepting only the values from the passed in String array:",
      "/**\n * @param strs\n *   All the strings that we want to allow in the returned language\n * @return\n *   An automaton that allows only the passed in strings\n */\npublic static Automaton stringUnionAutomaton(String[] strs) {\n    Automaton strUnion = null;\n    // Simply loop through the strings and place them in the automaton\n    for (String str: strs) {\n        // Basic building block, make an automaton that accepts a singl\n        // string\n        Automaton currStrAutomaton = BasicAutomata.makeString(str);\n        if (strUnion == null) {\n            strUnion = currStrAutomaton;\n        }\n        else {\n            // Combine the current string with the Automata for the\n            // previous string, saying that this new string is also valid\n            strUnion = BasicOperations.union(strUnion, currStrAutomaton);\n        }\n    }\n    return strUnion;\n}",
      "Notice how on every iteration, we create an automaton for the current string. The first iteration initializes the resulting automaton(strUnion) to the current strings automata (currStrAutomaton'). On subsequent iterations, we set the resultingstrUnionto the union ofcurrStrAutomatonand itself. Finally returningstrUnion` as the union of all the strings passed in.",
      "With this building block, we can build up a more complex Automaton for our FirstName LastName form:",
      "/**\n * @param firstNames\n *   Set of allowable first names\n * @param lastNames\n *   Set of allowable last names\n * @return\n *   An automaton that allows FirstName\\s+LastName\n */\npublic static Automaton createFirstBeforeLastAutomaton(String[] firstNames, String[] lastNames) {\n    List<Automaton> allAutomatons = new ArrayList<Automaton>();\n    // Use our builder to create an automaton that allows all the first names\n    allAutomatons.add(stringUnionAutomaton(firstNames));\n\n    // Add in an Automaton that allows any whitespace by using\n    // the regular expression\n    RegExp anyNumberOfSpaces = new RegExp(\"[ \\t]+\");\n    Automaton anySpaces = anyNumberOfSpaces.toAutomaton();\n    allAutomatons.add(anySpaces);\n\n    // Add in an Automaton that allows all our last names\n    allAutomatons.add(stringUnionAutomaton(lastNames));\n\n    // Return the concatenation off all these automatons\n    return BasicOperations.concatenate(allAutomatons);\n}",
      "In this code, were building three automata and returning the concatenation of all three. So to be a member of the concatenated automatons language, if a string passes the first automaton, then with additional characters passes the second automaton, and finally as characters are exhausted finishes the last automaton, well consider this a valid member of this language.",
      "Note the use of the RegExp class. This class supports basic regular-expression matching and allows us to match one-or-more tabs or spaces in the input.",
      "The LastName, FirstName form is similar:",
      "public static Automaton createLastBeforeFirstAutomaton(String[] firstNames, String[] lastNames) {\n    List<Automaton> allAutomatons = new ArrayList<Automaton>();\n    allAutomatons.add(stringUnionAutomaton(lastNames));\n\n    RegExp commaPlusAnyNumberOfSpaces = new RegExp(\",[ \\t]+\");\n    allAutomatons.add(commaPlusAnyNumberOfSpaces.toAutomaton());\n    allAutomatons.add(stringUnionAutomaton(firstNames));\n    return BasicOperations.concatenate(allAutomatons);\n}",
      "The only difference here is were validating last names before first and our regex separator now has a comma. Otherwise, this is very similar to the other form.",
      "To finish things off, we simply have to create an automata that takes the union of both forms, allowing strings of either language to be valid in the resulting language:",
      "public static Automaton createNameValidator(String[] firstNames,\n                                            String[] lastNames) {\n    return BasicOperations.union(createFirstBeforeLastAutomaton(firstNames, lastNames),\n                                 createLastBeforeFirstAutomaton(firstNames, lastNames));\n}",
      "Woohoo! You should be ready to go! Just keep in mind one or two things when building more complex automata:",
      "Automaton Creation Considerations",
      "One of the things that makes the Automaton special is the potential minimal in-memory representation of the data structure. This gives you powerful lookup capabilities against a complex language with a large vocabulary, but noticeably increases index time when compared to a traditional data structure.",
      "To ensure the minimal representation of an Automaton, its important to note that Lucene may not be always keeping the most optimal representation of the data structure in memory. Without minimizing, you could have problems with lookup speed and will certainly have problems exhausting the jvm heap.",
      "For example, if we said that \"Ed\" and \"Eddy\" are valid strings in our language, we might initially have something like:",
      "[]---E--->[]---d---[*]\n              \\\n               ---E--->[]---d---[]---d---[]---y---[*]",
      "Add this up over time, and we end up with a horrendous looking graph that leaves you wondering why anyone would ever bother using Automata!",
      "Part of the secret sauce to Lucenes Automatons is minimizations. Instead of representing the graph as a gnarly web of duplicated gobbly, gook, we can perform the minimization operation on the graph above to be simply:",
      "[]---E--->[]---d---[*]\n                            \\\n                             d---[]---y---[*]",
      "By periodically calling Automata.minimize you can reduce the memory footprint of your automaton.",
      "You can track roughly how big your automaton is getting with Automata.getNumberOfStates() and Automata.getNumberOfTransitions. Its generally a good idea to keep an eye on the size of your automaton and deal with any bloat that occurs during indexing.",
      "Hack away!",
      "I hope youve enjoyed this tutorial, hack away and let me know what you think! I hope to follow up with more information on how to more efficiently create and store automata and delve into their sister library in Lucene – Finite State Transducers!"
    ],
    "summary_t": "This article is intended to help you bootstrap your ability to work with Finite State Automata (note automata == plural of automaton). Automata are a unique ..."
  },
  {
    "id": "34152cde0155f51b8a467f67e779264a",
    "url_s": "https://opensourceconnections.com/blog/2013/03/06/why-foundationdb-might-be-all-its-cracked-up-to-be/",
    "title": "Why FoundationDB Might Be All Its Cracked Up To Be",
    "content": [
      "When I first heard about FoundationDB, I couldnt imagine how it could be anything but vaporware. Seemed like Unicorns crapping happy rainbows to solve all your problems. As Im learning more about it though, I realize it could actually be something ground breaking.",
      "NoSQL: Lets Review…",
      "So, I need to step back and explain one reason NoSQL databases have been revolutionary. In the days of yore, we used to normalize all our data across multiple tables on a single database living on a single machine. Unfortunately, Moores law eventually crapped out and maybe more importantly hard drive space stopped increasing massively. Our data and demands on it only kept growing. We needed to start trying to distribute our database across multiple machines.",
      "Turns out, its hard to maintain transactionality in a distributed, heavily normalized SQL database. As such, a lot of NoSQL systems have emerged with simpler features, many promoting a model based around some kind of single row/document/value that can be looked up/inserted with a key. Transactionality for these systems is limited a single key value entry (\"row\" in Cassandra/HBase or \"document\" in (Mongo/Couch) – well just call them rows here). Rows are easily stored in a single node, although we can replicate this row to multiple nodes. Despite being replicated, it turns out transactionally working with single rows in distributed NoSQL is easier than guaranteeing transactionality of an SQL query visiting potentially many SQL tables in a distributed system.",
      "There are deep design ramifications/limitations to the transactional nature of rows. First you always try to cram a lot of data related to the rows key into a single row, ending up with massive rows of hierarchical or flat data that all relates to the row key. This lets you cover as much data as possible under the row-based transactionality guarantee. Second, as you only have a single key to use from the system, you must chose very wisely what your key will be. You may need to think hard how your data will be looked up through its whole life, it can be hard to go back. Additionally, if you need to lookup on a secondary value, you better hope that your database is friendly enough to have a secondary key feature or otherwise youll need to maintain secondary row for storing the relationship. Then you have the problem of working across two rows, which doesnt fit in the transactionality guarantee. Third, you might lose the ability to perform a join across multiple rows. In most NoSQL data stores, joining is discouraged and denormalization into large rows is the encouraged best practice.",
      "FoundationDB is different",
      "FoundationDB is a distributed, sorted key-value store with support for arbitrary transactions across multiple key-values – multiple \"rows\" – in the database.",
      "To understand the distinction, let me pilfer an example from their tutorial. Their tutorial models a university class signup system. You know, the same system every CS major has had to implement in their programming 101 class. Anyway, to demonstrate the potential power here, I just want to share a single function with you, the class signup function:",
      "def attendsKey(s, c):\n    \"\"\" Key for student(s) attending class(c)\"\"\"\n    return fdb.tuple.pack(('attends', s, c))\n\ndef classKey(c):\n    \"\"\" Key for num available seats in class\"\"\"\n    return fdb.tuple.pack(('class', c))\n\n@fdb.transactional\ndef signup(tr, s, c):\n    rec = attendsKey(s, c) # generates key for a whether a student attends a class\n    if tr[rec].present(): return # already signed up (step 3)\n\n    seatsLeft = int(tr[classKey(c)]) ## Get the num seats left for a class\n    if not seatsLeft: raise Exception('no remaining seats') ## (step 3)\n\n    classes = tr[attendsKeys(s)] ## Count the number of \"attends\" records for this student\n    if len(list(classes)) >= 5: raise Exception('too many classes') ## (step 4)\n\n    tr[classKey(c)] = str(seatsLeft-1) ## decrement the available steps\n    tr[rec] = '' # mark that this student attends this class",
      "Ok, more than one function, but the other functions are just helpers to show you how keys are getting generated.",
      "Important here is that all work is done through signups first argument, tr, this is the transaction object where all work is done. First we check for the existence of a special key that indicates whether student s is attending class c. Then in the same transaction, we work on a completely different \"row\" – the count of students attending a class. If we are able to, we update that count and then create a row to store the fact that that student stores that class. More important than what is actually happening here, FoundationDB is able to attempt to perform this transaction atomically across the entire cluster.",
      "If this were a more traditional NoSQL store, we would have to take a bit more awkward tack to do this atomically. Wed have to chose either the class or the student to make the row that we can work with atomically. Implicitly, our key would become either a lookup for a class or a lookup for a student. For the sake of discussion, lets say we made our rows classes and we simply stored the id of all the students attending that class in that row. Its trivial to work on classes to add/remove students. We simply lookup a class and append the student id to sign them up.",
      "Conceptually this model is pretty simple, but its lacking if we suddenly want to lookup students in the database. What would that query look like? Can you do it atomically? Youll need to have another type of rows for students. Then you have to entities to work across outside of the transactionality guarantees.",
      "FoundationDB == unopinionated transactions",
      "A big reason that many NoSQL stores were simplified to the atomic row architecture is to get away from the forced large-scale transactionality (and performance hit) of SQL transactions. The solution was to go back to making everything a map and to make accesses to each entry/row/document a transaction. So we all bought into that and began working our schemas into that model.",
      "However, at the end of the day both SQL and traditional NoSQL are both very opinionated about a transaction should be. Despite the transaction manifesto, Foundation is completely unopinionated when it comes to how you define transactions. The same signup code above could easily be implemented as two or three transactions if that was truly what was called for.",
      "This power is expressed in how you access Foundation. Foundation gets exposed more as a library for defining transactions on an arbitrary key-value store. This narrower aim lets you write code in your language, not constrained to a second query language or awkwardly fitting your code to an ORM. Instead, You write natural code expressing the transactions that you want to perform over the key-value store. Pretty exciting stuff.",
      "Whoah whoah whoah, slow your roll sparky, looks cool and all but prove this isnt a giant boondongle?",
      "Ok Foundation is new and unproven. Theres plenty of unanswered questions about it. How does it perform vs {HBase/Cassandra/Mongo/Couch/…}? What is the cost of this transactionality? At what point does its transactional architecture stop scaling? What are the trade-offs? Etc Etc",
      "Yeah, yeah so dont start rewriting all your database code to use Foundation, that would be pretty crazy. Nevertheless, the unopinionated, highly client-controlled notion of transactionality is ground-breaking, obviously useful, and Im hopeful it can be successful."
    ],
    "summary_t": ""
  },
  {
    "id": "548164b382e5bb54d6424e1e699e9d25",
    "url_s": "https://opensourceconnections.com/blog/2013/03/07/the-anatomy-of-a-dismax-query/",
    "title": "The Anatomy of a DisMax Query",
    "content": [
      "",
      "While debugging the functionality of a new query parser, I had the poor fortune rare opportunity to dig deep into Solrs search scoring code. I ended up learning a ton about how queries are composed and scored, so I though Id pass the learning on to you so that perhaps you can avoid my fate benefit from my new-found understanding.",
      "Our discussion here will focus upon the composition of queries made through Solrs DisMax query parser (DisMaxQParser). This was the code that I was largely working with, but it serves as an excellent example because it touches upon all of the most important query types that you might regularly deal with: Boolean queries, DisMax queries, Phrase queries, and Term queries.",
      "The Query",
      "So without further adieu lets take a look our sample query:",
      "localhost:8983/solr/select\n?q=captain james kirk\n&defType=edismax\n&qf=Body Title^1.5\n&pf=Body Title\n&mm=2\n&debugQuery=true",
      "(Those who have been following our blog recently may realize that here we are using our favorite demo data set – the SciFi StackExchange data. Its easy to understand and fun to work with. If youd like to give it a try, our Indexing StackOverflow in Solr post will get you started in 10 minutes.) Here were looking for posts containing captain james kirk (q), were using the edismax parser (defType), were looking for pure term matches in the Body and Title fields (note the boost on the Title, qf), and were looking for phrases matches on the same two fields (pf). We insist that at least 2 of the terms must match (mm), and finally, since were interested in understanding how Solr thinks about our query and about scoring, we turn on debug output (debugQuery).",
      "The first thing that is interesting to look at is the parsedquery section of the response. This shows us how Solr breaks down that query we sent it:",
      "+(\n  ( DisjunctionMaxQuery((Body:captain | Title:captain^1.5))\n    DisjunctionMaxQuery((Body:james | Title:james^1.5))\n    DisjunctionMaxQuery((Body:kirk | Title:kirk^1.5)) )~2\n)\nDisjunctionMaxQuery((Body:\"captain james kirk\"))\nDisjunctionMaxQuery((Title:\"captain james kirk\"))",
      "Boolean Queries",
      "Here we actually have examples of all the commonly used queries, Boolean, DisjunctionMax, Term, and Phrase. Lets break it down. At the outer-most level we have a Boolean query with three clauses:",
      "+(( DisjunctionMaxQuery((Body:captain | Title:captain^1.5)) DisjunctionMaxQuery((Body:james | Title:james^1.5)) DisjunctionMaxQuery((Body:kirk | Title:kirk^1.5)) )~2)\n  DisjunctionMaxQuery((Body:\"captain james kirk\"))\n  DisjunctionMaxQuery((Title:\"captain james kirk\"))",
      "Now you might be asking yourself \"Where are the ANDs and ORs?\" Well, thats not quite how Lucene Boolean queries work. Instead, a Lucene Boolean query is composed of a set of clauses that are modified by either MUST match, MUST_NOT match, or SHOULD match. SHOULD match is the default, MUST match is indicated in the above query by a plus at the front of the clause, and MUST_NOT match, though not present in this example, is indicated by a minus at the front of the clause. So for this query, we are insisting that the first clause MUST match in every document returned from our search, the other two SHOULD match, but if they dont its ok (the document just gets a lower score). And as far as scoring goes, each matching clause of a Boolean query is scored and the final score of the Boolean query is the sum of the score of its clauses.",
      "Lets dig one level deeper on that first clause of the Boolean query. Here we actually find yet another Boolean query, and it has three SHOULD match clauses (these clauses are all DisjunctionMaxQueries… hold your horses, were getting there). One interesting thing to note about this Boolean query is the ~2 at the end of that query. Its there because you earlier specified mm=2 in the original query. This ~2 indicates that of the three optional SHOULD match subqueries, we are insisting that at least two of these queries actually match, otherwise we disregard that document.",
      "DisMax Queries",
      "Lets now take a closer look at the first actual DisMax query: DisjunctionMaxQuery((Body:captain | Title:captain^1.5)). DisjunctionMax is a funny name… but no ones laughing. It means that we should search for the term in question across the disjunction of the possible fields, and return the max score from all these sub-queries. So in this case, if captain matched in the Title and matched in Body, we would find the score in both cases and return the maximum of these two as the score for the DisMax clause",
      "Now why would you want to return only the max of the sub-query scores? Consider an example: Lets say you want to search for harry potter over the Title and Body fields. If we take the sum of the subqueries instead of the max, then a document that has \"Harry Potter\" in the Title but not in the Body will have the same score as a document that happens to have \"Harry\" in both Title and Body, but \"Potter\" nowhere. This is not what you want! If maximization still seems too harsh for your use case, then look up the \"tie\" parameter which allows you to do something between maximization and summing of clause scores.",
      "Term Queries",
      "Going one level deeper now, lets look at the first clause: Body:captain. Looks pretty simple, right? Were just looking for captain in the Body field. Actually, this is one of the most complex portions of the query. The score of this query is dependent upon several thing, among them:",
      "How common the term captain is in this document (Term Frequency)\n  How common the term captain is across all documents (Document Frequency)\n  How large the field is in this document (Field Norm)\n  How much this query is boosted",
      "This is actually a very interesting topic, and possibly worth a future post by itself. However, for the time being, I will just refer you to the very well documented TFIDFSimilarity class that covers these details.",
      "Phrase Queries",
      "This leaves one final piece, the Phrase query. Traversing back up the query parse tree, the next top level Boolean sub-clause is a DisMax query with a single clause: DisjunctionMaxQuery((Body:\"captain james kirk\")). As a DisMax query, this is a somewhat degenerate case because, as youll recall, DisMax takes the maximum score of all its sub-clauses; in this case there is only one clause. Regardless, its the sub-clause that were interested in right now: Body:\"captain james kirk\" This is a Phrase query. Basically it seeks all the documents that contain all of the specified terms and then throws away the documents in which these terms are not adjacent to one another and in this order. Scoring proceeds in a manner analogous to Term queries in that the score is proportional to the number of occurrence of the phrase in this document and inversely proportional to the number of occurrences of the these terms across the whole corpus.",
      "Holistic Understanding of the DisMax Query",
      "So now since weve broken down the DisMax query and taken a look at all the little pieces, lets build it back up and look once more at the DisMax query as a whole.",
      "+(\n  ( DisjunctionMaxQuery((Body:captain | Title:captain^1.5))\n    DisjunctionMaxQuery((Body:james | Title:james^1.5))\n    DisjunctionMaxQuery((Body:kirk | Title:kirk^1.5)) )~2\n)\nDisjunctionMaxQuery((Body:\"captain james kirk\"))\nDisjunctionMaxQuery((Title:\"captain james kirk\"))",
      "In English:",
      "Were looking first and foremost for documents that contain at least two of the three terms: captain, james, and kirk. This has to match!\n    \n      We are searching only in the Body and Title fields, and documents dont get a higher score for matching any one of these terms in both fields.\n        \n          Oh… and Title is really important, so its score is multiplied by 1.5.\n        \n      \n    \n  \n  Next if any of the documents from the last bullet also have the phrase \"captain james kirk\" in the Body, then well give them a higher score.\n  Again, if any of the documents from the last bullet also have the phrase \"captain james kirk\" in the Title, then well give them a higher score.",
      "And thats it! Its a little confusing at first, but once you understand whats going on, youll be better able to control the relevancy of your search results. Next up, take a look at a related post where I talk about finally killing Sea Biscuit!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": "While debugging the functionality of a new query parser, I had the poor fortune rare opportunity to dig deep into Solrs search scoring code. I ended up learn..."
  },
  {
    "id": "311cd23adf3b8e08e06c9cb9e28791d9",
    "url_s": "https://opensourceconnections.com/blog/2006/07/19/running-with-scissors-overriding-latest_revision-svn-check-in-capistrano/",
    "title": "Running with scissors: overriding latest_revision SVN check in Capistrano",
    "content": [
      "Ruby as a language sometimes feels like you are running with scissors. Its great to get from A to B quickly with your scissors, but there is a chance you may trip and impale yourself!",
      "Today I was struggling with overriding what revision Capistrano uses when checking code out of Subversion as a hack to get around the lack of support for svn+ssh:// connectivity. Be default Capistrano makes a check from your local box for the latest revision, and then uses that to check out content on the remote box. However I wanted to skip the check from my local box and just pass in \"HEAD\" as the revision name.",
      "My first solution was just to hack up the Subversion class and hard code the @latest_revision to be \"HEAD\". But I realized what an ugly solution tweaking my gem was!",
      "After banging my head a bit, I realized that Ruby is a dynamic language, and I can do all sorts of magic that maybe I shouldnt be doing. It was time to use instance_variable_set!",
      "I added :before_update_code that allows me to manually set the revision in my deploy.rb and therefore bypass the check:",
      "task :before_update_code, :roles => [:web, :app] do\n  revision = \"HEAD\"\n  puts \"Revision to be deployed is #{revision}\"\n  source.instance_variable_set(\"@latest_revision\",revision)\nend",
      "It does kind of make me wonder if all those OOP rules about encapsulation no longer apply!"
    ],
    "summary_t": ""
  },
  {
    "id": "780e9c72f9493225168163331e224c70",
    "url_s": "https://opensourceconnections.com/blog/2007/02/28/citconf-the-continuous-integration-testing-conference-is-coming/",
    "title": "CitConf: The Continuous Integration Testing Conference is coming!",
    "content": [
      "CitConf is a focused conference on Continuous Integration Testing, and takes place three time a year around the globe. This years North American location will be Fort Worth, Texas, on April 27th and 28th.",
      "CitConf uses the OpenSpaces format which was pioneered by Bruce Eckels as a way to lower the management overhead of typical conferences. At CodeMash I joined Bruce and some others in a OpenSpace format session, and as long as the group is small, and interested it works quite well. Since CitConf is capped at 100 people, it seems like an entire OpenSpaces format conference would work.",
      "At any rate, the conference is FREE, as most of the costs are either borne by sponsors or eliminated via the OpenSpaces format."
    ],
    "summary_t": ""
  },
  {
    "id": "c44e260a19ca8504172a2c79fc6ed948",
    "url_s": "https://opensourceconnections.com/blog/2013/03/08/beswarm4-mission-accomplished/",
    "title": "beSwarm4 – Mission Accomplished",
    "content": [
      "OpenSource Connections was pleased to host this years beSwarm. We had 36 people sign up and from the turn out at the event, most of those who signed up came. We had a lots of old familiar faces as well as some out-of-towners, (though Robert Marmorstein is beginning to fit into both categories). Im also excited at how many students showed up this time. We also pulled in at least a couple of people who werent even technologist! – Goal for next year – more women!",
      "Thanks again to Scitent (Sharon Barker) for helping to sponsor lunch, and thanks to Dr. Marmorstein for also pitching in generously. Thanks most of all to you, the community, for sharing your ideas, showing your craft, and helping to build Cvilles rich tech community.",
      "Here are a couple of shots I snapped at the event:",
      "See you guys next year!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": "OpenSource Connects was pleased to host this years beSwarm. We had 36 people sign up and from the turn out at the event, most of those who signed up came. We..."
  },
  {
    "id": "8152df8dbc4be03b63ae7c26dd6e778b",
    "url_s": "https://opensourceconnections.com/blog/2013/03/14/finally-a-hadoop-hello-world-that-isnt-a-lame-word-count/",
    "title": "Finally! A Hadoop Hello World that isnt a Lame Word Count!",
    "content": [
      "So I got bored of the old WordCount Hello World, and being a fairly mathy person, I decided to make my own Hello World in which I coaxed Hadoop into transposing a matrix!",
      "What? Whats that you say? You think that a matrix transpose MapReduce is way more lame than a word count? Well I didnt say that we were going to be saving the world with this MapReduce job, just flexing our mental muscles a little more. Typically, when you run the WordCount example, you dont even look at the java code. You just pat yourself on the back when the word \"the\" invariably revealed to be the most popular word in the English language.",
      "The goal of this exercise is to present a new challenge and a simple challenge so that we can practice thinking about solving BIG problems under the sometimes unintuitive constraints of MapReduce. Ultimately I intend to follow this post up with exceedingly more difficult MapReduce problems to challenge you and encourage you to tackle your own problems.",
      "So, without further adieu:",
      "The Matrix Transpose Problem",
      "The matrix transpose is a pretty simple concept. Lets say that you have some matrix M. Here are the rows of M (preceded by the row number):",
      "0    7 9 3 6\n1    4 2 9 8\n2    4 6 6 1",
      "You transpose this matrix by \"flipping\" the values about the diagonal. Here is transpose(M):",
      "0    7 4 4\n1    9 2 6\n2    3 9 6\n3    6 8 1",
      "For tiny matrices like this, transpose is trivial, but for giant, super-jumbo Big Data matrices this can be challenging to do in the constraints of one machines RAM. Thus, the matrix transpose is a good candidate for MapReduce.",
      "Outline of a Simple MapReduce Job",
      "As you may know by now, MapReduce is a two phase process composed of a mapping phase and a reducing phase. Simplistically, in the mapping phase, the mapper is given the contents of a Hadoop directory one bit at at time as a key-value pair. In this example, let the key be the rowIndex and the value be the values associated with that row.",
      "{0: [7 9 3 6]}\n{1: [4 2 9 8]}\n{2: [4 6 6 1]}",
      "It is the goal of the mapper to consume this information, process it, and then emit a number of other key-value pairs K and V. The keys and values can be of any type that you like, and for every iteration of the map function you can emit as many {K: V} pairs as you wish.",
      "Between the mapping phase and the reducing phase, all of these values V are grouped according according to their associated keys K. Then, one at a time, the reducer is given a key K, along with all of the corresponding values V[]. The goal of the reducer is then to consume this information, process it, and emit more key-value pairs. In the case of our example, this new set of key-value pairs will represent the transpose of the original matrix. The keys will be transposeRowIndexs and the associated values will be the associated elements in that row which we refer to as transposeValuess. So, for the example of this post, the reducer will return:",
      "{0: [7 4 4]}\n{1: [9 2 6]}\n{2: [3 9 6]}\n{3: [6 8 1]}",
      "The goal of the game, then, is simply to write two functions, map and reduce, which achieve all of the above:",
      "map(rowIndex,values) :\n  #DO STUFF\n  return {K:V}\n\nreduce(K,V[]) :\n  #DO STUFF\n  return {transposeRowIndex: transposeValues}",
      "So, how do you implement this? Think about it! No peeking.",
      "…",
      "My Solution",
      "Here is my solution",
      "map(rowIndex,values) :\n  for(columnIndex = 1 to length(values)) :\n    K = columnIndex\n    V = {rowIndex: values[columnIndex]}\n  return {K:V}\n\nreduce(K,V[]) :\n  transposeValues = []\n  for(x in V[]) :\n    transposeValues[x.key] = x.value\n  transposeRowIndex = K\n  return {transposeRowIndex: transposeValues}",
      "And if we run our MapReduce algorithm, during the mapping phase we get",
      "map(0,[7 9 3 6]) #=> {0:{0,7}},  {1:{0,9}},  {2:{0,3}},  {3:{0,6}}\nmap(1,[4 2 9 8]) #=> {0:{1,4}},  {1:{1,2}},  {2:{1,9}},  {3:{1,8}}\nmap(2,[4 6 6 1]) #=> {0:{2,4}},  {1:{2,6}},  {2:{2,6}},  {3:{2,1}}",
      "In the \"shuffle and sort\" time before the reduce phase we cluster the values together according to key",
      "{0:   [{0,7},  {1,4},  {2,4}]}\n{1:   [{0,9},  {1,2},  {2,6}]}\n{2:   [{0,3},  {1,9},  {2,6}]}\n{3:   [{0,6},  {1,8},  {2,1}]}",
      "And finally, during the reducing phase we get",
      "reduce({0: [{0,7},{1,4},{2,4}]})  #=>  {0: [7 4 4]}\nreduce({1: [{0,9},{1,2},{2,6}]})  #=>  {1: [9 2 6]}\nreduce({2: [{0,3},{1,9},{2,6}]})  #=>  {2: [3 9 6]}\nreduce({3: [{0,6},{1,8},{2,1}]})  #=>  {3: [6 8 1]}",
      "And now since weve tested out the algorithm, its time to transpose something a bit larger…",
      "In Practice",
      "Im putting together a collection of Hadoop tutorials and Im calling it Hadoopadoop. Check it out! Once you download it, you can quickly run my MatrixTranspose example by issuing the command ./build.sh MatrixTranspose. But please dont let this just be like running the typical WordCount example. Instead, take a look at the build script and see what its doing to build the project. Im intentionally keeping everything as lightweight as possible. No extra libraries, no Maven, just some java files, some data zips, and a little bit of bash wrangling.",
      "The actual MatrixTranspose/MatrixTranspose.java code is pretty simple. Youll find that Im not using any bells and whistles to get the job done. Besides a little extra code to parse and assemble the input and output text, Im basically stating in code the same thing as I have stated above. However, there are still plenty of things to improve upon. This follow-up article describes such improvements as using a creating a custom combiner to reduce the network traffic during the shuffle/sort phase.",
      "While youre looking at my Hadoopadoop, take a glance at the WordCount tutorial that Ive adapted from the WordCount example that ships with Hadoop. Lame though it may be, its still remains a great, and simple example of MapReduce.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "4043a014aa79aee245b0c9cabb74b281",
    "url_s": "https://opensourceconnections.com/blog/2013/03/14/getting-started-quickly-with-hadoop-and-mapreduce/",
    "title": "Getting started quickly with Hadoop and MapReduce",
    "content": [
      "So heres the problem: Youve finally found a block of time to set down and get your head around Hadoop and MapReduce. You do a quick Google search for a tutorial to get your started and immediately, your problems are two-fold:",
      "You are a 23 step process and a cloud deployment away from having your first Hadoop cluster spun up.\n  The most interesting thing you will be able to do once you get your cluster up and running is to count all the words in the complete works of Shakespeare. Ho…hum.",
      "Well, if this is your situation, youll be please to find that the first problem goes away immediately upon downloading Hadoop. Doug Cutting in his infinite wisdom understood that it was intimidating to spin up an entire cluster just so that you can get started learning the platform; because of this he built in a little feature that allows you to get started immediately. As an example, lets say you have a giant 137 core cluster in the cloud and youve stored the complete and unabridged works of all the classic authors on HDFS in the books directory. You can run your WordCount MapReduce on the corpus and send the results to the words directory with the following command:",
      "${HADOOP_HOME}/bin/hadoop jar WordCount.jar org.myorg.WordCount books words",
      "On the other hand, if you have no such cluster, but you have Macbeth and Romeo and Juliet stored in the books directory on your local machine, then you can still run your WordCount MapReduce on your measly, wimpy corpus and send the results to the words directory (again, on your local machine) by issuing the exact same command.",
      "${HADOOP_HOME}/bin/hadoop jar WordCount.jar org.myorg.WordCount books words",
      "Pretty easy way to get started, eh?",
      "Issue number 2 is a bit more nefarious. Why? Because word counting is easy to understand and it really is probably the most straight-forward application of MapReduce.",
      "However I got bored of the old WordCount Hello World, and being a fairly mathy person, I decided to make my own Hello World with a mathematical twist! Take a look!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "f965500f91a2ed207cd08c5bdedc7176",
    "url_s": "https://opensourceconnections.com/blog/2013/03/14/using-solrs-new-atomic-updates/",
    "title": "Using Solrs New Atomic Updates",
    "content": [
      "A while ago we created a sample index of US patent grants roughly 700k documents big. Adjacently we pulled down the corresponding multi-page TIFFs of those grants and made PNG thumbnails of each page. So far, so good.",
      "You see, we wanted to give our UI the ability to flip through those thumbnails and we wanted it to be fast. So our original design had a client-side function that pulled down the first thumbnail and then tried to pull down subsequent thumbnails until it ran out of pages or cache. That was great for a while, but it didnt scale because a good portion of our requests were for non-existent resources.",
      "Things would be much better if the UI got the page count along with the other details of the search hits. So why not update each record in Solr with that?",
      "A quick caveat: When were prototyping we store all of the fields in our schema. That gives us flexibility on what we display and doesnt hurt performance too much. Once the dust around the design settles a bit we then look at schema optimization.",
      "So were running Solr 4.x and weve got all of our fields stored. Those are the two requirements for doing atomic updates! Enough with the history, on with the code…",
      "Each bundle of TIFFs comes with an index and a count of the number of pages. That looks like this:\n[gist id=5164194]",
      "The first column is a seven digit grant number, the second column is a DAT catalog number (which we ignore), and the third is the number of pages.",
      "Up next is some shell script that reads in those lines and makes an update request to Solr:\n[gist id=5164172]",
      "The only weird part about this script is the use of arrays and the funky whitespace trimming I had to do on the last element, and the zero-padding I had to do because our grant IDs are eight digits. To run it you just:",
      "./update_solr.sh < 2009_cumulative.csv",
      "Quick and crappy, but it works. If I have to do this again Ill figure out how to batch up the updates or maybe even (sigh) break out the Java code and use the SolrJ interface. In the meantime, it works and Im really glad Solr finally got this feature!"
    ],
    "summary_t": ""
  },
  {
    "id": "95b3e47e2ac1051c6905383b9bb9521e",
    "url_s": "https://opensourceconnections.com/blog/2013/03/15/visualizing-stackoverflow-data-when-does-jon-skeet-answer-questions/",
    "title": "Visualizing StackOverflow Data: When Does Jon Skeet Answer Questions?",
    "content": [
      "Last month we found the best time to ask a question on StackOverflow using the oft-missed ‘join’ feature in Solr.",
      "Numbers will get you far in isolating the top-performing times, but patterns and off-the-chart clusters are lost when scanning lists of numbers.",
      "Information-rich graphics lend themselves to naturally finding patterns and, and the punchcard template view to easily spot data densities grouped around times.",
      "So, let’s set up the visualization.",
      "The first thing we need is data, which we’ll get from Solr. See how to set up a working StackOverflow index here For now, since we’re just using a local instance of Solr, we make a jsonp request to get the data we need.",
      "In a future post we’ll discuss how to set up a single-page search app using EmberJS.",
      "A quick query to:",
      "http://localhost:8983/solr/select\n?q=OwnerUserId:22656\n&fq=PostTypeId:2\n&rows=0\n&facet=true\n&facet.pivot=CreationDay,CreationHour\n&wt=json\n&json.wrf=callback",
      "This returns a summary (facet.pivot=CreationDay,CreationHour) of all of the answers (PostTypeId:2) written by Jon Skeet (OwnerUserId:22656).",
      "D3 is primarily a visualization library; it does not come with data processing tools. Therefore all scrubbing must happen before D3. In this case, Solr provides clean JSON output by selecting wt=json, so our cleaning is reduced to converting UTC dates into Eastern Standard Time.",
      "We set up the visualization by calling draw() from our callback method to Solr. This takes the data, processes it, and then sends it through our visualization. And here is what we get:",
      "",
      "Conclusion",
      "So, what have we learned?",
      "Jon Skeet does in fact sleep! This is kind of a let down considering all the other cool things Jon Skeet can do!\n  Your best chance for getting your question answered by Jon Skeet is somewhere between 8 and 4 EST, Monday through Friday. You can refine your question by using Solr JOIN, an advanced feature similar to a MySQL JOIN.",
      "The best approach to discovering patterns in large data sets is to iteratively investigate new hypotheses. From simple charts and summary statistics to detailed, interactive diagrams, all visualizations provide a unique opportunity to literally see if the data makes sense.",
      "What other questions can you ask the data? What other visualizations would you like to see? Keep track of our progress as we explore this dataset further on Github. For some ideas, check out these related posts:",
      "Indexing StackOverflow In Solr\n  Using Solr Join To Find The Best Time To Ask Questions On StackOverflow"
    ],
    "summary_t": ""
  },
  {
    "id": "9c39245c009e7672b6ea2938a6ee5d05",
    "url_s": "https://opensourceconnections.com/blog/2013/03/17/solr-unleashed-mission-accomplished/",
    "title": "Solr Unleashed – Mission Accomplished",
    "content": [
      "This past Wednesday and Thursday (March 13th and 14th) OpenSource Connections held an on-site 2-day Solr training course called Solr Unleashed. We covered a broad range of Solr topics including beginner level concepts such as indexing and retrieving data, and more advanced concepts such as relevancy tuning.",
      "I know that Im biassed, but I think it went exceptionally well! Take a look at a couple of shots from the course:",
      "Setup Time",
      "Middle of Day One"
    ],
    "summary_t": ""
  },
  {
    "id": "a1f822f27df1eac5288888728d9eb1a3",
    "url_s": "https://opensourceconnections.com/blog/2013/03/18/redesigning-the-smallbizcontracting-homepage/",
    "title": "Redesigning the SmallBizContracting Homepage",
    "content": [
      "OSC came to me with an interesting task: To do a quick refresh of SmallBizContracting.com. The first thing that I did was to visit the site so that I could find out what it was about. What I discovered was that figuring out exactly what the site was and how it could benefit me wasnt all that easy. In fact I was confused by the homepage because even though it provided a lot of information it never explicitly said that the site is a searchable database of businesses. The primary heading told me that I could \"Quickly and easily find small businesses\" but the only search box seemed like an afterthought and was tucked away in the navbar. Without exploring beyond the homepage I could mistakenly assume that this is an informational site about contracting. These initial misconceptions helped to shape my main objectives for the redesign.",
      "",
      "Setting Objectives for the New Site",
      "To quickly explain the content of the site up front (a huge database of companies indexed by their business classifications and NAICS codes). This information is especially important for first-time visitors.\n  To make the benefits of SmallBizContracting.com immediately apparent (easily finding contractors to partner with). In other words: Why should they use this site instead of something else?\n  To direct the user to take the correct initial action (searching to find a specific contracting lead). This helps to ensure that the first business listings a user sees are related to their interests.\n  To make the site usable on more devices. The original site did not scale to accommodate smaller screen sizes.",
      "",
      "Content is King",
      "While reworking the homepage content my goal was to quickly explain the site and then guide the user to the search bar. Accurately defining the site up front is important so that users know what to search for. Instead of using vague terminology, I cut straight to the details:",
      "Easily search over 140,000 small businesses, categorized by NAICS codes and disadvantage classifications.",
      "That heading quickly sums up the site, and gives users a clear frame of reference for interacting with the rest of the content.",
      "Next I restructured the supporting content and broke it into small chunks so that the whole page is much easier to scan. In the new design there is still some explanatory text below the search bar but it feels minimal compared to the old content. For people who would rather browse through the site I added links to the business classifications on the homepage. This makes the classifications much more functional, and because they are in a list it is easier to get a quick overview at a glance.",
      "Putting the search bar in the center of the page makes it unmissable. One subtle design change is that the new search box is automatically selected when you arrive on the homepage. This further emphasizes it, and because it is selected you can easily type in keywords without having to click it.",
      "",
      "Responsive Design",
      "During the last few years mobile web browsing has skyrocketed, making it important for sites to work across many device types. The original site was fixed-width which means that the content did not reflow for small screens. By using Bootstrap responsive I was able to set up some good defaults for resizing elements and made sure that the content would adapt to mobile devices.",
      "",
      "The new mobile site (left) is a huge improvement over the old layout.",
      "The Little Details",
      "Adding variation in the fonts, and contrasting colors made everything more readable. Color is a great way to position a product and to make it unique. The old color palette was stark and made the site feel a little overbearing, so to add contrast I used colors that were more vibrant. Color was also used to de-emphasize the navigation bar and bring more attention to the content and headings. In place of the large illustrations from the previous design I created some custom icons to break up the page and highlight key concepts. The new icons don’t overpower the page, and they serve as visual markers for some of the supporting information.",
      "",
      "My goal for the icons was to keep them simple and unobtrusive while still reinforcing the three main points of the site: contracting leads, targeted results, and small businesses.",
      "In Closing",
      "Overall this redesign improves SmallBizContracting.com’s position as a great resource for finding contracting partners. The new design has a clear focus and the content is easier to understand. Hopefully this glimpse into some of the thoughts behind the redesign shows the important role that content plays in a project.",
      "Go check out the new site, and let us know what you think. http://smallbizcontracting.com/"
    ],
    "summary_t": ""
  },
  {
    "id": "8e65da8d5578be29af91c654c5e9bbfe",
    "url_s": "https://opensourceconnections.com/blog/2013/03/20/how-cloud-computing-is-revolutionizing-pharma-and-genomics/",
    "title": "How cloud computing is revolutionizing pharma and genomics",
    "content": [
      "Yesterday I attended an event hosted by Booz Allen/Amazon around Big Data and Cloud Computing for life sciences. It was a fascinating event that brought together folks from data science, software, and pharma background.",
      "The takeaway was clear: Cloud Computing, Full Text Search, and Big Data are revolutionizing genomics and pharmaceutical research. So much computing power and so much data is in the hands of todays researchers. Tools like Hadoop, Mahout, and Solr stand ready to ensure that the medicine of tomorrow will look nothing like the medicine of today.",
      "Below is a breakdown of the technologies presented.",
      "Illumina links gene sequencers to the cloud",
      "What a computer might have looked like when the human genome was originally sequenced.",
      "Dr. Alex Dickinson from Illumina showed off their BaseSpace cloud platform for genomics. BaseSpace provides a platform for cloud analytics of gene sequences. Instead of needing to create and manage a massive super computing cluster like the original crackers of the human genome, BaseSpace provides as seamless and scalable EC2 backed platform to help researchers and clinicians analyze gene sequences.",
      "BaseSpace transforms the advanced and ivory tower field of genomics computing into something accessible to patients, clinicians, and researchers. They even have an app store for providing custom applications around genomics. So if you want to write an application that utilizes the latest bioinformatics algorithms and tools, it might be a fun weekend hack instead of a million dollar research project. The potential for improving lives and patient outcomes is massive.",
      "I had a fascinating discussion with Dr Dickinson and Sriram Sridhar at Booz about how Lucene might also be a potential tool for analyzing sequences of genes. Lucene provides a set of data structures ideal for dealing with sequences of symbols, allowing efficient lookup and sequence pattern matching of gene sequences. For example, if there was a gene sequence a researcher wanted to use to lookup all the samples that had that sequence, they might be able to index all genes into Lucene and perform a query to lookup those samples. Additionally Lucene/Solr provide an ability to add parameters such as Levenshtein distance to a query, allowing a researcher to lookup DNA sequences very similar to a query sequence. Hopefully this is a conversation we can continue, theres definite potential for analytics tools here.",
      "OpenSource Connections helps develop drug recommender using Big Data and Search",
      "Partnering with LucidWorks and Booz Allen, I worked with a cross-company team to develop a new drug recommender for pharma researchers using Lucids Big Data platform. In the demo, users search for an indication such as \"headache\" against drug product labels from Dailymeds database of product labels. Based on active ingredients already approved for headache, for example, acetimeophan or ibuprofin, the demo uses a similarity metric to identify candidate compounds from the PubChem dataset, scoring each compound with how similar it is to currently approved compounds.",
      "One of these could be the next big headache drug!",
      "Additionally, through natural language processing, weve linked additional data sets such as safety data through FAERS, NIH Clinical Trials, and NIH Funding Reports to give researchers additional information about candidate compounds at a single glance. This part of the project was fun as it incorporated building out some D3 visualizations to give users a responsive, discovery oriented user interface. This demonstrates a core part of our mission at OpenSource Connections – linking backend data structures with rich, user-focused front-ends.",
      "The demo gives researchers powerful leads into potential new areas of research, potentially having a real impact on peoples lives. It demonstrates that full-text search and big data can be brought together to enhance one another, with big data providing a lot of the backend scale, smarts, data structures and search providing a natural invitation for users to interact with large sets of data. This demo also demonstrates how full-text search is more than just a dumb search box. Full-text search provides friendly, natural-language aware ways perform analytics and discovery with features such as facets, deep text analysis, and an ability to index most, if not all, of your data.",
      "Cycle Computing makes supercomping cheap and available",
      "Cycle Computing showed how its platform affordably gathers EC2 spot instances to maximize compututational power to work on computationally hard problems in the pharma space. By doing this, Cycle can capture a massive amount of computational power. Their platform deals with the \"spotty\" nature of these instances – adding fault tolerance and managing the dynamic nature of the instances. Cycles platform can also balances stable local resources in conjunction with spotty cloud resources to provide these services. I dont think anyone puts it better than Cycle themselves:",
      "Utility Supercomputing is the on-demand creation and use of high performance computational environments equivalent to those on the Top 500 Supercomputer list, as a metered, pay for use service.",
      "Cycle provides a nice suite of tools to help manage the large cluster. They showed off how their UI can show you the status of 10000 simultaneously running EC2 instances by showing users a nice grid based visualization. Additional features include deployment integration of Spot instances with Chef. Its fascinating how simple and accessible massive amounts of computational power is becoming. Elastic, super computing capacity is truly becoming an everyday phenomena, and Cycle is showing us how we can maximize whats out there to perform amazing analytics.",
      "To sum up",
      "This was a great event and it was amazing to get to meet professionals from so many diverse backgrounds. Thanks so much to Booz Allen Hamilton and Amazon for hosting the event, I certainly learned a lot. Im really excited about where the future is going to take us now that so much computational power is in the hands of so many smart researchers. The implications are simply revolutionary."
    ],
    "summary_t": ""
  },
  {
    "id": "f978466c923d26004c53f9def30637c3",
    "url_s": "https://opensourceconnections.com/blog/2013/03/24/hdfs-debugging-wrong-fs-expected-file-exception/",
    "title": "Debugging \"Wrong FS expected: file:///\" exception from HDFS",
    "content": [
      "I just spent some time putting together some basic Java code to read some data from HDFS. Pretty basic stuff. No map reduce involved. Pretty boilerplate code like the stuff from this popular tutorial on the topic.",
      "No matter what, I kept hitting my head on this error:",
      "Exception in thread \"main\" java.lang.IllegalArgumentException: Wrong FS: hdfs://localhost:9000/user/hadoop/DOUG_SVD/out.txt, expected: file:///",
      "If you checkout the tutorial above, whats supposed to be happening is that an instance of Hadoops Configuration should encounter a fs.default.name property, in one of the config files its given. The Configuration should realize that this property has a value of hdfs://localhost:9000. When you use the Configuration to create a Hadoop FileSystem instance, it should happily read this property from Configuration and process paths from HDFS. Thats a long way of saying these three lines of Java code:",
      "// pickup config files off classpath\n Configuration conf = new Configuration()\n // explicitely add other config files\n conf.addResource(\"/home/hadoop/conf/core-site.xml\");\n // create a FileSystem object needed to load file resources\n FileSystem fs = FileSystem.get(conf);\n // load files and stuff below!",
      "Well… My Hadoop config files (core-site.xml) appear setup correctly. It appears to be in my CLASSPATH. Im even trying to explicitly add the resource. Basically Ive followed all the troubleshooting tips youre supposed to follow when you encounter this exception. But Im STILL getting this exception. Head meet wall. This has to be something stupid.",
      "Troubleshooting Hadoops Configuration & FileSystem Objects",
      "Well before I reveal my dumb mistake in the above code, it turns out theres some helpful functions to help debug these kind of problems:",
      "As Configuration is just a bunch of key/value pairs from a set of resources, its useful to know what resources it thinks it loaded and what properties it thinks it loaded from those files.",
      "getRaw() – return the raw value for a configuration item (like conf.getRaw(\"fs.default.name\"))\n  toString() – Configuration`s toString shows the resources loaded",
      "You can similarly checkout FileSystems helpful toString` method. It nicely lays out where it thinks its pointing (native vs HDFS vs S3 etc).",
      "So if you similarly are looking for a stupid mistake like I was, pepper your code with printouts of these bits of info. They will at least point you in a new direction to search for your dumb mistake.",
      "Drumroll Please",
      "Turns out I missed the crucial step of passing a Path object not a String to addResource. They appear to do slightly different things. Adding a String adds a resource relative to the classpath. Adding a Path is used to add a resource at an absolute location and does not consider the classpath. So to explicitly load the correct config file, the code above gets turned into (drumroll please):",
      "// pickup config files off classpath\n Configuration conf = new Configuration()\n // explicitely add other config files\n // PASS A PATH NOT A STRING!\n conf.addResource(new Path(\"/home/hadoop/conf/core-site.xml\"));\n FileSystem fs = FileSystem.get(conf);\n // load files and stuff below!",
      "Then Tada! everything magically works! Hopefully these tips can save you the next time you encounter these kinds of problems."
    ],
    "summary_t": ""
  },
  {
    "id": "297a750c3ef06f87cee0c6af47332ab3",
    "url_s": "https://opensourceconnections.com/blog/2013/04/01/opensource-connections-will-be-presenting-at-dc-hadoop-user-group/",
    "title": "OpenSource Connections to present at DC Hadoop User Group",
    "content": [
      "OpenSource Connections is investigating the use of Mahout(/Hadoop) to incorporate recommendations and Latent Semantic Indexing (LSI) into Solr search. In this fast-paced talk well cover the theory behind recommenders and LSI (two-sides of the same coin), well discuss why this project is a Big Data project and well present our approach using a combination of Solr and Mahout (via Hadoop). Our approach to presentation is very conversational, so there will be plenty of opportunities for question and answer.",
      "Speakers",
      "Doug Turnbull loves learning about data structures. As a bit twiddler, Doug spearheaded performance gains on his companys C++ product. Now as a Big Data and Search consultant, Doug enjoys learning about the guts of todays broad variety of NoSQL options and applying that knowledge to customer solutions. Doug enjoys hacking on Python, C++, Java and is an enthusiastic participant on Stackoverflow.",
      "Coming from a background of Aerospace Engineering, John Berryman soon discovered that his true interest lay at the intersections of information technology and entrepreneurship (and when applicable – math). In early 2011, John stepped away from his day job to take up software consulting. Finally John found permanent employment at Opensource Connections where he currently consults large enterprises about full-text search and Big Data applications. Highlights to this point have included prototyping the future of search with the US Patent and Trademark Office, implementing the search syntax used by patent examiners, and building a Solr search relevancy tuning framework called SolrPanl.",
      "Heres a link to the DC Hadoop User Group event site.\n  And heres a link to our presentation."
    ],
    "summary_t": ""
  },
  {
    "id": "04c650840559e78111ded86ccbdc8e89",
    "url_s": "https://opensourceconnections.com/blog/2007/03/01/using-sqlite3-capistrano-mongrel-cluster-oh-my/",
    "title": "Using SQLite3 & Capistrano & Mongrel Cluster oh my!",
    "content": [
      "The biggest slam against using file based databases like SQLite is that they are actual files embedded in your application, and that you have to go through extra hoops when upgrading your application. For example, if you have your production prod.sqlite3 file in ./db, then every time you update your application using Capistrano youll replace your production database with a fresh one!",
      "Well, my first attempt to work around this involved excluding the ./db directory from checkout and then creating a db directory in /shared/db. Symlinking from /shared/db to ./db worked, but was clumsy, and required extra code in my deployment recipe.",
      "A light bulb finally dawned that the path in databases.yml for my production database could be something like:",
      "production:\nadapter: sqlite3\ndatabase: ../../shared/db/prod.sqlite3",
      "Just go up two directories and over to the shared/db directory and locate the database file there! I tried it out, and it worked great. I ran cap deploy_with_migrations and soon enough had a shared sqlite3 database. I then fired up the app with cap cold_deploy and everything ground to a halt :( . I was getting an error about \"database cant be opened. Eventually, after pulling my hair out I discovered that while starting mongrel with ./server/script works great with relative paths to the database, starting with mongrel_rails requires a complete path:",
      "production:\nadapter: sqlite3\ndatabase: /opt/apps/horseshow/shared/db/prod.sqlite3",
      "Victory at last!",
      "To automate the generation of the shared/db directory, just add to your deploy.rb recipe:",
      "desc 'A setup task to put shared system, log, and database directories in place';\ntask :setup, :roles => [:app, :db, :web] do\nrun <<-CMD\nmkdir -p -m 775 #{release_path} #{shared_path}/system #{shared_path}/db &&\nmkdir -p -m 777 #{shared_path}/log\nCMD\nend"
    ],
    "summary_t": ""
  },
  {
    "id": "bea4d464ad25eea4e72f49106096bfc0",
    "url_s": "https://opensourceconnections.com/blog/2013/04/04/complete-n00bs-guide-to-enhancing-solrlucene-search-with-mahouts-machine-learning/",
    "title": "Beginners guide to enhancing Solr/Lucene search with Mahouts Machine Learning",
    "content": [
      "Learn you some Mahout for great good!",
      "Yesterday, John and I gave a talk to the DC Hadoop Users Group about using Mahout with Solr to perform Latent Semantic Indexing – calculating and exploiting the semantic relationships between keywords. While we were there, I realized, a lot of people could benefit from a bigger picture, less in-depth, point of view outside of our specific story. In general where do Mahout and Solr fit together? What does that relationship look like, and how does one exploit Mahout to make search even more awesome? So I thought Id blog about how you too can get start to put together these pieces to simultaneously exploit Solrs search and Mahouts machine learning capabilities.",
      "The root of how this all works is with a slightly obscure feature of Lucene-based search – Term Vectors. Lucene-based search applications give you the ability to generate term vectors from documents in the search index. Its a feature often turned on for specific search features, but other than that can appear to be a weird opaque feature to beginners. What is a term vector, you might ask? And why would you want to get one?",
      "What is a term vector?",
      "To answer the first question, a documents term vector is simply of listing for each term in the entire corpus with how frequent the term occurs in this document. So if our corpus is these three documents:",
      "doc 1: brown dog\n doc 2: brown unicorn\n doc 3: unicorn eats unicorn",
      "We could choose to represent a document by assigning a column to each term in the corpus and a row to each document. The number at each row/column would correspond to the terms frequency in that document. We refer to each row as a term vector for a document and the entire set of term vectors as a term-document matrix. Heres an example of the term-document matrix for the above corpus, containing three term vectors for each of our documents:",
      "doc\n  \n\n  \n    brown\n  \n\n  \n    dog\n  \n\n  \n    eats\n  \n\n  \n    unicorn\n  \n\n  \n    \n      doc1\n    \n\n    \n      1\n    \n\n    \n      1\n    \n\n    \n      0\n    \n\n    \n      0\n    \n  \n\n  \n    \n      doc2\n    \n\n    \n      1\n    \n\n    \n      0\n    \n\n    \n      0\n    \n\n    \n      1\n    \n  \n\n  \n    \n      doc3\n    \n\n    \n      0\n    \n\n    \n      0\n    \n\n    \n      1\n    \n\n    \n      2",
      "As stated above, each documents term vector has a spot for every possible term that occurs in the entire corpus of text. As you can imagine, with real documents in real corpuses with lots of terms this turns into term vectors that are mostly 0. The mostly 0 nature of this matrix leads us to calling these matrices (and their component vectors) sparse.",
      "What can you do with this information?",
      "Using the mathematical concept of a vector has pretty specific implications. If you recall your high school Geometry, youre probably remembering a vector as a kind of arrow coming from the origin of a graph (0,0) with a pointy thingy at the vectors coordinates ((1,2) for example). So youd get something like this. If you learned anything past High School math, you likely got that theres such things as 3D vectors. They have three numbers because in 3D space we have numbers for x, y, and z.",
      "Remember this guy?.",
      "Ok well stretch your mind and think about the term vectors above. These vectors have frequencies for all four terms in the corpus – they are four dimensional vectors. And this is a very trivial example. In our testing with the SciFi Stackexchange data set, there are roughly 36000 terms meaning vectors 36000 wide, with each individual vector mostly 0s. Luckily the same math that applied to two and three dimensions can be applied to vectors of any dimensionality.",
      "Now that weve brought it back to our high school math, lets remember what kinds of things we learned about vector back then:",
      "We can take the dot product of two vectors (x1x2 + y1y2 + z1*z2 … for more dimensions)\n  calculate vector magnitude sqrt(x1^2 + y1^2 ... )\n  Add two vectors [x1+x2, y1+y2, … ]\n  etc etc\n  Hazy memories of some trig stuff\n  and a guy name Oscar that always wanted to copy my homework",
      "When you realize that the x,y,z… above are term frequencies in a term vector, vector math turns out to have certain very practical implications. Lets think through an example that might awaken your dormant math neurons! When term vectors point in the same direction in highly dimensional space, its because theres a lot of terms that overlap between those documents. One way to figure that out is the dot product. For example consider the dot product of document 1 (\"brown dog\") and document 3 (\"unicorn eats unicorn\"). We can detect theres no overlap by taking the dot product of the two four-dimensional vectors:",
      "doc\n  \n\n  \n    brown\n  \n\n  \n    dog\n  \n\n  \n    eats\n  \n\n  \n    unicorn\n  \n\n  \n    \n      dotprod(doc1,doc3)\n    \n\n    \n      1*0+\n    \n\n    \n      1*0+\n    \n\n    \n      0*1+\n    \n\n    \n      0*2\n    \n\n    \n      ==0",
      "Compare this to the dot product of two documents that overlap, document 2 (\"brown unicorn\") and document 3 (\"unicorn eats unicorn\") then the \"unicorn\" parts of the dot product amplify each other, letting us know of some similarity:",
      "doc\n  \n\n  \n    brown\n  \n\n  \n    dog\n  \n\n  \n    eats\n  \n\n  \n    unicorn\n  \n\n  \n    \n      dotprod(doc2,doc3)\n    \n\n    \n      1*0+\n    \n\n    \n      1*0+\n    \n\n    \n      0*1+\n    \n\n    \n      1*2\n    \n\n    \n      ==2",
      "Pretty cool, huh! This is a pretty minor example. If you like finding practical ways to exploit cool math, heres an area just for you. It gets even cooler when you work with our term vectors as if its a matrix, and heck theres really neat stuff you can do with that too!",
      "So where do I go to get gobs of good math to apply to giant vectors/matrices?",
      "This is where Mahout steps in",
      "Mahout is a machine learning library built on top of Hadoop. Machine learning sounds hard, but its just marketing speak for math you dont know yet. Hadoop is a map reduce framework for managing jobs and a storage system to work on gigantic data sets. So a more realistic term-document matrix, maybe one with one billion documents and one million terms might actually be usable.",
      "Mahout has all the tidbits, advanced and beginner, that can help us do very interesting machine learning (read:math) processing on our giant term-document matrices. For example we can cluster the rows, use similarity metrics to calculate distances between our term vectors. We can fill in some of the empty spots in our sparse vectors by detecting relationships between terms (\"banana\" and \"peel\" commonly co-occur, lets score this document for banana too!) and much much more.",
      "Each of these processes has its own mathematics behind it. Many of these processes like singular-value decomposition and K-means clustering you can get at least get a basic appreciation for what its doing. With a little work you can get your head around the math by brushing off some old textbooks. Weve also got a great presentation on Singular Value Decomposition, which might be a great place to equip some machine learning with your search application.",
      "Get Started!",
      "So great, where do you start playing around using Mahout with your term vectors? Youll need to get term vectors into a format that Hadoop/Mahout can understand. Mahout gives us a nice utility called lucene.vector for dumping term vectors from a Lucene index and saving the information you need to feed data back into the search index. From this point on, the sky is the limit. Learn some cool math and machine learning and find interesting/fun ways you can play with your term vectors! Have fun!"
    ],
    "summary_t": "Yesterday, John and I gave a talk to the DC Hadoop Users Group about using Mahout with Solr to perform Latent Semantic Indexing – calculating and exploiting ..."
  },
  {
    "id": "507cbf097667ee101edfa8be91092076",
    "url_s": "https://opensourceconnections.com/blog/2013/04/10/modifying-solr-result-relevancy-via-an-auxiliary-boost-field/",
    "title": "Modifying Solr Result Relevancy via an \"Auxiliary Boost\" Field",
    "content": [
      "English is a confusing language. I mean, does it really make sense that you can park in a driveway or drive in a parkway? Also, Ive always been amused that there actually exists a class of words that are their own antonym – so called \"auto-antonyms\":",
      "cleave – 1] Split or sever (something) 2] Stick fast to\n\n  awful – 1] worthy of awe 2] very bad\n\n  to overlook – 1] to inspect 2] to fail to notice",
      "Unfortunately, the confusing nature of English (and of all natural languages) sometimes has consequences that can affect our bottom line. Consider a situation that Zappos! was facing sometime back with their search results: If I am looking for a pair of \"dress shoes\", then what should I expect to see?",
      "You would expect that I would see a page full of brown or black leather shoes right? Unfortunately Solr had some different opinions. By default, the page was filled not only with dress shoes, but with sundresses, and tennis shoes, and with dress pants! And in some ways it makes sense, right? Under the hood, Solr is really just a sophisticated and performant token matching engine.",
      "Fortunately for Zappos, much of their problem was alleviated by boosting higher on phrase matches. So that if \"dress\" and \"shoes\" occurred next to each other in text, then that document would rise toward the top. However, some e-commerce sites have a great deal of difficulty with this problem and it drives them toward extreme and even somewhat detrimental approaches. For instance, some companies build in special case solutions – bandaid solutions – so that if they see a particular query string then they completely circumvent their search engine and provide a hand tailored set of results. This is a very brittle approach because with every update to the inventory, with every new partnership, and with every new advertising campaign, someone must review each of these bandaid fixes and make sure they are still relevant.",
      "Theres a better approach, beautiful in its simplicity and its flexibility. Solr, and ElasticSearch view each item in your inventory as a document which has various fields which correspondingly have their own values. So for Zappos, a document might contain a SKU, an item name, a brand name, a description, and a price. But theres no reason that you cant include additional fields that are used to modify the relevancy of a particular document in a particular search. We call these fields auxiliary boosting fields and they work like this: Consider again the dress shoes problem. If every document in your index has two additional fields, AuxiliaryBoost and AuxiliaryBust, then we can tightly control the search results and the way they are sorted. As a merchandizing expert, if you see a document that should not appear in the search results, a sundress for example, then you add the offending query string to the AuxiliaryBust field. Accordingly, if you find a document that really should be sorted higher in the result set, then you add the query string to the AuxiliaryBoost field. The final piece of this puzzle is a slight modification that you make to the actual query that goes to Solr. To get rid of all bad results you add a filter query to remove those documents that have a match in the AuxiliaryBust field:",
      "fq=-AuxiliaryBust:(dress AND shoes)",
      "To promote those documents that really deserve to be at the top, you simply add the AuxiliaryBoost field to the set of fields that youre searching over and apply appropriate boosting.",
      "qf=SKU^10 ItemName^5 ItemDescription^3 Brand^4   AuxiliaryBoost^1\npf=ItemDescription^3   AuxiliaryBoost^2",
      "",
      "Now, if youre a merchandizing expert reading this, youre probably becoming upset again at this point because you have no easy way of adding fields or of modifying the text they contain. Furthermore, if you have to adjust boosting of particular fields, your hands are equally tied. We have recognized this issue over and over again and as a result we are in the process of building SolrPanl – a merchandizer-facing search behavior dashboard. As a merchandizer, SolrPanl will allow you to create a test case of \"troubled searches\" to monitor and modify. If you see a search that has particularly bad results then you will be able adjust the boosting of various fields with a simple UI composed of sliders and selection boxes. As you modify these parameters, you can see immediately how the search results are effected. (In the past, you would have to tell your tech team to make a modification and then check back later to see the results.) If you find that a document appears lower in a particular search result set than it should, then we will provide you the tools to understand why that is happening. Finally, you will also be able to modify the documents directly by adding query strings to fields such as AuxiliaryBoost and AuxiliaryBust. You can even do simple things such as fixing typos!",
      "If youre interested, then please follow our ongoing development of SolrPanl here. Also, ask us about becoming a beta tester!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "8c9135dd47496a1b39a0853a3de5d4bf",
    "url_s": "https://opensourceconnections.com/blog/2013/04/13/how-to-debug-solr-with-eclipse/",
    "title": "How to Debug Solr With Eclipse",
    "content": [
      "Next Stop Debuggersville!",
      "Recently I was puzzled by some behavior Solr was showing me. I scratched my head and called over a colleague. We couldnt quite figure out what was going on. Well Solr is open source so… next stop – Debuggersville!",
      "Running Solr in the Eclipse debugger isn’t hard, but there are many scattered user group posts and blog articles that you’ll need to manually tie together into a coherent picture. So let me do you the favor of tying all of that info together for you here.",
      "First step is of course to grab the source code for the version of Solr you want to debug. Unpack this somewhere, and browse to the top directory. Let’s get comfortable with the directory structure. Most importantly, I want to point out these three pieces:",
      "build.xml – top level Solr/Lucene ant build.xml file\n  lucene/ – directory containing Lucene’s source\n  solr/ – directory containing Solr’s source",
      "The takeaway here is that it’s important to note that Solr and Lucene are deployed together. This has a lot of conveniences, but is a bit confusing. We think we just downloaded Solr, but really we have Solr and Lucene bundled together. To help keep things clear, I’m going to call the whole shebang we just downloaded the Solr/Lucene source tree and I’ll call the solr/ subdirectory the Solr source tree.",
      "My Solr/Lucene Source Tree",
      "OK enough nonsense, let’s get started. Browse to the top-level Solr/Lucene source tree in your command line and lets bootstrap the Solr/Lucene Eclipse project.",
      "~/workspace/solr-4.2.1$ ant eclipse",
      "Once this completes, load up Eclipse, and select File -> Import. Select Existing Eclipse Project from the dialog and browse to the top of your Solr/Lucene source tree. Once this directory is selected, click \"Finish\". Now you have an Eclipse project with both Solr and Lucene.",
      "Import Solr/Lucene Into Eclipse",
      "Now back to the command prompt, we’ll go into the Solr source tree and build the example/ directory that holds the Solr/Jetty web application. The same example/ directory distributed with Solr`s binaries.",
      "~/workspace/solr-4.2.1$ cd solr/\n~/workspace/solr-4.2.1/solr$ ant example",
      "Once the example app has been built, we can cd into the example directory, and use start.jar in the same way we would with the binary distribution of Solr. We can still include the usual solr arguments for items such as changing solr.home, etc. The only difference here is we’re going add Java arguments needed for debugging.",
      "~/workspace/solr-4.2.1/solr$ cd example\n~/workspace/solr-4.2.1/solr/example$ java -jar -Xdebug \\\n     -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=1044 \\\n     start.jar",
      "Because we set suspend=y, Java is waiting for us to attach our debugger before beginning execution. Switch back to Eclipse. Right click on your Solr/Lucene Java project and select Debug As and then Debug Configurations. Under the Remote Java Application category. Click New to create a new debug configuration. Enter in the port we just specified to Java at the command line – 1044. Hit Apply and Debug. Over in the command line you should see Solr starting up.",
      "Debug Config For Attaching to Example Web App",
      "Huzzah! Youve done it! You should now be able to set breakpoints throughout the Solr/Lucene code base, step through code, and do all the other fun stuff you can do with any other hunk of source code. In the future, you can simply recall this debug configuration in the \"Debug As\" menu and Eclipse will attempt to attach to any Solr instance launched with Javas debug commands. Have fun hacking on Solr!"
    ],
    "summary_t": "Recently I was puzzled by some behavior Solr was showing me. I scratched my head and called over a colleague. We couldnt quite figure out what was going on. ..."
  },
  {
    "id": "5b1cc25a63c0f8fcab69e00d4d64ab12",
    "url_s": "https://opensourceconnections.com/blog/2013/04/15/querying-more-fields-more-results-stop-wording-and-solrs-mm-min-should-match-argument/",
    "title": "Querying More Fields != More Results – Stop wording and Dismaxs mm (min should match) argument",
    "content": [
      "Let’s recall from Anatomy of a Dismax Query some key components to the dismax query parser:",
      "qf – the fields we will search over (we’ll take the highest score out of all the fields that match)\n  mm – the minimum number of fields that MUST match the query",
      "OK, now weve had plenty of time to study John’s post (and hey you should be able to even debug Solr). Let’s take our new knowledge for a test drive with this puzzler: Why would adding a field to qf cause our result set to actually shrink in size? Consider these two Solr queries:",
      "(A) http://localhost:8983/solr/select?\n    q=captain+of+enterprise&qf=body&mm=3&defType=dismax\n\n(B) http://localhost:8983/solr/select?\n    q=captain+of+enterprise&qf=title+body&mm=3&defType=dismax",
      "The only difference between A and B is qf. Query B adds \"title\" to qf.",
      "Why would query A return more results than query B? In query B we added a field, so shouldnt there be more fields to match on and therefore more documents? Not necessarily as it turns out. Why? Well lets start with something that might help us solve this problem: body is stop worded at query time. Title is not. Well lets dig a little deeper. Lets set debugQuery=true to take a gander at what’s happening under the hood with query parsing & analysis. When we dig into query parsing, A and B turn into the following two dismax queries:",
      "(A) +((body:captain body:enterprise)~2)\n\n(B) +(((title:captain | body:captain)\n       (title:of) (title:enterprise | body:enterprise))~3)",
      "Notice how in both cases body’s stop wording has removed our search for \"of\" in the body field. In query A this reduces mm to 2, as dismax nicely figures out that after stop wording, we only have 2 clauses in our query – \"body:captain\" and \"body:enterprise\".",
      "What has the addition of an extra field done in B? Well notice it’s introduced a 3rd clause between \"captain\" and \"enterprise\". Query-time stop wording has removed \"body:of\". However title is not stop worded. Therefore, Solr can still potentially match on \"title:of\" so this component of the middle clause stays in place.",
      "The result of query parsing is that now we have a mandatory clause requiring title to have \"of\". Therefore, the result set for query B is limited to the number of titles that have \"of\" in them. If no titles have \"of\" in them, then we’ll get no results.",
      "This sounds like an unlikely scenario, but consider if instead of \"title\" you have another field. Something with a very tightly controlled vocabulary. Something like, titles of laws. Then you could hit this problem very easily.",
      "Solutions?",
      "It’s a bit hard to figure out what’s expected of Solr in this case. Should the \"title:of\" query be mandatory? Should it be coupled with a \"body:?\" clause that will match on any term in body (effectively letting body off the hook?).",
      "As a user, it doesnt seem to make sense to avoid stop wording entirely just to avoid this behavior. It’s a useful tool. More importantly, I feel that we probably want dismax to continue to be able to search over heterogeneous fields with their own analysis chains. Why should the behavior of dismax constrain how we decided to slice up individual fields?",
      "Nevertheless, one takeaway is clean – don’t get aggressive with mm. Think carefully about mm in terms of the percentage of stopwords you’ll likely encounter – realizing that might upgrade some parts of the dismax query to even more mandatory than they are. For long queries q=Where in the world is Carmen Sandiego? this could be quite a few stopwords. For short queries, you’re likely to encounter few stop words like in the query q=Carmen Sandiego. Luckily Solr lets us control mm as a function of the number of clauses in the query.",
      "Id love to get your thoughts though. Have you encountered this issue before in the wild? How have you solved it?"
    ],
    "summary_t": ""
  },
  {
    "id": "ef8ef865d490dff6585e0a4b4e1c92a1",
    "url_s": "https://opensourceconnections.com/blog/2013/04/21/beginner-tips-for-elastic-mapreduce/",
    "title": "Beginner Tips for Elastic MapReduce",
    "content": [
      "By this point everyone is well acquainted with the power of Hadoops MapReduce. But what youre also probably well acquainted with is the pain that must be suffered when setting up your own Hadoop cluster. Sure, there are some really good tutorials online if you know where to look:",
      "here is a great one for setting up a single node cluster\n  and here is the equally good follow-on tutorial for setting up a complete cluster",
      "However, Im not much of a dev ops guy so I decided Id take a look at Amazons Elastic MapReduce (EMR) and for the most part Ive been very pleased. However, I did run into a couple of difficulties, and hopefully this short article will help you avoid my pitfalls.",
      "Elastic MapReduce, Step by Step",
      "Elastic MapReduce makes everything simple. In just a few steps and in just a few minutes, you can have your own MapReduce job running on a cluster of whatever size and configuration youd like. First you just log on to your AWS console, navigate to the Elastic MapReduce page, and click the \"Create New Job Flow\" in the top left of the main panel.",
      "",
      "Next you name your job and indicate that you will supply your own custom jar.",
      "",
      "On the next screen you tell EMR exactly where to find your jar file. This is going to be a path that starts with the name of an S3 bucket that you have access to in AWS. You also specify the inputs to your job. For most people this will likely include the path to an input directory and the path to an output directory (as Ive indicated here), but you can use this space to include any argument youd like. There are a couple of potential gotchas here. First, if you did not specify a main class in your jar manifest, then the fully qualified class must be the first argument here. Second, if you are pointing to directories in S3, make sure that you include the S3 scheme at the front of the URI: \"s3://yourBucket/yourDirectory\"",
      "",
      "The next screen is where you get to experience the awesome efficiency of Elastic MapReduce. No futzing with hackneyed deployment scripts; you simply select the type of machine you want for the master, the type and number of machines you want for slaves. As you can see here, Im building a 1021 machine cluster of the largest machines that AWS has to offer! (…And then after I took this screen shot I got scared and returned it to a 3-machine cluster of \"smalls\".)",
      "",
      "The next screen has a lot of goodies that its well worth paying attention to. If you ever want to be able to ssh into the cluster youre building – which is often a good idea – then you must make sure to specify an Amazon EC2 Key Pair. While youre debugging the setup, its also a good idea to set a directory to send log files to. And while I havent yet found it as useful, its probably also a good idea to enable debugging. And finally, the ssh key is of no use if the job has failed and AWS has shut down your cluster, so click the \"Keep Alive\" option in case you need to get into the machine and do a post-mortem. As well see in a little bit, being able to ssh into the Hadoop master is also useful for running experiments in the same environment that that your job will run.",
      "",
      "While, I havent had need of them quite yet, the next screen gives you the option to \"bootstrap\" your Hadoop cluster. So, for instance, you can install software, run custom scripts, or tweak the nitty-gritty configuration parameters of Hadoop itself. For the power user, this should be quite useful.",
      "",
      "In the final screen, Elastic MapReduce allows you to review the configuration before you push the big \"Create Job Flow\" button and things get rolling. Once you create your job flow, then you can check back at the main screen of Elastic MapReduce and watch as your cluster builds, runs, and completes – or in my case FAILS!",
      "Avoiding my Pitfalls in Elastic MapReduce",
      "So lets get into that discussion: What are some of the common pitfalls, and how can you avoid them? Above, Ive already described a couple: make sure to specify the main class of your jar file (if its not already specified in the jars manifest), and since youre probably pulling from and pushing to S3, make sure to use fully qualified URIs including the S3:// scheme.",
      "That last one caused me some pain. Lets take a look at how I tracked down my problem and hopefully it will help you in the future when you are tracking down your own problem.",
      "Notice the big \"FAILED\" message for my jobs state. That was my first clue that something might be wrong.",
      "",
      "Now notice the \"Debug\" button in the header of the table. By clicking on this button, youll have access to several useful debugging files.",
      "",
      "In my case, the \"stderr\" file was the most helpful. This is the stderr output that occurred while running the job – so youll get to see where the error occurred in your java code.",
      "For me, the error was about a Null value that basically indicated that a file did not exist. This was troubling because Id run the job several times locally with no problem, and Id double checked, and these files were exactly where (I thought) Id specified. So something was different between my local environment and the environment of my Elastic MapReduce job. But what?!",
      "Fortunately, Id specified an ssh key and indicated that the cluster should be kept alive after running or failing the job, so I could easily ssh onto the box. But heres where debugging gets a little tricky. The error had occurred deep within my code, and I didnt really want to make little tweaks in the java on my local machine, jar everything up and, and send it to the remote machine. The turn around for this process would have been 15 minutes and I would be destroying my code in the process.",
      "Instead, I opted to write Java on the remote machine for a relatively fast debugging cycle. I found that I could use vim to replicate the problematic portion of my code, and in a single line of bash, I could compile my code (Test.java), jar it (as Test.jar), and run it against Hadoop. Heres my magic line:",
      "javac -cp $HADOOP_HOME Test.java;  jar cf Test.jar Test.class;  hadoop jar Test.jar Test s3://myBucket/myDirectory",
      "And, as it turns out, the bug I was having was because Id used the default Hadoop FileSystem",
      "FileSystem fs = FileSystem.get(conf);",
      "Instead, I should have retrieved the filesystem from the input path, which takes into account that were talking with S3",
      "FileSystem fs = new Path(\"s3://myBucket/myDirectory\").getFileSystem(conf);",
      "And finally, there was one more gotcha in my case that might be of help for you. Its important to know that all JVMs are not equal! Im doing image processing, so Im making heavy use of the Java Advanced Imaging (JAI) API. On my local machine, this comes with the JVM, however the JVM used by Elastic MapReduce doesnt have these dependencies (and possibly doesnt have any of the javax dependencies). In order to fix this problem I had to pull down these extra dependences and make sure they were bundled into my jar file. But, once I did that, it ran as smooth as you please!",
      "All in all, I really like Elastic MapReduce. It gives you the ability to get to work fast without having to worry much about configuration or administration tasks.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "6cc39d60bca6b45ee990261cc334e7fd",
    "url_s": "https://opensourceconnections.com/blog/2013/04/25/understanding-solr-soft-commits-and-data-durability/",
    "title": "Understanding Solr Soft Commits and Data Durability",
    "content": [
      "I ran into an interesting problem today. I was working with the first project where we legitimately needed Solr soft commits and in testing my configuration I wanted to prove to myself that the soft commits were performing as expected. Namely, I expected soft commits to flush all added documents to an in-RAM index so that they would appear in search results. Furthermore, and importantly, I expected soft commits not to flush the indices to disk. Disk IO is expensive and since we are very IO constrained in this project, my main goal was to prove that soft commits were not writing to disk. The problem was – I couldnt prove it! At least not at first. However, by the time I finally understood what was happening I had also gained a much better understanding of how Solrs new soft commits actually work. In the remainder of this post I will walk you through my experimentation and have you think through it with me as if you were setting beside me during my experimentation.",
      "Heres the setup: In Solrs solrconfig.xml, I indicated that I wanted Solr to soft commit all pending documents every 10 seconds. Then at 30 seconds, I wanted to hard commit.",
      "<updateHandler class=\"solr.DirectUpdateHandler2\">\n\n  <updateLog>\n    <str name=\"dir\">${solr.ulog.dir:}</str>\n  </updateLog>\n\n  <autoCommit>\n    <maxTime>30000</maxTime>\n  </autoCommit>\n\n  <autoSoftCommit>\n    <maxTime>10000</maxTime>\n  </autoSoftCommit>\n\n</updateHandler>",
      "I used SolrJ to load about a 100 documents into Solr and then I jumped over quickly to a browser and periodically issued a series of \"select all\" query to solr. Heres about how it went:",
      "Time 0:01 – no documents\n  Time 0:02 – no documents\n  Time 0:03 – no documents\n  Time 0:04 – no documents\n  Time 0:05 – no documents\n  Time 0:06 – no documents\n  Time 0:07 – no documents\n  Time 0:08 – no documents\n  Time 0:09 – no documents\n  Time 0:10 – 100 documents",
      "So far so good. Soft commit appeared to be working; I just needed verify that nothing was written to disk. I immediately stopped Solr (a control-C in the terminal running Solr), and then restarted Solr and what did I see? – Documents – 100 of them!",
      "\"So,\" I said grumpily \"Solr is actually hard committing when Im telling it to soft commit!\" But then it occurred to me that maybe Solr was doing something smart to protect me from data loss. \"What about that updateLog thing I see in the updateHandler?\"",
      "After tracking down the log files, (theyre stored sibling to the data directory), I found that, sure enough, they are keeping track of all the items that havent yet been hard committed to Solr. So, first lesson:",
      "Whenever Solr wakes up, it looks for these logs and replays them back to recover from any potential data loss.",
      "So, now it was clear what I had to do. I spun up the same experiment, I submitted about 100 docs to solr, and I kept refreshing the solr \"select all\" query until after 10 seconds my documents appeared. I then stopped Solr (control-C) and this time, smiling smugly, I deleted the log files and restarted Solr, resubmitted my \"select all\" query and found … 100 documents.",
      "At this point, words dont readily express the confusion that I felt. However, this emoticon does a pretty good job:",
      ":-/",
      "Im becoming upset that I was wrong and still concerned that maybe Solr was writing my soft commits to disk. Then it occurred to me that perhaps my method of killing Solr – control-C – was simply not violent enough.",
      "I created one more experiment. This time I",
      "turned on Solr\n  submitted 100 or so documents\n  re-queried \"all docs\" repetitively until after 10 seconds they appeared\n  then I killed the task with a kill -9 <processId>\n  I removed the log files\n  I restarted Solr",
      "And finally, once I again started Solr and queried for all documents – there were none! So, my second lesson for the day:",
      "When Solr is shut down politely, it does you the favor of hard committing all outstanding documents to disk.",
      "And so my final assessment of the situation is that not only do the soft commits perform as expected, but Solr does several things to make sure youre not losing any of your data.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "2f7a4487ea44e890d5b8d05317e93079",
    "url_s": "https://opensourceconnections.com/blog/2013/04/28/indexing-millions-of-documents-using-tika-and-atomic-update/",
    "title": "Indexing Millions of Documents using Tika and Atomic Update",
    "content": [
      "On a recent engagement, we were posed with the problem of sorting through 6.5 million foreign patent documents and indexing them into Solr. This totaled about 1 TB of XML text data alone. The full corpus included an additional 5 TB of images to incorporate into the index; this blog post will only cover the text metadata.",
      "Streaming large volumes of data into Solr is nothing new, but this dataset posed a unique challenge: Each patent documents translation resided in a separate file, and the location of each translation file was unknown at runtime. This meant that for every document processed we wouldnt know where its match would be. Furthermore, the translations would arrive in batches, to be added as they come. And lastly, the project needed to be open to different languages and different file formats in the future.",
      "Our options for dealing with inconsistent data came down to: cleaning all data and organizing it before processing, or building an ingester robust enough to handle different situations.",
      "We opted for the latter and built an ingester that would process each file individually and index the documents with an atomic update (new in Solr 4). To detect and extract the text metadata we chose Apache Tika. Tika is a document-detection and content-extraction tool useful for parsing information from many different formats.",
      "On the surface Tika offers a simple interface to retrieve data from many sources. Our use case, however, required a deeper extraction of specific data. Using the built-in SAX parser allowed us to push Tika beyond its normal limits, and analyze XML content according to the type of information it contained.",
      "First, we define the schema to use in Solr.",
      "Our two copy fields, text_en and text_cn will contain all of the text that is searchable by default, and we will match each document by id. We also set up two separate field types for the different languages.",
      "<field name=\"id\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\"/>\n<field name=\"text_en\" type=\"text_general\" indexed=\"true\" stored=\"false\" multiValued=\"true\"/>\n<field name=\"text_cn\" type=\"text_general_cn\" indexed=\"true\" stored=\"false\" multiValued=\"true\"/>",
      "Next we specify the actual patent metadata; first in English and then in Chinese.",
      "<field name=\"invention_title_en\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n<field name=\"inventors_en\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n<field name=\"abstract_en\" type=\"text_general\" indexed=\"true\" stored=\"true\" />\n...\n\n <field name=\"invention_title_cn\" type=\"text_general_cn\" indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n<field name=\"inventors_cn\" type=\"text_general_cn\" indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n<field name=\"abstract_cn\" type=\"text_general_cn\" indexed=\"true\" stored=\"true\" />\n...",
      "Now that we have our schema written we can start writing content handlers to parse out the XML.",
      "First we need to create our own custom mimetype and register the parsing process with Tika at runtime.",
      "<mime-type type=\"application/x-chinese+xml\">\n  <sub-class-of type=\"application/xml\"/>\n  <root-XML localName=\"cn-patent\"/>\n  <expanded-acronym>Chinese Patent</expanded-acronym>\n</mime-type>",
      "Next, we can take advantage of the fact that both the English and Chinese XML are validated against the same DTD and use a single event handler to capture the various data pieces.",
      "return new ElementPathHandler(metadata, \"invention_title_\" + la, \"cn-patent/cn-bibliographic-data/invention-title\");",
      "Now we can pass in any file, regardless of language, and extract the important data.",
      "The data is then rendered into a binary key-value store that we stream into Solr as an atomic update. If the patent does not exist in the Solr index, a new document is written. If the patents translation or original Chinese text is present, then all data is simply appended to the document.",
      "While robustness simplifies working with messy data, there are a couple drawbacks to our approach. Indexing Performance: Atomic updates are actually not updates – Solr reads all of the information for each document, merges the new data together, and then re-indexes the set. This means that every document is indexed 1.5 times. Solr Performance: Because Solr has to re-read the document, all fields must be stored, bloating the index.",
      "Overall, the tradeoffs were worth the challenge: merging the documents outside of Solr would have increased complexity unnecessarily, and data size can be addressed through sharding and distributed storage. Building a robust ingester allowed us to handle an incomplete dataset and match up the pieces during processing."
    ],
    "summary_t": ""
  },
  {
    "id": "d6646941889dd3f9925ee4a6d5062937",
    "url_s": "https://opensourceconnections.com/blog/2013/05/06/does-foundationdb-beat-the-cap-conjecture-hackathon-on-friday/",
    "title": "Does FoundationDB beat the CAP conjecture?  Hackathon on Friday!",
    "content": [
      "",
      "Were cohosting a hackathon with FoundationDB on Friday. For those of you not in the know, FoundationDB is a pretty exciting addition to the NoSQL space. It brings flexible, arbitrary transactionality (including atomic cross-row joins in a distributed system) to NoSQL. My colleague Doug Turnbull got very excited about the technology a few months ago and blogged an introduction to it shortly after it became public.",
      "The extra cool part is that several APIs exist (Java, Python, others) that allow you to access this key-value store and to even connect it to other SQL and NoSQL technologies. For instance, someone has already built a SQLite connector so that your API can think that its talking to SQLite, but actually be talking to FoundationDB. (And all SQLite tests pass.) During the hackathon, we hope to get a good start on implementing a connector between Solr and FoundationDB.",
      "So join us and the super smart folks at FoundationDB on Friday at our offices in Charlottesville! Were excited, and hope you can make it!",
      "Heres the info and sign up: http://bit.ly/11LwzCs"
    ],
    "summary_t": "Were cohosting a hackathon with FoundationDB on Friday. For those of you not in the know, FoundationDB is a pretty exciting addition to the NoSQL space. It b..."
  },
  {
    "id": "ee91462f1c4af081eb9046a614977ccc",
    "url_s": "https://opensourceconnections.com/blog/2013/05/06/search-is-eating-the-world-recap-of-lucene-revolution/",
    "title": "Search is Eating The World | Recap of Lucene Revolution",
    "content": [
      "The team recently spent time at Lucene/Solr Revolution",
      "Much of the crew just got back from Lucene Revolution. It was an incredible experience to hang out with the cream-of-the-crop of the Lucene/Solr community. It continues to be clear that modern applications of all stripes are increasingly driven by search as the primary UI component. Users of these applications expect rich interactivity. And because search is becoming smarter and smarter, search is becoming the centerpiece for interacting with big data and complex applications.",
      "Search as the primary UI",
      "Google and Siri have trained us all to expect and work with smart search as a primary user interface. One fascinating example of this at Lucene Revolution was ADP’s HR system that forgoes a user interface in its entirety in favor of using search to understand verb/noun pairs of actions, presenting users with search results that reflect actions that can be taken in an HR system. \"Hire John Smith\" comes back with \"Onboard John Smith\" as the most relevant action, with perhaps actions for the unfortunately named \"John Hire\" being less relevant for the search result.",
      "As with Google and Siri, this product reflects our changing expectations when interacting with even everyday applications. We no longer see computing resources as strict executors of specific commands. Rather, we expect fuzzy understanding and inference of what we mean. In other words, we like search interfaces, but expect search to be more than just text search. We want search to be backed by intelligence – machine learning and natural language processing. We want it to be user-centric and focused on our needs.",
      "Once you commit to focusing on search as your user-interface component, you commit yourself to enriching search with other systems. An obvious example of this paradigm is in Big Data. As we believe at OpenSource Connections, search is the most accessible mechanism for working with Big Data. However, once you commit to search as your primary UI component, you must find ways to deal with the results of whatever data science you might be applying to your data set. The \"smarts\" circle back to the friendly, user-facing search box, creating the richest possible experience for exploring this data.",
      "An example of LucidWork’s Big Data System. Demonstrating a reference architecture where search is the central UI component to a Big Data set enriched by Machine Learning and OpenNLP",
      "Grant Ingersoll and Ted Dunning presented a reference architecture that captures many of the pieces of this idea. One can use Solr as the primary method of exploring data. Feedback from machine learning/batch processing of data can enrich search results by simply adding/modifying a field. Once in place, features of full-text search can take over.",
      "Solr as the Ideal Data Structure",
      "Why is a search engine like Solr the ideal means of exploring all this data science? Solr has few constraints on what can be indexed. The default is to index everything. Databases (NoSQL or otherwise) however, require us to think carefully about the extremely narrow subset of columns we’re going to choose to index/lookup later. Later if you look up on a non-indexed column, you’ll unknowingly create a performance mess, bogging down the system as the entire column is linearly scanned for the data you’re looking on.",
      "Solr is different. The inverted index data structure is written from an index-first point of view. Fields are frequently even indexed without being stored – the actual storage being unimportant or done elsewhere. The ability to lookup anything clearly allows us to perform full-text search on any field, but in the context of Big Data, faceted navigation is where search really shines. As demonstrated in Trey Grainger’s talk, facets allow a very broad way of breaking down counts of values in a field. This simple tool provides often provide surprising analytic capabilities. Nothing special is required other than the field be indexed – no sweat in Solr.",
      "A simple example of this capability is by enriching Solr documents with clustering algorithms. Once each document has indexed a field indicating its cluster, one can explore the nature of these clusters extremely easily in Solr. First, users view a breakdown of clusters in facets, with their respective counts. Users can then filter by cluster value, viewing how other facets are broken down after the filter. For example, we may discover in our document set that the strongest clusters form around specific natural languages. As we filter on a facet, suddenly only \"Chinese\" remains in the natural language facet, with other natural languages returning zero documents. Doing a similar thing with a traditional database (SQL or NoSQL) would require many columns be indexed, something seen as onerous to most data modelers.",
      "The future of Solr",
      "In his keynote, Yonik pointed out future goals of Solr. With SolrCloud, Solr looks more like a NoSQL solution with search baked into its bones. As Mark Miller said in his talk \"Solr started with search and backed into the storage problem; other solutions started with a storage problem and are trying to back into search\". More and more, folks are seeing Solr as a primary data store. It’s friendlier for analysis than databases for most users and is increasingly doing a better job of being a true distributed storage engine.",
      "Enriching the database features of Solr also includes adding more and different types of join functionality. Data can’t always be denormalized without many annoying side-effects and limitations. Solr is well suited to provide very sophisticated joining capabilities across documents, including potentially adding fuzzier/natural-language joins. Perhaps relevancy from the original query could be brought to bear on the boosting of documents from the second query. This is something I talked about in my talk – searching databases of legal jargon with layman’s terms (car ->motor-vehicle), once we’ve arrived at the top 5 most relevant pieces of jargon from a database, we can then search laws with the technical terms that actually exist in the jargony corpus of law.",
      "More and more search is the centerpiece rather than an add-on. And people want more than just search. The future is very exciting as adjacent technologies like machine learning and natural-language processing become more and more regular components of average applications. We’re truly living in a smarter world where search is the ear and the mouth of the computer!"
    ],
    "summary_t": "Much of the crew just got back from Lucene Revolution. It was an incredible experience to hang out with the cream-of-the-crop of the Lucene/Solr community. I..."
  },
  {
    "id": "62e96600166ae6adb4526777a4ce207b",
    "url_s": "https://opensourceconnections.com/blog/2013/05/20/how-does-a-search-engine-work-an-educational-trek-through-a-lucene-postings-format/",
    "title": "How does a search engine work? An educational trek through a Lucene Postings Format",
    "content": [
      "Little did you know, Lucene stores its index on hundreds of Post-It Notes",
      "A new feature of Lucene 4 – pluggable codecs – allows for the modification of Lucene’s underlying storage engine. Working with codecs and examining their output yields fascinating insights into how exactly Lucene’s search works in its most fundamental form.",
      "The centerpiece of a Lucene codec is it’s postings format. Postings are a commonly thrown around word in the Lucene space. A Postings format is the representation of the inverted search index – the core data structure used to lookup documents that contain a term. I think nothing really captures the logical look-and-feel of Lucene’s postings better than Mike McCandless’s SimpleTextPostingsFormat. SimpleText is a text-based representation of postings created for educational purposes. I’ve indexed a few documents in Lucene using SimpleText to demonstrate how postings are structured to allow for fast search:",
      "Sample Documents",
      "doc0:\nauthor: doug-turnbull\ntext: where oh where are my Unicorns!\ndoc1:\nauthor: scott-stults\ntext: Unicorns! Unicorns!",
      "If we tell Lucene to split up words on whitespace (Lucene’s WhitespaceAnalyzer), specify the SimpleText codec, and write these documents out using an IndexWriter, we get Lucene’s postings serialized to disk in this nice human readable format:",
      "field author\n  term Doug\n    doc 0\n      freq 1\n      pos 0\n  term Scott\n    doc 1\n      freq 1\n      pos 0\n  term Stults\n    doc 1\n      freq 1\n      pos 1\n  term Turnbull\n    doc 0\n      freq 1\n      pos 1\nfield text\n  term Unicorns!\n    doc 0\n      freq 1\n      pos 5\n    doc 1\n      freq 2\n      pos 0\n      pos 1\n  term are\n    doc 0\n      freq 1\n      pos 3\n  term my\n    doc 0\n      freq 1\n      pos 4\n  term oh\n    doc 0\n      freq 1\n      pos 1\n  term where\n    doc 0\n      freq 2\n      pos 0\n      pos 2\nEND",
      "What we end up with is the text of our documents reorganized into something resembling an index in the back of a book. In search jargon, this is referred to as an \"inverted index\". After our text gets tokenized into terms, the resulting terms form the key to the inverted index data structure. Instead of the document pointing to a series of terms, a term points to a list of the documents that contain it. This is what makes the index \"inverted\". Now when we search a field, we can simply take a search term, look in the inverted index for that term, and retrieve a list of documents that contain that term. All of this much in the same way that a set of terms in the index in the back of a book points to page numbers that contain that term.",
      "More than just term -> page number",
      "Although the index in a book is a useful metaphor that we can and will dutifully abuse, book indices don’t contain the frequency of that term on a page. Nor do they specify the position on the page the terms occur. This information IS captured in a postings format. The section for the \"Unicorns!\" term captures this notion well:",
      "term Unicorns!\n    doc 0\n      freq 1\n      pos 5\n    doc 1\n      freq 2\n      pos 0\n      pos 1",
      "Why are term frequency and position very useful to search engines?",
      "The term frequency helps us determine how relevant a document is to that term. If there are comparatively more \"Unicorns\"\" in document 1, then it will be scored higher. This model is known as the TF-IDF model, named for the very simple math that captures this idea. As an example, extend this to our book metaphor. Let’s say you’re searching for \"Edgar Allen Poe\" in a book about 19th century literature. If you knew through that book’s index that there were more mentions of Edgar Allen Poe on page 15 and fewer on page 5, you’d probably be more likely to flip to page 15. This is because you probably discern that page 15 with its more frequent mentions of \"Edgar Allen Poe\" is more likely to be the best page to flip to. This notion falls in line with most people’s expectations when searching and forms the basis for Lucene’s search ranking.",
      "Term positions are also useful. Imagine in our previous example if we didn’t list \"Edgar Allen Poe\" in the book’s index. Instead we simply had individual words \"Edgar\", \"Allen\" and \"Poe\". We could, however, figure out if there was a strict mention of the phrase \"Edgar Allen Poe\" if we recorded the position of each term on a page in our book’s index. For example, \"Edgar\" is word number 3 on page 5; \"Poe\" is word number 17 on page 5, etc. This would be painful and annoying for us humans, but we could do it. Nevertheless this is easy for a computer – Computers have an easier time chopping up text on whitespace and a harder time recognizing names of accomplished authors. Therefore, this is precisely what Lucene does. Lucene can quickly determine the distance and arrangement of search terms in a document at query time. If the terms \"Edgar\", \"Allen\", and \"Poe\" are hits AND are adjacent and in the correct order, then Lucene can determine this page is a hit. This ability allows Lucene to support term position aware queries such as phrase queries and span queries.",
      "Devil in the details",
      "So how do search engine’s work? That’s about it. Well kind-of. Its more like teaching you how to add and subtract and expecting you to derive algebra. The hard part is often actually creating and using this inverted index for a real, user-facing application. One problem alluded to above is how do you correctly analyze and tokenize content – tricky depending on your language and nature of the content. Whitespace does not cut it in most cases– like maybe youre indexing snippets of code and want to tokenize on camelCase. Maybe youd like to normalize your text to the root form of words. Or incorporate synonyms into the index. Also alluded above is that while TF-IDF forms the root of Lucene’s default scoring, it doesnt capture all the variables included. There’s many tweaks along the way, including norms – biasing search results toward shorter fields – and custom boosting – users giving priority to hits from specific fields or from specific queries. There’s also the question of the search strategy you use for your application. Are you only doing direct lookup? Do you need range queries? Are single fields being searched, or multiple simultaneously? Would you like to include features such as faceting? These and many other considerations come into play when trying to get Lucenes inverted index to do your bidding.",
      "More fundamentally, there’s the fact that this is of course the educational implementation of an inverted index. A good, fast data structure that won’t chug for minutes to give you an answer is really required. To make a long story short, there’s lots and lots of hardcore data structures thought placed into how exactly that works. If you’d like that CS PhD inducing experience I recommend diving into the Lucene42PostingsFormat at your earliest convenience. Heres a taste of something related to get you started. Once you get to the bottom of this postings format– call me… for a job :)."
    ],
    "summary_t": "A new feature of Lucene 4 – pluggable codecs – allows for the modification of Lucene’s underlying storage engine. Working with codecs and examining their out..."
  },
  {
    "id": "0dadec49eaca17bf34edaeda2453c2f5",
    "url_s": "https://opensourceconnections.com/blog/2007/03/05/a-simple-example-of-tomcat-realms-and-security/",
    "title": "A simple example of Tomcat Realms and Security",
    "content": [
      "Last week a colleague of mine called me up because he was struggling with setting up Tomcat JDBCRealm for use with a relatively simple straightforward webapp. Wed recently released the 1.0 version of GateKeeper, and he wanted to use it to manage his users and roles. The GateKeeper side of things went just fine, but the JDBCRealm setup on his webapp was proving very balky.",
      "He fired up VNC, and I logged in and checked out his settings. And there was nothing wrong! At least, nothing that looked odd in any way. We then googled for some help, maybe a simple sample setup, but to no avail! Oddly enough, nowhere on the internet is a good example of using a Tomcat Realm to protect a webapp. There are various examples, but they just show a part of the puzzle, maybe just the <security -constaint/> section in the web.xml, or a Realm definition: <realm className=\"org.apache.catalina.realm.MemoryRealm\" />. Reading the servlet spec for web.xml helps, but doesnt give you details on the Tomcat specific details.",
      "So, over the weekend I set out to create a simple example, and I planned on using the jsp-examples app the ships with Tomcat. After poking around jsp-examples I discovered what I had been looking for all along! There is a full example of protecting a webapp using a Realm! The example for some odd reason is NOT linked from the home page of jsp-examples, but it is there. http://localhost:8080/security/protected and log in as user role1, with the password tomcat.",
      "Ill walk you really quickly through the relevant portions of the configuration files. First off you define what URLs you want protected by which roles in your web-apps web.xml file:",
      "<security -constraint>\n<display -name>Example Security Constraint</display>\n<web -resource-collection>\n</web><web -resource-name>Protected Area</web>\n<!-- Define the context-relative URL(s) to be protected -->\n\n\n<url -pattern>/security/protected/*</url>\n<!-- If you list http methods, only those methods are protected -->\n\n\n<http -method>DELETE</http>\n<http -method>GET</http>\n<http -method>POST</http>\n<http -method>PUT</http>\n\n<auth -constraint>\n<!-- Anyone with one of the listed roles may access this area -->\n\n\n<role -name>tomcat</role>\n<role -name>role1</role>\n</auth>\n</security>\n\n<!-- Default login configuration uses form-based authentication -->\n\n\n<login -config>\n<auth -method>FORM</auth>\n<realm -name>Example Form-Based Authentication Area</realm>\n\n</login>\n\n<!-- Security roles referenced by this web application -->\n\n\n<security -role>\n<role -name>role1</role>\n</security>\n<security -role>\n<role -name>tomcat</role>\n</security>",
      "The first <security -constraint/> stanza specifies the URLs of the pages that we want to protect, as well as what roles are required to allow access. Anything in the /security/protected/* pattern will require the user has either the role role1 or tomcat.",
      "The next stanza, <login -config/> specifies how you gain access to the protected URLs. In this example, we are using FORM based security, so we specify the URLs for the login/logout pages. If you are using BASIC authentication then your stanza is much simpler:",
      "<login -config>\n<auth -method>BASIC</auth>\n<realm -name>Example BASIC Authentication Area</realm>\n</login>",
      "The remaining stanzas, <security -role/> are the roles that are used in this webapp, and they are elements that are often forgotten in a web.xml.",
      "Now that we have defined that only users who have roles role1 or tomcat can access the application, we then need to configure Tomcat. This is done through server.xml, and can be a bit of a rat hole to figure everything out. So, extracted from server.xml we have the following XML stanzas. Ive removed most of the commenting so you can see the overall structure of the document:",
      "<server port=\"8005\" shutdown=\"SHUTDOWN\">\n\n<!-- Global JNDI resources -->\n\n\n<globalnamingresources>\n\n<!-- Editable user database that can also be used by<br />\n\t         UserDatabaseRealm to authenticate users -->\n\n\n<resource name=\"UserDatabase\" auth=\"Container\"\ntype=\"org.apache.catalina.UserDatabase\"\ndescription=\"User database that can be updated and saved\"\nfactory=\"org.apache.catalina.users.MemoryUserDatabaseFactory\"\npathname=\"conf/tomcat-users.xml\" />\n\n</globalnamingresources>\n\n<!-- Define the Tomcat Stand-Alone Service -->\n\n\n<service name=\"Catalina\">\n\n<!-- Define a non-SSL HTTP/1.1 Connector on port 8080 -->\n\n\n<connector port=\"8080\" maxHttpHeaderSize=\"8192\"\nmaxThreads=\"150\" minSpareThreads=\"25\" maxSpareThreads=\"75\"\nenableLookups=\"false\" redirectPort=\"8443\" acceptCount=\"100\"\nconnectionTimeout=\"20000\" disableUploadTimeout=\"true\" />\n\n<!-- Define the top level container in our container hierarchy -->\n\n\n<engine name=\"Catalina\" defaultHost=\"localhost\">\n\n<!-- Because this Realm is here, an instance will be shared globally -->\n\n<!-- This Realm uses the UserDatabase configured in the global JNDI<br />\n\t           resources under the key \"UserDatabase\".  Any edits<br />\n\t           that are performed against this UserDatabase are immediately<br />\n\t           available for use by the Realm.  -->\n\n\n<realm className=\"org.apache.catalina.realm.UserDatabaseRealm\"\nresourceName=\"UserDatabase\"/>\n<host name=\"localhost\" appBase=\"webapps\"\nunpackWARs=\"true\" autoDeploy=\"true\"\nxmlValidation=\"false\" xmlNamespaceAware=\"false\">\n\n</host>\n\n</engine>\n\n</service>\n\n</server>",
      "Now, instead of going from top to bottom, well start from the definition for the specific host, and work our way out. The challenge that my colleague and I had was that we thought we needed to somehow map a <security -constraint/> in the web.xml to some sort of definition in server.xml. We were mucking around with all sorts of <realm /> stanzas embedded in <host /> stanzas and trying to match <realm -name/> in the web.xml to the <realm name=XX/> element in server.xml. What we didnt realize until I found the jsp-examples sample security code is that because the realm is defined inside of the <engine /> stanza, it applies to all hosts! And that there is NO explicit mapping between webapps and realms at a level more detailed then at an Engine level. All webapps defined in all hosts that share the same engine share the same realm! So the complete stanza looks like:",
      "<engine name=\"Catalina\" defaultHost=\"localhost\">\n<realm className=\"org.apache.catalina.realm.UserDatabaseRealm\"\nresourceName=\"UserDatabase\"/>\n<host name=\"localhost\" appBase=\"webapps\"\nunpackWARs=\"true\" autoDeploy=\"true\"\nxmlValidation=\"false\" xmlNamespaceAware=\"false\">\n</host>\n</engine>",
      "Now, I dont know what you would do if you wanted to use different Realms for each webapp, maybe define multiple Engines I guess. Our UserDatabaseRealm is not fully defined here, instead it refers to a global resource called UserDatabase. That is defined outside in our <globalnamingresources /> stanza:",
      "<globalnamingresources>\n\n<!-- Editable user database that can also be used by<br />\n\t         UserDatabaseRealm to authenticate users -->\n\n\n<resource name=\"UserDatabase\" auth=\"Container\"\ntype=\"org.apache.catalina.UserDatabase\"\ndescription=\"User database that can be updated and saved\"\nfactory=\"org.apache.catalina.users.MemoryUserDatabaseFactory\"\npathname=\"conf/tomcat-users.xml\" />\n\n</globalnamingresources>",
      "Weve defined a resource call UserDatabase that reads in data in the conf/tomcat-users.xml file and exposes it as a user database.",
      "Hopefully this will walk you through all the setup steps required for adding a Realm to your webapp. For me, the key insight was that the realm definition inside of the engine definition applies to all the web-apps in all the hosts. I am working on cutting our 1.0.1 version of Gatekeeper which will demo using a JDBCRealm. I will post a follow up on that."
    ],
    "summary_t": ""
  },
  {
    "id": "fb81535504aa68e039a048c3db962642",
    "url_s": "https://opensourceconnections.com/blog/2013/05/20/initial-impressions-of-elasticsearch-flexible-open-source-distributed-search/",
    "title": "Initial Impressions of Elasticsearch – Flexible Open Source Distributed Search",
    "content": [
      "I’m Jonathan Thompson, a rising 4th year CS major at UVA. This is my first blog post as an intern at OSC, I hope you enjoy it!",
      "While my teammate Krystal worked on integrating OSCAR, the OSC Automated Robot, with Solr, I forked the project and used it as an opportunity to demo Elasticsearch. Was a multi-node distributed solution overkill for this relatively simple project? Probably, but I learned a lot about Elasticsearch and had fun setting everything up. I’m going to give you my impressions of Elasticsearch from the past few days of researching and using it.",
      "Elasticsearch is a real time \"distributed RESTful search and analytics\" engine. The easiest way to explore Elasticsearch is to download it, but here’s a synopsis. JSON documents are stored in primary shards. These primary shards are associated with any number of replica shards. These shards are stored on different nodes in your cluster. A node is a single Elasticsearch instance (usually corresponding to a single server). A cluster is a logical partitioning of nodes that contains one master node. This master node controls the distribution of shards across nodes, cluster health, and node status. This network is completely peer to peer and distributed, so any queries (URI or http post with a JSON payload) can go to any node in the cluster to be handled. If any nodes go offline, the data will be redistributed accordingly, and if a master node goes offline, the cluster will automatically elect a new one. An index with multiple types is similar to a Solr instance with multiple collections, but with Elasticsearch, any number of indicies can be stored over any number of nodes or clusters. What this means is that a single index with a large number of documents of a single type can be split among many machines, but also many indicies with many types can be run on a single machine, assuming a small number of documents for each type. You can start your build with a single server or EC2 instance, and easily add more as your database grows.",
      "]2 The whiteboard saw a little use while exploring Elasticsearch!",
      "When Elasticsearch is brought up in conversation Solr has to be mentioned as well, seeing as both are open source search and analytic engines built on top of Lucene. Solr is the stable incumbent with many advanced and refined features. Solr has some distributed capabilities, but these have only been added on fairly recently. Elasticsearch is the challenger, and it lacks some of the advanced features of Solr, but it is built from the ground up for distributed search, and offers fast spin up times, powerful automatic configuration and management options, and lots of flexibility.",
      "I’m probably going to tackle a direct comparison of the two in a later post, but for more information now, Sematext wrote an excellent 6 part breakdown of Solr vs Elasticsearch. Without further adieu, here are my impressions of Elasticsearch.",
      "Elasticsearch is fast and easy",
      "For spin up, I downloaded the code, ran an elasticsearch \"node\", and sent it a http PUT request with my data in json form as the payload. Elasticsearch automatically created an index, determined the data type of each field, and created a mapping for the \"type\" of document I had just submitted. If that weren’t cool enough, just by starting another node in another terminal, it detected the second node and started to distribute the index load among the nodes. Realistically, each node in your system should correspond to a single server or EC2 instance, but for testing you can run multiple nodes on a single machine. I started another few nodes just for kicks, and in 5 minutes had a distributed system that could handle node removals and additions with ease!",
      "\"Schema free\" is a lie… but in a good way!",
      "Elasticsearch is schema free by default, but you can very easily specify \"mappings\" to override the auto-indexing features. In fact, this principle holds to many other features of Elasticsearch. All cluster, node, and shard management is automatically handled, but you can directly control almost anything if necessary. For example, you can specify document and shard placement on nodes explicitly with \"routing\", create advanced network topologies by altering per node options, and change cluster wide configuration settings with ease.",
      "Powerful plugins and flexibility",
      "In addition to the main API being exposed over http, thrift, and memcached, there are also many other ways to access and extend Elasticsearch. An official Java and Groovy API give you access to the main API and more. The \"River\" system pulls streams of data into your cluster. Last, but not least, Elasticsearch’s plugin system lets you extend and tweak Elasticsearch to fit your needs. There’s everything from custom mapping types, analyzers, request handlers, and network management plugins to site plugins that let you build a web front end that lives on your cluster. A few quick examples I found useful:",
      "The elasticsearch adapter for Ember.js lets you use elasticsearch as a storage engine. I was able to quickly modify elasticsearch’s Ember integration tutorial to create a Task list from all emails sent to OSCAR that contained \"task\" in the email subject line.\n  An EC2 discovery module is included, and other plugins available add features to make deployment to EC2 even easier.\n  A Solr plugin parses incoming queries in Solr format and emulates Solr responses, and a Solr River plugin lets you import data from a Solr instance.",
      "In summary, Elasticsearch is a young but powerful open source solution for distributed search solutions, and while a specialized search problem will probably benefit greatly from Solr’s optimizations, both Solr and Elasticsearch should be carefully compared if a problem appears to need a large distributed solution."
    ],
    "summary_t": ""
  },
  {
    "id": "6862e6cdf2b5c624203853eca154b970",
    "url_s": "https://opensourceconnections.com/blog/2013/05/20/national-day-of-civic-hacking-at-opensource-connections/",
    "title": "National Day of Civic Hacking at OpenSource Connections",
    "content": [
      "National Day of Civic Hacking at OpenSource Connections",
      "What",
      "OSC is excited to join 86 other venues across the US for civic hacking and camaraderie. Come join us and find like minded folks to help you save the world. Better yet, lend your skills to someone else's great idea!",
      "When",
      "Saturday, June 1 from 9:00AM to 6:00PM",
      "Where",
      "Opensource Connections International Headquarters (here’s the MAP) Parking available on the street.",
      "Schedule",
      "9:00 Donuts and Introductions.\n  10:00 Present your master plan! Call for help.\n  10:30-5:30 Hacking\n  12:30 Lunch\n  5:30 Lightning presentations: What did you do? What's next? How can we help?",
      "Links",
      "It's free (of course!) and you can Sign Up by going to http://hackforchangecville.eventbrite.com.",
      "More information about the national event, including events near you, is at theNational Day of Civic Hacking Website."
    ],
    "summary_t": ""
  },
  {
    "id": "dfcfb1e4c0d4324eef8fd6a5ecf5c214",
    "url_s": "https://opensourceconnections.com/blog/2013/05/21/introducing-oscar-search-your-notes-no-matter-where-they-live/",
    "title": "Introducing OSCAR! Search your notes no matter where they live!",
    "content": [
      "In our daily adventures on our computers and the Internet, we often find/create notes or documents that we would like to keep for later, that we attempt to organize. We put them in our email, in Google Docs, in Dropbox, in Gist, and countless other places. Then when we want to find them later, it’s often a hassle to look everywhere.",
      "What if you had a fast, smart search that would look through all these places at once? What if you could also search for files shared by others in your company, and selectively share your own documents without reorganizing them into yet another service?",
      "Let me introduce \"OSCAR\", the OpenSource Connection Automated Robot. Keep your docs wherever you want; Oscar will find them and index them. Oscar is being developed as an (initially) internal tool which functions basically as a virtual assistant that helps you manage your personal digital content and easily retrieve them later. Eventually of course, it will cater to your every whim and need. If you’re lucky.",
      "The past week I developed the very first version of Oscar with a functioning Solr backend and simple online frontend. Right now the function is pretty basic – you send an email to Oscar’s Gmail address and he puts your email into a Solr server. If you include any links, Oscar crawls the pages a little and remembers some keywords so you can easily find the content later. Finally, you can go to Oscar’s site and make queries for these emails.",
      "See it here. (Warning: Under active development. May explode upon contact.)",
      "The app itself is hosted on Google App Engine (Python), which was something new I learned to use in the past week in addition to Solr. GAE is pretty awesome – in my opinion much easier to setup and deploy to than Heroku or AWS… though thats a post for another time. It’s especially fun to work with if you have a cool IDE like PyCharm. GAEs webapp2 library and templating system is like a lighter, smaller version of Django, and Django itself is supported. Another reason why GAE is awesome.",
      "Over the next week, we will begin fully integrating the app with Django and moving the backend to ElasticSearch. In addition, we will begin adding new features to OSCAR as we begin the trek of bringing it into its vision. I’m excited!",
      "We’d love to hear about your experiences with organizing your notes and documents. How do you find stuff across multiple locations and services? Do you have a preferred service that you try to pack everything into, or do you have another way of saving and finding things? Let us know by commenting below!"
    ],
    "summary_t": ""
  },
  {
    "id": "a67199059546eb6d56e39e983ca4803e",
    "url_s": "https://opensourceconnections.com/blog/2013/05/22/what-happened-to-charstreamcharreader-in-lucene/",
    "title": "What happened to CharStream/CharReader in Lucene?",
    "content": [
      "A pretty subtle change happened in the transition from Lucene/Solr 3 to 4. The abstract method for CharFilterFactory changed from",
      "public CharStream  create(CharStream input);",
      "to",
      "public abstract Reader create(Reader input);",
      "What ’s up with this change? Why did it happen?",
      "Well first, let’s take a step back and explain why CharStream existed. CharStream inherited from and wrapped a Java’s Reader class. Prior to Lucene 4, it was used by a Tokenizer to pull characters out of a document one-by-one. CharStream could be any number of things that wrap the underlying text, including a CharFilter that might filter in or out certain characters from being indexed. To support filtering, it added one piece of functionality, the abstract method correctOffsets.",
      "What does correctOffsets do? Recall that in Lucene, you can store for each token the character offset within a document of the begin and end of that term. Lucene also allows you to filter in/out the characters of the document before tokenization in your own custom CharFilter. The result is that if you took the offsets of the characters after character filtering, they’ll be wrong. You can see this if we perform HTML filtering on this text:",
      "01234567890123456789012345\n    <p>Doug is <b>cool</b></p>",
      "After filtering the HTML chars out we have",
      "012345678901\n    Doug is cool",
      "If we didn’t correct the offsets, we’d end up saving the following character positions of each token:",
      "0  3 56 8  11\n    Doug is cool\n\n    Doug – begins 0, ends 3   -> Really should be begins 3, ends 6\n    is – begins 5, ends 6     -> Really should be begins 8, ends 9\n    cool – begins 8, ends 11  -> Really should be begins 14, ends 17",
      "Something in the CharFilter needs to remember the original offsets and correct them. This is where \"correctOffsets\" comes in. It’s the magic black box that takes the offsets after filtering and converts them back to the original offsets in the document.",
      "Prior to Lucene 4.0, the intermediate CharStream abstract class was passed around. There really were only two implementation of CharStream – CharFilter – which filtered out characters and corrected offsets – and CharReader – which did no filtering and therefore no offset correcting. Nobody else really cared about CharStream. If they needed to filter (and in turn correct offsets) they inherited from CharFilter. CharStream was therefore a noisy intermediary between Reader and CharFilter. Most API users would probably feel more comfortable interfacing with a Reader or inheriting from a CharFilter. At least if I was the one who did the change that’s what I would be thinking .",
      "So things were simplified for API users. Now in Lucene 4 and beyond, CharFilter simply inherits from Reader directly. Either a Tokenizer works with a Reader or a CharFilter. To decide how to correct offsets during tokenization, Tokenizer has this single line of hackery to decide how to proceed:",
      "// (taken from Tokenizer)\n protected final int correctOffset(int currentOff) {\n       return (input instanceof CharFilter) ?\n            ((CharFilter)input).correctOffset(currentOff) : currentOff;\n }",
      "In short, Lucene has hidden a little naughty direct type checking craziness to simplify the API. The Tokenizer now just directly figures out if it’s talking to a Reader or CharFilter and proceeds accordingly. Either keeping the current offset or correcting the offset as needed. Theres no longer a need for the CharStream (or CharReader). We either dont correct offsets if theres a Reader. Or we correct offsets if the Reader is really a CharFilter.",
      "So what does this mean for API users? You can safely change your CharFilterFactories to return Readers. If they returned CharFilters, they can still return CharFilters (as these are Readers). If you don’t care about filtering, you can return a Reader from this method as well. As an API user you can rest assured that Lucene won’t barf tokenizing either a CharFilter or a Reader."
    ],
    "summary_t": ""
  },
  {
    "id": "0b3bad2eb1d85d920fd7d9b296e87863",
    "url_s": "https://opensourceconnections.com/blog/2013/05/31/deploying-elasticsearch-to-amazon-ec2-with-ansible/",
    "title": "Deploying Elasticsearch to Amazon EC2 with Ansible",
    "content": [
      "Over the past few weeks Ive been working with Elasticsearch. However, what I really have been missing is a good deployment system. Elasticsearch has a chef provisioning tutorial on their site, but I decided to check out another open source alternative, Ansible. Ansible claims to be \"dead simple\". I have to agree with the statement. Compared to the custom language of Puppet and the quirks of Chef, Ansibles simple human readable syntax using yaml is breath of fresh air. Ansible also avoids some of the chicken-and-egg issues of deployment with puppet and chef by managing machines purely over SSH. I used the built in AWS module (one of many built in modules!), and created a convenient solution for creating an Elasticsearch cluster with a single command.",
      "I pushed a simple example to Github as a quick bootstrap. This code is based on the Eucalyptus deploy sample in the Ansible example playbooks. These files will let you create a variable number of EC2 instances, and automatically provision them so that Elasticsearch is running and all your nodes are in the same cluster. I used Ubuntu 13.04 64 bit machines, specifically the AMI image ami-c30360aa. Other Ubuntu images should work as well, but your success might vary. The Git readme explains the details of the setup and what you have to do to deploy. The main goals are to setup an EC2 security group, setup your local environment, and customize some configuration files. In this post, Im going to walk through how everything works.",
      "In Ansible, you list out tasks to be executed on a group of host machines in a play. Multiple plays can be combined into a single playbook, like a Chef recipe or Puppet manifest. In my playbook (deploy_elasticsearch.yml), I have two plays.",
      "The first play stages the AWS instances. I want my local machine to send the commands, so I target the host group local. Ansible can collect information about machines, but we dont need any facts about our local machine, so the gather_facts flag is set to false. Ansible supports tags, which allow you to later call groups of tasks individually, so I included them for future use. In the first play, the first task launches instances, and register the outputs of that task to the variable ec2. The next task adds every single amazon instance that was created to the \"deploy\" group of hosts, so that we can reference them later. The commented task relating to attaching volumes is from the original example, and I kept that in case anyone wants to use that feature. Back in the first task, the \"wait\" parameter was set to yes. This waits to go to the next step until the amazon instance is running. However, \"running\" doesnt mean that the OS is fully booted, so we wait for SSH to be available to determine that the OS is fully booted. The last task in the play gives all the instances a few seconds of breathing room before jumping to the next step. This seems like a \"hackish\" solution, but was the best I could find after several hours of troubleshooting a bug with Ansibles apt module*.",
      "The second play is targeted at the hosts group \"deploy\", and provisions each of them with Elasticsearch. The tasks are fairly simple, installing JRE, downloading and installing Elasticsearch, installing Elasticsearch plugins. The last task copies over the Elasticsearch configuration file on our local machine to the remote one. At the very end, we see something new – a handler. Similar to Puppets state management systems, if any handlers are notified during the play, they will execute after the play, but if the handler is notified multiple times it will only execute once.",
      "The biggest feature of Ansible that I havent used yet is roles. My playbook is great for many instances of a single type of server (which works well if you use Elasticsearchs default setting that automatically elects a master node). But what if we have multiple specific node configurations we wanted? Well, instead of making tons of playbooks that all say the same thing, we can make playbooks that include other playbooks when necessary, and set up a system where every node is configured with a common set of plays, and then specialized with a different playbook.",
      "Ansible is a quick and surprisingly powerful deployment and provisioning tool. Anybody tired of Chef or overwhelmed with Puppet should definitely take a look at Ansible.",
      "*The Ansible module for apt control is built on python-apt. I ran into issues with files not existing within 1 second of boot time that python-apt needs to function correctly."
    ],
    "summary_t": ""
  },
  {
    "id": "f161be0595df6c5d57c8afbf26129b07",
    "url_s": "https://opensourceconnections.com/blog/2013/06/02/searching-through-hierarchical-documents-for-dummies-like-me/",
    "title": "Searching with hierarchical fields using Solr",
    "content": [
      "In our recent and continuing effort to make the world a better place, we have been working with the illustrious Waldo Jaquith on a project called StateDecoded. Basically, were making laws easily searchable and accessible by the layperson. Here, check out the predecessor project, Virginia Decoded. StateDecoded will be similar to the predecessor but with extended and matured search capabilities. And instead of just Virginia state law, with StateDecoded, any municipality will be able to download the open source project index their own laws and give their citizens better visibility in to the rules that govern them.",
      "For this post, though, I want to focus upon one of the good Solr riddlers that we encountered related to the hierarchical nature of the documents being indexed. Laws are divided into sections, chapters, and paragraphs and we have documents at every level. In our Solr, this hierarchy is captured in a field labeled \"section\". So for instance, here are 3 examples of this section field:",
      "<field name=\"section\">30</field> – A document that contains information specific to section 30.\n  <field name=\"section\">30.4</field> – A document that contains information specific to section 30 chapter 4.\n  <field name=\"section\">30.4.15</field> – A document that contains information specific to section 30 chapter 4 paragraph 15.",
      "And our goal for this field is that if anyone searches for a particular section of law, that they will be given the law most specific to their request followed by the laws that are less specific. For instance, if a user searches for \"30.4\", then the results should contain the documents for section 30, section 30.4, section 30.4.15, section 30.4.16, etc., and the first result should be for 30.4. Other documents such as 40.4 should not be returned.",
      "Initially we tackled the problem with the PathHierarchyTokenizerFactory.",
      "<field name=\"section\" type=\"path\" indexed=\"true\" stored=\"true\"/>",
      "and",
      "<fieldType name=\"path\" class=\"solr.TextField\">\n  <analyzer>\n    <tokenizer class=\"solr.PathHierarchyTokenizerFactory\" delimiter=\"/\" />\n  </analyzer>\n</fieldType>",
      "Using this analyzer, heres how several example documents get tokenized:",
      "30       -->  <30>\n30.4     -->  <30>    <30.4>\n30.4.15  -->  <30>    <30.4>    <30.4.15>\n30.4.16  -->  <30>    <30.4>    <30.4.16>\n30.5     -->  <30>    <30.5>\n40.5     -->  <40>    <40.5>",
      "And if anyone searches for a law, the same tokenization occurs. So that if someone wants to look at law \"30.4\" then this gets tokenized to <30> and <30.4>. In this case the first 5 documents match. In particular, documents 30.4, 30.4.15, and 30.4.16 all have two matching tokens while 30 and 30.5 have one matching token. Additionally, because of the field length normalization, a match in a shorter field – say 30.4 with 2 tokens gets boosted higher than a match on a field with 3 tokens such as 30.4.15 or 30.4.16.",
      "All things considered, the resulting documents should come in the following order:",
      "30.4\n  30.4.15\n  30.4.16\n  30\n  30.5",
      "BUT, as fate would have it, our perfect plan has a hole in it somewhere and the documents come back in this order:",
      "30.4.15\n  30.4\n  30.4.16\n  30\n  30.5",
      "So… whats up?! (Solr experts reading this, do you know where our theory breaks down?)",
      "Our theory is actually sound, but the problem lies in the practical matter of storing the field norm. Each field of every document is stored with an additional piece of information called the field norm. Ideally, the field norm would be able to hold very specific information about how long a document is, but the more accurate the field norm is, the more space you need to store that information. In the end, the Lucene developers decided to store the field norm as a single byte. This means that the field norm can store exactly 256 different values. And if you look at the Lucene TFIDFSimilarity class (the class responsible for dealing with field norms and scoring documents) you can see exactly where these byte values are getting translated into floating point values. Theres even a comment that gives a hint into what our problem might be:",
      "The encoding uses a three-bit mantissa, a five-bit exponent, and the zero-exponent point at 15, thus representing values from around 7×10^9 to 2×10^-9 with about one significant decimal digit of accuracy. Zero is also represented. Negative numbers are rounded up to zero. Values too large to represent are rounded down to the largest representable value. Positive values too small to represent are rounded up to the smallest positive representable value.",
      "Because of this lack of precision, the field norm for fields of length 2 and length 3 is the same! … And thats why we get the weird ordering. The first three documents have the same score and are therefore returned in index ordering.",
      "So, how do we fix this? Well, one way would be to implement our own Similarity class that uses a different byte-to-float conversion than the default. But one of the main goals for this particular project is to keep this installation of Solr as simple as possible so that anyone can set it up. Therefore, we found that an easier solution was to use a slightly different analysis technique. Consider the following schema fragments",
      "<field name=\"section_descendent\" type=\"descendent_path\" indexed=\"true\" stored=\"true\"/>\n<field name=\"section_ancestor\" type=\"ancestor_path\" indexed=\"true\" stored=\"true\"/>",
      "and",
      "<fieldType name=\"descendent_path\" class=\"solr.TextField\">\n  <analyzer type=\"index\">\n    <tokenizer class=\"solr.PathHierarchyTokenizerFactory\" delimiter=\"/\" />\n  </analyzer>\n  <analyzer type=\"query\">\n    <tokenizer class=\"solr.KeywordTokenizerFactory\" />\n  </analyzer>\n</fieldType>\n\n<fieldType name=\"ancestor_path\" class=\"solr.TextField\">\n  <analyzer type=\"index\">\n    <tokenizer class=\"solr.KeywordTokenizerFactory\" />\n  </analyzer>\n  <analyzer type=\"query\">\n    <tokenizer class=\"solr.PathHierarchyTokenizerFactory\" delimiter=\"/\" />\n  </analyzer>\n</fieldType>",
      "In this case were splitting up section into two fields, section_descendent and section_ancestor which are tokenized slightly differently than before. The difference is that in the previous example, the PathHierarchyTokenizer was used on both index and query. But in this case descendent_path uses the PathHierarchyTokenizer only at index time and ancestor_path uses the PathHierarchyTokenizer only at query time. (The KeywordTokenizer tokenizes the entire field as a single token.) As a result section_descendent will only match queries for sections that are a descendent of the query – so 30.4 will match 30.4 and 30.4.15 and 30.4.16, but not 30. And similarly section_ancestor will only match queries for sections that are an ancestor of the query – so 30.4 will match 30.4 and 30, but not 30.4.15.",
      "The final piece of the puzzle is the request handler. Here we simply use an edismax request parser with a qf (query fields) set to include both descendent_path ancestor_path. Now, whenever someone queries for 30.4, then in the descendent_path they get matches on 30.4, 30.4.15, and 30.4.16; and in the ancestor_path they get matches on 30.4 and 30. The only document that matches in both fields is 30.4 thus it gets an extra boost and appears at the top of the search results followed by the other documents. Problem solved!",
      "Now it is interesting to note that weve lost sibling documents (for instance 30.5 in this example). Personally, that seems fine to me, but if we really wanted it, all we have to do is again include the original section field path-tokenized on both query and at index.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "22399a7c36c0a11da2dc2aa3e9dca6ac",
    "url_s": "https://opensourceconnections.com/blog/2013/06/03/cville-hacks-for-change/",
    "title": "Cville Hacks for Change",
    "content": [
      "Thanks to everyone for an excellent first Charlottesville Civic Hacking Day! Im thrilled that everyone was willing to contribute their Saturdays to a great cause.",
      "Heres your todo list to really make Saturday count:",
      "Today: Send me screen shots of your hacking work!\nIll use them to encourage even more great people to participate in the future.\n  \n  \n    Tomorrow: Submit your project! http://www.hackforchange.org/submit.\n15 teams who participated Saturday across the US will be invited to the White House in July to show off their work. It doesnt need to be finished to be submitted, and early submissions might get more attention. Charlottesville has a lot to be proud of, so make sure to show it off!\n  \n  \n    Ongoing: Keep hacking, and keep us posted.\nFrom Fire App to Merriweather Lewis/Trademark Navigator to State Decoded, Im proud of every app we worked on. Lets figure out how to make them all real!",
      "On that last note, would you like a free hand to tackle some part of you project thats out of your wheelhouse? How about web hosting for a year? Keep an eye on your email for some amazing gifts from eLance, Amazon and others as a no-strings Thank You for participating."
    ],
    "summary_t": ""
  },
  {
    "id": "42e5511e9c267cc6b9d0c94ccc372ff9",
    "url_s": "https://opensourceconnections.com/blog/2013/06/05/build-your-own-lucene-codec/",
    "title": "Build Your Own Lucene Codec!",
    "content": [
      "A Lucene Codec, a bit twiddlers dream!",
      "Ive been having a lot of fun hacking on a Lucene Codec lately. My hope is to create a Lucene storage layer based on FoundationDB – a new distributed and transactional key-value store. It’s a fun opportunity to learn about both FoundationDB and low-level Lucene details.",
      "But before we get into all that fun technical stuff, there’s some work we need to do. Our goal is going to be to get MyFirstCodec to work! Here’s the source code:",
      "public class MyCodec extends FilterCodec {\n    public MyCodec() {\n        super(\"MyCodec\", new Lucene42Codec());\n    }\n}",
      "Great! Done. Well not quite. A real codec looks more like this",
      "public final class SimpleTextCodec extends Codec {\n  // pretend there’s private vars here\n\n  @Override\n  public PostingsFormat postingsFormat() {\n    return postings;\n  }\n\n  @Override\n  public StoredFieldsFormat storedFieldsFormat() {\n    return storedFields;\n  }\n\n  @Override\n  public TermVectorsFormat termVectorsFormat() {\n    return vectorsFormat;\n  }\n\n  @Override\n  public FieldInfosFormat fieldInfosFormat() {\n    return fieldInfosFormat;\n  }\n\n  @Override\n  public SegmentInfoFormat segmentInfoFormat() {\n    return segmentInfos;\n  }\n\n  @Override\n  public NormsFormat normsFormat() {\n    return normsFormat;\n  }\n\n  @Override\n  public LiveDocsFormat liveDocsFormat() {\n    return liveDocs;\n  }\n\n  @Override\n  public DocValuesFormat docValuesFormat() {\n    return dvFormat;\n  }\n}",
      "This example is a bit more explicit. This is Mike McCandless’s SimpleText codec that is a great codec to browse for browsing for educational purposes. In this codec, each part of the class is being customized.",
      "Typically you only want to implement a subset of the components of the codec. Lucene provides a convenient base class called \"FilterCodec\". You can customize whatever pieces you’d like to, delegating the rest to another codec. For example, If we were, however, to implement a custom storage implemenation for only term vectors, we can override like so:",
      "public class MyCodec extends FilterCodec {\n    final private TermVectorsFormat myTermVectorsFormat;\n\n    public MyCodec() {\n        super(\"MyCodec\", new Lucene42Codec());\n        myTermVectorsFormat = new MyTermVectorsFormat();\n    }\n    // Use custom TermVectorsFormat, default everything else to Lucene42Codec\n    public TermVectorsFormat termVectorsFormat() {\n        return myTermVectorsFormat;\n    }\n}",
      "Here, we’re delegating to Lucene42Codec for everything except our special TermVectorsFormat.",
      "Each of these individual formats are separate pieces responsible for serializing to a backing store during indexing and deserializing into memory when read back into memory. For many of the formats though, it’s a bit more than that. For example, for the postings format, the format responsible for storing the inverted index, it’s vital to be able to efficiently iterate the inverted index. Storage of the inverted index must be done in such a way that we can easily iterate all the indexed fields, then all the terms indexed into that field, then in turn the documents with term frequencies and positions that contain that term in that field.",
      "You’ll find similar constraints as you implement the interfaces of the other pieces of the codec. Each of these pieces is a topic in its own right worth writing about. They all deserve their own blog articles. For now, I encourage you to explore the JavaDocs to see what might be fun to customize on the Lucene backend! Before I leave you to the Javadocs though, it’s important to tackle a few bits of plumbing – building & running Lucene’s tests against your codec.",
      "Plumbing! Unit Tests & More",
      "Let’s take care of a bit of plumbing. How do we setup a project for a codec? How do we run Lucenes tests against our implementation to confirm Solr/Lucene will function with our changes?",
      "Using maven to setup the project is fairly straight-forward. Luckily I’ve created a Lucene Codec hello world project on github to get you started. It captures setting up the project with Maven. Feel free to fork it to skip the first two steps below. You’ll still need to read below to learn how to run the Lucene tests against your codec.",
      "First, we’ll start by creating a straightforward maven project with a pom that depends on lucene-core at the version you’re targeting your codec for.",
      "Second we’ll need to publish via our META-INF/services directory that we have a class that implements the Codec interface. This advertises our codec to Lucene’s class loader. Under src/main/resources/services create a file called org.apache.lucene.codecs.Codec. In the file should be a single line with the full name of your Codec class:",
      "com.o19s.MyCodec",
      "We’ll need to tell mvn to copy this into the target/META-INF by specifying it as a resource to be copied into the target folder:",
      "<build>\n       <resources>\n           <resource>\n               <directory>src/main/resources/services</directory>\n               <targetPath>META-INF/services</targetPath>\n           </resource>\n       </resources>\n </build>",
      "Third pull down the full Lucene/Solr source tree. Lets test our codec from the command line!",
      "Package your codec into a jar:",
      "mvn package",
      "Under the Lucene source tree, run a single Lucene Test. Pass it the codec to use with the -Dtests.codec argument. Pass the jar with the codec you just packaged up with the lib argument. Executing this will prove that Lucene can find and load your codec. If Lucene can’t find load your codec, you’ll get an appropriate error right away.",
      "C:\\solr\\solr-4.3.0\\lucene>ant -Dtestcase=TestSegmentTermDocs -\nDtests.codec=MyCodec –lib \"C:\\path\\to\\target\\codec-1.0-SNAPSHOT.jar\" test",
      "Now Run all the tests!",
      "C:\\solr\\solr-4.3.0\\lucene>ant -Dtests.codec=MyCodec –lib\n\"C:\\path\\to\\target\\codec-1.0-SNAPSHOT.jar\" test",
      "Fourth, Naturally its going to be convenient to debug Lucene unit tests running our codec in Eclipse. Here’s what we need to do.",
      "Load your codec and the Lucene source code into Eclipse.\n  Create a new Junit debug configuration\n  Select the Radio button for \"Run all the tests in the selected project, package, or folder\"\n  Enter in the folder /path/to/lucene/core/src/test\n  Select the JUNIT 4 test runner\n  \n    In the arguments tab, for vm arguments specify:\n\n    -ea -Dtests.codec=MyCodec\n  \n  In the \"classpath\" tab, make sure both your codec project and the solr/lucene projects are selected",
      "Now you should be able to launch this debug configuration and go to town! Go forth and make some awesome codecs! Let us know about the codec you’re working on, love to hear about it!"
    ],
    "summary_t": "Ive been having a lot of fun hacking on a Lucene Codec lately. My hope is to create a Lucene storage layer based on FoundationDB – a new distributed and tran..."
  },
  {
    "id": "1c73a9ea2745663f0d550c5f34a83863",
    "url_s": "https://opensourceconnections.com/blog/2013/06/05/cassandra-tutorial-at-strataconf-hadoop-world-ny/",
    "title": "Cassandra Tutorial at StrataConf + Hadoop World NY",
    "content": [
      "Its just been confirmed that Ill be giving a tutorial at StrataConf + Hadoop World this October in New York.",
      "My tutorial is: \"An Introduction to Real-Time Analytics with Cassandra and Hadoop\". More information to come!"
    ],
    "summary_t": ""
  },
  {
    "id": "75b444440920acbb93377841700b3f76",
    "url_s": "https://opensourceconnections.com/blog/2007/03/06/adding-tls-support-to-tracs-wikinotifyscript/",
    "title": "Adding TLS Support to Tracs WikiNotifyScript",
    "content": [
      "Weve been using the WikiNotifyScript from trac-hacks.org for quite a while now. It helps us stay appraised of changes to all of our various wikis and its pretty easy to set up. However, when I had the brilliant idea of switching it to use Gmail as the SMTP server I ran into a snag: Gmail only supports logins through TLS. Recent Trac releases support TLS through smtplib, but unfortunately WikiNotifyScript doesnt share that code.",
      "After a quick grep through the trac source I found the magic incantation nestled inside notification.py. It was a pretty simple matter to port what trac does over to WikiNotifyScript, so after a brief round of testing I posted the patch over on WikiNotifyScripts trac. Hopefully thatll save somebody some trouble."
    ],
    "summary_t": ""
  },
  {
    "id": "508c0530ba6f3cea4c43add8608e32fd",
    "url_s": "https://opensourceconnections.com/blog/2013/06/06/forget-search-lets-talk-discovery-a-recap-of-enterprise-search-summit/",
    "title": "Forget Search, Lets Talk Discovery – A Recap of Enterprise Search Summit",
    "content": [
      "The middle of May saw me road tripping to not just one, but two Enterprise Search Summits to tell some war stories in doing search at scale. I attended my first ESS back in New York in 2010, and I was curious to see how the conversation had changed in the intervening three years. Would we still be talking about the same things three years later?",
      "So first the bad news…",
      "The Search industry as weve known it for the past 15 years is dead. There used to be many large pure search platform vendors like Autonomy, Endeca, FAST, and theyve all been driven into the arms of larger companies. These deep pocketed companies, and their deep pocketed customers, made Enterprise Search a niche cozy business to be in for years. There was a lot of conversation around \"What Next\" for the market. And some mourning of the commoditization of search, as evinced by the rise of both open source search engines like Solr and ElasticSearch, as well as the various small companies (BasisTech, SmartLogic, Raytion) building products that extend search engines, but arent search engines of their own.",
      "Martin White",
      "Additionally, as",
      "Martin White puts it, Enterprise Search is only (only!) a billion dollar business, compared to Business Intelligence at $14 billion, or the ERP market at $45 billion or so. And since Sharepoint 2013, with Microsofts acquisition of FAST, has turned itself into the dominant enterprise search engine/CMS solution, this redirects much of that billion dollars of revenue to Microsoft. So this sounds like a stagnating small market right where wed all better learn Sharepoint right?",
      "Stephen Arnold",
      "Not so fast!",
      "Stephen Arnold had something different to say in his very memorable talk on Big Data vs Search. The most interesting slide, which I wish I could find a original of to share, was of the word \"Search\" with the word \"Big Data\" splatted all over it. Conveying the idea that there was this massive buzzword coming that was going to steam roll everything in its way, and we all better get ready for its arrival because its going to change everything. He did write about some of his observations of how the search market is going to have to adapt to this.",
      "]11 The impact of Big Data on Enterprise Search",
      "However, I think he has the impact of the \"splat\" backwards. I think that Big Data is finally going to let the search community deliver what it has been promising for so many years. I like to talk about the impact of Machine Learning and Natural Language Processing in building smart search experiences. The search engine can move beyond being a statistical token matching tool to an actual discovery interface. One of the attendees commented that \"That sounds like an Autonomy pitch from 2004″, and yet that pitch is now becoming a reality, at a cost that any enterprise can afford, not just the deep pocketed ones. Big Data is going to let us build the search engines that weve promised our users so many years, and its re-invigorating the search market.",
      "So heres the good news for the search market:",
      "Once you move beyond the definition of our market as the narrowly defined \"Enterprise Search\" to the more general \"Discovery for Enterprises\", our market size explodes. Gartner in their 2012 Magic Quadrant for Business Intelligence and Analytics Platforms report led off with the quote:",
      "The dominant theme of the market in 2012 was that data discovery became a mainstream BI and analytic architecture. The market also saw increased activity in real time, content and predictive analytics.",
      "And search engines are the dominant platform for delievering discovery applications.",
      "As more evidence of the value of search, have you noticed that all the major Big Data vendors have been integrating search into their offerings? Cloudera came out this week with Cloudera Search that puts search on Hadoop. LucidWorks, DataStax, and Elasticsearch all have offerings that marry Big Data and Search together.",
      "So lets all get beyond Search meaning a query box and 10 blue links on a web page, and lets embrace the new world of Discovery that Big Data enables! And next year, instead of meeting up at the Enterprise Search Summit, lets attend the Discovery Summit for Enterprises."
    ],
    "summary_t": ""
  },
  {
    "id": "1d2bc3c66063bc4c50236834d420695c",
    "url_s": "https://opensourceconnections.com/blog/2013/06/07/search-as-you-type-with-solr/",
    "title": "Search-As-You-Type with Solr",
    "content": [
      "In my previous post, I talked about implementing Suggest-As-You-Type using Solr. In this post I’ll cover a closely related functionality called Suggest-As-You-Type.",
      "Several years back, Google introduced an interesting new interface for their search called Search-As-You-Type. Basically, as you type in the search box, the result set is continually updated with better and better search results. By this point, everyone is used to Googles Search-As-You-Type, but for some reason I have yet to see any of our clients use this interface. So I thought it would be cool to take a stab at this with Solr.",
      "Lets get started. First things first, download Solr and spin up Solrs example.",
      "cd solr-4.2.0/example\njava -jar start.jar",
      "Next click this link and POOF! you will have the following documents indexed:",
      "Theres nothing better than a shiny red apple on hot summer day.\n  Eat an apple!\n  I prefer a Grannie Smith apple over Fuji.\n  Apricots is kinda like a peach minus the fuzz.",
      "(Kinda cool how that link works isnt it?) Now lets work on the strategy. Lets assume that the user is going to search for \"apple\". When the user types \"a\" what should we do? In a normal index, theres a buzillion things that start with \"a\", so maybe we should just do nothing. Next \"ap\" depending upon how large your index is, two letters may be a reasonably small enough set to start providing feedback back to your users. The goal is to provide Solr with appropriate information so that it continuously comes back with the best results possible.",
      "Attempt number 1: Directly query Solr with current search string.",
      "User has typed \"ap\", so you search for q=\"ap\":",
      "http://localhost:8983/solr/browse?q=ap",
      "… and nothing is returned. But youre nobodys fool. A quick glance at the schema and you see that the \"title\" field (which weve indexed these sentences into) is tokenized with the StandarTokenizer, so of course \"ap\" wont match either of the plausible tokens \"apricot\" or \"apple\". Therefore:",
      "Attempt number 2: N-gram Tokenize the documents.",
      "Just add this fieldType to your schema,",
      "<fieldType name=\"text_general_edge_ngram\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n   <analyzer type=\"index\">\n      <tokenizer class=\"solr.LowerCaseTokenizerFactory\"/>\n      <filter class=\"solr.EdgeNGramFilterFactory\" minGramSize=\"2\" maxGramSize=\"15\" side=\"front\"/>\n   </analyzer>\n   <analyzer type=\"query\">\n      <tokenizer class=\"solr.LowerCaseTokenizerFactory\"/>\n   </analyzer>\n</fieldType>",
      "set the \"title\" field to be of type=\"text_general_edge_ngram\", restart, reindex and viola! The queries above will now work just fine.",
      "You can do this… but I dont like it. Why? Because the average English word is 5 letters long. And if you account for stopwords, probably up to 6 or 7. So BAM! your index is now substantially larger than it was before. Lets try something else.",
      "Attempt number 3: Use wildcards.",
      "Rather than trying number 3, lets just add an asterisk \"\" at the end of the users search string. This wildcard will match zero or more additional characters at the end of the users search term. So when the user types \"ap\", the search string becomes \"ap\":",
      "http://localhost:8983/solr/browse?q=ap*",
      "Thats more like it! We can see that we are now catching all 4 documents because of the terms apple and apricot. And as the user keeps typing, the appropriate thing happens. When the user adds an extra \"p\", the search becomes",
      "http://localhost:8983/solr/browse?q=app*",
      "And with that the document for apricot appropriately disappears.",
      "So we have solved the problem, right? … Not so fast, what happens when the user has typed the complete word \"apple\". In this case the search becomes:",
      "http://localhost:8983/solr/browse?q=apple*",
      "Notice anything fishy? I do… Why on earth is \"Eat an apple!\" the second results. This sentence is only three terms long, it should be sorted to the top. Similarly, the first result is the longest sentence in the bunch, it should be sorted to the bottom! As proof that this is the \"proper\" order of things, lets do a plain search for apple:",
      "http://localhost:8983/solr/browse?q=apple",
      "As you can see, the results are now returned in order of length of sentence – as they should be. But why the confusing arrangement? Its because wildcard queries are not scored at all! Thats right! The strange ordering from before is the order in which the documents were indexed. Theres gotta be something we can do.",
      "Attempt number 4: Use wildcards and the last token in the search.",
      "Heres an idea, what if we just stick both the wildcard and the users current input into the search. So for instance, if the user types \"appl\" then the query is for q=appl appl*. And if the user types \"apple\" then the query is for q=apple apple*. In the first case token \"appl\" returns nothing and the wildcard \"appl*\" at least returns reasonable results. Take a look:",
      "http://localhost:8983/solr/browse?q=appl+appl*",
      "In the second case the search term \"apple\" matches and the results all come back correctly ordered:",
      "http://localhost:8983/solr/browse?q=apple+apple*",
      "The only think left is for finishing touches. If the user types in multiple terms, for instance \"red apple\", then its only the final term that should be queried with and without the wild card.",
      "http://localhost:8983/solr/browse?q=red+apple+apple*\n\n  http://localhost:8983/solr/browse?q=red+appl+appl*",
      "Implementation",
      "Of course the strategy presented above is nothing without a swanky AJAXy UI. As a predominantly back-end developer – heres the best I have to offer.",
      "Its not much to look at, but save it to your desktop and youll immediately be able to use it with the demo above. Take a look at the code and youll see that were implementing the strategy above.",
      "Additionally this little example makes use of a really nice auto-suggest pattern that I described in my previous post.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "a015d1a6a477b6df31d5e4115f5362a0",
    "url_s": "https://opensourceconnections.com/blog/2013/06/08/advanced-suggest-as-you-type-with-solr/",
    "title": "Advanced Suggest-As-You-Type with Solr",
    "content": [
      "In my previous post, I talked about implementing Search-As-You-Type using Solr. In this post Ill cover a closely related functionality called Suggest-As-You-Type.",
      "Heres the use case: A user comes to your search-driven website to find something. And it is your goal to be as helpful as possible. Part of this is by making term suggestions as they type. When you make these suggestions, it is critical to make sure that your suggestion leads to search results. If you make a suggestion of a word just because it is somewhere in your index, but it is inconsistent with the other terms that the user has typed, then the user is going to get a results page full of white space and youre going to get another dissatisfied customer!",
      "A lot of search teams jump at the Solr suggester component because, after all, this is what it was built for. However I havent found a way to configure the suggester so that it suggests only completions that that correspond to search results. Rather, it is based upon a dictionary lookup that is agnostic of what the user is currently searching for. (Please someone tell me if Im wrong!) In any case, getting the suggester working takes a bit of configuration. – Why not use a solution that is based upon the normal, out-of-the-box Solr setup. Heres how:",
      "Facet Based Suggestions",
      "As your user searches through your inventory, the Solr query q parameter will constantly be changing. Additionally, you may add any number of filter queries fq to further constrain the search results. As the search results change, consider what is happening to your facets. The only facet values displayed will be those values that are consistent with the current search results. Additionally, the facet values are sorted according to the number of documents associated with that value.",
      "Now consider what happens if you include a text field as a faceted field. When the user makes a query, a list will be returned that contains all the words that exist in the remaining documents sorted by count. Whats more, the user can type these terms in and be guaranteed to find results.",
      "But theres one small matter, to contend with – Ideally, you would want the suggestions to complete the word that you user is currently typing. However the values remaining in the field may start with any letter of the alphabet. Fortunately, theres is a less commonly used parameter that fixes our problem: facet.prefix. By specifying facet.prefix, the facet values will be constrained to only those values that start with the stated prefix.",
      "Implementation",
      "This is all theoretical so far. Its a lot easier to understand if you have an example in hand. To start things off download and start the example Solr",
      "cd solr-4.2.0/example\njava -jar start.jar",
      "Next click this link and POOF! you will have the following documents indexed:",
      "There’s nothing better than a shiny red apple on hot summer day.\n  Eat an apple!\n  I prefer a Grannie Smith apple over Fuji.\n  Apricots is kinda like a peach minus the fuzz.",
      "(Kinda cool how that link works isn’t it?) Weve indexed these sentences into the \"title\" field of the example setup. This is because there is no stemming on the titles here. This is important! If you facet over a stemmed field, then the users will see suggestions that arent English!",
      "In your case, youll probably be searching over several fields, so make sure that youre dumping the text of each of these fields into a single non-stemmed field.",
      "Now lets take a look at a quick search and get oriented:",
      "http://localhost:8983/solr/select?q=ap*&facet=true&facet.field=title",
      "Here, I appear to be searching for \"apricot\" or \"apple\", but I havent completed my search. (Theyre both so tasty… who can choose?) Ive also turned on faceting and I am faceting over the title field. Lets take a look at the contents of that facet:",
      "<lst name=\"facet_fields\">\n    <lst name=\"title\">\n      <int name=\"a\">3</int>\n      <int name=\"apple\">3</int>\n      <int name=\"an\">1</int>\n      <int name=\"apricots\">1</int>\n      <int name=\"better\">1</int>\n      <int name=\"day\">1</int>\n      <int name=\"eat\">1</int>\n      <int name=\"fuji\">1</int>\n      <int name=\"fuzz\">1</int>\n      <int name=\"grannie\">1</int>\n      <int name=\"hot\">1</int>\n      <int name=\"i\">1</int>\n      <int name=\"is\">1</int>\n      <int name=\"kinda\">1</int>\n      <int name=\"like\">1</int>\n      <int name=\"minus\">1</int>\n      <int name=\"nothing\">1</int>\n      <int name=\"on\">1</int>\n      <int name=\"over\">1</int>\n      <int name=\"peach\">1</int>\n      <int name=\"prefer\">1</int>\n      <int name=\"red\">1</int>\n      <int name=\"shiny\">1</int>\n      <int name=\"smith\">1</int>\n      <int name=\"summer\">1</int>\n      <int name=\"than\">1</int>\n      <int name=\"the\">1</int>\n      <int name=\"there's\">1</int>\n    </lst>\n</lst>",
      "As you can see. This is a list of every token in the title field sorted by count. Lets add the prefix parameter.",
      "http://localhost:8983/solr/select?q=ap*&facet=true&facet.field=title&facet.prefix=ap",
      "Now, the facets are constrained to only those that complete the users current term. Whats more either of these terms are guaranteed to provide results to the user. We even know how many results will be returned!",
      "<lst name=\"facet_fields\">\n  <lst name=\"title\">\n    <int name=\"apple\">3</int>\n    <int name=\"apricots\">1</int>\n  </lst>\n</lst>",
      "Demo",
      "The only thing left is to stick these results into a drop down box. And thats just what Ive done here. Nope, its not much to look at, but Ive build a simple interface that implements this pattern, works with this example here, and displays suggestions back to the user on a per-term basis. (Whats more, this also demonstrates the Search-As-You-Type described in my previous post.)",
      "Improvements",
      "There are plenty of fun things you could do to improve upon this pattern. As far as user experience, rather than a drop-down, consider providing the suggestions as greyed-out text ahead of the users cursor. If they see a suggestion they like, they can tab complete and be confident that when they press return the result page will not be blank.",
      "In the past, Ive implemented this pattern to make suggestions from two separate fields. In that instance, our client wanted to provide autocomplete suggestions of user names, first or last. So we simply extended this pattern to both the first_name and last_name fields and when users typed \"joh\" in the search box, they would be quickly suggested John from first_name and Johnson from last_name. The suggestions were presented in a drop-down menu separated according to first and last names.",
      "You can also extend this pattern to make multi-term suggestions. How? Just send the text to a shingling field and facet over that. (Text shingling is basically like n-gramming, but for terms rather than individual characters.) Obviously, one thing youll have to look out for with this method is pretty substantial growth in index size.",
      "Use With Caution",
      "This pattern provides an excellent user experience, but you should make sure to test it out first to see that it works well in your situation. Consider the following points:",
      "Response time: The most obvious problem is that this technique performs yet another facet search so the request will take longer.\n  Memory: Calculating facet values requires allocation of memory for each token in the field. This is never a problem for a field composed of enumerated tags, but for a text field you may want to keep an eye on memory usage.\n  Inappropriate Content: Be very cautious about content of the fields being used for suggestions. For instance, if the content has misspellings, so will the suggestions. And dont include user comments unless you want to endorse their opinions and choice of language as your search suggestions!",
      "",
      "Check out my LinkedIn Follow me on Twitter",
      "Thanks to David Smiley who pointed this pattern out to me a while back."
    ],
    "summary_t": ""
  },
  {
    "id": "4c702f991e45d3d953e1540bf4884cfe",
    "url_s": "https://opensourceconnections.com/blog/2013/06/17/lockdown-solr-with-iis-as-a-reverse-proxy/",
    "title": "Lockdown Solr with IIS as a Reverse Proxy",
    "content": [
      "We’ve been developing rich client-side applications that talk directly to Solr’s HTTP interface from Javascript – requiring a publicly accessible Solr. One concern that you’ll naturally have with Solr is that by default Solr’s HTTP API has no concept of security. For example, after standing up Solr, anyone, from anywhere can browse to your index and delete everything.",
      "At this point you could go and hack up the Jetty or <insert servlet container> configs to block dangerous Solr request handlers. You’re likely to make life hard on yourself.",
      "Alternatively, you could simply put Solr behind a reverse proxy. By proxying Solr, you move the outside world to their own HTTP endpoint, blocking direct access Solr with a firewall. Internal users behind the firewall have full access to Solr to do the everyday work of deleting and updating the index. External users coming through the proxy have an extremely limited set of actions they can perform, based on the HTTP requests that the proxy forwards.",
      "Turns out this is the recommended approach to take if public Solr access is needed.",
      "If hosting on Windows, IIS provides a very straight-forward and easy to maintain method for creating a reverse proxy. It’s a nice solution, as IIS will be updated automatically and fits in seamlessly in with the Windows ecosystem. The steps are fairly straightforward.",
      "Assuming you have a Windows Server 2012 box with a running Solr and Solr’s port (i.e. 8983) is blocked to the outside world but accessible over localhost:",
      "Install IIS on the Server\n  Install Application Request Routing and URL Rewrite Modules\n  \n    Open IIS Manager, selecting the server find Application Request Routing\n\n    \n  \n  Step 3\n\n  \n  \n    Double click, and select \"Proxy\". Click the checkbox to enable the proxy.\n\n    \n  \n  Step 4\n\n  \n  \n    Browse to the config directory for the default web page\n\n    \n  \n  Step 5\n\n  \n  \n    For the default web page, modify or add the web config with this rewrite rule:\n\n    <configuration>\n  <system.webServer>\n    <rewrite>\n      <rules>\n        <rule name=\"Proxy To Solr Search\" stopProcessing=\"true\">\n          <!-- capture solr args for select and fwd -->\n          <match url=\"^solr/select(.*)\" />\n          <action type=\"Rewrite\" url=\"http://127.0.0.1:8983/solr/select{R:1}\" />\n        </rule>\n      </rules>\n    </rewrite>\n  </system.webServer>\n</configuration>\n    \n  \n  Back in IIS Manager, restart your the default webpage and, browse to http://yoursolr/solr/select?q=*:* and confirm you can query Solr. Browse anywhere else, and youll get IISs 404. Nice!",
      "Note, to help define your rules, you can use the \"URL Rewrite\" UI in IIS manager under \"Default Web Page\".",
      "Now go have fun and tell us about your rich (and now secure) Javascript Solr apps!"
    ],
    "summary_t": ""
  },
  {
    "id": "b7ded3040b1d9f1767dc3161f3d4de46",
    "url_s": "https://opensourceconnections.com/blog/2013/06/25/getting-started-with-cassandra-overview/",
    "title": "Getting Started with Cassandra: Overview",
    "content": [
      "Over the next couple of weeks, we will be posting a 4-part series introduction to Cassandra. This series will take you through all of the basic components of setting up a Cassandra cluster.",
      "This is the first of the 4-part series.",
      "Overview",
      "Cassandra has been in the news a lot recently: Many companies are turning to NoSQL solutions to address their growing data needs.",
      "With any new technology comes fear, uncertainty, and the never-ending barrage of SQL-vs-NoSQL flame wars. These posts seek to address these topics in a way to give you a good sense of what Cassandra is, and why you might want to implement a new technology in your stack.",
      "What is it good for?",
      "How do I set up my data model?",
      "Is Cassandra the silver bullet for all of my data storage needs?",
      "Why is my data model wrong?",
      "Cassandra is not a single-serve solution – it’s highly-specialized for certain use cases. To understand what Cassandra is best at, we need to take a look at why a new database was created in the first place.",
      "History",
      "Cassandra began at Facebook in 2007 as a joint effort between Avinash Lakshman and Prashant Malik to power the new company’s massively-growing inbox search platform. Lakshman, originally from Amazon, had co-invented Amazon’s Dynamo platform for storing distributed data. Thus, Cassandra borrows heavily from the Dynamo model, Google’s BigTable system, and many other distributed systems.",
      "Facebook needed to handle a very high throughput – up to billions of writes per day – while replicating their data across geo-graphically distributed systems to keep search latency down. The system also needed to be able to scale with the growth of users, as Facebook was adding many, many users per day.",
      "Fast, Available, Eventually Consistent",
      "From this came Cassandra, a geographically-distributed, very available, highly-scalable columnar storage system.",
      "Fig 1: A 3-node Cassandra cluster.",
      "It’s constantly available, even with server failures.",
      "Data is replicated across many different nodes: If one goes down, there are many more to take its place.",
      "It handles fast writes.",
      "Because any node can respond to a request, the entire system can handle throughput of tens of thousands of writes per second.",
      "Read speed is configurable, making the data eventually consistent.",
      "Data consistency is adjustable, meaning you decide to focus on speed or consistency.",
      "Performance scales linearly.",
      "To get more performance, add another node. Cassandra was built to handle hundreds of terabytes of data across hundreds and hundreds of nodes.",
      "No Silver Bullet: The Tradeoffs of Using Cassandra",
      "A common misconception is that Cassandra simply replaces extant relational databases. Cassandra is not a relational database. In fact, this is so important, I’ll repeat it: Cassandra is not a relational database.",
      "This means a number of things:",
      "Cassandra doesn’t value ACIDity, it holds an eventually consistent model, so data may be outdated when a query is sent.\n  Deletes are hard in append-only systems. Data must be continually compacted to ensure tombstones are properly propagate throughout the entire system.\n  And, of course, JOINs do not exist in Cassandra; operations like this are simply too expensive.",
      "Instead, Cassandra column families (tables) are modeled around the queries you intend to ask.",
      "An Example",
      "To clarify the image, let’s look at a quick example. Say you run a social networking site, and you want to display all of a user’s followers, as well as everyone that they follow.",
      "In a relational world, we set this up using two tables: a User table, and a User_Follower_Link table.",
      "We set up our schema thusly:",
      "User\n\n  user_id     name\n\n  1           Patricia\n\n  2           John\n\n  3           Scott\n\n  4           Doug\n\n  User_Follower_Link (userA follows userB)\n\n  id      userA       userB\n\n  x           1           2\n\n  x           1           3\n\n  x           4           1\n\n  x           1           4\n\n  x           3           1\n\n  x           4           2\n\n  x           3           4",
      "",
      "So, to find the list of John’s followers, we’d run:",
      "select * from User\n\n  join User_Follower_Link on user_id = userB\n\n  where name = 'John';\n\n  Finding out who each user follows is similar:\n\n  select * from User\n\n  join User_Follower_Link on user_id = userA\n\n  where name = 'John';",
      "This is fine for small datasets, but what happens when you grow to millions of users? Your simple JOIN will take a very long time to scan through and compare each row, or your database could even time out from the strain.",
      "Modeling this problem in Cassandra is quite simple, and requires two separate column families.",
      "Each user composes a new row, each containing multiple columns. Note that the actual columns need not match.",
      "Users_Followers",
      "patricia: ['Scott'], ['Doug']\n  john: ['Scott'], ['Patricia']\n  scott: ['Patricia']\n  doug: ['Patricia'], ['Scott']",
      "Users_Following",
      "patricia: ['John'], ['Doug'], ['Scott']\n  john:\n  scott: ['Patricia'], ['Doug']\n  doug: ['John'], ['Patricia']",
      "",
      "",
      "In reality, each column comprises a key-pair, a timestamp, and an optional TTL, though it’s been simplified for our model.",
      "Retrieving the data is quite similar using CQL (Cassandra Query Language).",
      "To get a list of John’s followers:",
      "select * from Users_Followers where name = 'john';",
      "To get a list of who John is following:",
      "select * from Users_Following where name = 'john';",
      "Notice how the actual data is duplicated across different column families. This is encouraged – data should be optimized for the application, and not the other way around.",
      "Conclusion",
      "Cassandra works very well for a specific set of problem areas. If you have a use case that you think warrants Cassandra, let us know! What are you considering at your company? What tradeoffs are you concerned about?",
      "Next week we will discuss the Cassandra architecture, and go over how each component fits together."
    ],
    "summary_t": ""
  },
  {
    "id": "2d529d3ee9ebafad593f0b6d6ed86bd3",
    "url_s": "https://opensourceconnections.com/blog/2013/06/25/how-to-delete-millions-of-amazon-s3-objects-in-one-swell-foop/",
    "title": "How to delete millions of Amazon S3 objects in one swell foop",
    "content": [
      "Did you accidentally upload millions of documents into S3 under the wrong key in your bucket? Maybe ran 40 servers over night reprocessing data only to discover that its in the wrong place? Dont want to run 40 servers over night deleting said data? Or watching Cyberduck churn for a week deleting each object one by one?",
      "Then use S3s lifecycle option to delete those documents. On a recent project we stored all our objects under a year tag in our bucket:",
      "mybukket/1999/\nmybukket/2000/\nmybukket/2001/",
      "And we did this for data ranging from 1985 to 2011. We decided that to support future datasets we wanted to aggregate them under dataset name. So we then copied the data to:",
      "mybukket/datasetA/1999/\nmybukket/datasetA/2000/\nmybukket/datasetA/2001/",
      "And then contemplate the delete process. Fortunately it turned out to be very easy by setting up a lifecycle rule to delete all the keys that started with 19, and then 20, which covered everything from 1995 to 2011, and left everything under the /datasetA/ subdirectory:",
      "",
      "It does take 24 hours to happen, but is much faster, and feels much less error prone then running your own job to do this!"
    ],
    "summary_t": ""
  },
  {
    "id": "bb9f307b8bb2e3e06ba90588e0aa1fe5",
    "url_s": "https://opensourceconnections.com/blog/2013/06/30/coming-soon-to-state-decoded/",
    "title": "Coming Soon to State Decoded",
    "content": [
      "",
      "Updated 8/19/2017 to fix web links",
      "State codes are wretched. Seriously, look at a few: California’s, New York’s, Illinois’, and Texas’ are all good examples of how stunningly difficult that it is to understand state laws. They don’t have APIs. Virtually none have bulk downloads. You’re stuck with their crude offerings.",
      "The State Decoded is a platform that displays state codes, court decisions, and information from legislative tracking services to make it all more understandable to normal humans. With beautiful typography, embedded definitions of legal terms, and a robust API, State Decoded aims to make law more accessible by the very people that are governed by the law.",
      "State Decoded is a great resource and OSC recently completed a project that will soon make it even better! The focus of this project was to improve the State Decoded search functionality. In the remainder of this post we will describe the current status of State Decoded search, we will describe the changes coming soon, and we will describe the long term goals for State Decoded search.",
      "Current Status: Good, but Could be Better",
      "In an earlier post, Waldo Jaquith describes our initial work with State Decoded. In that project OSC equipped State Decoded with basic search features using the Solr search technology. Among other things, we gave Stated Decoded users the ability to search the text of laws and of legal definitions, to find related laws, and to filter the results according to the types of documents that they are seeking.",
      "However, after completing this initial project, State Decoded still lacked some of the polished features available through Solr. For example in the current implementation:",
      "Keyword highlighting is only present in the search results; there is no highlighting in the text of the laws or legal definitions.\n  Tag clouds and facet displays are present but are disorganized and are therefore not very helpful.\n  There is no suggest-as-you-type functionality to help the users as they search.",
      "Coming Soon to State Decoded!",
      "In the recently completed project, OSC has built several new search futures that will greatly aide State Decoded users as they search through laws. Our contributions can be found in this Github code repository, which will soon be added back to the original code base.",
      "Improved Faceting",
      "Facets are collectively the lists of menus commonly found on search pages that help users filter through search results. For instance, if you search for \"running shoes\" on Amazon then on the side of the page, there are several facets that allow you to filter through results based upon color, brand, and price.",
      "Here is a screen capture of the current facets as seen in the current version of Stated Decoded.",
      "",
      "As you can see, these facets are functional in that you can use them to filter through search results. However they are not very informative: the formatting is inconsistent, there is no information about exactly how many documents are remain in each category, and the tag clouds are disorganized and difficult to read.",
      "Here is an example of what Stated Decoded facets may look like in the near future:",
      "",
      "There are several things to point out here. For one, the facets are all now consistently formatted as clickable hyperlinks. Also, all items in the facets are associated with a document count so that the user gets a more intuitive understanding of which areas of law are pertinent to their search. Finally, the \"title\" facets are presented in a hierarchical fashion preserving the structure of the law.",
      "Law Search by Section Number",
      "Commonly, State Decoded users will search for specific laws by typing in their section number (e.g. 13.1). Soon, State Decoded will detect when users are searching for a law by section number and will provide the appropriate results. Obviously, at the top of the result list will be that specific law (e.g. 13.1), but this will be followed by subsections (e.g. 13.1-6) and higher-level sections (e.g. 13). The results will also include laws that refer to the section specified in the users query.",
      "Highlighting",
      "Soon, State Decoded will make much more use of highlighting. On the search results page, results will include highlighted snippets of text that match the users query. When a user clicks on a particular result, the full text of that section of the law will be highlighted with the users search terms.",
      "Suggest-as-you Type",
      "One of the most exciting new features will be \"suggest-as-you-type\". As a user types in their search string, they will be prompted with terms that match match laws and legal definitions in the search index. This should make it much easier for users to find to information they are searching for.",
      "An Even Brighter Future",
      "Our current work with State Decoded represents a great leap forward for the search user experience. However there is always more work to do! If we have an opportunity to work with State Decoded in the future, one of the main goals will be to tackle the fact that legal code was written using a jargon that is sometimes difficult to understand for the lay person. As a simple example, a user may search using terms that they are familiar with, such as \"car license\", however the laws that are most relevant to this search may only refer \"motor vehicle licenses\". In the future this problem can be addressed through the use of rich synonyms for legal jargon."
    ],
    "summary_t": "State codes are wretched. Seriously, look at a few: California’s, New York’s, Illinois’, and Texas’ are all good examples of how stunningly difficult that it..."
  },
  {
    "id": "2469642eae8b59f7c19a631f33082556",
    "url_s": "https://opensourceconnections.com/blog/2013/06/30/finding-relationships-in-trademark-data/",
    "title": "Finding relationships in Trademark Data",
    "content": [
      "At the recent National Day of Civic hacking here at OSC we dug into a few ways to find relationships between Trademarks files with the USPTO.",
      "If youve ever played with the US trademark data youll know that its both plentiful and scarce. There are lots of trademark fillings, each with the minimum possible data to make them uniquely identifiable.",
      "Thats great for streamlined government and citizen anonymity, but no fun for finding the relationships between filings. We needed to suss out more information about the graph of trademarks.  Thats when we Eric and Wes tripped over the translations included in many of the patent filings.  We wondered if the term space for these translations might be smaller and more consistent then the space defined by the actual trademarks.  Translations were less likely to play games with spelling or grammar the way one might with the actual mark.",
      "Some Hacking with the data and Neo4j resulted in an intriguing dataset that we are still unpacking.  Want to play with the data? Neo4J loaded with data is at this url: http://rosetta.bloom.sh:7474/webadmin/",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "873456f1f911c5e126a5ca842f5716eb",
    "url_s": "https://opensourceconnections.com/blog/2013/07/02/getting-dissed-by-dismax-why-your-incorrect-assumptions-about-dismax-are-hurting-search-relevancy/",
    "title": "Getting Dissed by Dismax – Why your incorrect assumptions about dismax/edismax are hurting search relevancy",
    "content": [
      "In the competition between field scores, theres little fairness.",
      "When you learn about the dismax query parser for the first time, one of the first things you learn about is qf, pf and friends. To use (e)dismax you take a google-like query >laws about hats< and use qf/pf/etc to specify which fields to search.",
      "What we next learn is that if we apply boosts to those fields, we can carefully tune how much impact each field has on scoring. If we take my favorite open-source law search project State Decoded as an example, we might boost fields accordingly:",
      "qf=text^1 catch_line^5 tags^3",
      "Weve made some estimation of how text, catch_line (the law title), and tags should be weighted. The naive user looks at this and thinks holistically, catch_line has been defined as five times as important as text. That catch_line matches will get a gentle nudge with respect to other matches. As if this:",
      "qf=text catch_line tags",
      "meant all matches would all be treated fairly.",
      "This thinking is largely a fallacy. Why? Two reasons:",
      "Lucene’s scores are field-relative and\n  Dismax does not apply \"weights\". It chooses the max of a field score: causing either-or scoring behavior.",
      "Different Scoring Universes",
      "In Lucene, each field is its own little scoring universe. Usually (but not always) the laws of physics are basically the same in these universes – TF-IDF. However, the fundamental constants that feed these formulas differ per field.",
      "TF-IDF tends to be the predominate scoring model in use. At its core, a documents score is proportional to the number of query terms it contains and inversely proportional to the number of times those query terms occur in the entire corpus. Effectively, TF-IDF scoring serves to boost the documents that are most densely filled the users query terms.",
      "Solr/Lucene scores on a per-field basis. The factors that feed into TF-IDF can change dramatically between fields. For example, the scoring calculations that happen in a short title field, such as catch_line are going to be very different from the characteristics of a free text field, such as text.",
      "Given these two Lucene queries:",
      "q=catch_line:police\nq=text:police",
      "The document frequency for the term \"police\" will be radically different between the short catch_line field vs. the larger, free-form text field. Fields may also have configuration options enabled that influence scoring, such as omitNorms or omitTermFrequenciesAndPositions. A given field might even be configured to use some kind of custom scoring, rendering scores almost completely unrelated.",
      "So when we switch to dismax, and search State Decoded with qf=text catch_line, what will the scoring look like? To figure that out, need to search separately first with qf=text to get text’s best matches, then with qf=catch_line to get catch_lines best matches. Lets search for both >insurance< and >police< to get an idea for what each fields scores look like:",
      "Best Matches for catch_line",
      "3.4429686 = (MATCH) sum of:\n    3.4429686 = (MATCH) weight(catch_line:polic in 1768) [DefaultSimilarity], result of: (OMITTED)\n3.4429686 = (MATCH) sum of:\n    3.4429686 = (MATCH) weight(catch_line:polic in 7674) [DefaultSimilarity], result of: (OMITTED)\n5.2182307 = (MATCH) sum of:\n    5.2182307 = (MATCH) weight(catch_line:insur in 7902) [DefaultSimilarity], result of: (OMITTED)\n3.3893402= (MATCH) sum of:\n    3.3893402 = (MATCH) weight(catch_line:insur in 894) [DefaultSimilarity], result of: (OMITTED)",
      "Best Matches for text:",
      "1.8226589 = (MATCH) sum of:\n  1.8226589 = (MATCH) weight(text:polic in 13273) [DefaultSimilarity], result of: (OMITTED)\n    1.8226589 = score(doc=13273,freq=4.0 = termFreq=4.0\n1.7184192 = (MATCH) sum of:\n  1.7184192 = (MATCH) weight(text:polic in 11078) [DefaultSimilarity], result of: (OMITTED)\n1.5556965= (MATCH) sum of:\n    1.5556965 = (MATCH) weight(text:insur in 6536) [DefaultSimilarity], result of: (OMITTED)\n1.5397402 =  (MATCH) sum of:\n    1.5397402 = (MATCH) weight(text:insur in 13116) [DefaultSimilarity], result of: (OMITTED)",
      "Notice how the numbers line up. Scoring, being a field-relative calculation, seems to have placed a good score in the catch_line field in the 5ish range. The text field seems to be happy to report \"good\" in the 1.8’s.",
      "Well we say \"good\" but we don’t quite have a notion of if 1.8 is truly an amazing result for that field, or a truly abysmal result. Maybe it turns out that because of the characteristics of these fields, 5 is a terrible score for catch_line while 1.8 is a truly off-the-charts amazing score for text.",
      "Take a second to let this sink in. Our inclination is often to give title fields a big \"boost\" due to importance. But they may already get a pretty nice boost relative to other fields just by the nature of the scoring universe created for that field.",
      "In short, the scores and their relative scales are completely specific to each field and unrelated to each other. This is a crucial bit of information as we consider the most important feature of a dismax:",
      "Dismax causes either-or scoring behavior",
      "Dismax takes the maximum score of multiple field’s scores. As weve seen, field scores come from independent measurement universes, rendering this not much better then ranking college applicants by taking a maximum of their SAT score and GPA.",
      "Because of this, dismax can create a winner-takes-all scenario where one field’s score dominates the final ranking. All the top results could be scored best simply because one field’s scores tend to be higher by default. According to dismax, students always get sorted by SAT score, not by GPA because we can pretty much guarantee that: max(SAT(student), GPA(student)) == SAT(student).",
      "The same thing can happen when scoring fields. In the example above, catch_line matches just happened to be higher than text matches out-of-the-gate, so our results could be hundreds of good catch_line matches first, followed by the good text matches.",
      "Heres an example that illustrates how destructive this behavior can be. Say we have a tags field, and sometimes our query matches a tag, and sometimes it doesnt. If the tags fields scores are very high, when a tag does match, this might completely overwhelm the value of the other fields scores. So if we queried for >car insurance< laws and >insurance< happens to match on our tags, our results might look quite odd:",
      "Search results for >car insurance<",
      "Law about car insurance (tag score == 100, text score = 1.9)\n  Law about life insurance (tag score == 99, text score = 1.6)\n  Law about travel insurance (tag score == 98, text score = 1.6)\n  Law about health insurance (tag score == 97, text score = 1.5)",
      "Laws we SHOULD have",
      "Law about car insurance (text score = 1.9)\n  Law about kids with car insurance (text score = 1.9)\n  Laws about dogs with car insurance (text score = 1.8)",
      "Suddenly the mere presence of a match on this field causes our results to look rather odd. Dismax’s \"either or\" winner-take-all behavior has preferred the match on the Tags rather than the other fields, causing us to blow away relevant matches from other fields. Effectively dismax moves the large block of good Tags matches up to the top and disregards other potentially valuable matches.",
      "Boosting, tie, and other mitigation options",
      "In our previous example, the introduction tags dominated the results. Does downboosting tags help? What if we downboosted to roughly the range of the other scores? If tags tend to go from 10 (bad tags match) to 100 (good tags match) we could boost by adding ^0.01. to tags in qf.",
      "While this does help, its not perfect. It assumes that scores will have identical distributions through the scoring space. You’ll still have winner-take-all situations occasionally. It also doesn’t work great if instead of a scale of 10 to 100, we have 99 as a terrible score and 100 as a good score for a field.",
      "Does tie help? The tie parameter lets you layer in other field matches into your score. Instead of the maximum of the score you get:",
      "score = max(scores) + tie * sum(otherscores)",
      "A tie of 1.0 effectively just turns the score into something of a \"DisSum\" score. The sum of all the matching fields scores becomes the overall score. This might help, but if one field’s score tends to be in the 100s for a good score while another tends to be in the single digits, scoring is still going to be a \"winner takes all\" scenario as the sum is dominated by the larger score. Trying to normalize scoring via boosting though may make a tie parameter more valuable.",
      "Relevancy is hard, let’s go shopping",
      "Dismax is a great solution for a needle-in-a-haystack problem. You have many fields and it will be relatively rare that query terms match in more than one field. Dismax breaks down when we’re searching for hay in a haystack. When matches are common and more fields are brought into the dismax equation, it becomes increasingly hard to balance out the diverse measurements. Scoring can fall apart as the dismax equation gets increasingly hard to balance for more-and-more use cases.",
      "Carefully picking the right parameters to make google-like search meaningful and relevant is hard. It’s also likely never going to be perfect, especially as we add new fields. Though you can get pretty close, eliminating all cases of weird dismax behavior will be met with diminishing returns.",
      "What do you think? Do you have a tough relevancy problem? Tell us about it. We’d love to help!"
    ],
    "summary_t": "When you learn about the dismax query parser for the first time, one of the first things you learn about is qf, pf and friends. To use (e)dismax you take a g..."
  },
  {
    "id": "ee61e8bcd47278a57cd4895996d39674",
    "url_s": "https://opensourceconnections.com/blog/2013/07/04/friend-recommendations-using-mapreduce/",
    "title": "Friend Recommendations using MapReduce",
    "content": [
      "So Jonathan, one of our interns this summer, asked an interesting question today about MapReduce. He said, \"Lets say you download the entire data set of whos following who from Twitter. Can you use MapReduce to make recommendations about who any particular individual should follow?\" And as Jonathans mentor this summer, and as one of the OpenSource Connections MapReduce experts I dutifully said, \"uuuhhhhh…\"",
      "And then in a stoke of genius … I found a way to stall for time. \"Well, young Padawan,\" I said to Jonathan, \"first you must more precisely define your problem… and only then will the answer be revealed to you.\" And then darn it if he didnt ask me what I meant! Left with no viable alternatives, I squeezed my brain real hard, and this is what came out:",
      "How to make \"follow\" recommendations on Twitter",
      "In my mind, the more precise definition of the problem goes something like this:",
      "If Jonathan is following Frank, Fred, Frida, and Fernando,\n  and if Frank, Fred, Frida, and Fernando are all following Zeke,\n    and if Jonathan is **not** currently following Zeke\n      **then Jonathan should follow Zeke.**",
      "Or a bit more generally, \"if a lot of my friends are following someone who I am not yet following, then I might be interested in following that person\".",
      "Now, given my only somewhat more precise statement of the problem, lets look at the inputs and outputs to the problems. Lets say that capital letters stand for people on Twitter. The information that we download from Twitter can be formatted to look something like this:",
      "A -follows-> [B C D M P Q X Y Z]\nB -follows-> [A C D F P Q X Y Z]\nC -follows-> [A B D F G M X Y]\nD -follows-> [A B C F G M Q]\n...",
      "The ultimate output might then be an ordered list of who each person should follow",
      "A -should-follow-> [F G]\nB -should-follow-> [M]\nC -should-follow-> [Q P]\nD -should-follow-> [X Y Z]\n...",
      "And the big question is how do we generate this list? Now I was unfortunately too slow to answer this question on the spot. However after staring at the ceiling for several hours, the answer came to me in a flash of insight:",
      "We can do this with MapReduce, but were going to need to spread it out into two iterations.",
      "Iteration 1: Find friends of friends.\n  Iteration 2: Count and sort the friends of frieds on a per person basis, and find friends of friends that arent already being followed by said person.",
      "Friend recommendations in pseudocode:",
      "To reiterate, before the we run the map reduce, we have the following input rows",
      "people       listOfWhoTheyFollow\nA            [B C D M P Q X Y Z]\nB            [A C D F P Q X Y Z]\nC            [A B D F G M X Y]\nD            [A B C F G M Q]\n...",
      "Iteration 1: Find Friends of Friends: Mapper",
      "map(person,listOfWhoTheyFollow) :\n  emit [person, 'follows'] -> listOfWhoTheyFollow\n  for(friend in listOfWhoTheyFollow) :\n      emit [friend, 'followed_by'] -> person",
      "This mapper will emit things that look like this:",
      "[A, 'follows'] -> [B C D M P Q X Y Z]\n    [B, 'followed_by'] -> A\n    [C, 'followed_by'] -> A\n    [D, 'followed_by'] -> A\n    ...\n    [B, 'follows'] -> [A C D F P Q X Y Z]\n    [A, 'followed_by'] -> B\n    [C, 'followed_by'] -> B\n    [D, 'followed_by'] -> B\n    ...",
      "Here, the keys are compound, containing a person and then their relationship to the people mentioned in the values. Why would I want to do this? Because Im getting ready to take advantage of some less commonly used feature of MapReduce – custom partitioning, custom grouping, and custom sorting. Heres how it will work:",
      "Iteration 1: Find Friends of Friends: Shuffle Sort",
      "Define a custom partitioner that partitions on the first element of the mapper outputs keys. This will ensure that [A, 'follows'] -> [B C D M P Q X Y Z] goes to the same machine as [A, 'followed_by'] -> D.\n  Define a custom sort that sorts on person first and on the relationship second. Make sure that followed_by comes after follows. Now [A, 'follows'] -> [B C D M P Q X Y Z] goes to the same machine and comes before [A, 'followed_by'] -> D.\n  Define a custom grouping so that all keys with the same person go to the same reducer. In this case [A, 'follows'] -> [B C D M P Q X Y Z] goes to the same reducer as [A, 'followed_by'] -> D. This seems like just the same thing as the custom partitioner, but if we hadnt taken this extra step, these two key value pairs would have gone to different reducers on the same machine.",
      "(For more details about custom partitioning, sorting, and grouping, see the Secondary Sorting section of Chapter 8 of Tom Whites Hadoop: The Definitive Guide. Alternatively, look at our post: Custom Partitioning, Sorting, and Grouping in Hadoop MapReduce.) After the shuffle and sort, then a single reducer will get all the key values associated with a person sorted so that the \"follows\" relationship comes first. For instance, for A the reducer will receive these inputs:",
      "[A, 'follows'] -> [B C D M P Q X Y Z]\n[A, 'followed_by'] -> [B C D ... ]",
      "And for B the reducer will receive these inputs:",
      "[B, 'follows'] -> [A C D F P Q X Y Z]\n[B, 'followed_by'] -> [A C D ... ]",
      "Iteration 1: Find Friends of Friends: Reducer",
      "For each person, the associated reducer will be called twice. The first time with the \"follows\" list and the second time with the \"followed_by\" list. The goal of the reducer is to convert this information to a list of friends of friends.",
      "listOfWhoTheyFollow = null\n\nreduce(compoundKey,people) :\n    if(compoundKey[1] = 'follows') :\n        listOfWhoTheyFollow = people\n        # and do nothing else because the next call to the\n        # reducer will have the corresponding 'followed_by' list\n    else if(compoundKey[1] = 'followed_by') :\n        listOfPeopleThatFollowThem = people\n        for(followed in listOfWhoTheyFollow) :\n            for(follower in listOfPeopleThatFollowThem) :\n                emit follower -> followed",
      "As seen here, whenever the reducer is first called it sets aside the list of people that this person follows. On the subsequent call to this reducer, the reducer iterates through all of the people that follows this person and all of the people that this person follows, and for each pair, the follower -> followed key,value is emitted.",
      "Effectively we are iterating through all possible friends of friends that exist in Twitter. Considering that there are roughly 500M Twitterers and the average number of followers (and followees?) is 200, and considering that the id of users are probably stored as longs, then the result of this iteration is roughly 730TB of stored data. Is this a problem (well maybe!) of course not! This is Big Data! (…whimper…)",
      "Half way there.",
      "This post is getting long, so Ill truncate it here for the time being. In the next iteration of this algorithm, well count up all of the friend-of-friend relationship, weed out the ones that theyre already following, and present the users with the best people to follow – namely, those friends of friends who are most popular among friends.",
      "Interested in seeing the rest of the algorithm? Comment below!",
      "",
      "Check out my LinkedIn Follow me on Twitter",
      "Improvements:",
      "First reducer or second mapper to eliminate friends of friends who are already being followed which will be plentiful.\n  When FoF relationships are redundant – just output them once, with a count. Combiner."
    ],
    "summary_t": ""
  },
  {
    "id": "f9aedc6a25e14b1e3e225e9dd9bb27d2",
    "url_s": "https://opensourceconnections.com/blog/2007/03/06/re-system-logs-as-reproducible-scripts/",
    "title": "RE: System logs as reproducible scripts",
    "content": [
      "Duane Gran recently pinged me about thoughts on \"System logs as reproducible scripts\". Its a topic that Ive been running into recently as weve gone into testing a web based application, and dealing with some of our users very difficult to reproduce bug reports.",
      "The challenge with a log that can be \"played back\" is that often the system will prevent the same set of events from happening again. For example, if you record a series of events that includes creating a user, then when you play them back the user creation step will fail with a \"user already exists\" type of error.",
      "However, this isnt to say that this isnt a good idea. With in narrow scopes, this has been done. For example, there are tools that intercept your SQL transactions and record them for use in rebuilding indexes and debugging after the fact issues. Ive also seen a tool that attempted to record all your SQL statements, and then replay them in reverse to restore your database to a known state!",
      "Right now the tools available dont help us follow the path of execution through a web app for a specific user very well. We can see part of the picture, via grepping through a webserver log for a session id, but its difficult to tie that into a log of SQL statements issued for that session. And there arent any good simple tools for connecting data collected from Google Analytics, and matching it up to other data sources like server logs or SQL transaction logs.",
      "On the server side, there are tools like DTrace that provide a developer with indepth visibility into what is happening to an application. DTrace was used by some of the PostgresSQL developers at OSCON 2006 to find many bottlenecks in the performance of PostgresSQL for example. Now we need to bubble up one more level up and be able to view the actions performed by a specific user that may cause activity on multiple tiers and systems!",
      "I used to work with Canoo WebTest, a tool for functional testing of websites. And one thing it did that was wonderful was to record on the local filesystem a copy of every HTML page that it recieved back from the server. This meant that you could go back and \"see\" what your Canoo Functional Test \"saw\" at each step. Selenium is my current favorite for Functional testing. But it doesnt record each page as it runs through them, so you have to figure out what tests failed by just the error message that was created when running a suite of tests.",
      "Maybe what we need, at least from a web application perspective, is the ability to save each page displayed to the user? I can envision the webserver sending a copy of each page back to the user, but also saving a copy locally, and aggregating them based on session id, and naming (and thus ordering) them based on time? Ajax calls of course might be just the raw XML that was sent back to the client between full HTML page loads. While this might put inordinate load on a large public website, it might be just the ticket for when you are doing development/testing of a website. A bug report with a timestamp and session id could be tied back to the sequence of webpages, and the developer could visually see each step that was taken to generate the error.",
      "Ideally the list of pages could be tied together in some sort of slideshow that the developer could play back to see what the user did. While this approach isnt the generic \"System logs as reproducible scripts\" that Duane was looking for, it is a step in the right direction!"
    ],
    "summary_t": ""
  },
  {
    "id": "bf99f9a9916ec8fc466f26dfb8642275",
    "url_s": "https://opensourceconnections.com/blog/2013/07/04/multiple-objective-search-scoring/",
    "title": "Multiple Objective Search Scoring",
    "content": [
      "Over the past few years as Ive worked with search, and Solr in particular, I have witnessed a commonly recurring problem associated with multiple objective search and how documents are scored. Perhaps the best way to understand this problem is to look at an example:",
      "Lets say that you are Zappos and you sell apparel online. When your users search for \"green shoes\", you want to display green shoes, but you also want to promote the newest product first. So you set up your Solr request handler like so:",
      "http://www.zappos.com/solr/select?green shoes\n &defType=dismax\n &qf=product_description\n &pf=product_description\n &bf=recip(ms(NOW,product_date),3.16e-11,1,1)",
      "In this case part of the score is based upon how well \"green shoes\" matches in the product description, and (thought its a bit cryptic) part of the score is based upon how new the product is. These two score components are summed together to form the final score. Or in equation format, for a given product,",
      "(total score for product) = (how well product matches the search) + (newness of product)",
      "Now that search has been appropriately configured, you issue the search for \"green shoes\" and to your chagrin, you find that instead of green shoes, you have a mixture of a bunch of really new products that happen to be either green or shoes, but rarely both at the same time. This stinks, because you know there are plenty of green shoes in the index, but theres just not any recent products!",
      "Well no problem, you can just change the weight the text match so that its more important than the newness of the product:",
      "http://www.zappos.com/solr/select?green shoes\n &defType=dismax\n &qf=product_description^10\n &pf=product_description^10\n &bf=recip(ms(NOW,product_date),3.16e-11,1,1)",
      "Or again in equation form:",
      "(total score for product) = 10*\n        (how well product matches the search) + (newness of product)",
      "And upon issuing the search again, you see a lineup of the newest green shoes you have to offer.",
      "Problem solved! Right?",
      "Not so fast. Do a search for \"dress shoes\". To your surprise youll find, not dress shoes, but a collection of recently introduced sun dressed and tennis shoes! What on earth happened?!",
      "Here is what happened: Dresses and dress shoes are much more common in the index than \"green shoes\", and because they are so much more common, they get a lower score! (See TF-IDF scoring for more information.) Because \"dress shoes\" are weighted so much lower, the relative importance of newness is increased and you get this mess were talking about here.",
      "From this point its not difficult to see that there is no best setting. For instance, if we again increase the weight on text match, then in all likelihood we would match too heavily on the text \"green shoes\", ignore the newness, the results would now contain only the oldest, most out-of-style green shoes in our inventory!",
      "A visual interpretation",
      "Perhaps youre like me. I always feel a lot better when Im looking at a visual interpretation of new concept. So here is a good visual way of thinking about the problem above.",
      "Considering the \"green shoes\" search, every product in the inventory has a certain score for the text match and a certain score for the newness as seen here.",
      "",
      "But based on these two scoring dimension, theres no real obvious order for displaying these products? So we do the best thing that we can think of, and we sum the scores together and turn them into one score. This can be represented as a 45˚ line on the diagram. And the further along this line the product is, the higher its total score.",
      "",
      "But as illustrated here, there are far too many high-scoring items that are not \"green shoes\" – theyre just newer. So we boost the text score – effectively stretching their placement along the vertical axis. The more we do this, the more influential the text match becomes on the result ordering.",
      "",
      "And, as we saw in the \"dress shoes\" example, our choice for the best text match weight for \"green shoes\" might not be appropriate at all for \"dress shoes\".",
      "",
      "Exploring Better Options",
      "The most obvious way of dealing with multiple objective search is to simply sum the individual scores together. However weve proven above that the best thing you can do for this approach is to optimize for a single search and then hope that the rest of searches arent too bad. Theres got to be something better. In the following sections Ill discuss two ideas that weve recently been exploring.",
      "Auto-Scaled Weights",
      "Perhaps you know that one of the objectives is more important and that the other objectives should be secondary. For instance, in both of the examples above, the text matching is primary. If a customer searches for \"green shoes\" then by golly they should show them green shoes, and only after that should we care about the newness of the green shoes. Ditto for \"dress shoes\".",
      "One way of achieving this is to have two pass scoring as follows. In the first pass, the text score and the newness score are handled individually and we collect statistics for both scores. For instance, it will probably be useful to track the maximum, average, and lowest score along both dimensions, along with the variance of those stores. Then, on the second scoring pass, we add the two scores together, but we give the newness component a weight based upon the statistics that weve captured for the other components of score:",
      "(total score) = (text score) + (newness score weight)*\n         \n        (newness score)",
      "There are certainly several ways to automatically determine the values for the secondary scoring weight. One example for now is to make the newness score weight equal the to the average text score divided by the average newness score. This way average contribution of the newness to the total score will be equal to the average contribution of the text score; the two components will be on equal footing.",
      "The principle drawback here is that all matching documents have to be scored twice, however it might be possible mitigate this problem by smart use of caches.",
      "Non-Dominated Results \"Ordering\"",
      "A completely different approach would be to return results sorted so that so-called \"non-dominated\" results are shown first. Whats a non-dominated result? Consider a new example: Lets now imagine that we are CareerBuilder and our users use our search engine to find new jobs. A user is going to be interested in simultaneously optimizing several criteria. For now we consider two of these criteria: distance and salary. In a particular search, a user sees the following results scored according to both criteria:",
      "",
      "A \"non-dominated\" result is any result X for which there is no other result that is in every way better that X. The collection of all such non-dominated results is called the non-dominated frontier (or sometimes the Pareto frontier).",
      "Now since weve defined what a non-dominated result is, it should be easier to understand why you would want to present your user with these results first. For example, why on earth would our user want to see result H before seeing results B or C which in both cases are both closer and provide a higher salary?",
      "One of greatest reasons that a non-dominated ordering of results is beneficial is because we cant ask our users how strongly they weight salary vs. distance. For some users, they simply dont want to move and therefore salary is secondary to distance. For others, the oposite will be true. The great thing about a non-dominated ordering is that you dont have to ask, the user will only be presented with results that are not dominated in either dimension.",
      "While this non-dominated ordering provides some great benefits, there are drawbacks as well. For one, its not a strict ordering. There are several items that are equally non-dominated (those on the non-dominated frontier). What order do we present these results to the user? Also, once weve exhausted the completely non-dominated results, how do we present the remaining results? Finally, calculating the non-dominated frontier is computationally much more complex that simply sorting the documents by some scalar score. This complexity, however, might be mitigated by making some approximations in our ordering of the results.",
      "Implementing this in Solr",
      "And all of this begs the question, \"How do you actually build this in Solr?\" The answer, for now at least, is that we dont quite know. We know the component that does the scoring; its called the Similarity. And we know the component that searches the index; its called the IndexSearcher. And we even know the thing that collects all of the documents from the searcher; its called TopDocs. But we dont know if all of the necessary requirements above are met. (For instance, do document scores have to be scalars?) – But what I do know is that our clients need this type of search. And the sooner the better!",
      "If you are such a client, then tell us! The more people that are mutually benefitted by this, the better and the faster well get this implemented! Are you a developer? Were fun to hack with. If youre interested, help us build this!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "b5b9e6d85943381772e68fcf49d4ba68",
    "url_s": "https://opensourceconnections.com/blog/2013/07/18/tomorrow-is-semantic-day-at-opensource-connections/",
    "title": "Tomorrow is Semantic Day at OpenSource Connections",
    "content": [
      "Tim Berners-Lee original vision was for the web is what we today call the Semantic Web:",
      "I have a dream for the Web [in which computers] become capable of\nanalyzing all the data on the Web – the content, links, and\ntransactions between people and computers. A \"Semantic Web\", which\nmakes this possible, has yet to emerge, but when it does, the\nday-to-day mechanisms of trade, bureaucracy and our daily lives will\nbe handled by machines talking to machines. The \"intelligent agents\"\npeople have touted for ages will finally materialize.",
      "Today, we are drowning in an acronym soup of RDF, RDFa, SKOS, OWL, SPARQL, FOAF, mixed up in various Triple Stores and Schemas. And for the past 10 (15?) years, weve seen the key enabler of the Semantic Web, working metadata, never quite arrive. Indeed, Cory Doctorow calls it metacrap… http://www.well.com/~doctorow/metacrap.htm",
      "However, much of the promise of search engines is based around the \"understanding\" of the queries and the data indexed. And maybe, just maybe, we are finally at the point, with emergence of Web 3.0, that the promise of the Semantic Web is ready to happen. Not hip to Web 3.0? Read this: http://en.wikipedia.org/wiki/User:Davidjcmorris/Web_3.0",
      "Alex Strumminger, who some of you know, did much of the semantic work for UNICEFs website, but ran up against all the limitations of the historical suite of semantic technologies. Tomorrow hell be joining us to work through how to use Semantic in a practical way.",
      "Alex and his colleagues have been experimenting with an open source project, OpenStructs.org, as a platform, and Alex will walk us through their setup, done in EC2. Hes working on having some of the core developers at OpenStructs.org be available to us that day.",
      "Come join us at lunch time at OSC and play around with some fun semantic technologies. Well be working in small teams on various topics. Hopefully this goes a long way towards taking the Semantic web from a set of acronyms to a real tool!"
    ],
    "summary_t": ""
  },
  {
    "id": "fc2b0805f9d57f4fa1e50fea37bd8ee7",
    "url_s": "https://opensourceconnections.com/blog/2013/07/21/improve-search-relevancy-by-telling-solr-exactly-what-you-want/",
    "title": "Improve search relevancy by telling Solr exactly what you want",
    "content": [
      "Please find my piece of hay!",
      "To be successful, (e)dismax relies on avoiding a tricky problem with its scoring strategy. As we’ve discussed, dismax scores documents by taking the maximum score of all the fields that match a query. This is problematic as one field’s scores can’t easily be related to another’s. A good \"text\" match might have a score of 2, while a bad \"title\" score might be 10. Dismax doesn’t have a notion that \"10\" is bad for title, it only knows 10 > 2, so title matches dominate the final search results.",
      "The best case for dismax is that there’s only one field that matches a query, so the resulting scoring reflects the consistency within that field. In short, dismax thrives with needle-in-a-haystack problems and does poorly with hay-in-a-haystack problems.",
      "We need a different strategy for documents that have fields with a large amount of overlap. We’re trying to tell the difference between very similar pieces of hay. The task is similar to needing to find a good candidate for a job. If we wanted to query a search index of job candidates for \"Solr Java Developer\", we’ll clearly match many different sections of our candidates resumes. Because of problems with dismax, we may end up with search results heavily sorted on the \"objective\" field. Our top scoring result might have something like:",
      "Goal: Work with Solr some day!",
      "Clearly not what we want! We need the hardcore experienced folks!",
      "I’ve switched to using a different strategy for search relevancy in these kinds of cases. Start with rudimentary yet simple scoring avoiding the wild swings of dismax. Once this is in place, give Solr a list of additive queries (via bq/bf) that describe the ideal document. Tune the multiplier on each qualification through testing and experimentation.",
      "Simple Base Scoring",
      "Instead of relying on qf/pf to search and take the best of multiple fields, I’ll create a grab-bag field. I’ll use Solr’s copyField directives to copy all text I want to match on into this field in the schema:",
      "<copyField source=\"resume_goal\" dest=\"text_all\"/>\n<copyField source=\"resume_experience\" dest=\"text_all\"/>\n<copyField source=\"resume_skills\" dest=\"text_all\"/>",
      "The field \"text_all\" becomes what Solr initially searches. The assumption here is that it’s appropriate to tokenize what goes into text_all the same way. In this kind of setup, you might also want to consider omitTermFreqsAndPositions for text_all, otherwise your scoring will be heavily biased toward the field that contributes the most tokens to text_all.",
      "Now we can set",
      "qf=text_all",
      "and start searching!",
      "Describe job qualification to Solr",
      "Once there’s baseline, predictable scoring in place, let’s describe our ideal candidate by passing solr multiple boost queries that help bubble up the the best documents for the problem were trying to solve:",
      "The candidate has at least 75% of the required skills\n\n    bq={!edismax qf=resume_skills mm=75% v=$q bq=}\n  \n  \n    The candidate wants to work with the technology\n\n    bq={!edismax qf=resume_goals v=$q bq=}\n  \n  \n    The candidate has a high StackOverflow reputation\n\n    bf=log(resume_stackoverflow_reputation)",
      "Each of these queries lets Solr layer in an extra factor into the sorting. Notice how in the bq we set v=$q. We’re using Solr’s local param syntax to reprocess the original query against a new set of criteria. We’re also making an assumption in the first bq that resume_skills will utilize an analysis chain that will filter out tokens that are non-job skills through a combination of synonyms and filtering. It’s also important to note that this wouldn’t be the finished product. Each boost needs to be carefully tuned through testing, tweaking its impact with the ^(multiplier) syntax.",
      "Which one of you is the perfect document for this query?",
      "One nice thing about this strategy is we’re directly telling Solr exactly what we want in an awesome candidate. It’s a bit like using Solr for a fuzzy sorter, explicitly feeding it pieces of criteria we think are \"good\", tuning those criteria, then using it to find the answers that match as many pieces of criteria as we specify. It’s also easy to decide later that we want to layer on additional criteria (does the candidate have code on github that utilizes skills in the query? – how much code? – how recent is it?). We could even apply additional queries based on additional criteria like salary requirements. It’s a pretty exciting strategy. John Berryman and I have even been wondering whether this might help get at his multiple objective scoring ideas. In any case, I hope to be using it more!",
      "Let us know what you think of this strategy! If youve got a tough relevancy problem, let us know, weve got this and plenty other relevancy tricks up our sleeves and we’d love to talk with you!"
    ],
    "summary_t": "To be successful, (e)dismax relies on avoiding a tricky problem with its scoring strategy. As we’ve discussed, dismax scores documents by taking the maximum ..."
  },
  {
    "id": "18b1f9e7883780670de6c10ca3dd00fc",
    "url_s": "https://opensourceconnections.com/blog/2013/07/24/understanding-how-cql3-maps-to-cassandras-internal-data-structure-sets-lists-and-maps/",
    "title": "Understanding How CQL3 Maps to Cassandra’s Internal Data Structure: Sets, Lists, and Maps",
    "content": [
      "Refer to my previous post for more in depth details into what Im doing here. This is just a quick reference regarding how Cassandra sets, lists, and maps work under the hood:",
      "Maps",
      "Defining:",
      "cqlsh:test> CREATE TABLE phonelists (\n        ... user text PRIMARY KEY,\n        ... phoneNumbers map<text,text> );",
      "Inserting:",
      "cqlsh:test> INSERT INTO phonelists (user, phonenumbers)\n        ... VALUES ('john',{'patricia':'555-4326','doug':'555-1579'});\ncqlsh:test> INSERT INTO phonelists (user, phonenumbers)\n        ... VALUES ('scott',{'bill':'555-7382','patricia':'555-4326', 'jane':'555-8743'});",
      "Retrieving:",
      "cqlsh:test> SELECT * FROM phonelists;\n\n user  | phonenumbers\n-------+------------------------------------------------------\n scott | {bill: 555-7382, jane: 555-8743, patricia: 555-4326}\n  john |                 {doug: 555-1579, patricia: 555-4326}",
      "Updating:",
      "cqlsh:test> UPDATE phonelists\n        ... SET phonenumbers = phonenumbers + {'daniel':'555-0453'}\n        ... WHERE user='john';",
      "The internal representation:",
      "[[email protected]] list phonelists;\n-------------------\nRowKey: scott\n=> (column=, value=, timestamp=1374684062860000)\n=> (column=phonenumbers:bill, value='555-7382', timestamp=1374684062860000)\n=> (column=phonenumbers:jane, value='555-8743', timestamp=1374684062860000)\n=> (column=phonenumbers:patricia, value='555-4326', timestamp=1374684062860000)\n-------------------\nRowKey: john\n=> (column=, value=, timestamp=1374683971220000)\n=> (column=phonenumbers:doug, value='555-1579', timestamp=1374683971220000)\n=> (column=phonenumbers:patricia, value='555-4326', timestamp=1374683971220000)",
      "Note that the above text components, e.g. doug and 555-1579′, are returned by cassandra-cli in their hex encoding. Ive taken the liberty to decode them so that the print out is more understandable. I will do this throughout the post.",
      "Lists",
      "Defining:",
      "cqlsh:test> CREATE TABLE friendlists (\n        ... user text PRIMARY KEY,\n        ... friends list <text>\n        ... );",
      "Inserting:",
      "cqlsh:test> INSERT INTO friendlists (user, friends)\n        ... VALUES ('john',['doug','patricia','scott']);\ncqlsh:test> INSERT INTO friendlists (user, friends)\n        ... VALUES ('patricia', ['john','lucifer']);",
      "Retrieving:",
      "cqlsh:test> SELECT * FROM friendlists;\n\n user     | friends\n----------+-------------------------\n     john | [doug, patricia, scott]\n patricia |         [john, lucifer]",
      "Updating:",
      "cqlsh:test> UPDATE friendlists\n        ... SET friends = friends + ['matt','eric']\n        ... WHERE user='john';\ncqlsh:test> UPDATE friendlists\n        ... SET friends = friends - ['lucifer']\n        ... WHERE user='patricia';",
      "The internal representation:",
      "[[email protected]] list friendlists;\nUsing default limit of 100\nUsing default column limit of 100\n-------------------\nRowKey: john\n=> (column=, value=, timestamp=1374687324950000)\n=> (column=friends:26017c10f48711e2801fdf9895e5d0f8, value='doug', timestamp=1374687206993000)\n=> (column=friends:26017c11f48711e2801fdf9895e5d0f8, value='patricia', timestamp=1374687206993000)\n=> (column=friends:26017c12f48711e2801fdf9895e5d0f8, value='scott', timestamp=1374687206993000)\n=> (column=friends:6c504b60f48711e2801fdf9895e5d0f8, value='matt', timestamp=1374687324950000)\n=> (column=friends:6c504b61f48711e2801fdf9895e5d0f8, value='eric', timestamp=1374687324950000)\n-------------------\nRowKey: patricia\n=> (column=, value=, timestamp=1374687352290000)\n=> (column=friends:3b817b80f48711e2801fdf9895e5d0f8, value='john', timestamp=1374687243064000)",
      "Here the internal column name is more complicated because a UUID is appended to the name of the CQL field \"friend\". This is used to keep track of the order of items in the list.",
      "To be determined: Does a list item delete take more time than a list insert? I suspect so – I dont see how Cassandra can delete an element in the list without reading in all the elements in the list and then deleting column that has the value indicated in the delete. – UPDATE This is the case.",
      "Set",
      "Defining:",
      "cqlsh:test> CREATE TABLE friendsets (\n        ... user text PRIMARY KEY,\n        ... friends set <text>\n        ... );",
      "Inserting:",
      "cqlsh:test> INSERT INTO friendsets (user, friends)\n        ... VALUES ('john',{'doug','patricia','scott'});\ncqlsh:test> INSERT INTO friendsets (user, friends)\n        ... VALUES ('patricia', {'john','lucifer'});",
      "Retrieving:",
      "cqlsh:test> SELECT * FROM friendsets;\n\n user     | friends\n----------+-------------------------\n     john | {doug, patricia, scott}\n patricia |         {john, lucifer}",
      "Updating:",
      "cqlsh:test> UPDATE friendsets\n        ... SET friends = friends + {'matt','eric'}\n        ... WHERE user='john';\ncqlsh:test> UPDATE friendsets\n        ... SET friends = friends - {'lucifer'}\n        ... WHERE user='patricia';",
      "The internal representation:",
      "[[email protected]] list friendsets;\nUsing default limit of 100\nUsing default column limit of 100\n-------------------\nRowKey: john\n=> (column=, value=, timestamp=1374688135443000)\n=> (column=friends:'doug', value=, timestamp=1374688108307000)\n=> (column=friends:'eric', value=, timestamp=1374688135443000)\n=> (column=friends:'matt', value=, timestamp=1374688135443000)\n=> (column=friends:'patricia', value=, timestamp=1374688108307000)\n=> (column=friends:'scott', value=, timestamp=1374688108307000)\n-------------------\nRowKey: patricia\n=> (column=, value=, timestamp=1374688151386000)\n=> (column=friends:'john', value=, timestamp=1374688116595000)"
    ],
    "summary_t": ""
  },
  {
    "id": "2e4b6d98c0e86ebf4d53f747ab5db1ba",
    "url_s": "https://opensourceconnections.com/blog/2013/07/24/understanding-how-cql3-maps-to-cassandras-internal-data-structure/",
    "title": "Understanding How CQL3 Maps to Cassandras Internal Data Structure",
    "content": [
      "CQL3 appears to be the newly ordained, canonical, and best-practices means of interacting with Cassandra. Indeed, the Apache Cassandra documentation itself declares that the Thrift API as \"legacy\" and recommends that CQL be used instead. However at the same time, Ive heard several people express their concern over the added layer of abstraction and an uncertainty about whats really happening inside of Cassandra.",
      "In this post we will open up the hood and take a look at exactly how Cassandra is treating CQL queries. Our methodology is simple! We will create CQL statements and then in the \"legacy\" cassandra-cli we will look at exactly how the corresponding tables get written. For your further edification, consider following along: To get started, download cassandra, cd to the cassandra directory, start it (bin/cassandra -f), and then in two new terminal windows, start cassandra-cli and cqlsh, (respectively bin/cassandra-cli and bin/cqlsh). Well be switching back and forth between cqlsh and cassandra-cli.",
      "A Simple Example",
      "First off well create and use a test keyspace:",
      "cqlsh> CREATE KEYSPACE test\n       WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}\ncqlsh> USE test;",
      "Go ahead and flip over to cassandra-cli and use the test keyspace there as well.",
      "[default] use test;",
      "In our first example, lets look at the simple table composed of three fields using a simple primary key.",
      "cqlsh:test> CREATE TABLE example (\n        ... field1 int PRIMARY KEY,\n        ... field2 int,\n        ... field3 int);",
      "And we will toss in a few example rows:",
      "cqlsh:test> INSERT INTO example (field1, field2, field3) VALUES ( 1,2,3);\ncqlsh:test> INSERT INTO example (field1, field2, field3) VALUES ( 4,5,6);\ncqlsh:test> INSERT INTO example (field1, field2, field3) VALUES ( 7,8,9);",
      "And as we would expect, the resulting entry as viewed through CQL is",
      "cqlsh:test> SELECT * FROM example;\n\n field1 | field2 | field3\n--------+--------+--------\n      1 |      2 |      3\n      4 |      5 |      6\n      7 |      8 |      9",
      "However this looks very different when viewed from cassandra-cli:",
      "[[email protected]] list  example;\n-------------------\nRowKey: 1\n=> (column=, value=, timestamp=1374546754299000)\n=> (column=field2, value=00000002, timestamp=1374546754299000)\n=> (column=field3, value=00000003, timestamp=1374546754299000)\n-------------------\nRowKey: 4\n=> (column=, value=, timestamp=1374546757815000)\n=> (column=field2, value=00000005, timestamp=1374546757815000)\n=> (column=field3, value=00000006, timestamp=1374546757815000)\n-------------------\nRowKey: 7\n=> (column=, value=, timestamp=1374546761055000)\n=> (column=field2, value=00000008, timestamp=1374546761055000)\n=> (column=field3, value=00000009, timestamp=1374546761055000)",
      "Even though this presentation of the data is much harder to decipher, its important to understand whats going on here because cassandra-cli presents the information exactly as its stored internally in Cassandra. For each item above, there are 3 important things to look at: the row key (RowKey: <?>), the column name (column=<?>) and the column value (value=<?>). From these examples, we can make a couple of initial observations about the mapping from CQL statements to their internal representations.",
      "The value of the CQL primary key is used internally as the row key (which in the new CQL paradigm is being called a \"partition key\").\n  The names of the non-primary key CQL fields are used internally as columns names. The values of the non-primary key CQL fields are then internally stored as the corresponding column values.",
      "You may have also noticed that these rows all contain columns with no column name and no column value. This is not a bug! Its actually a way of handling the fact that it should be possible to declare the existence of field1=<some number> without necessarily specifying values for field2 or field3. (See here for further details.)",
      "A More Complex Example",
      "Now since were getting used to just how the CQL and cassandra-cli results are organized, lets jump into the thick of it with another more complicated example. Back in CQLSH:",
      "cqlsh:test> DROP TABLE example;\ncqlsh:test> CREATE TABLE example (\n        ... partitionKey1 text,\n        ... partitionKey2 text,\n        ... clusterKey1 text,\n        ... clusterKey2 text,\n        ... normalField1 text,\n        ... normalField2 text,\n        ... PRIMARY KEY (\n                (partitionKey1, partitionKey2),\n                clusterKey1, clusterKey2\n              )\n        ... );",
      "Here weve named the fields to indicate where theyll end up in the internal representation. And weve also pulled out all the stops. Our primary key is not only compound, but it also uses compound partition keys, (this is represented by the double nesting of the parentheses of the PRIMARY KEY) and compound cluster keys (more on partition vs. cluster keys in a bit).",
      "Correspondingly we will also insert data where, again, the name gives us a hint of where the data is headed in the internal Cassandra data structure.",
      "cqlsh:test> INSERT INTO example (\n        ... partitionKey1,\n        ... partitionKey2,\n        ... clusterKey1,\n        ... clusterKey2,\n        ... normalField1,\n        ... normalField2\n        ... ) VALUES (\n        ... 'partitionVal1',\n        ... 'partitionVal2',\n        ... 'clusterVal1',\n        ... 'clusterVal2',\n        ... 'normalVal1',\n        ... 'normalVal2');",
      "If we query this in CQL, we get the results that we would expect:",
      "cqlsh:test> SELECT * FROM example;\n partitionkey1 | partitionkey2 | clusterkey1 | clusterkey2 | normalfield1 | normalfield2\n---------------+---------------+-------------+-------------+--------------+--------------\n partitionVal1 | partitionVal2 | clusterVal1 | clusterVal2 |   normalVal1 |   normalVal2",
      "But again, this looks very different in cassandra-cli:",
      "[[email protected]] list example;\n-------------------\nRowKey: partitionVal1:partitionVal2\n=> (column=clusterVal1:clusterVal2:, value=, timestamp=1374630892473000)\n=> (column=clusterVal1:clusterVal2:normalfield1, value=6e6f726d616c56616c31, timestamp=1374630892473000)\n=> (column=clusterVal1:clusterVal2:normalfield2, value=6e6f726d616c56616c32, timestamp=1374630892473000)",
      "One by one, lets consider where every element of the CQL SELECT statement ends up in the cassandra-cli list statement. Again, its important to remember that the cassandra-cli representation closely corresponds to how the data is stored in the actual Cassandra data structure!",
      "Looking at partitionVal1 and partitionVal2 we see that these are concatenated together and used internally as the RowKey (a.k.a. the partition key).\n  Similarly, clusterVal1 and clusterVal2 are concatenated together and used in the names of the columns. But look what else theyre concatenated with – the non-primary key field names – specifically normalfield1 and normalfield2.\n  What came of the actual values of normalfield1 and normalfield2? They are encoded as the values of columns clusterVal1:clusterVal2:normalfield1 and clusterVal1:clusterVal2:normalfield2 respectively. That is 6e6f726d616c56616c31 becomes normalVal1 and 6e6f726d616c56616c32 becomes normalVal2.\n  \n    Look whats missing: partitionkey1, partitionkey2, clusterkey1 and clusterkey2 are not even present at all! This is because the components of the primary keys are being tracked seperately in the system.schema_columnfamilies table. Go ahead and try the CQL query:\n\n    SELECT key_aliases, column_aliases FROM system.schema_columnfamilies WHERE keyspace_name=test AND columnfamily_name=example’;",
      "And youll find our missing field names:",
      "key_aliases                       | column_aliases\n-----------------------------------+-------------------------------\n [\"partitionkey1\",\"partitionkey2\"] | [\"clusterkey1\",\"clusterkey2\"]",
      "A More Practical Example",
      "In the example above, we still dont have the big picture because were only looking at a single entry. Its also a little difficult to identify with the example here because its abstract. So in our final example we look at something that includes several entries while at the same time being something that we can all readily identify with – tweets. Consider the following table:",
      "CREATE TABLE tweets (\n        ... user text,\n        ... time timestamp,\n        ... tweet text,\n        ... lat float,\n        ... long float,\n        ... PRIMARY KEY (user, time)\n        ... );",
      "Take special notice of how the primary key is defined. Again here we have a partition key, user. The partition key is always the first field in the primary key, and it can optionally be compound as in the previous example. We also have a clustering key, time. The clustering key or keys are the fields contained in the primary key asides from the partition key. It will become clear in a moment why the pieces of the primary key are labeled as \"partition\" or \"clustering\".",
      "After entering in several items we select the contents of the table and heres what we get:",
      "cqlsh:test> SELECT * FROM tweets;\n user         | time                     | lat    | long    | tweet\n--------------+--------------------------+--------+---------+---------------------\n softwaredoug | 2013-07-13 08:21:54-0400 | 38.162 | -78.549 |  Having chest pain.\n softwaredoug | 2013-07-21 12:15:27-0400 | 38.093 | -78.573 |   Speedo self shot.\n      jnbrymn | 2013-06-29 20:53:15-0400 | 38.092 | -78.453 | I like programming.\n      jnbrymn | 2013-07-14 22:55:45-0400 | 38.073 | -78.659 |     Who likes cats?\n      jnbrymn | 2013-07-24 06:23:54-0400 | 38.073 | -78.647 |  My coffee is cold.",
      "Whats more, we can easily (and efficiently) select all tweets for a particular user",
      "cqlsh:test> SELECT * FROM tweets WHERE user='jnbrymn';",
      "And we can easily (and efficiently) select all tweets for a particular user within a particular time slice:",
      "cqlsh:test> SELECT * FROM tweets WHERE user='jnbrymn' AND time>='2013-07-01';",
      "In all of the queries, it is also significant that we are efficiently retrieving all information associated with each tweet (tweet text, lat, long, …) without having to issue multiple queries.",
      "In order to understand why these queries are efficient, its informative to look again at the cassandra-cli listing of tweets:",
      "[[email protected]] list tweets;\n-------------------\nRowKey: softwaredoug\n=> (column=2013-07-13 08:21:54-0400:, value=, timestamp=1374673155373000)\n=> (column=2013-07-13 08:21:54-0400:lat, value=4218a5e3, timestamp=1374673155373000)\n=> (column=2013-07-13 08:21:54-0400:long, value=c29d1917, timestamp=1374673155373000)\n=> (column=2013-07-13 08:21:54-0400:tweet, value=486176696e67206368657374207061696e2e, timestamp=1374673155373000)\n=> (column=2013-07-21 12:15:27-0400:, value=, timestamp=1374673155407000)\n=> (column=2013-07-21 12:15:27-0400:lat, value=42185f3b, timestamp=1374673155407000)\n=> (column=2013-07-21 12:15:27-0400:long, value=c29d2560, timestamp=1374673155407000)\n=> (column=2013-07-21 12:15:27-0400:tweet, value=53706565646f2073656c662073686f742e, timestamp=1374673155407000)\n-------------------\nRowKey: jnbrymn\n=> (column=2013-06-29 20:53:15-0400:, value=, timestamp=1374673155419000)\n=> (column=2013-06-29 20:53:15-0400:lat, value=42185e35, timestamp=1374673155419000)\n=> (column=2013-06-29 20:53:15-0400:long, value=c29ce7f0, timestamp=1374673155419000)\n=> (column=2013-06-29 20:53:15-0400:tweet, value=49206c696b652070726f6772616d6d696e672e, timestamp=1374673155419000)\n=> (column=2013-07-14 22:55:45-0400:, value=, timestamp=1374673155434000)\n=> (column=2013-07-14 22:55:45-0400:lat, value=42184ac1, timestamp=1374673155434000)\n=> (column=2013-07-14 22:55:45-0400:long, value=c29d5168, timestamp=1374673155434000)\n=> (column=2013-07-14 22:55:45-0400:tweet, value=57686f206c696b657320636174733f, timestamp=1374673155434000)\n=> (column=2013-07-24 06:23:54-0400:, value=, timestamp=1374673155485000)\n=> (column=2013-07-24 06:23:54-0400:lat, value=42184ac1, timestamp=1374673155485000)\n=> (column=2013-07-24 06:23:54-0400:long, value=c29d4b44, timestamp=1374673155485000)\n=> (column=2013-07-24 06:23:54-0400:tweet, value=4d7920636f6666656520697320636f6c642e, timestamp=1374673155485000)",
      "We see here that internally in Cassandra, these tweets are stored in two different rows. The internal rows are keyed by the user names – the partition keys. This is why it is so efficient to retrieve all the tweets for a single user. The internal columns are named by the tweet times – the clustering keys. This is why it is so efficient to query for slices along the cluster keys. Also demonstrated here, the names of the non-private key fields are appended to the internal column names. This makes it possible to grab all fields that are associated with a given tweet without having to make independent queries.",
      "Why? Whats the Rationale?",
      "Why would so much trouble be taken to carefully disorganize the field names and field values like this? The answer is that this is actually a fantastic Cassandra data modeling pattern – so fantastic in fact, that the Cassandra community has decided to bet the farm on it, so to speak, and build an interface to it that abstracts away the messy bits. The interface – CQL.",
      "But what does this pattern provide you? As it turns out, many things!",
      "It provides fast look-up by partition key and efficient scans and slices by cluster key.\n  It groups together related data as CQL rows. This means that you can do in one query what would otherwise take multiple queries into different column families.\n  It allows for individual fields to be added, modified, and deleted independently.\n  It is strictly better than the old Cassandra paradigm. Proof: you can coerce CQL Tables to behave exactly like old-style Cassandra ColumnFamilies. (See the examples here.)\n  It extends easily to implementation of sets lists and maps (which are super ugly if youre working directly in old cassandra) – but thats for another blog post.",
      "And although I havent addressed the matter here, the new CQL protocol allows for asynchronous communication as compared with the synchronous, call-response communication required by Thrift. As a result, CQL is capable of being much faster and less resource intensive than Thrift – especially when using single threaded clients.",
      "Still not certain that youre ready for CQL? Well dont worry, legacy Cassandra clusters will still have to be supported by Thrift, so Thrift will be with us for quite some time yet. However if youd like to take the leap into modernity, then take a look at this Datastax blog post. It should get you started in the right direction.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "5850d76a9df7f9a8bdd7f4ba5694f4ff",
    "url_s": "https://opensourceconnections.com/blog/2013/08/08/aws-autoscaling-queues/",
    "title": "AWS Autoscaling & Queues",
    "content": [
      "In GPSN we have this task stereotype of spinning up a group of VMs to process task items in an SQS queue. While reading through the autoscaling docs two things jumped out at me: The minimum size of size of an autoscale group is zero, and autoscaling can be triggered off of the size of a queue. In other words, we could have a fifteen-step process based on fifteen stacks and fifteen queues, and we dont have to manually start any of them – we can just let the stacks autoscale down to zero instances when their queue is empty.",
      "A great side-effect of this is that each stack will start worker VMs when items start popping up in the queue theyre watching. While not as fine-grained as a full-blown Actor Model framework, this still lets multiple types of tasks through the system in parallel and at their own pace."
    ],
    "summary_t": ""
  },
  {
    "id": "db62d73cb798876bedc2277d36d97914",
    "url_s": "https://opensourceconnections.com/blog/2013/08/11/conference-recap-defcon/",
    "title": "Conference Recap: Defcon",
    "content": [
      "Last week I went to Defcon, the premiere security conference that attracted 15,000 security experts and Will Smith to Las Vegas.",
      "Here are a few lessons I learned hanging out with the worlds finest:",
      "All your data are belong to us.\nSecurity is often relegated to an after-thought in the application development realm, when data security should be a first priority, especially for startups that deal with a users personal data.\n  \n  \n    Big data analytics are not immune to infiltration.\nWhen companies run analytics over their data, their datas quality is never brought into question. In reality, spoofing data or simply inserting enough fake data points is enough to bring any clustering algorithm to its knees. The main solution: guard your input sources, and make sure you are not opening yourself up to attack.\n  \n  \n    When in doubt, check for hands.\nMost cases of data breach have some human element involved: A lone USB drive that was plugged into the network, or a password that was written down in plain sight. Address any form of input as a potential attack vector.",
      "Security in Big Data will only increase in importance as more companies look to seek patterns through their data.",
      "That is, until next year!",
      "Good night, Defcon."
    ],
    "summary_t": ""
  },
  {
    "id": "b701f28b7e9f5de2936a6666896cb360",
    "url_s": "https://opensourceconnections.com/blog/2013/08/11/creating-a-search-html-element-with-angularjs/",
    "title": "Creating a Solr <search> HTML element with AngularJS!",
    "content": [
      "Note: The following post is fantastic… but this one is about the same topic and is even better.",
      "Of late weve been playing around with EmberJS for putting together slick client-side apps. But one thing that bothers me is how heavy-weight it feels. Another thing that concerns me is that AngularJS is really getting a lot of good attention and I want to make sure Im not missing the boat! Here, look, just check out the emberjs/angularjs Google Trends plot:",
      "",
      "So since all my major life decisions are dictated by Google Trends plots, I took a little time this weekend to familiarize myself with AngularJS. And frankly, Im impressed!",
      "When building a JavaScript framework, the framework designer has to strike a delicate balance between power and heft (that is, how intrusive and heavy-weight the framework is). Usually the more powerful the framework is, the more invested you have to be in the framework: the more the difficult it is to learn, and the more you simply have to accept its opinions a being the right way to go (even if you secretly disagree). AngularJS, somewhat miraculously, appears to at once be both powerful and lightweight! After taking a casually stroll through the main page of the AngularJS website I was immediately able to start working with it.",
      "Currently one of my favorite things about AngularJS is the ability to create your own custom HTML tags. For instance, in a prototype Im building, I just used AngularJS to create a new <lawsearch> element which loads up a set of search results and displays them on the page. Lets take a look at how this works.",
      "First I create code that defines this new element, enables the data binding, and governs its behavior, (make sure to read the comments here):",
      "//define a module (yay! modular code!)\nangular.module('law', [])\n//within that module, define the idea of a lawsearch element\n.directive('lawsearch', function () {\n    return {\n        restrict: 'E', // this indicates that we're describing a new element\n        scope: {}, //every element will have it's own scope\n\n        //this is where the behavior is defined\n        controller: function ($scope, $element) {\n            $scope.init = function (q) {\n                $scope.q = q;\n\n                //here we issue the actual query using jQuery\n                $.ajax({\n                    url: \"http://ec2-75-101-214-153.compute-1.amazonaws.com:8983/solr/statedecoded/search\",\n                    data: {\n                        \"q\": $scope.q,\n                        \"wt\": \"json\",\n                        \"rows\":3\n                    },\n                    traditional: true,\n                    cache: true,\n                    async: true,\n                    dataType: 'jsonp',\n                    success: function (data) {\n                        //and when we get the query back we\n                        //stick the results in the scope\n                        $scope.$apply(function () {\n                            $scope.results = data.response.docs;\n                        });\n                    },\n                    jsonp: 'json.wrf'\n                });\n            }\n        },\n        //finally, here's the presentation our new lawsearch element\n        template:\n            '<div>'+\n            '    <h1></h1>'+\n            '    <ul class=\"unstyled\">'+\n                     //check out how easy it is to iterate through elements\n            '        <li ng-repeat=\"result in results\">'+\n            '            <h4></h4>'+\n            '            <p></p>'+\n            '        </li>'+\n            '    </ul>'+\n            '</div>',\n        replace: true\n    };\n})",
      "I love that AngularJS has totally nailed the model-view-controller pattern. Usually when someone says theyre using a MVC-based framework I enjoy asking them what that means and how their framework fulfills the MVC pattern. I guess Im sadistic, but I like hearing them mumble and sputter as they try to answer the question.",
      "But here, AngularJS has made it pretty clear. The model is the data that hiding behind the lawsearch element (this is the data that we keep shoving into the $scope object), the view is the template that describes how the data will be presented, and the controller is the function (labeled \"controller\" above) that acts as the intermediary between the model and the view. When the model changes, the controller relays the necessary information to the view and when users interact with the view, the controller can relay changes back to the model. I love it.",
      "Now that the JavaScript piece is done, lets see how one would use this (again, read the comments here):",
      "<html>\n<head>\n    <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\">\n    <title>Law Search</title>\n    <script type=\"text/javascript\" src=\"//code.jquery.com/jquery-2.0.2.js\"></script>\n    <!-- We've got to include angular... duh... -->\n    <script src=\"https://ajax.googleapis.com/ajax/libs/angularjs/1.0.7/angular.min.js\"></script>\n</head>\n<body ng-app=\"law\"><!-- this extra bit indicates that the law\n                 module is in charge of everythin in this tag -->\n    <!-- and then we just create several searches!\n         the init calls code in the controller -->\n    <lawsearch ng-init=\"init('vehicle')\"></lawsearch>\n    <lawsearch ng-init=\"init('house')\"></lawsearch>\n    <lawsearch ng-init=\"init('pet')\"></lawsearch>\n</body>\n</html>",
      "Its hard to get much simpler than that. Want to see this in action? Here it is, live.",
      "And… if you didnt find that impressive, youre sure to love this: Ive used AngularJS to resurect the blink element!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "e05762d7b02011a734e28dbb58868da7",
    "url_s": "https://opensourceconnections.com/blog/2013/08/12/osc-welcomes-you-to-our-friday-after-12-hack-and-tell/",
    "title": "OSC welcomes you to the Friday after 12 Hack-and-Tell",
    "content": [
      "For the past several months OpenSource Connections headquarters has served as an underground, no-cost coworking hot spot, and weve had such a good time with it that weve decided to formalize and expand the arrangement.",
      "Introducing the OSC Friday after 12 Hack-and-Tell",
      "Weekly OSC has been holding internal Friday afternoon hacks, and the best of these hacks culminate in afternoon short talks in which OSC employees and friends spread knowledge about what interests them. Now we would like to officially invite you to join us. Heres how: Join our OSC coworking mailing list and we will keep you updated as to when well hold the next Friday after 12 Hack-and-Tell. Its important to join the mailing list because we wont have the Friday after 12 Hack-and-Tell every week, and we wouldnt want you to show up and be dismayed! But we do plan to hold Friday after 12 Hack-and-Tell about once a month, and when it happens, we want you to be there.",
      "OpenSource Connections Coworking",
      "\"But I dont want hack, I just want a change of scenery and a place to work for free!\" Well your demands are high, but I still think we can help you out. OSC is a great place to cowork because 1) its free and 2) its free. The caveat, though, is that you cant simply show up every day (because hey after all… its free). If you want to join us, then you should still sign up for our coworking mailing list so that you can be aware of the latest news – and importantly, the days that no one is in the office!",
      "Spread the Wealth",
      "Want to know our big, secret, ulterior motive for opening our doors to the community for free? Its because we have a lot to learn! And theres no one better to learn from than you, Cville techizens. Whats more, we believe that youll profit from and learn from us, and from the others that show up. Oh yeah.. the office is a little trick to find, so this might be of help to you",
      "So I hope to see you soon, if not at a Friday after 12 Hack-and-Tell, then at least sitting at a desk next to me coworking. ~Cheers!"
    ],
    "summary_t": ""
  },
  {
    "id": "8c72e9e3928b97e38820e77472ad2833",
    "url_s": "https://opensourceconnections.com/blog/2013/08/20/two-solr-training-courses-coming-to-charlottesville-in-october/",
    "title": "Two Solr Training Courses Coming to Charlottesville in October",
    "content": [
      "Coming this October, OpenSource Connections, in partnership with LucidWorks, is pleased to present two separate Solr training courses. Read on for all the details!",
      "Solr Unleashed- October 21-22",
      "Solr Unleashed is a 2-day Solr training course right here in Charlottesville. If youre new to Solr, then this course will put you well on your way toward building a best-practices search application customized to your particular needs. And even if you have plenty of Solr experience, this course will introduce interesting, new, and useful aspects of Solr that you need to be aware of. Here, take a look at the course outline and see what were talking about. Register here.",
      "Solr Under the Hood – October 23-24",
      "Already feel like you know the basics? Ready to turn your Solr into a lean-mean searching machine? Solr Under the Hood will give you all you need to know to tune performance, build custom Search components, and use SolrCloud to build a fast, distributed, and fault-tolerant search infrastructure. Here is the course outline for more details. Register here.",
      "Introducing your Trainers",
      "",
      "Scott Stults",
      "Scott began his career in 1995 by building a dial-up Internet Service Provider in his hometown. During the years since he has gained a wide range of IT experience from start-ups and Fortune 100 companies alike. His critical thinking and scientific approach has ensured the measurable success of a wide variety of projects. He is currently developing patterns for securing Solr deployments.",
      "",
      "",
      "Matt Overstreet",
      "Usability is Matt Overstreet’s mission. He has worked with Federal, Fortune 500, and small businesses to help collect, mine and interact with data. He solves problems by synthesizing his experiences drawn from a liberal arts and technical background.",
      "",
      "Location",
      "Classes will take place at the OpenSource Connections office conveniently located just southeast of the Downtown Mall. Lunch, snacks, and coffee will be provided.",
      "",
      "Heres what Verizon had to say about the course:",
      "\"It was a great course and [the trainers] were extremely helpful answering our questions.\" – Luis A.",
      "And heres what BestBuy had to say about the course:",
      "\"I found the training course to be an informative and comprehensive dive into the SOLR platform, it touched on many key points and important features of SOLR. I found the course to be a perfect introduction for those new to the SOLR or intermediate users who might want to delve deeper into the platform.\" – Kurt B.",
      "If you are interested in learning more, then please contact us.",
      "Once you are ready, sign up!.",
      "Solr Unleashed",
      "Solr Under the Hood"
    ],
    "summary_t": ""
  },
  {
    "id": "56420805eaf165becf13d93607fd54f7",
    "url_s": "https://opensourceconnections.com/blog/2007/03/09/congratulations/",
    "title": "Congratulations!",
    "content": [
      "Congratulations to OSC principal Scott Stults and his wife on the birth of their first baby, a healthy and as of yet unnamed son.",
      "Naturally, Scott cannot allow something like this to pass without introducing technology, so he has a webcam set up in the hospital room.",
      "Our hopes are that the baby takes after his mother…"
    ],
    "summary_t": ""
  },
  {
    "id": "3b931e271b4ff4ec877b5edb751c6a12",
    "url_s": "https://opensourceconnections.com/blog/2013/08/21/name-search-in-solr/",
    "title": "Name Search in Solr",
    "content": [
      "Remember the good ole days of \"Alpha by Author\"?",
      "Searching names is a pretty common requirement for many applications. Searching by book authors, for example, is a pretty crucial component to a book store. And as it turns out names are actually a surprisingly hard thing to get perfect. Regardless, we can get something pretty good working in Solr, at least for the vast-majority of Anglicized representations.",
      "We can start with the assumption that aside from all the diversity in human names, that a name in our Authors field is likely going to be a small handful of tokens in a single field. We’ll avoid breaking these names up by first, last, and middle names (if these are even appropriate in all cultural contexts). Lets start by looking at some sample names in our \"Authors\" field:",
      "Authors:",
      "Doug Turnbull\n  Turnbull, Douglas\n  Turnbull, Douglas G.\n  Turnbull, D. G.\n  D. Graeme Turnbull",
      "Ok, you can already see the variations in how we represent Anglicized names. This lets us construct our strategy. First, for the record, we’re going to use this pretty basic analysis chain for our authors field. It will do the job of stripping punctuation and lowercasing names:",
      "<fieldType name=\"AuthorsType\" class=\"solr.TextField\"  positionIncrementGap=\"100\">\n  <analyzer>\n    <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n    <filter class=\"solr.LowerCaseFilterFactory\" />\n  </analyzer>\n</fieldType>",
      "Ok down to business. There are two main issues that if we could tackle, would get at a very large portion of the name search problem",
      "Author names get rearranged, either in the document or query, with some parts omitted: (Douglas Turnbull vs Turnbull, Douglas vs Turnbull Douglas G)\n  \n  \n    Many names are abbreviated: (Doug Turnbull vs D. Turnbull vs D. G. Turnbull vs Douglas G. Turnbull)",
      "Rearranged Names",
      "The reordering of author tokens is a fairly straight forward exercise in using lucene proximity search. This feature of Lucene query syntax lets us take a user’s query and a proximity P:",
      "Douglas Turnbull",
      "And search for phrases that are the phrase the user entered or phrases where the terms occur within P proximity to each other. So, expressing this in Lucene query syntax",
      "Authors:\"Douglas Turnbull\"~2",
      "Will happily return results where Douglas and Turnbull occur within 2 token moves of each other (regardless of order). The following matches would be acceptable:",
      "Douglas Turnbull\n  Turnbull Douglas",
      "Accounting for middle initials, we could set our proximity to 3, so a query for:",
      "Authors:\"Douglas Turnbull\"~3",
      "Will now match:",
      "Douglas G. Turnbull\n  Turnbull, Douglas G.",
      "Great, we’ve solved this problem! Well actually this works in most, but not all, cases. Can you spot the subtle bug? Here’s a hint, it has to do with using a phrase query. Is there a class of queries that this strategy won’t work for?",
      "Initials & Short Forms",
      "What happens when a user searches for Doug Turnbull and all Solr has indexed are references to Douglas Turnbull? To help with efficient prefix queries, Solr gives us the EdgeNGramFilterFactory. The EdgeNGramFilterFactory takes a token, say Douglas, and generates tokens based on slicing the string from either the front or the back of the string. For example, with minGramSize=1 and side=\"front\" the token \"Douglas\" will result in the following tokens:",
      "Input:  douglas\nTokens: [d] [do] [dou] [doug] [dougl] [dougla] [douglas]",
      "An important note about this filter (and many others in Solr) is that each generated token ends up occupying the same position in the indexed document. In reality, we can envision the document as two dimensional:",
      "Position N:     Position N+1\n    [d]         ->  [t]\n    [do]            [tu]\n    ...             ...\n    [douglas]       [turnbull]",
      "So a phrase query for \"do turnbull\" will hit on this document in the same location in this document as the phrase \"douglas turnbull\". What a nice characteristic! So we’ll be able to match the abbreviated \"D. Turnbull\" by simply employing this filter in our analysis chain as follows:",
      "Field:",
      "<field name=\"AuthorsPre\" type=\"AuthorsPrefix\" indexed=\"true\" multiValued=\"true\"/>",
      "Copy Field:",
      "<copyField source=\"Authors\" dest=\"AuthorsPre\"/>",
      "Field Type:",
      "<fieldType name=\"AuthorsPrefix\" class=\"solr.TextField\"  positionIncrementGap=\"100\">\n  <analyzer type=\"index\">\n    <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n    <filter class=\"solr.LowerCaseFilterFactory\" />\n    <filter class=\"solr.EdgeNGramFilterFactory\" minGramSize=\"1\" maxGramSize=\"200\" side=\"front\"/>\n  </analyzer>\n  <analyzer type=\"query\">\n    <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n    <filter class=\"solr.LowerCaseFilterFactory\" />\n  </analyzer>\n</fieldType>",
      "Let’s walk this analysis chain through an example query on the AuthorsPre field to see how it works. The name \"Douglas G. Turnbull\" gets indexed as follows",
      "Position            N           N+1     N+2\nStandard Tokenizer: [Douglas]   [G]     [Turnbull]\nLower Case Filter:  [douglas]   [G]     [turnbull]\nNGram Filter:       [d]         [g]     [t]\n                    [do]                [tu]\n                    …(etc)…             …(etc)…\n                    [douglas]           [turnbull]",
      "Now let’s take a user’s query: \"D G Turnbull\". This gets tokenized rather simply using the query analysis chain to [d] [g] [turnbull]. These tokens occur in the index everywhere that the name Douglas G. Turnbull was indexed! (and also where David G. Turnbull was indexed)",
      "Putting it together",
      "Ok time to step up your game. Now the user has entered \"Turnbull, D.\" into the search box. What now? Well we simply apply the same trick we used before. Instead of searching for:",
      "AuthorsPre:\"Turnbull, D.\"",
      "Let’s turn this into a proximity query:",
      "AuthorsPre:\"Turnbull D.\"~3",
      "There’s lots of bits of knowledge coalescing at once to see how this works. First, as we discussed, all the generated ngrammed tokens share the same position in the token stream. So [D.] and [Douglas] are in the same position in the indexed document. This means that when position matters (like in a phrase query) \"D. Turnbull\" and \"Douglas Turnbull\" will both match a document containing \"Douglas Turnbull\". Our proximity search, on the other hand, gives Solr some freedom in rearranging the tokens a bit to satisfy the match, allowing a bit of freedom in how names are arranged – resulting in a match on many rearranged and abbreviated forms of our name.",
      "This is just a start",
      "This is a pretty good start, but search is a heuristic that can always be improved. There’s lots of work left to do to get this perfect! Aside from the numerous conflicting cultural conventions I’m sure I’ve violated, there’s plenty of exercises left to the reader:",
      "Come to Solr training and stump this chump with solutions to these problems!",
      "How do you prefer exact matches over prefixed names?\n  How could you identify which query tokens are meant for middle names vs first names vs last names?\n  The standard tokenizer breaks up hyphenated names, how could you preserve the hyphenated name as one token?\n  Many name abbreviations are not simply prefixes of the original name. For example, how would you match \"Thomas\" when the user enters \"Tom\"?",
      "So theres some fun puzzles to up your Solr game! And if you really want to up your Solr game on these and other problems, be sure to check out our Solr Training!",
      "Share your thoughts! Hopefully this article can get you started building a reasonable name search. Have you had issues with name search in the past? How have you approached these problems with Solr? And of course contact us for help with your name search problems!"
    ],
    "summary_t": "Searching names is a pretty common requirement for many applications. Searching by book authors, for example, is a pretty crucial component to a book store. ..."
  },
  {
    "id": "0f7dae252149994d362a2f54d6834224",
    "url_s": "https://opensourceconnections.com/blog/2013/08/22/four-reasons-were-really-excited-about-foundationdb/",
    "title": "Four reasons we’re really excited about FoundationDB",
    "content": [
      "",
      "As you might have heard, FoundationDB became publicly available this week. As we’ve discussed in the past, we’re very excited here at OpenSource Connections about this emerging database. After experimenting with the beta, we can definitely assert FoundationDB represents an important milestone – combining ACID style transactions with the scalability of today’s NoSQL key-value stores. Why is this important? Here’s four reasons why you ought to be paying attention:",
      "1. Unopinionated Transactions",
      "Most applications need transactions. A big part of application design is mapping the application’s expectation of transactionality to the transactional features the database provides. Unfortunately, many of the new NoSQL databases frequently have rigid transactionality features. HBase, for example, only gives you transactionality guarantees for a single row. If you can map your application to HBase’s opinions, then you’ll get the awesome performance that HBase promises. If you can’t then you’ll need to find another database for your application.",
      "FoundationDB lets you tune the dial in whichever direction your application requires. A transaction may be a simple operation impacting a single key/value pair. Or a transaction might be a complex operation impacting multiple parts of your database. The decision is the programmers. In effect FoundationDB lets the application’s opinions on transactions reign. Moreover, when multiple applications share the same data store, each applications opinion matters. FoundationDB was built around few assumptions about what you’ll need, understanding that every application is different when it comes to transactions.",
      "2. Unopinionated Normalization",
      "Databases these days come with very specific notions of how you ought to (de)normalize your data. In columnar data stores like HBase, the opinion is that keeping everything in a single row, as denormalized as possible, is the best and safest way to use the database. Relational data stores, of course, expect high levels of normalization, with data broken up based on the normal forms I’m sure you learned at school. Normalization is often good – it helps us avoid duplication and update anomalies. And sometimes denormalization is good – its frequently simpler to manage in a distributed system – avoiding the performance cost of joining across multiple nodes in the store.",
      "FoundationDB lets you decide on the appropriate level of (de)normalization based on what the application calls for. Do you need very wide rows of data? That’s fine; FoundationDB will happily store a giant BLOB away for you. Do you need to clump together related keys/values in the database? That’s also great, FoundationDB can achieve this through the sorted and hierarchical nature of its keys. Would you rather give each piece of data its own id, turning FoundationDB into redis? That’s perfectly ok too. You start to get the feeling that FoundationDB serves your diverse needs rather than your application conforming to its narrow opinions of how data should be stored. This kind of thinking in the NoSQL space is certainly refreshing!",
      "3. Layers with opinions – Your applications opinions",
      "Of even greater importance, I’m excited that FoundationDB lets me build the layers that bake in the transactional/normalization opinions I need for my application. I can define, in code, the subtle tradeoffs that defined how I store data. Instead of choosing half-a-dozen different databases each with a slightly different set of features, promises, and tradeoffs, I build layers on top of a single data store, thus avoiding the burgeoning complexity of dealing with many duplicated databases.",
      "Building the layers is not a hard task that requires low-level database knowledge. The API is elegant, simple, has multiple language bindings, and is well documented. The storage model is simple and unobtrusive. This makes it straight-forward to wrap in a completely different data storage abstraction. For example, FoundationDB has replaced the backend storage engine of SQLlite with FoundationDB. You may think you’re talking SQL, but in reality, there’s a piece of FoundationDB client code sitting underneath the SQLite query parser. Suddenly a small embedded database has turned into a distributed, scalable data store!",
      "4. The People",
      "Most importantly, we’re impressed with the team. After interacting with the FoundationDB, including hosting a hackathon with the team, I can verify that it’s a great group of folks. The FoundationDB team combines the two features I find most valuable in technologists:",
      "They are really smart and deep technically\n  They are human beings: friendly, outgoing, humble, and can communicate",
      "When the founder takes a personal interest in bugs you find, or technical feedback you provide, you know you’ve found a technology that feels like home. It seems every member of the team interacts frequently community site they’ve setup, helping with support issues and triaging bugs. It’s a good sign for me to see a product company so engaged with the community around their product.",
      "Closing Thoughts",
      "In short, we’re excited for the future of this new company and new technology! Do you have questions about FoundationDB and how it stacks up against other (No)SQL solutions? We’d love to hear from you or leave us a comment below!"
    ],
    "summary_t": "As you might have heard, FoundationDB became publicly available this week. As we’ve discussed in the past, we’re very excited here at OpenSource Connections ..."
  },
  {
    "id": "662961d766a92c971bd3dce244ab042d",
    "url_s": "https://opensourceconnections.com/blog/2013/08/24/cisco-proud-to-work-with-opensource-connections/",
    "title": "Cisco, proud to work with OpenSource Connections",
    "content": [
      "We recently wrapped up Solr project with Cisco that turned out to be one of our most interesting and challenging projects to date. While we can not disclose some of the most interesting details of our project, the work entailed pushing Solr to limits that we had not previously witnessed. In addition, the use-case was significantly outside of what we normally see with Solr. Solr has become a cornerstone for full-text search, but it turns out to be very fast and well-suited as a general analytics engine! In our project, rather than dealing with fields composed of natural text, we were concerned with documents composed almost exclusively of numerical values and collections of hierarchical tags. At the pinnacle of our work, we were able to ingest 30K such documents per second while at the same time providing rich, highly faceted search. This will allow users to slice and dice the data set and return only the documents of interest.",
      "Heres what Cisco had to say about our work:",
      "Thanks to OpenSource Connections for so quickly picking up our requirements and coming up with multiple plausible solutions. We really appreciated your focused approach to problem solving. I know that \"search\" per se is a wide field but you helped us narrow it down to our context quite effectively with Solr.",
      "Do you have a unique use case for Solr? Are you pushing Solr to new extremes? Let us know, we can probably help you out!"
    ],
    "summary_t": ""
  },
  {
    "id": "03f94cfccf1fcfcac8747e680bd92f51",
    "url_s": "https://opensourceconnections.com/blog/2013/08/25/instant-search-with-solr-and-angular/",
    "title": "Instant Search with Solr and AngularJS",
    "content": [
      "",
      "Previously, we gave an example of a super simple search results widget that utilized jsonp and AngularJS. Since then, we’ve polished our Angular skills quite a bit. Enough so, that I was able to throw together this instant search widget, in less than an hour. Angular truly is an amazing UI tool.",
      "Below is the source for the widget (and here it is on github and jsfiddle). Like our previous example, we use Angular directives to create a custom HTML element. Unlike our previous example, weve allowed some interactivity with a simple input tag. Enjoy!",
      "'use strict';\n\nangular.module('solrAngularDemoApp')\n  .directive('searchResults', function () {\n    return {\n      scope: {\n        solrUrl: '=',\n        displayField: '=',\n        query: '&',\n        results: '&'\n      },\n      restrict: 'E',\n      controller: function($scope, $http) {\n        console.log('Searching for ' + $scope.query + ' at ' + $scope.solrUrl);\n        $scope.$watch('query', function() {\n          $http(\n            {method: 'JSONP',\n             url: $scope.solrUrl,\n             params:{'json.wrf': 'JSON_CALLBACK',\n                    'q': $scope.query,\n                    'fl': $scope.displayField}\n            })\n            .success(function(data) {\n              var docs = data.response.docs;\n              console.log('search success!');\n              $scope.results.docs = docs;\n\n            }).error(function() {\n              console.log('Search failed!');\n            });\n        });\n      },\n      template: '<input ng-model=\"query\" name=\"Search\"></input>' +\n                '<h2>Search Results for </h2>' +\n                '<span ng-repeat=\"doc in results.docs\">' +\n                '  <p></p>'  +\n                '</span>'\n    };\n  });",
      "Notice how we added the $scope.$watch call in our controller. This directly binds some behavior to occur when a value in the $scope changes. Angular is well known for rerendering the page when $scope changes. Whats even more powerful is that Angulars $watch lets you perform arbitrary behavior when a value in $scope changes. In our case, were reexecuting the search when the query member changes. The variable query is bound to the input tag, so nothing special is needed to invoke this code other than the user typing into the widget. Pretty snazzy!",
      "Here’s how the widget is used:",
      "<search-results\n    solr-url=\"'http://ec2-75-101-214-153.compute-1.amazonaws.com:8983/solr/statedecoded/search'\"\n    query=\"'car tax'\"\n    display-field=\"'catch_line'\">\n</search-results>",
      "We continue to enjoy working in Angular. Like any other framework, it requires a bit of reworking your mind into how it thinks, but once you get there, the productivity gains are pretty enormous. In many ways the tight binding of user behavior and display reminds me of my favorite desktop UI framework, QT that I used to blog about here. Very analogous to Angulars philosophy of two-way bindings, QT uses signals/slots to tightly integrate input, display, and behavior. Once figured out, the tight binding of these things can be enormously productive in either the web or the desktop environment.",
      "Of course, we’d love to hear about your search UI needs! Contact us! You too can take advantage of our expertise creating beautiful search and discovery interfaces (like say this one!)"
    ],
    "summary_t": "Previously, we gave an example of a super simple search results widget that utilized jsonp and AngularJS. Since then, we’ve polished our Angular skills quite..."
  },
  {
    "id": "c45e3109849a86f5d6d07098fdb05cb5",
    "url_s": "https://opensourceconnections.com/blog/2013/08/25/semantic-search-with-solr-and-python-numpy/",
    "title": "Semantic Search with Solr and Python Numpy",
    "content": [
      "Built upon Lucene, Solr provides fast, highly scalable, and easily maintainable full-text search capabilities. However, under the hood, Solr is really just a sophisticated token-matching engine. Whats missing? – Semantic Search!",
      "Consider three, somewhat silly documents:",
      "Yellow banana peels.\n  A banana is a long yellow fruit.\n  This mystery fruit is long and yellow and has a peel.",
      "Now what happens if you search for the term \"banana\". Under normal circumstances you only get back the first and second document. But why shouldnt you also get back the third document? Its obviously talking about bananas!",
      "Semantic Search via Collaborative Filtering",
      "Colleague Doug Turnbull and I recently set about to right this wrong with help from a machine learning technique called collaborative filtering. Collaborative filtering is most often used as a basis for recommendation algorithms. For example, collaborative filtering algorithms were the central focus of the now-famous Netflix Prize which awarded $1Million to the team which could build the best movie recommendation engine. When dealing with recommendations, collaborative filtering works by mathematically identifying commonalities in groups of users based upon the movies that they enjoyed. Then, if you appear to fall in one of those groups, the recommendation engine will point you towards a movie that a) you havent watched and b) you are likely to enjoy.",
      "So what does this have to do with Semantic Search? Everything! In just the same way that certain users gravitate towards certain movies, certain words commonly co-occur in the same documents. When working with Semantic Search, rather than recommending user to movies that they would likely enjoy, we are going to identify words that are likely to belong in a given document, whether or not they actually occurred there. The math is exactly the same!",
      "Heres how the process works:",
      "First we identify a text field of interest in our documents and extract the associated term-document matrix for external processing. Each element of this term-document matrix indicates the strength of a particular term within a particular document (where strength can be anything, but will likely be either term frequency or TF*IDF).\n  Next, collaborative filtering is applied to the term-document matrix which effectively generates a pseudo-term-document matrix. This pseudo-term-document matrix is the same size and shape as the original term-document matrix and references the same terms and documents, but the numbers are slightly different. These new values indicate the strength that a particular term should have in a particular document once noisy data is removed.\n  Finally, the high-scoring values in the pseudo-term-document matrix are mapped back to the associated terms. These terms are then injected back into Solr in a new field which can be used for Semantic Search.",
      "Demo Time!",
      "So lets consider an example case. As in plenty of our previous posts, we will be using the Science Fiction Stack Exchange. Why? Because were all nerds and with such a familiar topic, we can quickly intuit whether or not a search is returning relevant results. In this data set, the field of interest is the Body field because it contains the contents of all questions and answers.",
      "So now that weve decided upon our demo dataset, were ready run the analysis. If youd like to follow along, then please take a look at our git repo. This repo contains the example SciFi data set, the Semantic Search code, and README to get you going. However Im going execute everything from within Python:",
      ">>> from SemanticAnalyzer import *\n>>> stvc = SolrTermVectorCollector(field='Body',feature='tf',batchSize=1000)\n>>> tdc = TermDocCollection(source=stvc,numTopics=150)",
      "That last line takes a few minutes. If its in the AM where you are, grab a coffee. If its in the PM, grab a beer. Once that line completes, we will have successfully extracted the term-document matrix from Solr. Now lets play with it for a bit. One of the cool side effects of this analysis is the ability to quickly find words that commonly occur together. Lets give it an easy test; here are the 30 most highly correlated words with the word `vader (as in Darth Vader).",
      ">>> tdc.getRelatedTerms('vader',30)",
      "Did you notice that pause when you called the function? That was the collaborative filtering taking place. The results of that process have now been saved, so additional calls will return quite quickly.",
      "vader luke emperor darth palpatin anakin sith skywalk sidiou apprentic empir luca side star son forc turn kill death rule suit father question jedi command obi tarkin dark wan plan",
      "Hey not bad! Everything here seem very reasonably connected with Mr. Vader. You may notice some odd spellings here, thats because these are the indexed terms, therefore they are stemmed. Lets try again with a different term; this time everyones favorite wizard:",
      ">>> tdc.getRelatedTerms('potter',30)",
      "harri potter voldemort wizard snape death magic jame love spell time rowl lili eater travel seri hous hand hogwart three find wormtail kill slytherin hallow secret deathli muggl order lord",
      "Again, pretty good! One last try, and well make it a little more challenging – a vague adjective:",
      ">>> tdc.getRelatedTerms('dark',30)",
      "dark side jedi sith eater lord death mark snape magic curs evil forc luke mercuri cave yoda jame palpatin dagobah anakin black call wizard slytherin live light siriu matter voldemort",
      "Indeed, most of these terms are like a hall of fame of dark things from Star Wars and Harry Potter.",
      "Now since the word correlation has proven itself out, its time to generate the pseudo terms and post them back to Solr.",
      ">>> SolrBlurredTermUpdater(tdc,blurredField=\"BodyBlurred\").pushToSolr(0.1)",
      "This line will probably see you to the end of your coffee or beer (it takes about 10 minutes on my machine). But once its done, you can start issuing searches to Solr.",
      "Solr Results",
      "Heres an example of Semantic Search using Solr:",
      "http://localhost:8983/solr/select/?q=-Body:dark +BodyBlurred:dark",
      "The Body field contains the original text while the BodyBlurred contains the pseudo-terms. So this finds all documents that do not include the term dark, but presumably contain dark content. Take a look at the documents that come back:",
      "{\n  Body: \"In the John Carter movie (2012), he shows off some of his powers, like jumping abnormally high, but I have difficulty evaluating his strength. On the one side, he shows great strength, as when he kills a thark warrior with one hand, but he is also quite mistreated by them. He also seems helpless when he is strangled by Tars Tarkas. Why does the strength he shows seem so inconsistent? \",\n  BodyBlurred: \"tv great movi control kill consid hand dark side power long mutant fight machin light abil sauron wormtail hulk\"\n},\n{\n  Body: \"In the movies, the Nazgul ride black horses with armour. I was wondering if that is all they are, or do they have some sort of magic? Are they evil?\",\n  BodyBlurred: \"movi black magic dark demon engin hous aveng slytherin\"\n},\n{\n  Body: \"The remaining Black Brother from the prologue of A Game of Thrones is apparently the deserter who is beheaded in the beginning of the book. But how did he manage to get to Winterfell from the other side of The Wall? Or did the show throw me off track and in the book there weren't any survivors, so the deserter is someone else? \",\n  BodyBlurred: \"book watch black hole dark side plai long game demon engin light turn district\"\n},\n{\n  Body: \" Was this ever discussed in any episode, or as a side-plot somewhere? \",\n  BodyBlurred: \"episod dark side light\"\n}",
      "Not bad – most of those topics are rather… dark. Though check out that last result. So… maybe there are still some improvements we can make! But you also have to remember that were dealing with word correlation here and I can only guess that somewhere else in the corpus dark side-plots and dark episodes were surely discussed.",
      "Speaking of word correlations, check out this gem:",
      "{\n  Body: \"You're correct, Enterprise is the only Star Trek that fits into both the original and the new 2009 movie timelines. From the perspective of the Enterprise characters, both are possible futures, given the over-arcing conceit of the show was a Temporal Cold War, so its future is in flux and could line up with either of the timelines we're familiar with, or with an entirely different future. \",\n  BodyBlurred: \"answer charact place klingon star trek design travel crew watch work movi happen enterpris featur futur exist origin 2009 chang altern timelin war to version event captain gener pictur tng creat iii galaxi theori return alter voyag entir fry turn kirk paradox biff doc marti feder 1955 starship 2015 class hero centuri tempor uss phoenix mirror river 800 ncc 1701 simon conner skynet alisha\"\n}",
      "The original document involves Star Trek and time-travel. And appropriately, the pseudo terms include Star Trek things and time-travel terms… but do you see anything funny? Thats right Biff Doc and Marti made their way into the pseudo terms – likely because of their role in the popular time-travel film \"Back to the Future.\"",
      "Speaking of the future …",
      "Future Work",
      "Semantic Search with Solr is hot right now. In the upcoming Dublin LuceneRevolution I know of at least 3 related talks that have been submitted (one of them my own); I have heard that MapR is working on a Solr Semantic Search/Recommendation engine built atop of their Hadoop offering; and I suspect that with Clouderas recent foray into Solr with Mark Miller, they will also be working on the same thing.",
      "Whats next for our work? Recommendations! (Remember, thats how we started this conversation.) E-commerce recommendations is a simple extension of the work presented above. Given an inventory catalogue (e.g. product title, description, etc.), and given a history of user purchases, we can build a search-aware recommendation engine. That is, when a customer searches for a particular item, they will receives results as usual, except that the results will be boosted with items that they are more likely to purchase. How? Because we know what type of customer they are and what products that type of customer is more likely to buy!",
      "Do you have a good case for Solr Semantic Search and Recommendation? Wed love to hear it, please contact us!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "a4c3d3bb616f6bccd7f0ddec7ce52768",
    "url_s": "https://opensourceconnections.com/blog/2013/08/27/solrcloud-as-a-windows-service/",
    "title": "SolrCloud as a Windows Service",
    "content": [
      "Want to setup a simple solrCloud network on Windows? If you are just running a couple nodes and intend to run the embedded ZooKeeper heres how to get it done.",
      "First, make a list of the ip addresses (or better yet, hostnames) of your servers. Youll need this for the ZooKeeper bit of the solr configuration.",
      "Download a copy of Solr 4.3 or greater. Extract it somewhere reasonable on your first server. For simplicity well assume C:\\SolrCloud, if not find/replace in your favorite editor will be handy.",
      "Next, grab the Apache Commons daemon for Windows. Unzip it somewhere off to the side. Inside youll find two executables: prunmgr and prunsrv. Copy prunmgr.exe to C:\\SolrCloud and rename it to solrCloudw.exe. Do the same for prunsrv.exe, but name it solrCloud.exe (no \"w\" at the end of the name).",
      "Open a command prompt (Start->Run then \"cmd.exe\" and enter).",
      "cd C:\\SolrCloud",
      "Then, run the following to register your first server service:",
      "solrCloud.exe //IS/solrCloudZk --DisplayName=\"Solr Cloud with Zookeeper\"\n  --Install=c:\\solr-cloud\\solr-cloud\\example\\solrCloud.exe\n  --LogPath=c:\\solr-cloud\\solr-cloud\\example\\logs\n  --LogLevel=Debug\n  --StdOutput=auto\n  --StdError=auto\n  --StartMode=java\n  --StopMode=java\n  --Jvm=auto\n  ++JvmOptions=-Djetty.home=c:\\SolrCloud\\example\\solr\n  ++JvmOptions=-DSTOP.PORT=8087\n  ++JvmOptions=-DSTOP.KEY=stopsolr\n  ++JvmOptions=-Djetty.logs=c:\\SolrCloud\\example\\logs\n  ++JvmOptions=-Djetty.port=8983\n  ++JvmOptions=-Dorg.eclipse.jetty.util.log.SOURCE=true\n  ++JvmOptions=-XX:MaxPermSize=512M\n  --Classpath=c:\\SolrCloud\\example\\start.jar\n  --StartClass=org.eclipse.jetty.start.Main\n  ++StartParams=OPTION=ALL\n  ++StartParams=c:\\SolrCloud\\example\\etc\\jetty.xml\n  --StopClass=org.eclipse.jetty.start.Main ++StopParams=--stop\n  ++JvmOptions=-Dsolr.solr.home=c:\\SolrCloud\\example\\solr\n  --StartPath=c:\\SolrCloud\\example\n  --JavaHome=\"c:\\Program Files\\Java\\jre6\"\n  ++JvmOptions=-DzkHost=10.5.0.1:9983;10.5.0.2:9983\n  ++JvmOptions=-DzkRun\n  ++JvmOptions=-DnumShards=1\n  ++JvmOptions=-Dcollection.configName=myconf\n  ++JvmOptions=-Dbootstrap_confdir=c:\\SolrCloud\\example\\solr\\conf",
      "A couple things to note here.",
      "Make sure to edit the line with \"++JvmOptions=-DzkHost=10.5.0.1:9983;10.5.0.2:9983″. This should be a list of your servers with \":9983″ on the end of each. And, yes, you could supply a custom ZooKeeper root here by doing something like \":9983/myroot\". No need since we are running the embedded ZooKeeper, but who wants to do that forever?\n  \n  \n    Check all the paths and make sure they reflect your setup. As mentioned, we are using \"c:\\SolrCloud\" for simplicity.\n  \n  \n    Dont forget that we are writing logs to the C:\\SolrCloud\\example\\logs directory. These can be very handy for trouble shooting.\n  \n  \n    This setup will reset the the Solr config in ZooKeeper every time this server is restarted. You could remove this behavior by reregistering the service without the last line after it has run once. For now, Id recommend keeping it until you are comfortable managing your own ZooKeeper servers. At that point you can drop the last for lines, and manage Zk as its on server/service.",
      "Start the service:",
      "solrCloud.exe //SS",
      "Repeat the above on the rest of your servers, with one change. Remove the last line of the service setup, \"++JvmOptions=-Dbootstrap_confdir=c:\\SolrCloud\\example\\solr\\conf\".",
      "You now have a SolrCloud cluster running on Windows!",
      "Thanks to Alexandre Rafalovitch and his article on Solr as a Windows Service. Have a look there if you want to run Solr on Windows, but not SolrCloud."
    ],
    "summary_t": ""
  },
  {
    "id": "08a7e88222cdd12659bd81698a56635b",
    "url_s": "https://opensourceconnections.com/blog/2013/08/28/investing-in-client-side-search/",
    "title": "Why we chose to invest in client-side search applications",
    "content": [
      "On a recent enterprise search project we were tasked with taking over six million documents and making them searchable by the general public, all within a very short development window.",
      "Our solution was to build a lightweight search application in Javascript that would return results directly from Solr. Using this architecture allowed us to bypass the conventional server requirements, and meant that we could make the application fast and performant on client machines without having to spend tons on server resources. This model also gave us a lot of flexibility during projects development phase and enabled us to complete the application within a condensed timeframe.",
      "To illustrate the difference in approach: in the traditional model the load is placed on the server to render resources and relay database queries.",
      "",
      "But, in the client-side model once the application is downloaded it talks directly to the database.",
      "",
      "So how does it all work? The javascript search application interfaces directly with a Solr database through JSON which means that there are no intermediary servers in the conventional sense. We decided to develop the application with EmberJS. Using Ember allowed us to cut down our development time and focus on the core functionality instead of re-inventing data bindings, routing, or memory management. While building the application we attempted to make the code as modular and database-agnostic as possible. This lead to a reusable codebase for full-text search interfaces that we are calling Spyglass.",
      "Spyglass is built with EmberJS and comes with many of the search components you need to create a lightweight search interface right out of the box. In Spyglass there is the concept of:",
      "Searchers that return a result from a given url,\n  a Search Box which is a simple input tied to its searcher,\n  Result Sets that automatically show the results returned by their searchers, and\n  Facets which toggle search parameters. Both result sets and facets are extensions of SearcherObservers which update automatically when their linked searcher has new objects.",
      "This framework is something that we have been working on over the last few months and will be open sourcing soon. Stay tuned.",
      "Conclusion",
      "Deciding to build a client-side search application allowed OpenSource Connections to rapidly iterate through development, and deliver a product that will maximize scalability while minimizing costs. It was an exciting project to work on and one where we really learned a lot about application design and working with massive amounts of data. In the end we proved that search applications can be built rapidly, with a minimal set of application components, while still functioning at a very large scale.",
      "We look forward to sharing more about SpyGlass and the details of this specific client project in the near future. If you are interested in working with us on a client-side search application, drop us a line."
    ],
    "summary_t": ""
  },
  {
    "id": "6798ef5dc0849c8c23516eade4d5e20a",
    "url_s": "https://opensourceconnections.com/blog/2013/08/31/building-the-perfect-cassandra-test-environment/",
    "title": "Building the Perfect Cassandra Test Environment",
    "content": [
      "A month back, one of our clients asked us to set up 15 individual single-node Cassandra instances, each of which would live in 64MB of RAM and each of which would reside on the same machine. My first response was \"Why!?\"",
      "Qualities of an Ideal Cassandra Test Framework",
      "So what are the qualities of an ideal Cassandra test framework?",
      "Light-weight and available – A good test framework will take up as little resources as possible and be accessible right when you want it.\n  Parity with Production – The test environment should perfectly simulate the production environment. This is a no-brainer. After all what good does it do you to pass a test only to wonder whether or not an error lurks in the differences between the test and production environments?\n  Stateless – Between running tests, theres no reason to keep any information around. So why not just throw it all away?\n  Isolated – Most often there will be several developers on a team, and theres a good chance theyll be testing things at the same time. Its important to keep each developer quarantined from the others.\n  Fault Resistant – Remember, were a little concerned here that Cassandra is going to be a resource hog or otherwise just not work. Being \"fault resistant\" means striking the right balance so that Cassandra takes up as little resources as possible without actually failing.",
      "Implementing the Ideal Cassandra Test Framework",
      "The first thing to do is to set up the test environment on a per-developer basis. This means changing a few paths. From cassandra.yaml change",
      "data_file_directories:\n  - /home/jberryman/cassandra/data\ncommitlog_directory: /home/jberryman/cassandra/commitlog\nsaved_caches_directory: /home/jberryman/saved_caches",
      "And then in log4j-server.properties change",
      "log4j.appender.R.File=/home/jberryman/cassandra/system.log",
      "Next, its a good idea to create a wrapper around whatever client youre using. This has several benefits. For one thing, creating a wrapper provides a guard against the client changing from under you. This is especially important right now since so many clients are scrambling to be CQL3 compliant. This wrapper is also a great place to stick any safeguards against horking up your production data when you think youre running a test. Perhaps the easiest way to safeguard against this is to issue the CQL DESCRIBE CLUSTER statement and make sure that the cluster name is \"TestCluster\". (If your CQL client doesnt honor this statement, you can just create a keyspace called \"Yes_ThisIsIndeedATestCluster\" and test for its existence.) Once the wrapper is complete, it can be used with functional parity on both the test and production cluster.",
      "The simplest way to make Cassandra light weight is to simply declare it so! In cassandra-env.sh, simply change",
      "MAX_HEAP_SIZE=\"64M\"\nHEAP_NEWSIZE=\"12M\"",
      "However, just because you have now declared Cassandra to be light weight doesnt mean that it will JustWork™. Given this little heap space to move in, Cassandra will happily toss you an OutOfMemory error on its first SSTable flush or compaction or garbage collection. To guard against this we have a bit of work to do!",
      "The first thing to do is to reduce the number of threads, especially for reading and writing. In cassandra.yaml there are several changes to make:",
      "rpc_server_type: hsha",
      "Here, hsha stands for \"half synchronous, half asynchronous.\" This makes sure that all thrift clients are handled asynchronously using a small number of threads that do not vary with the amount of thrift clients.",
      "concurrent_reads: 2;\nconcurrent_writes: 2\nrpc_min_threads: 1;\nrpc_max_threads: 1",
      "As stated, the first two lines limit the number of reads and writes that can happen at the same time. 2 is the minimum number allowed here. The second two lines limit how many threads are available used for serving requests. Everything to this point serves to make sure that writes and reads can not overpower Cassandra during flushes and compactions. Next up:",
      "concurrent_compactors: 1",
      "If you are using SSDs then this will limit the number of compactors to 1. If youre using spinning magnets, then youre already limited to a single concurrent compactor.",
      "Next we need to make sure that we do everything we can so that compaction is not hindered. One setting here:",
      "compaction_throughput_mb_per_sec: 0",
      "This disables compaction throttling completely so that compaction has full reign over other competing priorities.",
      "Next we turn all the knobs on memory usage as low as possible:",
      "in_memory_compaction_limit_in_mb: 1",
      "This is the minimal limit for allowing compaction to take place in memory. With such a low setting, much of compaction will take place in a 2-pass method that is I/O intensive – but I/O is not the thing were worried about!",
      "key_cache_size_in_mb: 0",
      "At the expense of read times, we can do away with key caches. But this may not even be necessary because we can do even better:",
      "reduce_cache_sizes_at: 0\nreduce_cache_capacity_to: 0",
      "The first line say \"As soon as youve used up this much memory, then reduce cache capacity.\" And since this is set to 0, cache capacity is reduced just about as soon as Cassandra starts being used. The second line then dictates that the caches should effectively not be used at all.",
      "Finally, on a test cluster, were not worried about data durability, so there are plenty of safeguards that we can simply do away with. For one, before starting the test cluster, go ahead and remove everything in the data dir and commitlog directories. Next, in cassandra.yaml set hinted_handoff_enabled: false. When creating a test keyspace, go ahead and set durable_writes = false so that the commit log is never even populated. Finally, when creating test tables, consider setting read_repair_chance = 0 and bloom_filter_fp_chance = 1. Though perhaps these modifications on keyspaces and tables are unnecessary because I was able to get pretty good performance without them.",
      "Testing the Test Framework",
      "Now since all of our changes are in place, lets fire up Cassandra and see how she performs!",
      "$ rm -fr /home/jberryman/cassandra && bin/cassandra -f",
      "So far good. \"Starting listening for CQL clients on localhost/127.0.0.1:9042″ means that were alive and ready to service requests. Now its time to slam Cassandra:",
      "$ bin/cassandra-stress\ntotal,interval_op_rate,interval_key_rate,latency/95th/99th,elapsed_time\n33287,3328,3328,8.0,54.6,277.0,10\n85059,5177,5177,7.5,33.3,276.7,20\n133153,4809,4809,7.4,34.0,274.8,30\n183111,4995,4995,6.9,31.6,165.1,40\n233177,5006,5006,6.8,32.0,123.5,51\n288998,5582,5582,6.7,26.7,123.5,61\n341481,5248,5248,6.7,26.3,129.7,71\n391594,5011,5011,6.7,26.7,129.7,81\n441645,5005,5005,6.5,29.0,122.5,92\n494198,5255,5255,6.3,28.3,122.9,102\n539406,4520,4520,6.4,24.4,122.9,112\n591272,5186,5186,6.4,26.8,122.9,122\n641202,4993,4993,6.6,27.9,122.9,132\n696041,5483,5483,6.6,28.2,122.9,143\n747078,5103,5103,6.5,26.1,274.4,153\n797125,5004,5004,6.4,25.3,274.4,163\n839887,4276,4276,6.1,23.9,273.6,173\n880678,4079,4079,6.0,22.9,273.6,184\n928384,4770,4770,5.8,21.7,273.6,194\n979878,5149,5149,5.7,20.2,273.6,204\n1000000,2012,2012,5.5,19.4,273.6,208\nEND",
      "Wow… so not only does it not die, its actually pretty darn performant! Looking back at the logs I see a couple warnings:",
      "WARN 17:15:57,030 Heap is 0.5260566963447822 full.  You may need to reduce\nmemtable and/or cache sizes.  Cassandra is now reducing cache sizes to free up\nmemory.  Adjust reduce_cache_sizes_at threshold in cassandra.yaml if you don't\nwant Cassandra to do this automatically",
      "Ah… this has to do with the reduce_cache_sizes_at, reduce_cache_capacity_to bit from earlier. After this warning we hits, we know that caches have been tossed out. Without caches, I wonder how that will affect the read performance. Lets see!",
      "$ bin/cassandra-stress --operation READ\ntotal,interval_op_rate,interval_key_rate,latency/95th/99th,elapsed_time\n34948,3494,3494,8.4,39.9,147.0,10\n95108,6016,6016,7.9,19.3,145.2,20\n155830,6072,6072,7.8,15.4,144.7,30\n213037,5720,5720,7.8,14.6,72.5,40\n274021,6098,6098,7.8,13.7,56.8,51\n335575,6155,6155,7.7,12.6,56.6,61\n396074,6049,6049,7.7,12.6,56.6,71\n455660,5958,5958,7.7,12.7,45.8,81\n516840,6118,6118,7.7,12.3,45.8,91\n576045,5920,5920,7.7,12.3,45.6,102\n635237,5919,5919,7.7,12.7,45.6,112\n688830,5359,5359,7.7,13.5,45.6,122\n740047,5121,5121,7.7,15.1,45.8,132\n796249,5620,5620,7.8,14.8,42.4,143\n853788,5753,5753,7.9,14.1,37.1,153\n906821,5303,5303,7.9,15.1,37.1,163\n963981,5716,5716,7.9,14.1,37.1,173\n1000000,3601,3601,7.9,13.3,37.1,180\nEND",
      "Hooray, it works! And its still quite performant! I was concerned about the lack of caches killing Cassandra read performance, but it seems to be just fine. Looking back at the log file again, there are several more warnings each look about like this:",
      "WARN 17:16:25,082 Heap is 0.7914885099943694 full.  You may need to reduce\nmemtable and/or cache sizes.  Cassandra will now flush up to the two largest\nmemtables to free up memory.  Adjust flush_largest_memtables_at threshold in\ncassandra.yaml if you don't want Cassandra to do this automatically\nWARN 17:16:25,083 Flushing CFS(Keyspace='Keyspace1', ColumnFamily='Standard1')\nto relieve memory pressure",
      "Despite the fact that were regularly having these emergency memtable flushes, Cassandra never died!",
      "Popping open jconsole, we can make a couple more observations. The first is that while the unaltered Cassandra process takes up roughly 8GB of memory, this test Cassandra never goes over 64MB. Second, we also see that that the number of threads on the unaltered Cassandra hovers around 120-130 while the test Cassandra remains somewhere between 40 and 50.",
      "Conclusion",
      "So you see, my clients request was actually quite reasonable and quite a good idea! Now they have a test framework that is able to support 15 developers on a single machine so that each developer has their own isolated test environment. This is a good example of how consultants sometimes learn from the companies theyre consulting.",
      "How do you test Cassandra? Wed love to know?"
    ],
    "summary_t": ""
  },
  {
    "id": "9d61510a3912ff76057d03b826f2495e",
    "url_s": "https://opensourceconnections.com/blog/2013/09/04/gunning-for-gold-competing-on-an-indian-tech-game-show/",
    "title": "Gunning for Gold – Competing on an Indian Tech Game Show",
    "content": [
      "Last week I left OSC headquarters in Charlottesville and set off towards Mumbai. I and a friend are competing in the Super Techies Show put on by CapGemini.",
      "Cheer on Team All Your Base as we fight for glory and deliver innovative technology solutions!",
      "We are filming this week, and the show will be broadcast later this year on ET in India, and posted to YouTube as well."
    ],
    "summary_t": ""
  },
  {
    "id": "762b036a79a46717af6d56de06d4b065",
    "url_s": "https://opensourceconnections.com/blog/2013/09/04/migrating-american-medical-associations-search-to-solr/",
    "title": "Migrating American Medical Association’s search to Solr",
    "content": [
      "Our client, Silverchair Information Systems, recently completed a successful migration of the American Medical Association’s search over to Solr. Leveraging Silverchair’s semantic platform, weve helped migrate Silverchairs SCM platform to Solr and away from Windows Search, dramatically improving performance and search quality. The AMA is the first of Silverchairs clients to benefit from this swap-out.",
      "Numerous search quality problems had to be tackled to get the AMAs search just right. These include:",
      "Research journal users value recent publications very highly. Users want to see recent research, not just documents that score well due to how frequently search terms occur in a document. If you were a doctor, would you rather see brain cancer research that occurred this decade or in the early 20th century?\n  Searching on an authors name is an important feature. It’s understandably important that if you’ve published findings in the AMA’s prestigious journals, you’ll want to be able to find your research.\n  Silverchair uses a semantic tagging system to improve search quality. Integrating the semantic features into AMA’s search required a lot of tuning and testing. Turns out we know a thing or two about this too.",
      "For these problems, weve been using a new tool, Quepid, to help fix, regression test, and collaborate with the team on the quality of search results. I hope to talk about this tool and the related methodology – Test Driven Relevancy – at the upcoming Lucene Revolution. (Go here and vote for it!)",
      "It’s been amazing to see how Quepid completely alters how teams of technologists, programmers, and content experts can collaborate on search quality. Quepid offers a platform where the team can monitor the impact of tuning on multiple searches simultaneously, rate the quality of results, demonstrate problems with troublesome queries, and troubleshoot and resolve those search problems. Furthermore, by laying out queries that represent all the search problems resolved in the past, it’s helped us avoid the problem of fixing one problem and breaking dozens of others.",
      "Are you stuck in a morass of never improving search quality? Do you want to make it easier to collaborate on search quality across disciplines? Quepid could be the right tool for you. Sign up if you’re interested in hearing about the upcoming alpha release of this product.",
      "And finally, congratulations to Silverchair and the AMA on their successful migration!"
    ],
    "summary_t": ""
  },
  {
    "id": "338cf4de3f144d3380f7e3bd5ffd44b9",
    "url_s": "https://opensourceconnections.com/blog/2007/03/13/a-nice-surprise-courtesy-of-continuum-easy-updating-of-repository-information/",
    "title": "A nice surprise courtesy of Continuum: Easy updating of repository information",
    "content": [
      "I discovered an unexpected feature that Continuum has. At one client site we are building around 30 different projects from CVS. Over the weekend the DNS name of the CVS server was changed as part of a corporate renaming process. I expected to have to go and update each projects collection of CVS\\Root files for the new repository, as well as update the CVS checkout information in Continuum.",
      "I started by updating one project in Continuum, and then when I went onto the filesystem I couldnt find the previously checked out project! My first thought was \"Oh nooooo, everything has blown up\". It turned out that Continuum was smart enough to pick up the changed SCM information, and to remove the currently checked out code and replace it with the new code!",
      "This is one of those features that I wouldnt have expected Continuum, especially at the 1.0.3 versions level of maturity, to support, and saved us a couple hours of linux work."
    ],
    "summary_t": ""
  },
  {
    "id": "c81c31e36b29f7cb2948464094f1aca1",
    "url_s": "https://opensourceconnections.com/blog/2013/09/08/osc-introduces-the-rimm-kaufman-group-to-cassandra/",
    "title": "OSC introduces the Rimm-Kaufman Group to Cassandra",
    "content": [
      "The Rimm-Kaufman Group (RKG) provides a wide range of Data-Driven online marketing solutions to online businesses. RKG clients range from startups to the Fortune 500, and of late business has been growing at an ever increasing rate. This is great! But this also means that ever increasing pressure is placed upon their current MySQL-based infrastructure. Since RKG anticipates their clientele to continue growing larger, RKG decided that now is the time to take action and upgrade their infrastructure. And their technology of choice: Cassandra.",
      "Cassandra is a distributed, columnar data store that is optimized for lightening fast writes and high performance reads. Cassandra is capable of scaling horizontally to hundreds of servers. And all data in Cassandra is replicated across servers so that a single or even multiple failures will result neither in loss of data nor service interruption.",
      "Over the course of our two month project, OpenSource Connections aided RKG in building and deploying at production-ready Cassandra cluster. Highlights of the work are as follows:",
      "Upon reviewing RKGs technical and business requirements, OpenSource Connections outlined the hardware specifications and defined the optimal cluster configuration. Based upon this information, new servers were purchased and assembled into the production Cassandra cluster.\n  Part of the challenge when moving from SQL to Cassandra is getting your head around the new, and somewhat foreign way that Cassandra expects you to model data. OpenSource Connections worked closely with RKG developers to port their current solutions to Cassandra.\n  Testing is almost as important as developing an appropriate data model. OpenSource Connections helped RKG design a Cassandra test framework that fit in well with their current MySQL testing framework.\n  Before leaving RKG, OpenSource Connections provide the RKG tech team with a full-day training course in Cassandra development and systems administration. As part of the training, the development team brainstormed solutions to actual in-house development objectives.",
      "By the time that OpenSource Connections left, RKG was up and running with their new 7-node Cassandra cluster – capable of storing terabytes of data and accepting more than 10K writes per second! But more important than this, RKG is now fully able to utilize and maintain their new Cassandra cluster on their own."
    ],
    "summary_t": ""
  },
  {
    "id": "b6acad0c12aa0811232610f8ea899a24",
    "url_s": "https://opensourceconnections.com/blog/2013/09/12/getting-started-with-cassandra-data-partitioning/",
    "title": "Getting Started with Cassandra: Data Partitioning",
    "content": [
      "Today well talk about how Cassandra partitions data.",
      "When data enters Cassandra, the partition key (row key) is hashed with a hashing algorithm, and the row is sent to its nodes by the value of the partition key hash.",
      "The Old Method",
      "To understand how data is distributed amongst the nodes in a cluster, its best to start with the old method of separating data. Each node was assigned to a range of values, so that the entire cluster covered the possible hashed values from 0 – 2^127-1.",
      "This proved problematic in a couple of ways:",
      "Adding and removing nodes meant that the entire scope of range assignments changed and needed to be reshuffled.\n  `Hot spots can occur across ranges of data",
      "",
      "Virtual Nodes",
      "Cassandra 1.2 introduced the option of virtual nodes. Now, instead of segmenting data into token ranges to assign to one node, the range of possible data is split up into many, many smaller tokens. Now, each real node gets many smaller tokens, not fewer large tokens.",
      "This minimizes the chances of accidental hotspots, and removes the need for reshuffling upon scale. New nodes are simply assigned a set of tokens from the extant token space, and other nodes re-shuffle smaller pieces to that new node.",
      "",
      "Enabling virtual nodes in your Cassandra cluster is simple – make sure you are running Cassandra > 1.2, and edit your cassandra.yaml:",
      "# This defines the number of tokens randomly assigned to this node on the ring\n# The more tokens, relative to other nodes, the larger the proportion of data\n# that this node will store. You probably want all nodes to have the same number\n# of tokens assuming they have equal hardware capability.\n#\nnum_tokens: 256\n\n# If blank, Cassandra will request a token bisecting the range of\n# the heaviest-loaded existing node.  If there is no load information\n# available, such as is the case with a new cluster, it will pick\n# a random token, which will lead to hot spots.\ninitial_token:",
      "Adjust num_tokens to reflect the capability of the given machine: Heartier machines are assigned more tokens, less-capable ones are given less of a load.",
      "Leave initial_token blank; this lets Cassandra choose the first token. Again, hot spots are not a problem, because virtual nodes minimizes each token space, which minimizes the chance of creating a hot spot.",
      "Next time well talk about the read and write paths of Cassandra."
    ],
    "summary_t": "Today we’ll talk about how Cassandra partitions data"
  },
  {
    "id": "6d1a5e6ecd06d5f12900334a78361f39",
    "url_s": "https://opensourceconnections.com/blog/2013/09/16/prototype-angular-uis-without-a-backend/",
    "title": "Prototype Angular UIs Without A Backend",
    "content": [
      "",
      "So youve got an AngularJS UI built out, but youll need a fleshed-out backend before being able to really take it for a test drive, right? Actually, it turns out, with the magic of Angular and its mocked $httpBackend, we dont need no stinking backend!",
      "If youve heard of $httpBackend, youve probably heard of it it terms of writing unit tests. Tests that look like:",
      "",
      "// tell http backend what to do when in gets a GET request at a specific URL\n$httpBackend.when(\"GET\", \"/users/4\").respond( {userName: \"Doug\", userId: 4} );\n// Code we’re testing\nuserService.getUserById(4);\n// flush all pending requests (pretend the server just got back with response)\n$httpBackend.flush();\nexpect(userService.getUser(4)).toBe( {userName: \"Doug\", userId: 4});",
      "While unit testing like this can help test and play with bits of code, it can’t drive an overall vision the way that seeing, clicking, and typing into a prototype can. Therefore, Ive taken $httpBackend and implemented a complete mock-up of my backend to allow me to use my UI and iterate quickly. Basically, I can now simply open file://path/to/my/project/index.html in my browser use my application as if it were backed by a database on a server. (That is as long as I dont fully reload the page:) ).",
      "How have I done this? Read along with the corresponding jsfiddle",
      "First, I tell Angular to use the angular-mocks $httpBackend (by passing it the $httpBackend constructor function) as a decorator on top of the concrete $httpBackend service. In Angular’s dependency injection, a decorator wraps the original service, layering on some custom functionality. In this case, Angular’s mock $httpBackend is setup to receive the concrete $httpBackend service, and will pass through to it if you use the passThrough() function after creating a rule (examples further down). In other words, I reserve the ability to pass some requests along as real HTTP requests.",
      "myApp\n.config(function($provide) {\n    $provide.decorator('$httpBackend', createHttpBackendMock);\n});",
      "All our code to mock the backend will take place in this function we execute before running the angular application:",
      "myApp.run(function($httpBackend, $timeout) {\n        … <the cool stuff you’ll see below>\n});",
      "In this run function, I can use the mock $httpBackend API to specify some rules, with actions to perform on the receipt of HTTP requests:",
      "// Some state\nvar users = {};\nvar userId = 0;\n$httpBackend.when('PUT', '/users')\n.respond(function(method, url, data) {\n  data = angular.fromJson(data);\n  users[userId] = {userName: data.userName, userId: userId};\n  userId++;\n  return [200, users[data.userId]];\n});",
      "I can then specify some rules based on a regex if need be:",
      "var regexpUrl = function(regexp) {\n  return {\n    test: function(url) {\n      this.matches = url.match(regexp);\n      return this.matches && this.matches.length > 0;\n    }\n  };\n};\n\n$httpBackend.when('GET', regexpUrl(/users\\/(\\d+)/))\n.respond(function(method, url, data) {\n  data = angular.fromJson(data);\n  return [200, users[data.userId]];\n});",
      "And since I’m using a delegate, I can decide that certain requests will just get passed through to do real requests (say JSONP requests to solr from my instant search directive!):",
      "$httpBackend.when('JSONP', regexpUrl(/http:\\/\\/.*/))\n  .passThrough();",
      "$httpBackends way of simulating an asynchronous response is for you to call flush() from your tests. So none of our apps requests will be responded to until we tell $httpBackend to flush. To create a kind-of $flush run-loop, we’ll use the angular $timeout service to call flush every half-second. $httpBackend still wants to be used in a unit testing context, so it will throw an exception if there’s nothing to flush. So we catch and discard that exception.",
      "var flushBackend = function() {\n  try {\n    $httpBackend.flush();\n  } catch (err) {\n  // ignore that there's nothing to flush\n  }\n  $timeout(flushBackend, 500 /*ms*/);    };    $timeout(flushBackend, 500);",
      "Viola, now I can experiment with my UI much more efficiently! Compared to other solutions, like mocking individual angular services or creating a mockable backend service that wraps the $http calls, I have found this solution the cleanest and least imposing on my production code.",
      "Have you had to solve a similar problem? Please comment and let me know! Also, do you need help with rich search & discovery oriented user interfaces? Let us know. We know quite a bit about building rich UIs for Solr!"
    ],
    "summary_t": "So youve got an AngularJS UI built out, but youll need a fleshed-out backend before being able to really take it for a test drive, right? Actually, it turns ..."
  },
  {
    "id": "7d4ac12ed44f53bcfc7cf75ecf1b41a7",
    "url_s": "https://opensourceconnections.com/blog/2013/09/16/solr-index-speed-on-ebs/",
    "title": "Solr Index Speed on EBS",
    "content": [
      "If youve got your Solr index on an Amazon EBS volume, save yourself some headache and do this every time you make a new volume:",
      "sudo nohup dd if=/dev/xvdi of=/dev/null &",
      "(Use your own volume in place of xvdi.)",
      "That just writes the whole volume to /dev/null. Seems kind of dumb on the face of it, but the Amazon docs on EBS performance say there is a 5% to 50% reduction in IOPS when you first access data on a volume. I dont know what magic happens in Amazons datacenter, but the solution is to read every block on the volume.",
      "Thats all you have to know. If you want the backstory, read on…",
      "We found this out the hard way when trying to pin down performance variances on new installations. Our thinking was that in order to take advantage of AutoScaling wed want our index baked into an AMI so that we can have added query capacity in about 10 minutes (thats about how long it takes to spin up an instance off of an AMI). If instead we opted for instance (ephemeral) storage, wed have to wait for replication, which takes about three hours with our current index.",
      "So this all worked well except when we went to test performance. The weird thing was, we got wildly different performance results every time we created a new stack! A while ago I saw a great ops presentation (I forget who) at LuceneRevolution that talked about preemptively cating the index to /dev/null to prime the OS disk cache. Those keywords helped me find that EBS performance page. After doing the above dd (I think it stands for \"disk duplicate\") our performance was much more predictable.",
      "It still takes quite a bit of time to read every block on our EBS volumes. That means new instances in our AutoScaling Group will have degraded performance for a while. One thing I might try later is to have multiple processes reading from various parts of the volume in parallel."
    ],
    "summary_t": ""
  },
  {
    "id": "a518c8957d3103042273eebbfbc09451",
    "url_s": "https://opensourceconnections.com/blog/2013/09/18/using-solrcloud-to-clone-your-search-index/",
    "title": "Using SolrCloud to clone your search index",
    "content": [
      "The search engine GPSN has fairly static dataset of Chinese patents that is delivered via roughly 5 TB worth of very large ZIP files. The data for each patent is split among various zip files for English text, Chinese text, and image files. And each has its own vagaries of how they are stored. We wanted to be able to query for files and find out what Zip archives they were stored in when debugging issues that have arisen in matching all the data files up. We ended up using Solr to save us configuring some sort of more traditional database since this was the only use case, though at times I wish I had something a bit more expressive to work with!",
      "Well today I finally got to grips with putting a script together that would go over all the stored data and check it for various error conditions that we have identified over the course of the project. Initially the data_audit index was both what I was querying for my information about where a patent might have gone, and updating the associated metadata. The commits are set up to every 15 seconds, and queries were easily taking 15 to 20 seconds to occur under some decent, but not silly, amounts of load due to the the constant commit activity, and the caches never having a chance to fill up.",
      "So I refactored my audit code to have two properties, a solrDataWriter, which would be my primarily Solr index, and then a solrDataReader, which would be a clone of my data_audit index.",
      "I didnt want to go through the leg work of stamping out lots of Solrs, but fortunantly my colleague @omnifroodle had been working on over the past two months some Amazon CloudFormation scripts for SolrCloud. While still raw, they are available at http://github.com/o19s/cfn-solr.",
      "My \"RAID\" of shards",
      "I ran the CloudFormation template and stood up 3 SolrCloud servers. I then put my data_audit conf directory into ZooKeeper and created a single shard with two replicas. I was thinking of it being like RAID levels, in my case I only wanted performance of reads, minimal work to setup, and am not worried about writes being propagated.",
      "Next was how to get the data over? Well, replication has always been one of my favorite tools. So I used curl and copied the data over:",
      "",
      "curl \"http://ec2-68-202-9-80.compute-1.amazonaws.com:8983/solr/data_audit_raid2_shard1_replica1/replication?command=fetchindex&masterUrl=http://ec2-23-21-214-112.compute-1.amazonaws.com/solr/data_audit\"",
      "Now, Replication is actually meant in SolrCloud -land to manage moving bulk amounts of data from the leader shard to the replicas when things get out of sync, or to update a new replica, but invoking it manually seemed to work as well. The GUI doesnt show all the details, so browse the replication handler directly to monitor the progress:",
      "http://ec2-68-202-9-80.compute-1.amazonaws.com:8983/solr/data_audit_raid2_shard1_replica1/replication?command=details",
      "I did hope that if I replicate to the leader shard, it would in turn replicate to all the follow shards, but no joy."
    ],
    "summary_t": ""
  },
  {
    "id": "5536c22e941d9a657fbd6883fc0887e1",
    "url_s": "https://opensourceconnections.com/blog/2013/09/24/using-solrj-with-basic-authentication-and-ssl-wrapped-solr/",
    "title": "Using SolrJ with BASIC authentication and SSL wrapped Solr",
    "content": [
      "Do you have BASIC authentication turned on for Solr? Are you using a self signed SSL Certificate? Do you want to index to this server using SolrJ? Ive taken a couple of tries to get all the magic incantations to work, but finally have done it. Hopefully this saves you some of the pain I experienced in searching around StackOverflow and random blog posts.",
      "Some of the error messages you might get are javax.net.ssl.SSLPeerUnverifiedException: peer not authenticated or unauthorized access.",
      "To sum it up, you have to fake out the certificate checking and then wrap your http call with something called Preemptive Authentication.",
      "Here is the method, you can get it here.",
      "To use this class just pass in your regular client and username/password combo:",
      "By the way, the equivalent of all of this in curl is:",
      "curl --user admin:password --insecure \"https://localhost:443/solr/update/?commit=true\""
    ],
    "summary_t": ""
  },
  {
    "id": "0240b767fd0c5db704d19f8831e41409",
    "url_s": "https://opensourceconnections.com/blog/2013/09/29/classifying-non-patent-literature-to-aid-in-prior-art-searches/",
    "title": "Classifying Non-Patent Literature to Aid in Prior Art Searches",
    "content": [
      "",
      "Before a patent can be granted, it must be proven beyond a reasonable doubt that the innovation outlined by the patent application is indeed novel. Similarly, when defending ones own intellectual property against a non-practicing entity (NPE – also known as a patent troll) one often attempts to prove that the patent held by the accuser is invalid by showing that relevant prior art already exists and that their patent is actual not that novel.",
      "Finding Prior Art",
      "So where does one get ahold of pertinent prior art? The most obvious place to look is in the text of earlier patents grants. If you can identify a set of reasonably related grants that covers the claims of the patent in question, then the patent may not be valid. In fact, if you are considering the validity of a patent application, then reviewing existing patents is certainly the first approach you should take. However, if youre using this route to identify prior art for a patent held by an NPE, then you may be fighting an uphill battle. Consider that a very bright patent examiner has already taken this approach, and after an in-depth examination process, having found no relevant prior art, the patent office granted the very patent that you seek to invalidate.",
      "But there is hope. For a patent to be granted, it must not only be novel among the roughly 10Million US Patents that currently exist, but it must also be novel among all published media prior to the application date – so called non-patent literature (NPL). This includes conference proceeding, academic articles, weblogs, or even YouTube videos. And if anyone – including the applicant themselves – publicly discloses information critical to their patents claims, then the patent may be rendered invalid. As a corollary, if you are looking to invalidate a patent, then looking for prior art in non-patent literature is a good idea! While tools are available to systematically search through patent grants, it is much more difficult to search through NPL. And if the patent in question truly is not novel, then evidence must surely exists – if only you knew where to look.",
      "Locating Prior Art in Non-Patent Literature via Automatic Classification",
      "Knowing where to look for prior art within NPL is the big challenge. However, the goal of this post is to put you on the right path by proposing a method for automatically classifying NPL as if it were a part of the corpus of patents. First a little background on patent classification: Patents are a very well curated set of documents. Each patent is tagged with one or more classifications from a very large, hierarchical set of patent classifications. The set of classifications is so large, in fact, that the specification text of classifications alone consumes more than 100MB of disk space. Large and cumbersome though it is, this classification system provides an order to the chaos and makes it much easier for patent examiners to find the documents they are looking for and to do their job.",
      "If NPL was tagged with just the same classifications as the actual patents, then it stands to reason that patent researchers would have a much easier time identifying pertinent prior art. Rather than having to shovel through all NPL, the research can at least narrow the corpus down to only those areas of art that are pertinent to the IP they are investigating.",
      "It turns out that automatically tagging NPL may not be so hard as you would expect. We already have a training set; the 10Million hand-tagged US patents serves this purpose well! So, taking a chapter out of Taming Text, is it fairly straightforward to create a classification engine using Solr and a programming language of your choice. As a matter of fact weve already created a post detailing exactly how to accomplish this.",
      "Naturally, building a full-fledged Non-Patent Literature research engine will take plenty of effort. Part of the building this particular classifier it to first index all available patents into Solr, and from our experience, this is a grand challenge in and of itself. Patent storage formats have changed significantly over the years. And it currently is not even feasible to index all documents, because patents prior to 1970 – if theyre digitized at all – are just images of the patent pages themselves. Indexing these documents would require a preliminary OCR process before there is even text to index. Once the classifier is built, you then need to set up a pipeline to get NPL documents to the classifier. This likely means pulling in document from even more disparate sources – to start with, peer reviewed publications. And finally, once the classifier is built and documents are being classified, getting things \"just right\" will take a significant amount of tuning. For instance, you may need to build a significant list of synonyms (though we have another post that might help there).",
      "The main point is not that we can make this easy, but rather, we can at least help make the process more manageable. Are you working through similar problems related to IP research? We would love to hear about it!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": "Before a patent can be granted, it must be proven beyond a reasonable doubt that the innovation outlined by the patent application is indeed novel. Similarly..."
  },
  {
    "id": "d7e11e0cf7267277bb6effc055ab5062",
    "url_s": "https://opensourceconnections.com/blog/2013/09/30/easy-k-nn-document-classification-with-solr-and-python/",
    "title": "Easy k-NN Document Classification with Solr and Python",
    "content": [
      "Youve got a problem: You have 1 buzzillion documents that must all be classified. Naturally, tagging them by hand is completely infeasible. However you are fortunate enough to have several thousand documents that have already been tagged. So why not…",
      "Build a k-Nearest Neighbors Classifier!",
      "The concept of a k-NN document classifier is actually quite simple. Basically, given a new document, find the k most similar documents within the tagged collection, retrieve the tags from those documents, and declare the input document to have the same tag as that which was most common among the similar documents. Now, taking a page from Taming Text (page 189 to be precise), do you know of any opensource products that are really good at similarity-based document retrieval? Thats right, Solr! Basically, given a new input document, all we have to do is scoop out the \"statistically interesting\" terms, submit a search composed of these terms, and count the tags that come back. And it even turns out that Solr takes care of identifying the \"statistically interesting\" terms. All we have to do is submit the document to the Solr MoreLikeThis handler. MoreLikeThis then scans through the document and extracts \"Goldilocks\" terms – those terms that are not too long, not too short, not too common, and not too rare… theyre all just right.",
      "Finding a Data Set and Prepping Solr",
      "For this little experiment we need a good example data set. Lets use the same Sci-Fi StackExchange dataset that weve used several times in the past. This is a great dataset; it contains roughly 18,000 question and answer documents that cover every corner of sci-fi imaginable. Its nerdy. I love it. And importantly for the task at hand, of the 18,000 documents, roughly 6,000 are questions which come with a Tags field. This is precisely what we need to build our Solr/Python-powered document classifier. To give you a little idea of what youll find in the dataset, consider document 8723 which we will use further down as our example:",
      "Title: Would Star Trek holodecks physically affect you once you exit the Holodeck?\n  Body: Would Star Trek holodecks physically affect you once you exit the Holodeck? Meaning, if someone programmed a holodeck to dump a bucket of water over you head, would you have wet hair (outside later on)?\n  Tags: [star-trek] [holodeck]",
      "… I warned you that this is deep nerdery!",
      "If you wish to follow along, pull down the Solr Sci-Fi git repo. In the README youll find the directions for indexing the documents. The only additional setup required is to enable the MoreLikeThis requestHandler within solrconfig.xml",
      "<requestHandler name=\"/mlt\" class=\"solr.MoreLikeThisHandler\">\n</requestHandler>",
      "Using Solrs MoreLikeThis Handler",
      "Now (after having restarted Solr) you can issue MoreLikeThis queries like this:",
      "http://localhost:8983/solr/mlt      #Use MoreLikeThis\n    ?q=Id:8723                      #Classify document 8723\n    &mlt.fl=Title Body              #Use these text fields for classification\n    &fl=Tags                        #Only display the Tags field\n    &rows=10                        #Get the 10 most similar documents\n    &fq=Tags:*                      #Only retrieve documents that have Tags\n    &mlt.interestingTerms=list      #Display statistically interesting words",
      "This query basically identifies which document we intend to classify, specifies which fields to use for classification, and displays all the tags for the 10 most similar documents. Heres what the results looks like:",
      "<response>\n<result name=\"match\" numFound=\"1\" start=\"0\">\n  <doc>\n    <str name=\"Tags\">star-trek holodeck</str></doc>\n</result>\n<result name=\"response\" numFound=\"840\" start=\"0\">\n  <doc>\n    <str name=\"Tags\">star-trek star-trek-tng holodeck</str></doc>\n  <doc>\n    <str name=\"Tags\">star-trek star-trek-tng holodeck</str></doc>\n  <doc>\n    <str name=\"Tags\">star-trek holodeck</str></doc>\n  <doc>\n    <str name=\"Tags\">star-trek</str></doc>\n  <doc>\n    <str name=\"Tags\">star-trek star-trek-tng technology holodeck</str></doc>\n  <doc>\n    <str name=\"Tags\">star-trek holodeck</str></doc>\n  <doc>\n    <str name=\"Tags\">star-trek holodeck</str></doc>\n  <doc>\n    <str name=\"Tags\">star-trek time-travel</str></doc>\n  <doc>\n    <str name=\"Tags\">harry-potter diagon-alley magical-transportation</str></doc>\n  <doc>\n    <str name=\"Tags\">star-trek technology weapon</str></doc>\n</result>\n<arr name=\"interestingTerms\">\n  <str>holodeck</str>\n  <str>exit</str>\n  <str>affect</str>\n  <str>physic</str>\n  <str>trek</str>\n  <str>star</str>\n</arr>",
      "The first section named \"match\" contains the Tags field of the document that were searching for. Obviously, in practice our documents arent going to go into the Tagger already pre-tagged (duh), but when building a simple classifier for the purpose of testing the concept, this is the easiest way to go. In production you would post text to the MoreLikeThis handler as a content stream as mentioned in the Solr wiki.",
      "The next section named \"response\" is the list of the Tags field of the 10 most similar documents. And just as wed hoped, the most common tag is [star-trek]; and the second most common tag is [holodeck]! Now obviously, this isnt doing any classification just yet. Thats what were going to use Python for soon.",
      "The final section lists all of the statistically interesting terms. Though not strictly necessary, this provides insights into how MoreLikeThis is working and how it may be tuned for better precision or recall. For more information on tuning MoreLikeThis, please again refer to the Solr wiki.",
      "Building and Demonstrating the Actual Classifier",
      "Now for the fun part! Having indexed the documents, and demonstrated the functionality of the MoreLikeThis handler, its conceptually straightforward to put it all together. While you can find all of the code required to create in my iPython notebook, the crux of it is here:",
      "def classifyDoc(self,docId,\n              method=\"best\" #or \"sorted\" or \"details\"\n            ):\n    #send the MLT query to Solr\n    params = {\"q\": self.idField + \":\" + docId,\n              \"mlt.fl\": self.mltFields,\n              \"fl\": self.tagField,\n              \"fq\": self.filterQuery,\n              \"rows\": self.numNearest,\n              \"wt\":\"json\"\n              }\n    resp = sess.get(url=self.mltUrl,params=params)\n\n    #Perform error checking\n    if resp.status_code != 200:\n        raise IOError(\"HTTP Status \" + str(resp.status_code))\n    json = resp.json()\n    if int(json[\"match\"][\"numFound\"]) == 0:\n        raise RuntimeError(\"no document with that id\")\n    if int(json[\"response\"][\"numFound\"]) == 0:\n        raise RuntimeError(\"no interesting terms in document\")\n\n    #If no errors, then collect and count tags for each similar document\n    tagDict = defaultdict(int)\n    for tagList in json[\"response\"][\"docs\"] :\n        for tag in tagList[self.tagField].split(' '):\n            tagDict[tag] += 1\n\n    #Return the best tag, all of the tags sorted best\n    #to worst, or the list of tags and their count\n    if method == \"best\":\n        return max(tagDict, key=tagDict.get)\n    elif method == \"sorted\":\n        return sorted(tagDict, key=lambda x : tagDict[x], reverse=True)\n    elif method == \"details\":\n        return tagDict",
      "Upon quickly reading though the code, youll see that theres really not much to it. It pulls down the same MoreLikeThis results as listed above, counts up the tags, and returns the most common tag.",
      "But the question remains… does it work? To test this, I first instantiated our SciFi classifier and checked it against our example document above",
      "print c.classifyDoc(\"8723\",method=\"sorted\")",
      "Sure enough, the first two items in the list are [star-trek] and [holodeck]. Great, now lets test this against a larger set. First, though, we need to clearly define what a correct classification entails so that we will understand how to interpret our results. Therefore, given a classifier tag and a list of true tags, we define a correct classification to have occurred whenever the classifier tag is contained with the true tags. (And referring to the iPython notebook, you can see the classifierTester function in which this is all defined.) Running our classifier over all Tagged documents we see the following print out:",
      "4041 out of 5805 correct. That's 69.6124031008%",
      "70%, not bad, especially when you consider that vagaries of some of these tags. And the relatively high possibility that some of these questions are themselves will be tagged wrong to begin with. To get an idea of how accurate the classifier can be, I performed several tests on more constrained document sets. I found that if we only look at the questions for more popular topics (Harry Potter, story identification, Star Trek, and Star Wars), then the accuracy jumps up considerably:",
      "2136 out of 2344 correct. That's 91.1262798635%",
      "Obviously, we cant know a priori that a given document is in this more popular subset, but this does bring up an important point: k-NN classification will be most accurate for documents that are representative of the training data set.",
      "Conclusions",
      "Easy wasnt it? But there are still several things that you can do to improve the classifier for your purposes. There is still further information available from the data that were not making use of! In particular, it shouldnt be difficult to extend the current classifier so that it not only classifies the document, but it also conveys information about the certainty of prediction. This would be especially useful in building a human-in-the-loop classifier – basically you allow the computer to classify all the ones that are \"easy\" and then allow a human to review all the document that the classifier is less certain about.",
      "Tell us what you think! Do you have a classification problem that youre trying to solve? What is it? Have you found this article helpful? We want to hear about it!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "755b19224df8f951353b5d0ec93d846c",
    "url_s": "https://opensourceconnections.com/blog/2013/10/05/search-aware-product-recommendation-in-solr/",
    "title": "Search-Aware Product Recommendation in Solr",
    "content": [
      "Building upon earlier work with semantic search, OpenSource Connections is excited to unveil exciting new possibilities with Solr-based product recommendation. With this technology, it is now possible to serve user-specific, search-aware product recommendations directly from Solr.",
      "In this post, we will review a simple Search-Aware Recommendation using an online grocery service as an example of e-commerce product recommendation. In this example I have built up a basic keyword search over the product catalog. Weve also added two fields to Solr: purchasedByTheseUsers and recommendToTheseUsers. Both fields contain lists of userIds. Recall that each document in the index corresponds to a product. Thus the purchasedByTheseUsers field literally lists all of the users who have purchased said product. The next field, recommendToTheseUsers, is the special sauce. This field lists all users who might want to purchase the corresponding product. We have extracted this field using a process called collaborative filtering, which is described in my previous post, Semantic Search With Solr And Python Numpy. With collaborative filtering, we make product recommendation by mathematically identifying similar users (based on products purchased) and then providing recommendations based upon the items that these users have purchased.",
      "Now that the background has been established, lets look at the results. Here we search for 3 different products using two different, randomly-selected users who we will refer to as Wendy and Dave. For each product: We first perform a raw search to gather a base understanding about how the search performs against user queries. We then search for the intersection of these search results and the products recommended to Wendy. Finally we also search for the intersection of these search results and the products recommended to Dave.",
      "Here are the results:",
      "Potatoes",
      "SOLR_URL: http://localhost:8983/recommendation?q=potatoes",
      "",
      "Canned yams, potato bread, and other potato derivative products. These are unusual results, because when one searches for potatoes… they just want potatoes.",
      "Wendys Potatoes",
      "SOLR_URL: http://localhost:8983/recommendation?q=potatoes&fq=recommendToTheseUsers:46f7a9a9-c661-4497-81a9-64ed537b65b0",
      "",
      "Much better; with the exception of one item, everything pictured here is a potato.",
      "Daves Potatoes",
      "SOLR_URL: http://localhost:8983/recommendation?q=potatoes&fq=recommendToTheseUsers:973990af-1482-4b63-a83c-bd2d8fc1ff32",
      "",
      "Daves apparently a convenience shopper – besides actual potatoes, we have soups, frozen french fries, chips – perfectly reasonable!",
      "Steak",
      "SOLR_URL: http://localhost:8983/recommendation?q=steak",
      "",
      "Plenty of steak here – though there are a few things that arent steak. Whats the deal with that first result? Taco mix?",
      "Wendys Steak",
      "SOLR_URL: http://localhost:8983/recommendation?q=steak&fq=recommendToTheseUsers:46f7a9a9-c661-4497-81a9-64ed537b65b0",
      "",
      "Uh-on, only one steak recommendation for Wendy. Does this mean that the recommendation system has failed her? Not really. She probably doesnt grill much. And not having bought related products in the past, the recommender lacks sufficient information to make any more than one recommendation. But, when Wendy finally does search for steak, we can at least be assured that a this steak will be at the top of her search results rather than the Taco seasoning from the raw search.",
      "Daves Steak",
      "SOLR_URL: http://localhost:8983/recommendation?q=steak&fq=recommendToTheseUsers:973990af-1482-4b63-a83c-bd2d8fc1ff32",
      "",
      "Dave, on the other hand, apparently loves to grill. The richness of these recommendations indicate that his spending habits are probably akin to those Tim the Tool Man Taylor.",
      "Bananas",
      "SOLR_URL: http://localhost:8983/recommendation?q=banana",
      "",
      "Those mushroom looking things are muffins that contain bananas. No wait… theyre actually muffins that dont contain bananas; rather theyre produced by a company that has \"banana\" in their name. As a matter of fact, the only bananas on this page are decorations surrounding the actual product. Hm… :-/",
      "Wendys Bananas",
      "SOLR_URL: http://localhost:8983/recommendation?q=banana&fq=recommendToTheseUsers:46f7a9a9-c661-4497-81a9-64ed537b65b0",
      "",
      "Yumm… bananas.",
      "Daves Bananas",
      "SOLR_URL: http://localhost:8983/recommendation?q=banana&fq=recommendToTheseUsers:973990af-1482-4b63-a83c-bd2d8fc1ff32",
      "",
      "Here we again see several different options for bananas. The other non-banana results arent necessarily bad results; remember that above, Dave appeared to be somewhat of a convenience shopper, and banana chips and banana bread are certainly consistent with that.",
      "Incorporating Solr Search-Aware Product Recommendations into Your Search",
      "For even a moderately large inventory and user base, building the recommendation data is a relatively straightforward process that can be performed on a single machine and completed in minutes. Incorporating these recommendations into search results is as simple as adding a parameter to the Solr search URL. In every case outlined above, the recommended subset is clearly better than the full set of search results. Whats more, these recommendations are actually personalized for every user on your site! Incorporating this functionality into search will allow customers to find the items they are looking for much more quickly than they would be able to otherwise.",
      "Want to Improve Your Conversion Rates?",
      "We are currently seeking alpha testers for Solr Search-Aware Product Recommendation. If you are interested in trying out this technology then please contact us. We will review your current search application, get you set up with Search-Aware Product Recommendation, and help you collect metrics so that you can measure the corresponding increase in conversion rate."
    ],
    "summary_t": ""
  },
  {
    "id": "f10f0b9c84b8840c0ddf743d729bb16d",
    "url_s": "https://opensourceconnections.com/blog/2013/10/07/quepid-give-your-search-queries-some-love/",
    "title": "Quepid – Stop shipping broken searches",
    "content": [
      "Let’s face it, returning good search results means making money. To this end, we’re often hired to tune search to ensure that search results are as close as possible to the intent of a user’s search query. Matching users intent to results, what we call \"relevancy\" is what gets us up in the morning. It’s what drives us to think hard about the dark mysteries of tuning Solr or machine-learning topics such as recommendation-based product search.",
      "While we can do amazing feats of wizardry to make individual improvements, it’s impossible with today’s tools to do much more than prove that one problem has been solved. Search engines rank results based on a single set of rules. This single set of rules is in charge of how all searches are ranked. It’s very likely that even as we solve one problem by modifying those rules, we create another problem – or dozens of them, perhaps far more devastating than the original problem we solved.",
      "For example, if we’re a clothing site. We’ve noticed that sales of lady’s dresses are fairly profitable compared to our other wares. The search developers are given a problem to solve – boost the results such that dresses come up higher in the search results. When a user types in summer dress we ought to make sure that instead of shorts, t-shirts, etc, we surface more flower dresses and the like in the search results. Knowing our customers, we’ve decided that \"dress\" means the literal garment, so we feel pretty confident in tasking our developers. Luckily our search developers are pretty smart folks, and they modify the search relevancy parameters giving us great results:",
      "Customers search for \"summer dress\", and gets these results.",
      "These changes are great. Everyone goes out for a beer to celebrate the amazing new sales in the dress department.",
      "Little did the search developers know that the single most popular garment sold by our online store was dress shoes. Nobody bothered to run that query until sales suddenly plummet. With customers leaving in droves, and the company on the verge of bankruptcy, the search developers now sit down to try to see what went wrong. Typing in the query dress shoes they are suddenly greeted with:",
      "Some irrelevant results have snuck into the \"dress shoes\" results",
      "The developers had no way of knowing, until changes were pushed to production, that solving the narrow problem of surfacing more dresses has caused a catastrophic drop in search quality for the sites most important queries.",
      "This unfortunately is the current state of search relevancy work. Much like software development, it’s easy to fix one simple bug and create two catastrophic ones. Search developers frequently chase their tails as users, merchandisers, and content experts stumble upon yet another broken search that yields confusing results that bear no relevance to what the user was hunting for. Sometimes, as in our parable of the dress shoes, this happens far too late. All that developer can do is play the search quality game of \"whack-a-mole\", trying to beat down every problem, until the complaints and disasters lower to a mollified din.",
      "What can possibly be done?",
      "One strategy used to catch these issues is to track statistics on production search quality. By tracking user’s clicks and queries, it’s fairly easy to identify queries that cause most users to frequently return to the search or give up without a sale. We can also, hopefully point out the successful queries that result in conversions and happy users.",
      "While this is a useful tool for detecting existing issues, it’s the equivalent of waiting for the user to find the crash in your software. Can you detect that queries on your most popular item, dress shoes are failing fast enough? Can you respond with a remedy fast enough? More importantly, how do you know that the rushed fix you provide doesn’t create an even deeper catastrophic failure of your search quality?",
      "Another heavily used strategy is to feed the query tracking data back to the search algorithm itself. Basically the thinking goes, so what if our search is only mildly competent. We can track successful results (those that resulted in satisfied users or customer sales) for individual queries and boost the successful results over the unsuccessful ones.",
      "This is a valid strategy, however there are several significant hurdles to overcome. First, you have to deal with subtle differences in queries: dress shoes vs dressy shoes will store different sets of quality metrics. Second, your search index now must expand dramatically to store all of this information. Storing every possible query a user typed to get to a document can become impractical, given the extreme variation in human languages.",
      "Most importantly, now you’ve created a new relevancy \"meta problem\". Should you prefer the matches inside the actual content? Or should you prefer the query tracking? (a blog post in its own right…)",
      "Both of these potential solutions require us to get to production before finding our problems. What can be done before rolling changes out to measure the impact of changes to search results? Well, if this was software, I’d be advocating generating a suite of automated tests that captured every problem solved in the past. When a new problem was to be solved, I would work intimately with this suite of tests to ensure old features didn’t break. I’d be a good steward and create tests for my new feature as I develop it.",
      "Why can’t we do this with search?",
      "Introducing Quepid",
      "This is exactly why we built Quepid.",
      "Quepid is our instant search quality testing product. Born out of our years of experience tuning search, Quepid has become our go to tool for relevancy problems. Built around the idea of Test Driven Relevancy, Quepid allows the search developer to collaborate with product and content experts to",
      "Identify, store, and execute important queries\n  Provide statistics/rankings that measure the quality of a search query\n  Tune search relevancy\n  Immediately visualize the impact of tuning on queries\n  Rinse & Repeat Instantly",
      "The result is a tool that empowers search developers to experiment with the impact of changes across the search experience and prove to their bosses that nothing broke. Confident in that data will prove or disprove their ideas instantly, developers are even freer experiment more than they might ever have before.",
      "Quepid provides a canvas for the whole team to explore search relevancy and its impact across dozens of search queries instantly. Testers, marketing, and content experts can provide immediate search quality feedback to developers, intimately impacting their technical decisions. By actively participating in the process, by adding queries and even ranking individual results, the feedback provided by these groups causes much quicker iterations.",
      "In our Test Driven Relevancy work on Silverchair Information Systems medical search (which I’ll be speaking about at Lucene Revolution), Quepid has been a game changer. Silverchair had gone through several cycles of deploying search results with relevancy issues. It wasn’t until we brought Quepid to the scene that we could tame the beast.",
      "Much like the marketing wing of an eCommerce site, Silverchair’s medical experts would shower the developers with links to broken searches on the production site. We would work diligently to solve those problems, only to get hit again by a search we broke. (We replied with \"We’re search consultants, not doctors Jim!\" but they didn’t seem to listen). Tired of one-shot solutions that took days or longer and seemed to create more problems than they solved, we finally sat down and loaded Quepid up with queries. Working together, we crafted a holistic relevancy strategy that addressed all the representative queries instead of fighting one-at-a-time. We continued iterating, measuring our improvements in a development environment. Once we released to production, we were able to feel relatively confident no lurking dress shoes issues existed in our relevancy parameters.",
      "In Silverchair’s Director of Semantic Technology, Rena Morse’s, own words:",
      "Quepid has been a game-changer for us in the arena of search relevancy testing. With Quepid, the team can see the impact of planned search tuning changes immediately instead of waiting until changes are made live. Quepid’s search version comparison capability also allows us to understand how potential downstream results may be affected by each change, so we can select the version that’s best for the product. Weve solved and avoided numerous customer issues, and Ive been able to feel more confident that my feedback on search quality directly funnels into our search relevancy algorithm. Thanks, Quepid!",
      "After numerous client success stories, like Silverchair’s, we’re extremely happy to be offering Quepid directly as a product to help other search teams execute. So do you wish you had more say in your site’s search results? Do you wish you had better tools to test search quality? Let us know if you’d like to try Quepid today. We’d love to have a chat about how you can try this exciting product.",
      "So contact us! to setup a time to get a demo and speak about how Quepid and OpenSource Connections can help with your relevancy problems! Come by Lucene Revolution to see me and hear Silverchairs Rena Morse and I discuss how Quepid has fundamentally impacted how Silverchair has addressed their tough search problems."
    ],
    "summary_t": ""
  },
  {
    "id": "d6f27f9aaa32097d6f4e3622bc923e9a",
    "url_s": "https://opensourceconnections.com/blog/2007/03/21/comparing-datetime-and-date/",
    "title": "Comparing DateTime and Date",
    "content": [
      "For my project Novo, Ive been working on a script to sync my Apple iCal to Googles gCal. Think of it as a poor mans version of SpanningSync.",
      "One of our challenges is that sometimes we are dealing with Ruby DateTime objects, and other times we are dealing with Date objects. And of course, you cant compare the two to each other (argh!).",
      "So initially I would convert them to strings, and then create Time objects and then compare them. And if they were in UTC format, I had a hardcoded value that swapped in my offset from UTC. Of course, with the early arrival (ending???) of Daylight Savings Time change in the US, this caused one week all my appointments to be synced into gCal one hour off!",
      "So I took advantage of chatting with Chris Hapgood about his challenges with dates and timezones and implemented TZInfo.",
      "But, much to my disgust, when I hand in a DateTime to TZInfo, I get back a proper UTC formatted DateTime. And when I hand in a Time to TZInfo, I get back a proper UTC formatted Time object! So, I am still stuck converting to String, and then reparsing into Time for comparisions.",
      "Ive looked around, and the only thing Ive found was from a site that has Ruby Cookbook tips, but it just seems like I should be able to more simply parse/compare the two! Especially since I dont know if I am dealing with DateTimes or Times, and so have to put in some funky logic to figure it out and do the right parsing…."
    ],
    "summary_t": ""
  },
  {
    "id": "ebee6ace5c018f26f98fbdaf364dd6f4",
    "url_s": "https://opensourceconnections.com/blog/2013/10/14/detecting-reddit-voting-rings-using-hyperloglog-counters/",
    "title": "Detecting Reddit Voting Rings Using This Weird Little Data Trick",
    "content": [
      "A wolf in sheeps clothing",
      "Wow, so apparently the Reddit community has outed (and as of 15 minutes ago, ousted) colleague Doug Turnbull as the vile spammer that he is! Apparently he has been shamelessly promoting his own blog posts! And among his tactics, the use of voting rings. Yes, and even some of my own colleagues succumbed to his pressure to upvote his sales pitches – including this one masked as a post about test driven search relevancy. (Though I must admit… it is an interesting read… and Doug is an eloquent writer)",
      "But not I. No, Im as white as the wind driven snow, as pure as a mountain stream, as innocent as a newborn babe. And as a matter of fact, this has all got me thinking: How can I help prevent things like this from happening in the future? In particular, how can I help Reddit prevent voting rings from forming in the future? And then it occurred to me: the HyperLogLog counter.",
      "What is the HyperLogLog counter?",
      "The HyperLogLog counter is a really neat probabilistic data structure that estimates the count of unique elements in a list. For instance, lets say you have a really popular web page and you want to perfectly count the number of unique visitors. To keep a count of uniques, you have to store every IP address that you ever see. And upon receiving a new IP address, you have to first check that the new IP address has not been run across before, and only then do you increment the site counter. Under the best of situations, the storage and the computation probably scale as O(log(n)). (What would you use? Tries? Skip-lists?)",
      "With the HyperLogLog counter its all different. The size of the data structure is determined a priori (thus O(1)) and is a miniscule fraction of whatever data structure you would use from in the example of the previous paragraph. Data insertion and count tallying are super fast and also scales as O(1). But heres the catch, you cant actually know the exact count of unique views. Rather, you may only have an estimate of the current count. I encourage you to read more. This article has been the best resource Ive found. And make sure you try their cool JavaScript demo which really helps to understand how it all works.",
      "But how can the HyperLogLog counter help beat voting rings??",
      "Lets turn back to the sad example of disgraced colleague Doug Turnbull. Usually when Doug coerces others to upvote his posts he uses forms of extortion or even threats of physical harm. Naturally Ive never succumbed to this, but those that do go to Dougs most recent post (many of which can be found here) and upvote.",
      "But consider what would happen if we created a HyperLogLog counter for every user on Reddit, and any time that a user receives an upvote, we update the corresponding HyperLogLog counter with the id of the user who did the upvoting. Given this setup, heres how we detect the voting ring. First, for a given user, retrieve the estimated count of unique upvotes from that users HyperLogLog counter and lets label this quantity as estimated_unique_upvotes. Next, tally up the total number of actual upvotes…",
      "(these things)",
      "across all of that users posts. Lets call this quantity as global_total_upvotes. The probability that a given user is involved in a voting ring will correlate highly with the ratio of these numbers. In other words we can define",
      "voting_honesty  =  estimated_unique_upvotes / global_total_upvotes",
      "Think about it. If a user is not in a voting ring, then their upvotes are going to be organically generated from anyone one the world-wide-web that thinks their links are interesting. Because the internet is a big place, this users estimated_unique_upvotes and global_total_upvotes will tend toward the same number and their voting_honesty \"probability\" will tend toward 1. On the other hand, a user wrapped up in seedy underworld of Reddit voting rings will have many more global_total_upvotes across all posts than they have estimated_unique_upvotes. For this user, the voting_honesty will tend toward 0.",
      "Conclusion",
      "Now were talking 1s and 0s. Arent computers supposed to be good with those? Yes! So the method Ive outlined above should be a scalable way to automatically detect and expel those voting charlatans! Are there issues with this strategy? Sure. But I do think that the estimated_unique_upvotes to global_total_upvotes ratio is interesting. Help me think; where could this be used outside of Reddit voting rings?",
      "So what now? Well its obvious, right? Upvote this article!",
      "Update Idea: Throwing Away 90% of Reddits Infrastructure and Using HLL Approximations Instead.",
      "Hey, I got another idea! I wonder what reddits infrastructure looks like? Its gotta be hell to scale everything up. The most obviously complex thing is tracking all those up and down votes. Furthermore, the most important function of the infrastructure is in making sure that no one votes for a post more than once – otherwise colleague, Doug Turnbull, would cramp to death setting at the computer and click-upvoting his own posts! Since its so hard to scale, why not just rip all that stuff out and replace it with HLL approximations instead.",
      "Heres how: Each post gets 2 HLL counters, one positive votes and one negative. When a user upvotes or downvotes a post, their id is submitted to the corresponding HLL counter. There is never a chance that a vote can be counted twice, and theres no need for any of the infrastructure that ensures that no one votes twice.",
      "But there are a couple of obvious drawbacks to this approach. One – the vote is now just an estimate – a guess, and while one average this would work out fine, there would certainly be crappy posts getting promoted and great posts getting penalized. Secondly, a side effect of all this click tracking infrastructure is that you lose the ability to trace your own history! And if you again built infrastructure so that you could see your history, you might as well return back to the way Reddit currently does it. But at the very least this idea is worth remembering when building and scaling a website that has, but doesnt feature, votes.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "31f4e5e727a02cdd0a98952974864cbf",
    "url_s": "https://opensourceconnections.com/blog/2013/10/14/osc-has-7-speaking-engagements-the-fall-and-counting/",
    "title": "OSC has 9 speaking engagements the Fall (and counting)",
    "content": [
      "Just in case youre interested, Id like to add 6ish more speaking engagements to the list of speakers were taking to Dublin this November.",
      "Database History from Codd to Brewer and Beyond",
      "Doug Turnbull, NoSQL Matters Barcelona",
      "There are innumerable technical lessons to learn from database history. Its easy to go with what’s new and trendy. Its harder to appreciate technical reasons why one approach suddenly became more favored than another. History highlights the limitations and power behind database solutions. If we don’t learn from history we are doomed to repeat it: – What were the first databases like (Codasyl, etc)? Why did they start out this way? – Why was RDMS the right technical response to the non RDMS databases back in the day? – Why was the move away from RDMS to NoSQL the right technical solution for many problems today? A great introductory to the basic technical scaffolding and historic context for NoSQL, from this talk, you’ll have a deeper appreciation of the transition from vertically scaling Big Metal to horizontally scaling Big Data.",
      "One Million Books: Adventures in Discoverability with Cassandra and Solr",
      "patricia-gorla, Cassandra Summit Europe",
      "For any venture, storing your data is just the first step in making sense of it. How do you make your system discoverable? How do you tune your relevancy to accommodate real-time updates? In this session, we explore pairing Cassandra with Solr using Datastax Enterprise Search, and look at different search paradigms to help your users find patterns in your data.",
      "An Introduction to Real-Time Analytics with Cassandra and Hadoop",
      "patricia-gorla, Strata+Hadoop World NYC",
      "Cassandra is a distributed storage system for managing lots of structured data over many commodity servers, while providing a highly-available service with no single point of failure.",
      "Put another way, Cassandra is a solution to scaling out relational databases to the terabyte scale.",
      "Cassandra’s append-only structure makes it a perfect HDFS alternative to perform large scale mapreduce analytics on real-time data.",
      "In this session, we will cover:",
      "Introduction to Cassandra\n  Setting up a Cassandra Cluster with MapReduce\n  Pros and Cons of using Cassandra\n  Failure Mitigation: How to recover lost nodes\n  Performance Tuning",
      "At the end of the tutorial, participants will have set up a multi-node Cassandra/Hadoop cluster, indexed data into the cluster at high volumes, and run analyses against the cluster.",
      "Getting Started with Lucene and Solr",
      "John Berryman, All Things Open",
      "From intra-website search to full-on e-commerce applications, full-text search is ubiquitous on the web. And in the domain of search, Solr is arguably the most widely deployed search engine in the world. In this fast-paced session I will define the problem of full-text search and then introduce Lucene, the general-purpose software library upon which Solr is built. I will then demonstrate how Lucene may be used to build a basic search engine. After this, I will introduce Solr search engine. Solr can be though of as the best practices implementation of a Lucene search index wrapped inside a web server. I will present a wide variety of capabilities that are available with Solr right out of the box and I will demonstrated how easily Solr may be configured to meet various search needs. Attendees will leave with an understanding of the basic principles required to develop and deploy a full-text search engine to meet their own specific search needs.",
      "Understanding How CQL3 Maps to Cassandras Internal Data Structure",
      "John Berryman, Planet Cassandra",
      "CQL3 is the newly ordained, canonical, and best-practices means of interacting with Cassandra. Indeed, the Apache Cassandra documentation itself declares that the Thrift API as \"legacy\" and recommends that CQL3 be used instead. But I’ve heard several people express their concern over the added layer of abstraction. There seems to be an uncertainty about what’s really happening inside of Cassandra.",
      "In this presentation we will open up the hood and take a look at exactly how Cassandra is treating CQL3 queries. Our first stop will be the Cassandra data structure itself. We will briefly review the concepts of keyspaces, columnfamilies, rows, and columns. And we will explain where this data structure excels and where it does not. Composite rowkeys and columnnames are heavily used with CQL3, so well cover their functionality as well.",
      "We will then turn to CQL3. I will demonstrate the basic CQL syntax and show how it maps to the underlying data structure. We will see that CQL actually serves as a sort of best practices interface to the internal Cassandra data structure. We will take this point further by demonstrating CQL3 collections (set, list, and map) and showing how they are really just a creative use of this same internal data structure.",
      "Semantic Search and Recommendation with Solr Search Engine",
      "John Berryman, Semantic Technology & Business Conference",
      "Built upon Lucene, Solr provides fast, highly scalable, and easily maintainable full-text search capabilities. However, under the hood, Solr is really just a sophisticated token-matching engine. Whats missing? First, Solr lacks semantic search. If youre looking for documents about \"software architecture\" then it might be appropriate to retrieve documents about \"programming design patterns\" even if they dont explicitly contain the terms \"software\" or \"architecture\". Semantic search allows users to find documents by meaning rather than just by simple token matching. Second, Solr lacks the ability to make rich recommendations. If most customers who purchase cameras also tend to purchase tripods, then it is a good idea to recommend tripods when a new customer is purchasing a camera! So-called \"recommenders\" provide users with recommendations based upon the previous behavior of other similar users. In this fast-paced discussion I will describe the nature of token-based search and outline the need for semantic search and recommendation. Then, I will provide an audience-friendly, mathematical demonstration of how token-based search can be augmented so that both semantic search and recommendation are possible. Finally I will also demonstrate our ongoing work using Mahout to equip Solr with both of these useful capabilities.",
      "… and then of course theres the Solr training",
      "Scott Stults and Matt Overstreet"
    ],
    "summary_t": ""
  },
  {
    "id": "ff75122cbb01c98d3bdca184c4c9251b",
    "url_s": "https://opensourceconnections.com/blog/2013/10/14/osc-to-have-three-speakers-at-lucene-revolution/",
    "title": "OSC To Have Three speakers at Lucene Revolution!",
    "content": [
      "If youd like to catch up with us and youre in Europe, theres few better places to hear what were all about than at one of our talks at Lucene Revolution! Were happy to be presenting three talks that really highlight the kinds of skills, tools, and philosophy we bring to search applications:",
      "Test Driven Relevancy – How to Work with Content Experts to Optimize and Maintain Search Relevancy",
      "Presented by Doug Turnbull, Search & Big Data Architect, OpenSource Connections Rena Morse, Director of Semantic Technology at Silverchair Information Systems",
      "Getting good search results is hard; maintaining good relevancy is even harder. Fixing one problem can easily create many others. Without good tools to measure the impact of relevancy changes, theres no way to know if the \"fix\" that youve developed will cause relevancy problems with other queries. Ideally, much like we have unit tests for code to detect when bugs are introduced, we would like to create ways to measure changes in relevancy. This is exactly what weve done at OpenSource Connections. Weve developed a series of tools and practices that allow us to work with content experts to define metrics for search quality. Once defined, we can instantly measure the impact of modifying our relevancy strategy, allowing us to iterate quickly on very difficult relevancy problems. Get an in depth look at the tools we utilize when we not only need to solve a relevancy problem, we need to make sure it stays solved over the products life.",
      "This blog post will give you a nice preview – in this talk well be highlighting our relevancy testing and collaboration product Quepid.",
      "Building Client-side Search Applications with Solr",
      "Presented by Daniel Beach, Search Application Developer, OpenSource Connections",
      "Solr is a powerful search engine, but creating a custom user interface can be daunting. In this fast paced session I will present an overview of how to implement a client-side search application using Solr. Using open-source frameworks like SpyGlass (to be released in September) can be a powerful way to jumpstart your development by giving you out-of-the box results views with support for faceting, autocomplete, and detail views. During this talk I will also demonstrate how we have built and deployed lightweight applications that are able to be performant under large user loads, with minimal server resources.",
      "Implementing a Custom Search Syntax using Solr, Lucene, and Parboiled",
      "Presented by John Berryman Data Architect, OpenSource Connections",
      "In a recent project with the United States Patent and Trademark Office, Opensource Connections was asked to prototype the next generation of patent search – using Solr and Lucene. An important aspect of this project was the implementation of BRS, a specialized search syntax used by patent examiners during the examination process. In this fast paced session we will relate our experiences and describe how we used a combination of Parboiled (a Parser Expression Grammar [PEG] parser), Lucene Queries and SpanQueries, and an extension of Solrs QParserPlugin to build BRS search functionality in Solr. First we will characterize the patent search problem and then define the BRS syntax itself. We will then introduce the Parboiled parser and discuss various considerations that one must make when designing a syntax parser. Following this we will describe the methodology used to implement the search functionality in Lucene/Solr. Finally, we will include an overview our syntactic and semantic testing strategies. The audience will leave this session with an understanding of how Solr, Lucene, and Parboiled may be used to implement their own custom search parser.",
      "I hope youll come by and visit us in Dublin!"
    ],
    "summary_t": ""
  },
  {
    "id": "117d7ff42d375ecb04dde182875f486e",
    "url_s": "https://opensourceconnections.com/blog/2013/10/14/what-is-test-driven-search-relevancy/",
    "title": "What is Test Driven Search Relevancy?",
    "content": [
      "During a challenging search project, we realized that we were simply trying to accomplish too much. The demands to create relevant search results were intense. The deadline looming. Presented with dozens of unique search bugs, we provided one quick solution after another.",
      "What we found was that we quickly forgot why or what bug one piece of the relevancy formula had fixed. Why did we add this to the stopwords list? What made us think this boost was appropriate? Why did we add this field to the query fields?",
      "As we bandaided one solution on top of another, we realized we were creating a disaster. The search equivalent of a ball of mud. With the deadline looming, tension between the search developers and content experts was growing. Communication was breaking down. Even with a deadline looming, we knew we had to try another path.",
      "We needed to solve two problems. First we needed a way to remember and enforce all the ways the search needed to behave. Second, we needed to fix the collaboration problem. It was becoming too hard to communicate productively around broken search.",
      "As an engineer, the solution to the first problem was clear: automated testing. As a human being, the solution to the second problem was also clear: better collaboration processes and tools.",
      "I knew from extensive experience, that combining these two solutions– collaborating closely with others on creating automated tests– turned a potentially confrontational situation to a collaborative one. Instead of two people asserting that this way or that way was the One True Way(™), having them sit together and write tests allowed them to have a conversation on how a system should behave. Writing tests for the various corner cases allows the developers to turn to a tester or product owner and ask \"Well what do you think it should do!?\". Similarly, for marketing and content experts, helping the developer write automated tests for the nitty-gritty details suddenly creates a sense of being able to have a very in-depth say over what the system should do.",
      "So I applied those lessons to search and we built our testing & collaboration product, Quepid. Initially, Quepid had a few simple but effective features:",
      "Execute multiple search queries simultaneously, present all results in a single page\n  Rate the quality of results for those search queries\n  Provide an edit box to edit the relevancy params passed to Solr",
      "Marketing expert rates the quality of this juice for the \"apple juice\" query, giving developers instant feedback on search quality.",
      "The result was a simple tool that completely altered how we dealt with search relevancy problems at OpenSource Connections. With the ability to see the relative quality of several queries simultaneously allowed the whole team to work more efficiently:",
      "Developers had a workbench to experiment with relevancy parameters\n  Sales, testers, and content experts could add broken queries, and rate the results as poor, and provide meaningful feedback on their search",
      "With these ingredients, we had the ability to achieve a level of automated test coverage over dozens of representative queries. Testers and content experts could know before pushing changes to production whether or not the team was making measurably improvements. Developers could see right away the impact fixing new problems has on existing representative queries.",
      "More importantly, all search stakeholders could sit together and troubleshoot together rather than shoot emails at each other and argue. Developers could point out rather directly the challenges related to search. Marketing, content experts and testers could more readily communicate why the search ranking was broken.",
      "It was a pretty powerful experience that reinforced to me that automated testing is more than test coverage. Its an organizational dynamic that uses a meaningful artifact (tests) for communication. In other words, talk is cheap, tests matter.",
      "Can our products or services help your organization work more efficiently to improve relevancy? Do you struggle organizationally with how to communicate and fix broken searches? Would you be interested in the product that made this all possible. Contact us and let us know!"
    ],
    "summary_t": ""
  },
  {
    "id": "dfaa23b23d9e94993aaf34734c10d515",
    "url_s": "https://opensourceconnections.com/blog/2013/10/21/search-quality-is-about-effective-collaboration/",
    "title": "Search Quality is about effective Collaboration*",
    "content": [
      "*and Effective Collaboration is about Testing",
      "Sales expert not happy about the quality of her search. Shed like a better way to collaborate than yelling at the search devs.",
      "When we start on a search project, we never have a spec that tells us what good search is. The best we have is the expert opinion of marketing, content curators and the (sometimes forceful) opinion of users and suppliers. With such a diverse group of stakeholders involved, there’s plenty of opportunity for negative communication around search quality. Stakeholders fight over what users really intend with their queries–then fight again with developers over how to mold search in that direction.",
      "It makes sense. People do and should take the ordering of search results very seriously. It can mean real money for an eCommerce company. It can mean a doctor finding the right research to help her patients. When a non-technical sales expert or content curator can’t quite get the message across to search developers, disastrous search is bound to happen. Unfortunately these groups simply use different languages around search quality. Sales talks in conversions, sales, and profit. Developers talk in boosts, scoring, query parsing, and measuring a users intent.",
      "Given that there’s this distance between the techies and non-techies, how can one possibly keep all the stakeholders on the same page productively moving forward without more meetings and fights?",
      "Talk is cheap; Tests Matter",
      "The best solution we’ve found is test collaboration – having stakeholders work together creating automated tests for search quality. In other words: Talk is cheap, tests matter.",
      "Using automated tests to collaborate across disciplines is not new to software developers. Yet with search quality this practice is shockingly rare. I suppose this is because search is so fuzzy, heuristic, and psuedo-magical in a way that software development often is not. So at first automated tests would seem silly.",
      "In fact, collaborating on tests is even more fundamental to a successful search implementation than a normal software project. First, developers dont do a great job of measuring good vs bad search. Search correctness – what results should be returned for a given query – is best measured by marketing and content experts. In an eCommerce store, this means knowing your customers. It means measuring what is profitable, measuring convergence. It means knowing all the things that developers dont think about. Without non-technical domain experts helping developers measure search quality, no progress can truly be made on search quality. Without the testing being verified and blessed by these experts, search developers have little to develop against.",
      "From a technical point-of-view, collaborating on search testing is absolutely essential. Search quality for all searches is governed by a tiny set of heuristic rules crafted by developers. The changes your developers make to this algorithm are very global. Instead of thousands of lines of codes, search developers work using a handful of config files and search parameters. Modifications to search parameters almost never \"just impact this one case\" like a change to software might. Therefore fixing a broken search is almost certainly going to impact working queries. Thus getting everyone on the same page on the trade-offs of \"if you fix this query, this other query will change by X%\" is even more important. Without this testing, its pretty easy to inadvertently have a dress shoe moment where search tweaking accidentally breaks your highest grossing search, destroying your revenue.",
      "An example of this can be seen in our work at Silverchair Information Systems. While implementing search for medical research journals, we were tasked with weighting matches across three main variables:",
      "Expert curated medical tags attached to the document. IE [breast cancer] or [chemotherapy]\n  The text of the document\n  The recency of the research",
      "Giving too much weight to one of these variables could mean very poor results for crucial queries. Weigh the recency of the research too heavily, for example, and suddenly recent research trumps relevant research. If a user searches for \"breast cancer\", the highest ranking result might be yesterdays research article on toe fungus.",
      "Without tests around our important queries, our team would have no idea how catastrophic heavily boosting recency could be on search relevancy. In our example, they might not speak the right medical jargon to know this article on toe fungus has nothing to do with breast cancer. With a good test suite, the developers can incorporate the expertise of domain experts to instantly measure the impact of our weighting changes to both the queries that already work and the ones that are broken.",
      "Aside these kinds of catastrophic failures, weighting search a smidgen too far in one direction or another can nudge working queries out of whack. For example, a research article might never directly mention chemotherapy, instead perhaps its a study on a specific chemotherapy drug. Theres little mention of the term \"chemotherapy\" directly in the document. Nevertheless, our curators have expertly tagged this article with the [chemotherapy] tag so that user searches for chemotherapy pick up the article.",
      "Collaborative testing is crucial here. Without medical experts reporting in on what are good, mediocre, or abysmal results for a doctors search for \"chemotherapy\", search developers have no way to test the impact of their changes. Without collaboration around a single testing tool, developers dont get the information they need from non technical experts and experts dont have a way to get developers to really listen.",
      "Search Collaboration Needs Better Processes and Tools",
      "It’s shocking to us that until now, nothing really existed for building collaborative tests around search relevancy. That’s why we built Quepid around the idea of \"Test Driven Relevancy\". We built it to help marketing and content experts meaningfully provide their expert opinion and report broken searches. We built it to allow everyone to understand how search quality changed when developers altered relevancy parameters.",
      "Marketing expert rates the quality of this juice for the \"apple juice\" query, giving developers instant feedback on search quality.",
      "In the same way sitting together to talk about automated software tests lets everyone communicate about correct behavior, Test Driven Relevancy advocates getting everyone on the same page with search quality. Each \"test\" is a user query representing an important or representative search to be tested. Non technical folks (ie marketing) use a tool to manually or automatically rate the quality of individual results for each query. Developers use this data to improve the overall site search quality. Everyone can see in one glance how well the search is doing.",
      "Test Driven Relevancy using Quepid has helped us turn a number of difficult search stakeholder relationships into productive ones. It incorporates the feedback of all the stakeholders in a single glance. We’ve become accustomed to pairing with non technical content experts and letting them watch us tweak their search in real time. Simultaneously, the content experts can throw more curveballs – adding new queries that represent edge cases or long tail searches. Optimizing for the data they’ve put into our system, we can prove to the content experts in the moment that we are making progress on broken searches plus holding ground on working important searches. Its opened a whole new world for our relevancy work, letting us try whatever it takes to get search relevant to users.",
      "In short, I won’t be leaving home without Quepid on my next search relevancy engagement! Are you interested in trying out Quepid for your organizations search problems? Do you have tough search relevancy problems and would like help? Contact us! We’d love to talk to you about how we can help."
    ],
    "summary_t": "*and Effective Collaboration is about Testing When we start on a search project, we never have a spec that tells us what good search is. The best we have is ..."
  },
  {
    "id": "097bd01fc9f6fc16653481abf7ae5bca",
    "url_s": "https://opensourceconnections.com/blog/2013/10/27/why-is-multi-term-synonyms-so-hard-in-solr/",
    "title": "Why is Multi-term synonym mapping so hard in Solr?",
    "content": [
      "There is a very common need for multi-term synonyms. Weve actually run across several use cases among our recent clients. Consider the following examples:",
      "Ecommerce: If a customer searches for \"weed whacker\", but the more canonical name is \"string trimmer\", then you need synonyms, otherwise youre going to lose a sale.\n  Law: Consider a layperson attempting to find a section of legal code pertaining to their \"truck\". If the law only talks about \"motor vehicles\", then, without synonyms, this individual will go away uninformed.\n  Medicine: When a doctor is looking up recent publications on \"heart attack\", synonyms make sure that he also finds documents that happen to only mention \"myocardial infarction\".",
      "One would hope that working with synonyms should be as simple as tossing a set of synonyms into the synonyms.txt file and just having Solr \"do the right thing.\"™ And when were talking about simple, single-term synonyms (e.g. TV = televisions), synonyms really are just that straight forward. Unfortunately, especially as you get into more complex uses of synonyms, such as multi-term synonyms, there are several gotchas. Sometimes, there are workarounds. And sometimes, for now at least, youll just have to make do what you can currently achieve using Solr! In this post well provide a quick intro to synonyms in Solr, well walk through some of the pain points, and then well propose possible resolutions.",
      "Solr Synonyms – Back to Basics",
      "In principle, synonym mapping is not that complex. Solrs SynonymFilter searches through a stream of tokens and compares the contents to what it finds in the synonyms.txt file. The contents of the synonyms.txt file contains a series of line delimited entries that look something like this:",
      "spiderman, spider man\ntelevision => TV",
      "The SynonymFilter takes an \"expand\" parameter that can be set to either true or false. If set to false, then \"spider man television\" will be converted to",
      "| TOKEN 1 | TOKEN 2   | TOKEN 3 |\n+---------+-----------+---------+\n|         | spiderman | TV      |",
      "Notice that the first token is blank. This is because spiderman is a single token and spider man was two. In this way, Solr preserves the position information should it be useful for something like a phrase query. If you set expand to true, then the results are a little different. \"spider man television\" becomes",
      "| TOKEN 1 | TOKEN 2   | TOKEN 3 |\n+---------+-----------+---------+\n| spider  | man       |         |\n|         | spiderman | TV      |",
      "Theres a few things to note here. One, is that because we expanded the synonyms, we now have both spiderman and spider man. Also note that TV did not get expanded to both TV and television; this is because the \"=>\" notation in the synonyms.txt file overrides the \"expand\" parameter. Finally, you might be surprised to see that Solr can actually have several tokens in the same position. In this case, man and spiderman are both in token position 2.",
      "The final aspect is whether to do the synonym mapping at index time or at query time or both. The benefit of performing the mapping only at query time is that index-time synonyms expansion will bloat your index considerably. Furthermore, if you are using index-time synonym mapping, then upon any change to the synonym file, you must reindex everything! On the other hand, index-time synonym mapping will allow your term document frequencies to be more accurate, and some of the gotchas Ill present in a moment make index-time synonyms actually start to look more appealing. There are also plausibly times when it makes sense to use synonym mapping both at query and index time, for instance when doing complicated things with hyponyms and hypernyms (which are basically synonyms, but more specific or less specific, respectively – canine -> dog -> poodle ). But these details are beyond the scope of this post.",
      "Why are Multi-Term Synonyms so Hard?",
      "Basically, single-term synonyms work just about exactly as you would hope they work. But you start running into problems instantly when dealing with multi-word tokens. Lets say that your synonyms.txt file looks like this:",
      "spider man => spiderman",
      "Further lets say that you have documents in your index with the text:",
      "the adventures of spiderman\n  what hath man wrought?\n  spiders eat insects",
      "And lets say that synonym mapping only happens at query time. When a user searches for spider man, which documents do you think will match? The obvious and desired answer is that document 1 and only document 1 should match, right? Instead, very non-intuitively, document 2 and 3 match, but document 1 does not! Why!?",
      "Unfortunately, the Solrs query parser splits queries on whitespace before passing the terms to analysis.  This means that in the search for",
      "spider man, the term spider and the term man go through analysis independently. And since the SynonymFilter never sees \"spider man\", the rule \"spider man => spiderman\" is never matched. (For more information on this issue, refer to the LUCENE-2605 Jira issue).",
      "How to Deal with Multi-Term Synonyms",
      "For now, the simplest resolution to the problem outlined above is to perform synonym mapping at index time rather than query time, and make sure that synonyms are expanded. For instance, in the case above we could replace the line in synonyms.txt with the following:",
      "spider man, spiderman",
      "In this case the first document gets indexed with both spider man and spiderman and a search for spider man will match document 1.",
      "However, this resolution is also flawed. When someone searches for spider man, then very most likely, they mean the one and only Spider-Man. Unfortunately, the above query for spider man will match documents 2 and 3 as well as document 1. While there is no simple way to get around this, you can at least improve the situation by incorporating phrase matching behavior into your search. Most likely, you are going to be using the edismax query parser to handle user queries. By turning on the phrase field parameters (pf, pf2, and pf3) you can boost queries with phrase matches higher than queries without phrase matches. This can ensure that document 1 will be at the top of search results for spider man.",
      "The best resolution to the multi-term synonym problem, it seems, has yet to be invented. Some have built Solr modules that perform synonym-mapping before sending the query string to the query parser. And weve made good use of this solution. But it does come with the unfortunate side-effect in that synonym configuration, which rightly belongs in schema.xml, must copied over to the solrconfig.xml (which is intended to describe Solrs behavior).",
      "Ive got in mind the twinklings of a query parser that might just resolve these issues, but thats for a future time and later blog post. Till then…",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "91c90916f8c92d9b3aeaba7429bdf64f",
    "url_s": "https://opensourceconnections.com/blog/2013/10/29/adventures-in-cross-site-scripting/",
    "title": "Adventures in Cross-Site Scripting",
    "content": [
      "Dont cross the streams!",
      "One big component of making our search relevancy tool Quepid simple to use is the ability to just paste in a Solr search URL and go. Thankfully, Solr executes searches entirely with HTTP GET requests. Unfortunately, pasting in an arbitrary URL and executing HTTP requests to that arbitrary third-party violates the browser’s same-origin policy for performing HTTP requests. The browser wants to keep you sandboxed to the domain you started at (ie quepid.com).",
      "The most seamless way to get around this is to talk to Solr through a method known as JSONP. JSONP is certainly somewhat of a hack, but its a fairly well accepted hack. JSONP leverages the fact that your browser can load resources from any domain. A script tag is a resource, so if we dynamically insert a script tag like so:",
      "jsonpReq.src = 'http://mysolrsearch.com' +\n               '/solr/collection1/select?' +\n               'wt=json&json.wrf=loadResults&q=searchquery';\njsonpReq.type = \"text/javascript\";\ndocument.body.appendChild(jsonpReq);",
      "(see a full example in this jsfiddle)",
      "Solr takes a json.wrf argument (here \"loadResults\") which identifies a JavaScript function that the search results should be called on. Solr returns executable javascript that looks like:",
      "loadResults(/*search results as JavaScript object*/);",
      "So when this dynamic script tag is loaded, a global callback \"loadResults\" will be executed with our search results as an argument.",
      "We’ve used JSONP quite a bit in pure Javascript search applications. It lets us get rid of a middle layer of server-side glue code and focus on a rich, beautiful, client-side application. We love it!",
      "The problem with JSONP",
      "Unfortunately, JSONP has a pretty glaring hole. If your request fails to load, you get the same information you’d get if a script tag failed to load. You get extremely basic error information. You unfortunately don’t get an HTTP status code and you don’t get the data sent back with that error. Unfortunately for us, Solr reports errors via an HTTP error with the included HTTP data describing the exact error.",
      "For example, screwing up the echoParams parameter in this search request returns HTTP 400 with the following error message",
      "Solr Query:\n\nhttp://mysolrsearch.com/solr/collection1/select?echoParams=foo&#038;wt=xml\n\nSolr Response:\n<response>\n  <lst name=\"error\">\n    <str name=\"msg\">\n    Invalid value 'foo' for echoParams parameter, use 'EXPLICIT' or 'ALL'\n    </str>\n    <int name=\"code\">400</int>\n  </lst>\n</response>",
      "While this turns out to be not a big deal for a user-facing application, it stinks for a search developer workbench like Quepid. An advanced developer using Quepid needs more information than \"an error has occurred\" with their search. Perhaps in the search developers experimentation with the Solr relevancy parameters they mistyped a parameter, and Solr simply couldn’t parse what was sent. As a search developer, getting these errors is a pretty big deal. Missing them is tantamount to a code compiler that gives you no error messages. Not very helpful :).",
      "So knowing that JSONP stinks in this regard, how can we extract the Solr errors even if they come back with an HTTP error?",
      "CORS",
      "CORS, Cross-Origin Resource Sharing is a more standardized way of doing cross-site requests within the HTTP protocol. Perhaps this would be a better way to do cross-domain requests and extract both data and errors?",
      "I briefly looked into this for Quepid, but it quickly became apparent that it wasn’t nearly as seamless as JSONP. CORS requires that the server white-list domains that it will accept cross-domain requests from. Our users would need to white-list Quepid’s domain in their Solr web server’s config. I can just imagine our users calls to IT: \"Hey there’s this Quepid thing we’d like to try and it requires this other CORS thing. Could you please reconfigure Solr’s web server and add Quepid’s domain to this list?\" How long would that take to get approved? Some users may even be using hosted Solr solution where changing the web config is impossible. Even scarier, I’ve seen in a number of places that doing CORS in Solr may require a bit of Java code to be inserted, making it even less seamless.",
      "A big business goal behind Quepid is for it to be easy to try. Therefore, I don’t want to put an undue burden on users. The least friction to trying Quepid the better. Still, perhaps I should keep this in mind. Perhaps initially trying out the product would simply involve using JSONP and more advanced customers could white-list the domain to allow CORS to be used.",
      "Iframes Can Do Cross-Site Requests Too!",
      "One realization I had when playing around with cross-site scripting was that I can also insert iframes into a page using the same method (yes I know Im crazy). Simply by doing",
      "<iframe src=\"http://mysolrsearch.com/solr/collection1/select?echoParams=foo&wt=xml\"/>",
      "I’ll get search results inserted into the page. Furthermore, iframes will display returned data even if there’s an HTTP error. So could this be a way to expose Solr errors to devs?",
      "Initially I had hoped that I could simply dynamically add an iframe to the page and directly access the contents of the document body of the iframe from Quepid’s Javascript code, parse out the error, and show something pretty to the user. Hopefully I could grab the iframe’s contents by simply doing something like this code:",
      "iFrameBody = iFrame.contentWindow.document.getElementsByTagName(‘body’);",
      "Browser implementers are one step ahead of me here. This will work if my iframe is of the same domain as the requesting JavaScript. Unfortunately for my case theyve made sure that my domain’s JavaScript can’t access another domain’s content, so the browser complains with a Security Error that I’m violating the cross-domain security policy by trying to pull out my Solr data.",
      "Talking to Your Iframes",
      "The next realization I had was that I can talk to JavaScript code within an iframe using HTML’s new cross-document messaging. This new feature allows two window objects (the main window and an iframes for example) to post messages to each other. So if I could construct a Solr response that would respond to a message, I could post back the error data and do something meaningful with it. IE if I could do something like",
      "// ****************************************\n// main Quepid code\nvar receiver = function(e) {\n   console.log(e.data.error_text);\n}\n\nwindow.addEventListener('message', receiver, false);\n\n// ****************************************\n// iframe with Solr error\nvar receiver = function(e) {\n    if (e.origin == /*quepid domain*/) {\n      e.source.postMessage(solrErrorInfo, e.origin);\n    }\n};\n\nwindow.addEventListener('message', receiver, false);",
      "Great! Now the next problem is how can I make Solr generate a response that has the error along with the javascript to go along with it?",
      "At this point I got rather stuck. I tried various kinds of hackery. One technique that the browser’s dutifully didnt let me do was any kind of script injection from my Quepid code into the Solr response. Neither hacking the json.wrf argument I referred to earlier or trying out equivalent functionality in another Solr response format (the velocity template writer) got me what I wanted. When I was finally able to insert some JavaScript into the page, the browser detected the insertion as too similar to what I put in the URL and refused to execute the script (Good for you Chrome!).",
      "With these kinds of techniques, I clearly was going to hit walls. Using iframes to do this is already weird. Adding the extra level of script injection was even yuckier. Even if I got it to work, it certainly had little chance of continuing to work as browser security advanced.",
      "Enter XSLT (yes I said XSLT)",
      "However, there was one legal, though slightly inconvenient way I could get Solr to return the cross-document communication JavaScript. I can use Solr’s XSLT response writer. This feature of Solr lets you take Solr’s XML output and transform it to say HTML – with JavaScript and all the works. My XSLT can gin up some JavaScript code that implements the cross-document messaging safely. A preliminary version of this XSLT looks something like:",
      "<xsl:stylesheet version='1.0' xmlns:xsl='http://www.w3.org/1999/XSL/Transform'>\n  <xsl:output media-type=\"text/html\" encoding=\"UTF-8\"/>\n\n  <!-- Report errors to Quepid via cross-document communication\n       Quepid will post a message to this document and this\n       document will respond with the contents of the XML error\n       returned from Solr -->\n  <xsl:template match='/response/lst[@name=\"error\"]'>\n    <script type=\"text/javascript\">\n      String.prototype.endsWith = function(suffix) {\n        return this.indexOf(suffix, this.length - suffix.length) !== -1;\n      };\n      var errorText = \"<xsl:value-of select=\"str[@name='msg']\"/>\";\n      var solrErrorInfo = {\"error_text\": errorText,\n                           \"http_error_code\": <xsl:value-of select=\"int[@name='code']\"/>};\n\n      // respond to cross window messaging but only from *.o19s.com domains\n      var receiver = function(e) {\n        if (e.origin.endsWith(\"o19s.com\") || e.origin.endsWith(\"jshell.net\")) {\n          e.source.postMessage(solrErrorInfo, e.origin);\n        }\n      };\n\n      window.addEventListener('message', receiver, false);\n    </script>\n  </xsl:template>\n</xsl:stylesheet>",
      "If I save this XSLT as \"errors_to_quepid.xsl\", I can refer to it in an iframe:",
      "<iframe id=\"solr_errors\" style=\"display:none\" src=\"http://mysolrsearch.com/solr/collection1/select?echoParams=foo&wt=xslt&tr=errors_to_quepid.xsl\"></iframe>",
      "Quepid’s Javascript can communicate with this iframe as follows:",
      "var solrErrorWindow = document.getElementById(\"solr_errors\").contentWindow;\nsolrErrorWindow.postMessage(\"\", \"http://mysolrsearch.com\");\n\nvar receiver = function(e) {\n   console.log(e.data.error_text);\n}\n\nwindow.addEventListener('message', receiver, false);",
      "And guess what, it works! Check out this jsfiddle for a demo.",
      "The downside to this is that it requires users to add an XSLT file to the right place in their Solr config directory. A one-time inconvenience that users can opt-in to for better error reporting. An inconvenience Im imagining is likely easier to deploy then changes to the web server’s configuration.",
      "The upside to this approach is that it plays by the rules. I’m not doing hacky things trying to insert javascript into the solr response. I’m not circumventing any browser protections. And it works rather well.",
      "A Problem Thatll Drive You Insane",
      "Brendan Eichs favorite graphic to describe the Web. Evolution makes something thats often not very pretty, but it works!",
      "Reflecting on this problem leaves me wondering. While I’m aware how easy it is to inject cross-site scripts with malicious intent, I’m left wishing the web did a better job here. This is a rough problem to have to solve from an implementation point-of-view. It feels like if a service returns just data we ought to feel a little safer about how an application outside the domain consumes the content. For example, the browser could make decisions based on the mimetype coming back from the other end and relax the restrictions a bit. My naive understanding is that its cross-domain text/html we fear, not XML or JSON. Sure a malicious user that can examine my code can inject just the right XML or JSON into a response to exploit my lack of sanity checking. But its not in the same ballpark as doing cross-site requests where HTML and executable JavaScript is involved.",
      "Complaining aside, it was kind of a fun problem to work on. Getting cross-domain requests right certainly blurs the line between hacking and well \"hacking\". I’d be curious if you have any thoughts on solving this problem or a better solution? If so let me know. And of course, shameless plug. Check out Quepid! Its a neat tool if you care about managing your search quality."
    ],
    "summary_t": "One big component of making our search relevancy tool Quepid simple to use is the ability to just paste in a Solr search URL and go. Thankfully, Solr execute..."
  },
  {
    "id": "bde843a72cd333e732448b15f538a807",
    "url_s": "https://opensourceconnections.com/blog/2013/10/30/async-solr-queries-with-python/",
    "title": "Async Solr Queries in Python",
    "content": [
      "I frequently hit the wall of needing to work asynchronously with Solr requests in Python. Ill have some code that blocks on a Solr HTTP request, waits for it to complete, then execute a second request. Something like this code",
      "import requests\n\n#Search 1\nsolrResp = requests.get('http://mysolr.com/solr/statedecoded/search?q=law')\n\nfor doc in solrResp.json()['response']['docs']:\n    print doc['catch_line']\n\n#Search 2\nsolrResp = requests.get('http://mysolr.com/solr/statedecoded/search?q=shoplifting')\n\nfor doc in solrResp.json()['response']['docs']:\n    print doc['catch_line']",
      "(were using the Requests library to do HTTP):",
      "Being able to parallelize work is especially helpful with scripts that index documents into Solr. I need to scale my work up so that Solr, not network access, is the indexing bottleneck.",
      "Unfortunately, Python isnt exactly Javascript or Go when it comes to doing asynchronous programming. But the gevent coroutine library can help us a bit with that. Under the hood gevent uses the libevent library. Built on top of native async calls (select, poll, etc – the original async ), libevent nicely leverages a lot of low-level async functionality.",
      "Working with gevent is fairly straightforward. One slight sticking point is the gevent.monkey.patch_all() which patches a lot of the standard library to cooperate better with gevents asychrony. It sounds scary, but I have yet to have a problem with the monkey patched implementations.",
      "Without further ado, heres how you use gevents to do parallel Solr requests:",
      "import requests\nfrom gevent import monkey\nimport gevent\nmonkey.patch_all()\n\n\nclass Searcher(object):\n    \"\"\" Simple wrapper for doing a search and collecting the\n        results \"\"\"\n    def __init__(self, searchUrl):\n        self.searchUrl = searchUrl\n\n    def search(self):\n        solrResp = requests.get(self.searchUrl)\n        self.docs = solrResp.json()['response']['docs']\n\n\ndef searchMultiple(urls):\n    \"\"\" Use gevent to execute the passed in urls;\n        dump the results\"\"\"\n    searchers = [Searcher(url) for url in urls]\n\n    # Gather a handle for each task\n    handles = []\n    for searcher in searchers:\n        handles.append(gevent.spawn(searcher.search))\n\n    # Block until all work is done\n    gevent.joinall(handles)\n\n    # Dump the results\n    for searcher in searchers:\n        print \"Search Results for %s\" % searcher.searchUrl\n        for doc in searcher.docs:\n            print doc['catch_line']\n\nsearchUrls = ['http://mysolr.com/solr/statedecoded/search?q=law',\n              'http://mysolr.com/solr/statedecoded/search?q=shoplifting']\n\nsearchMultiple(searchUrls)",
      "Lots more code, and not nearly as pretty as the equivalent Javascript, but it gets the job done. The meat of the code is these lines:",
      "# Gather a handle for each task\nhandles = []\nfor searcher in searchers:\n    handles.append(gevent.spawn(searcher.search))\n\n# Block until all work is done\ngevent.joinall(handles)",
      "We tell gevent to spawn searcher.search. This gives us a handle to the spawned task. We can then optionally wait for all the spawned tasks to complete, then dump the results.",
      "Thats about it! As always, comment if you have any thoughts on pointers. And let us know how we can help with any part of your Solr search application!"
    ],
    "summary_t": ""
  },
  {
    "id": "cbc5fb033ac8f7df478c71ef709ff8ce",
    "url_s": "https://opensourceconnections.com/blog/2013/11/07/matrix-methods-for-term-sense-disambiguation/",
    "title": "Matrix Methods for Term Sense Disambiguation",
    "content": [
      "In earlier posts, weve talked about matrix methods to tweezing out semantics. Heres another quick idea for that can be used for term sense disambiguation.",
      "Steps",
      "Index all of your documents into Solr.\n  Extract the term/document matrix.\n  Multiply matrix with transpose of itself. This will you give you a term co-occurrence matrix. So if you have N terms, then the resulting matrix will be an N-x-N matrix. Lets examine one row, the row for \"tear\". The elements of this row correspond roughly to how commonly a term co-occurs with all other terms. So for the \"tear\" row, youll get large scores for words like \"paper\" and \"cry\". Although we know that you \"cry tears\" and \"paper tears\" and in both cases \"tear\" has different meaning, there is nothing immediately in the data of this row which indicates that \"tear\" has more than one sense. But…\n  Find the rows corresponding to the stronger values associated with tear and extract the portion of the co-occurrence matrix associated with these rows. For instance, in the case of \"tear\" you will go to the rows associated with \"cry\" and \"paper\" and \"apart\" and \"sweat\" and youll pull out only the columns associated with these terms. In this case it will be a 4-x-4 matrix.\n  Perform clustering on the rows from the sub-co-occurrence matrix. In the case of most words which, which practically have a single common sense, there will just be one obvious group. However with words that have multiple senses, as is the case for \"tear\", youll find that this matrices fairly well decouples into sub groups, and there will be severally small elements. For instance, while \"tear\" co-occurs highly with \"cry\", \"paper\", \"apart\", and \"sweat\", youll immediately see that \"cry\" does not commonly co-occur with \"paper\" or \"apart\", while it does co-occur moderately with \"sweat\" (e.g. \"blood, sweat, and tears\").",
      "Now, the big fat challenge here is going to be defining a cutoff criteria for the point at which multiple senses exist.",
      "And then, once you know about the senses… how do you use this information?",
      "… More on this later!"
    ],
    "summary_t": ""
  },
  {
    "id": "35004e0cc9ea6e60f06dde321e8ef981",
    "url_s": "https://opensourceconnections.com/blog/2013/11/11/datastax-mvps-for-cassandra/",
    "title": "Datastax MVPs for Cassandra",
    "content": [
      "Last month, John and I were named Datastax MVPs for Cassandra at the Cassandra Summit EU in London, and have now joined the rest of the Datastax Rebel Elite, a recognition for our involvement in the Cassandra community.",
      "",
      "What does this mean for us? You can still expect more blog posts about Cassandra, Solr, machine learning, and any other interesting topics that we come by."
    ],
    "summary_t": ""
  },
  {
    "id": "97ce6df3f23d69f8623afc25185a84a8",
    "url_s": "https://opensourceconnections.com/blog/2007/03/27/is-your-employees-time-your-time/",
    "title": "Is your employees’ time your time?",
    "content": [
      "A recent survey by Clearswift and Yougov shows that 59% of all employees aged 18-29 believe that they should be able to use Web 2.0 social networking sites for their personal use during work time and using corporate computers.",
      "This is certainly a shifting tide from the corporate culture of a bygone past when your time was the companys time, and you spent all day doing company work. But is this a bad thing?",
      "There are benefits to allowing employees to use the web for personal business during work hours. To expect almost any employee to work for eight hours straight (or even four hours straight, then an hour for lunch, and then four hours straight) is, almost without exception, unrealistic. The average persons attention span is somewhere between 50 and 55 minutes, and beyond that, concentration and results decline. They will need to take a break somehow. Many of them will use the time wisely and take care of other business which then eliminates a potential worry and allows them to focus on the task at hand.",
      "There are also risks to allowing them to use the web for personal use. One is the possibility of using social networking sites to air corporate dirty laundry. After all, if something has happened which the employee disagrees with, then its a simple matter to anonymously log into a social networking site and begin griping. Thats life. Theyre going to gripe sometimes, and to hide from the negative side of management is disregarding a duty.",
      "Where the line must exist is in giving away company secrets. However, that needs to be handled up front with non-disclosure agreements and employment agreements. Establish the rules and the punishments for failing to abide by the rules up front. Trying to stifle a leak later is a calamitous exercise.",
      "Companies that fail to embrace web 2.0s social networking powers are missing out on a great opportunity, particularly if they fail to do so out of fear of the airing of problems. Let others see a true picture of the company, and if it is one that does good work and takes care of its people, then an internal uprising of good words will quell the occasional disgruntled posting. Trying to overly monitor activity and stifle what is being said about a company is an Orwellian road to failure. Instead, focus on creating a good environment where people get to do the work that they enjoy and want to say positive things about the company."
    ],
    "summary_t": ""
  },
  {
    "id": "009fa738a55b419e53dbce57b545ce40",
    "url_s": "https://opensourceconnections.com/blog/2013/11/12/estimating-ad-conversion-rates-using-cassandra/",
    "title": "Estimating Ad Conversion Rates using Cassandra",
    "content": [
      "Dont let anybody fool you; a good bit of what a data scientist does is a glorified form of counting. And if you had a few billion fingers, you yourself could do data science on one hand. In my recent work with Cassandra Im finding that Cassandra is quite good at counting. As a matter of fact, you can treat Cassandra as a giant, distributed, redundant, \"infinitely\" scalable counting framework. Thus, for analysis applications that rely heavily upon accumulating counts, Cassandra can be a useful tool. In this and the next blog post well take a look at a couple of big data analytics applications that rely heavily upon counting and we will demonstrate how Cassandra can be put to good use.",
      "Lets say that you run an online ad company. When you started the company, your customers were happy enough to just have you host their advertising and handle the dirty work. But now, with ever increasing competition, you would provide your customers with feedback analytics so that you can retain your competitive edge. And the first thing that customers want to know is how their advertising is performing. The most straightforward metric for this: conversion rate. The conversion rate is portion of all ad clicks which lead to actual purchases. This number can be represented as a probability from 0 to 1 or as a percentage from 0 to 100%. And the easiest way to estimate the conversion rate, is to count! Basically, you need to keep track of how many times each ad has been clicked and how many times such a click leads to an eventual product purchase.",
      "Heres how I would put it together in Cassandra. First we create a table to hold all of our counts:",
      "CREATE TABLE conversion_rate (\n  company text,\n  ad text,\n  product text,\n  count counter,\n  PRIMARY KEY (company, ad, product)\n)",
      "Lets say that Apple is one of your most important clients and you hold ads specifically for iPhones, iPads, and iPods. And right now Apple is running a new ad campaign with ads corresponding to each of the above products. We will refer to these ads as iPhoneAd, iPadAd, and iPodAd respectively. When someone clicks on the iPodAd, you account for it by updating Cassandra:",
      "UPDATE conversion_rate\n  SET count=count+1\nWHERE company='Apple'\n  AND ad='iPodAd'\n  AND product='NO_PRODUCT';",
      "You might be wondering what product='NO_PRODUCT' is about. This is the way that we encode ad clicks, regardless of whether or not a purchase was eventually made. Of course, most of the time, a user who clicks an ad will look around for a bit and then leave. But in this case, lets assume this user goes on to buy an iPod:",
      "UPDATE conversion_rate\n  SET count=count+1\nWHERE company='Apple'\n  AND ad='iPodAd'\n  AND product='iPod';",
      "And so it goes; many different ads are shown to many different users. Some users click the ads and of those users, some go on to purchase products. Eventually, the marketing execs of Apple come to us to see how things are going with their new ad campaign. Specifically, they want to know which ads are performing best for their various products. To make this determination, all we have to do is make a single query:",
      "SELECT ad,product,count\n  FROM conversion_rate\n  WHERE company = 'apple'",
      "And the results will look something like this.",
      "ad    |    product    |  count\n----------+---------------+---------\n iPadAd   |  iPad         |      210\n iPadAd   |  iPhone       |        3\n iPadAd   |  iPod         |      152\n iPadAd   |  NO_PRODUCT   |    17533\n iPhoneAd |  iPad         |        7\n iPhoneAd |  iPhone       |     1409\n iPhoneAd |  iPod         |        4\n iPhoneAd |  NO_PRODUCT   |    28131\n iPodAd   |  iPad         |        6\n iPodAd   |  iPhone       |        2\n iPodAd   |  iPod         |       15\n iPodAd   |  NO_PRODUCT   |    11299",
      "The results here do not directly represent the conversion rate, but if youre thinking ahead then I bet you can see that were close. Rather than giving the direct counts to the Apple, we first need to divide the count of ad clicks that lead to a purchase by the total number of ad clicks. So for instance, here we see that 17533 people clicked the iPadAd and this lead to 210 iPad purchases. This implies a conversion rate of 210/17533 or roughly 1.2%. If we assemble this into a nice table for the customer, it will like something like this:",
      "iPad\n    \n\n    \n      iPhone\n    \n\n    \n      iPod\n    \n  \n\n  \n    \n      iPadAd\n    \n\n    \n      1.2%\n    \n\n    \n      0.0%\n    \n\n    \n      0.9%\n    \n  \n\n  \n    \n      iPhoneAd\n    \n\n    \n      0.0%\n    \n\n    \n      5.0%\n    \n\n    \n      0.0%\n    \n  \n\n  \n    \n      iPodAd\n    \n\n    \n      0.0%\n    \n\n    \n      0.0%\n    \n\n    \n      0.1%",
      "Now, at a glance, the customer will be able to immediately see how their ads are performing across all products. For instance, we can see that the iPod ad is underperforming while the iPhone ad is working quite well. But theres is other interesting information here as well; look how many iPods are being sold through the iPad ad. With some further research, there may be opportunity here to upsale customers to whichever product, iPod or iPad, has a higher margin.",
      "The feedback presented here is invaluable to the customer, and for you, the online ad company, gathering this data was a piece of cake! But there are a couple of things that you should look out for. 1) Be aware that you will get lower throughput with counter updates in Cassandra than with normal updates. Why? Because for each counter increment there must be a corresponding background query to make sure that the counter state is consistent. Since this is done in the background, this has little effect upon latency. 2) Counters can be a little more difficult to maintain. For instance, if growing or shrinking the cluster, care must be taken to make sure that counters remain in a consistent state. However the details of counter table maintenance is another post for another time!",
      "Next up – if you want to go one step further, check out our next post on using Cassandra to build a Naive Bayes customer classifier. Warning: I get pretty mathy with it!",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "e6354049b02b33869d73198e8393608d",
    "url_s": "https://opensourceconnections.com/blog/2013/11/15/using-cassandra-to-build-a-naive-bayes-classifier-of-users-based-upon-behavior/",
    "title": "Using Cassandra to Build a Naive Bayes Classifier of Users Based Upon Behavior",
    "content": [
      "In our last post, we found out how simple it is to use Cassandra to estimate ad conversion. Its easy, because effectively all you have to do is accumulate counts – and Cassandra is quite good at counting. As we demonstrated in that post, Cassandra can be used as a giant, distributed, redundant, \"infinitely\" scalable counting framework. During this post will take the online ad company example just a bit further by creating a Cassandra-backed Naive Bayes Classifier. Again, we see that the \"secret sauce\" is simply keeping track of the appropriate counts.",
      "In the previous post, we helped equip your online ad company with the ability to track ad conversion rates. But competition is steep and well need to do a little better than ad conversion rates if your company is to stay on top. Recently, suspicions have arisen that ads are often being shown to unlikely customers. A quick look at the logs confirms this concern. For instance, there was a case of one internet user that clicked almost every single ad that he was shown – so long as it related to the camping gear. Several times, he went on to make purchases: a tent, a lantern, and a sleeping bag. But despite this users obvious interest in outdoor sporting goods, your logs indicated that fully 90% of the ads he was shown were for womens apparel. Of these ads, this user clicked none of them.",
      "Lets attack this problem by creating a classifier. Fortunately for us, your company specializes in two main genres, fashion, and outdoors sporting goods. If we can determine which type of user were dealing with, then we can improve our conversion rates considerably by simply showing users the appropriate ads.",
      "Naive Bayes Classifiers",
      "With this goal in mind, lets look at the theory behind Naive Bayes Classifiers so that we can build our own. The purpose of a classifier is to identify which group a sample belongs to based upon the given evidence. In this case, our \"sample\", is an individual user, and based upon the evidence of which ads she clicks, we wish to identify which group she belongs to: fashion or outdoors. To put some math to the problem, consider the following question:",
      "What is the probability that user is from group    given the fact that this user has clicked on ads   ,   , and   ?",
      "To put this into equation form, we can write:",
      "",
      "This function returns a probably, a number from 0 to 1, representing how likely it is that this user is from a particular group based upon the fact that they have clicked on these ads. The goal, then, is to evaluate this equation with each group and then find which group leads to a bigger result. But how do you evaluate this equation? Fortunately for us, Thomas Bayes, a clergyman from the 18th century provided an answer in the form of Bayess equation:",
      "",
      "Heres weve turned one probability into the function of three separate probabilities:",
      "– the probability, in the absence of any evidence, that a user is from a particular group – this is called the prior\n    – the probability that a user from group    will have clicked ads   ,   ,   , etc.\n    – the probability that a user from any group will click ads   ,   ,   , etc.",
      "This looks a little confusing, but bear with me a moment and well see how this allows us to solve our classification problem. Lets first look at the probability   . We happen to know that both the fashion group and the outdoor group are about equally strong, so for simplicity sake, we assume that   . But remember, we ultimately intend to identify the group which maximizes this equation. Since    in both cases, it does not affect the outcome and can safely be disregarded. Next up,   . We could estimate the probability that users click on particular groups of ads, but here again were looking for the group that maximizes this above equation, and since the value of    is not a function of the group in consideration, this component remains constant across all groups and can also be safely be disregarded.",
      "The only piece left is   , the probability that a user from group G will click on ads   ,   ,   , etc. Since this piece is a function of   , we can not disregard it, so we must somehow compute it. And our goal, again, is to find the group    which maximizes this probability. … But we have a problem. This particular probability, as stated, can not be computed. Its intractable. Its mathematically infeasible to gather enough information to estimate the probability that a member of group    will click on any particular set of ads. So we do what any good applied mathematician will do when hitting a wall like this, well make a simplifying assumption. If we assume that ad clicks are completely independent from one another, then we can deal with them each separately. Thus:",
      "",
      "Here, each piece,   ,   ,   , etc., is actually quite simple to estimate. And though this assumption might be a bit naive – this is, after all, reason that this classifier is called the Naive Bayes Classifier – the resulting classifier has empirically been show to work quite well across a wide range of applications and even in certain cases where this assumption is not only naive, but actually quite wrong.",
      "Finally, we have arrived at something we can deal with. Lets take a moment to recap: In order to find the most likely group    that a user belongs to based upon their ad clicks   ,   ,   , etc., we must find which group maximizes the equation:",
      "",
      "But after using Bayess equation and discarding some unnecessary pieces we recognize that we can determine the most likely group by finding the group which maximizes this function:",
      "",
      "And finally, after making the simplifying assumption regarding the independence of clicks, we see that determining the most likely group for the user based upon ad clicks is as simple as finding the group which maximizes this equation",
      "",
      ".",
      "Building the Classifier using Cassandra",
      "Based upon the equation above, for each ad    and for both of the groups    we need to keep estimates of the probabilities   . To make a definitive determination of which group a user belongs to, we wait until a user makes a purchase. Based upon which of our customers he buys a product from, for instance Zappos vs. REI, its easy to determine whether this user is in the fashion group or the outdoor group. As an example, consider the user that we alluded to earlier – the person that purchased the tent, lantern, and sleeping bag. Hes obviously an outdoors person. Whats more, since we can track each users click history, we know which ads that he has clicked in the past. And we can use counts of these ads to estimate the probability, that an outdoors person will click on an ad for hiking boots –   .",
      "Heres how we do this using Cassandra. First, we create a table:",
      "CREATE TABLE probability_of_view_given_group (\n  ad text,\n  group text,\n  count counter,\n  PRIMARY KEY(ad,group)\n);",
      "Then, whenever a user makes a new purchase, we establish their group, fashion or outdoors, based upon the customer they purchased from. Next, we scoop together all of the ads that this user has clicked and for each ad we make two updates. Lets demonstrate this with HikingBootsAd for the user above:",
      "UPDATE probability_of_view_given_group\n  SET count=count+1\n  WHERE ad='HikingBootsAd'\n  AND group='ANY_GROUP';\n\nUPDATE probability_of_view_given_group\n  SET count=count+1\n  WHERE ad='HikingBootsAd'\n  AND group='HikingBootsAd';",
      "The first update keeps track of the number of ad clicks no matter the group; you make this update whether or not the user in question is an outdoors enthusiast or a fashion aficionado. The second update is the group specific count. Once you have these two values, then you have all you need to estimate the requisite probabilities. Again in the case of the HikingBootsAd, the probability that someone who clicks on this ad is a outdoors person is roughly equal to the number of outdoor people who have clicked the HikingBootsAd in the past divided by the total number of times that any product purchaser has clicked the HikingBootsAd. In equation form:",
      "",
      "Putting it all together",
      "Lets see it in action. A user has just opened a web page that belongs to one of your ad affiliates and its time to serve up an ad. A quick check reveals that this user has looked at three ads in the past couple of days: LittleBlackDressAd, CuteHighHeelsAd, and most recently HikingBootAd. Heres how we automatically determine which group this user is in so that we can serve them an ad appropriate to their tastes:",
      "First we grab all available data for the ads in question:",
      "SELECT * FROM probability_of_view_given_group\n  WHERE ad='LittleBlackDressAd';\n\n         ad         |   group   | count\n--------------------+-----------+-------\n LittleBlackDressAd | ANY_GROUP | 11843\n LittleBlackDressAd | Outdoors  | 142\n LittleBlackDressAd | Fashion   | 11701\n\nSELECT * FROM probability_of_view_given_group\n  WHERE ad='CuteHighHeelsAd';\n\n       ad        |   group   | count\n-----------------+-----------+-------\n CuteHighHeelsAd | ANY_GROUP | 54127\n CuteHighHeelsAd | Outdoors  | 53\n CuteHighHeelsAd | Fashion   | 54074\n\nSELECT * FROM probability_of_view_given_group\n  WHERE ad='HikingBootAd';\n\n      ad      |   group   | count\n--------------+-----------+-------\n HikingBootAd | ANY_GROUP | 71534\n HikingBootAd | Outdoors  | 63241\n HikingBootAd | Fashion   | 8293",
      "Next we calculate the requisite probabilities by dividing the Outdoors and Fashion counts by the ANY_GROUP counts.",
      "",
      "",
      "",
      "As is often the case when you start tracking metrics like this, you find other interesting facts as a byproduct of your work. In this case we see that the HikingBootAd is surprisingly popular among the fashionistas.",
      "Finally, recall once more that the classification we choose is based upon whichever group maximizes the equation   . Therefore we apply this equation to both groups:",
      "\\quad = 0.00001$$",
      "",
      "Based upon these calculations, it is roughly 11,000 times more likely that this person belongs to the fashion group than to the outdoor group. Based upon this classification, we confidently serve up an ad for a cute pink designer purse. As it turns out, this user goes on to purchase the purse. Because she made a purchase from one of our fashion clients, we now know that this user is in the fashion group, and dutifully, we complete the circle of analysis, by collecting the ads that she has viewed and using them to update our counts in Cassandra. Note that this does include another count in favor of that HikingBootAd being attractive for members of the fashion group.",
      "Conclusion",
      "Now that Ive concluded my sketch for building a Cassandra-based user classification engine, should you go out and pull this all into production? Nah… probably not. As a matter of fact, there are a few issues with the application as Ive built it up here. For one, just because a person makes one outdoor purchase, this doesnt mean that forever and always they will be the quintessential outdoorsman. Indeed, in our example here we have identified a particular pair of hiking boots that the fashion folks seem to fancy. Then on the Cassandra side of things, we are required to make several queries before serving up a each individual ad. While Cassandra is quite performant with reads, Cassandra most excels when it comes to quick writes and the example use case outlined in this post is a bit read heavy.",
      "However, Cassandra is still going to be quite performant for accumulating the various counts necessary to implement the Naive Bayes Classifier. There are countless possible applications for such a classifier, and so long as your particular use case does not require an overwhelming number of queries, Cassandra might just be a great tool for your application.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "69d06744fc2664a0f795ba326448f26e",
    "url_s": "https://opensourceconnections.com/blog/2013/11/22/parameterizing-and-organizing-solr-boosts/",
    "title": "Parameterizing and Organizing Solr Boosts",
    "content": [
      "One of my clients has some pretty heavy-duty requirements for boosting functions. Its actually right on the boundary of what I think is appropriate for Solr. BUT, while I choose to continue within the bounds of Solr, I might as well expect the boosting functions to be as readable, and well-organized as possible. So lets take a look at my strategy.",
      "The Setup",
      "Lets say that were Amazon, and were allowing our users to search over books. But rather than just return the books based upon straight TF-IDF search, we need to control the boosting behavior to guide users towards newer books and books with a higher margin. The text of the book is stored in the text field, and the margin and release date of the books are stored in the corresponding fields margin and release_date.",
      "The Problem",
      "The problem is that the syntax that we must assemble to create such a query is utterly unwieldy. Allow me to demonstrate:",
      "<requestHandler name=\"/booksearch\" class=\"solr.SearchHandler\">\n     <lst name=\"defaults\">\n       <str name=\"defType\">edismax</str>\n       <str name=\"qf\">text</str>\n       <str name=\"pf\">text</str>\n       <str name=\"boost\">sum(product(margin,0.34),product(div(1,ms(NOW,release_date)),1100)</str>\n     </lst>\n</requestHandler>",
      "Check out that boost parameter. Can you tell what its doing? Well, can you? (Im pausing to let you try and figure it out.) Yeah… so the answers no. And as a matter of fact, I cant tell what it does either – and I just wrote it. Whats more, if your eyeballs are a little better than mine at reading this stuff, youll notice that there are some hardwired constants in this equation: 0.34, and 1100. What do these do? Beats me! But they must be important, so lets never ever touch them ever again.",
      "I think Ive made a good case for the problem. This type of function munging leads to brittle, inscrutable, and unchangeable configuration. Lets take another swing at it!",
      "The Solution",
      "Heres my second attempt. Take a moment to read over it and see what you think.",
      "<requestHandler name=\"/booksearch\" class=\"solr.SearchHandler\">\n     <lst name=\"defaults\">\n       <str name=\"defType\">edismax</str>\n       <str name=\"qf\">text</str>\n       <str name=\"pf\">text</str>\n\n       <str name=\"boost\">$totalBoost</str>\n       <str name=\"totalBoost\">sum($marginBoost,$recencyBoost)</str>\n       <str name=\"marginBoost\">product(margin,$valMarginBoost)</str>\n       <str name=\"recencyBoost\">product($inverseRecency,$valRecencyBoost</str>\n       <str name=\"inverseRecency\">div(1,ms(NOW,release_date))</str>\n\n       <str name=\"valMarginBoost\">0.34</str>\n       <str name=\"valRecencyBoost\">1100</str>\n     </lst>\n</requestHandler>",
      "So the first thing that you might notice, is that its a little more verbose than the previous request handler, but I maintain that this verbosity is actually incredibly helpful. Because now, you can almost read this configuration as if its explaining to you exactly what its doing.",
      "YOU: How is the total boost formed?\n\n  MR.REQUEST HANDLER: Oh, well its the sum of the margin boost and the recency boost. Duh!\n\n  YOU: Yeah, well whats the margin boost?\n\n  MR.REQUEST HANDLER: Simple! We just multiply the value stored in margin field with the constant called valMarginBoost.\n\n  YOU: Oh… so I can just modify the valMarginBoost and change how important the margin is in the results?\n\n  MR.REQUEST HANDLER: Bingo!",
      "Personally I dont like Handlers tone, but hes right, this is lots easier to read, and therefore maintain and modify. The labeling of the functional pieces makes it easier to keep track of everything and understand how each piece builds up to the total boost. The ordering of the named pieces is also important. I made sure that the definition of each piece is located just below the place where it is first mentioned. The only exception is the section at the bottom where Ive placed the constants that the content curator or merchandising expert can fiddle with – thus there are no longer any magic constants in our configuration.",
      "Shameless Plug for Quepid",
      "Content curators, merchandising experts – now since the search team has built up the Solr request handler, and exposed the tunable parameters, its your job to find the perfect value for these parameters. This is hard! Why? Because you might find that the perfect parameter values for your top product is actually the worst possible configuration for all other products. And its hard to know this without looking at all those queries at once.",
      "Quepid solves this. Look at tens or even hundreds of queries at once and watch how they change as you modify configuration parameters. Look here for details. And also read further here.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "2b69a57ddf3ad4cce205fc2d7b1ee558",
    "url_s": "https://opensourceconnections.com/blog/2013/11/27/quick-start-with-neo4j-using-your-twitter-data/",
    "title": "Quick Start with Neo4J using YOUR Twitter Data",
    "content": [
      "When learning a new technology its best to have a toy problem in mind so that youre not just reimplementing another glorified \"Hello World\" project. Also, if you need lots of data, its best to pull in a fun data set that you already have some familiarity with. This allows you to lean upon already established intuition of the data set so that you can more quickly make use of the technology. (And as an aside, this just why we so regularly use the StackExchange SciFi data set when presenting our new ideas about Solr.)",
      "When approaching a graph database technology like Neo4J, if youre as avid of a Twitter user as I am then POOF you already have the best possible data set for becoming familiar with the technology – your own Social network. And this blog post will help you download and setup Neo4J, set up a Twitter app (needed to access the Twitter API), pull down your social network as well as any other social network you might be interested in. At that point well interrogate the network using the Neo4J and the Cypher syntax. Lets go!",
      "Installing and setting up Neo4J",
      "Since were not setting Neo4J up for production use, this parts real easy. Just go to the Neo4J download page, click on that giant blue download button, and 36.1M later youll have your very own copy of Neo4J. Unzip it to some reasonable place on your machine, cd into that directory, and simply issue the command bin/neo4j start. (Once youre finished, a bin/neo4j stop will shut Neo4J down.) Now if you point your browser at http://localhost:7474 and see stuff (rather than lack of stuff), then youre ready to start shoveling data into Neo4J.",
      "Prepping Twitter",
      "Youll need to create a Twitter app before you can start pulling down your connections because you need the apps credentials in order to access Twitters API. But dont sweat it, this literally takes less than a minute. Just go to the Twitter developer apps page, sign in, and there will be yet another big blue button, this time labeled \"Create a new application\" – click it! After filling out a really short form, checking the \"I blindly agree to whatever is included in this legal contract\" checkbox, entering a CAPTCHA string, and clicking the \"Create your own Twitter application\" button, you will indeed have your very own Twitter app. Youll be taken to a screen that contains the details for your new app, but most importantly the OAuth credentials. Initially, you wont have the access tokens, but you can click the \"Create access tokens\" button at the bottom and next time you refresh the page (wait a few seconds) youll see that the access keys are available. Keep track of the credentials here because youll need to refer to them soon.",
      "Scraping Your Social Circles from Twitter",
      "Check out my Python TwitterScraper script. Though its not yet the most beautiful code, it doesnt really matter, because theres not much here! Lets take a moment to walk through it. The first section is where you set up Twitter and Neo4J. Naturally youll need to pip install the Tweepy and Py2Neo libraries, but they dont have any weird dependencies, so this shouldnt be a problem. Also notice, this is where all the access keys for your Twitter app should be used. Go ahead and copy and paste your credentials there. Now you should be ready to go.",
      "The remaining code includes two functions. The first, create_or_get_node, creates, or gets a node (in this case a Twitter user) from Neo4J by id_str, and if its creating the node for the first time, it also inserts all of the relevant user metadata into Neo4J. Also, the create_or_get_node optionally takes a list of labels that will later be used to group certain users together. The second function. insert_user_with_friends, takes a Twitter user (via their screen name), pulls that all relevant metadata for that user from the Twitter API and inserts it into Neo4J. This function will then do the same thing for all the individuals that this Twitter user follows. And finally, insert_user_with_friends will establish a FOLLOWS relationship linking the source Twitter user to those that she follows. Again here, insert_user_with_friends takes an optional list of labels that can be used to group the seed nodes (those that are followed do not get labeled).",
      "The last bit of the script is the fun part. This is where you programmatically lay out the social networks and individuals that you want to stalk… er, uh… observe. For your convenience, Ive added all of the OpenSource Connections team, as well as several notable individuals from the Neo4J community. Ive also included grouping labels that I though were pretty reasonable descriptors for these individuals and groups. As that last comment in the code states, make sure to add several people that you follow as well. Remember, the goal here is to create a data set that you are eminently familiar with. Once youre happy with the data set, the run it: python TwitterScraper.py. It will pull down twitter users 200 at a time and insert them into Neo4J as fast as possible. Soon the program will hit Twitters rate limit cutoff, at which point, the script will wait until the rate limit has been lifted and will continue pulling down the rest of the data. All together, you can plan on getting around 200 updates per minute.",
      "Start Infiltrating the Social Network!",
      "Now for the fun part; lets start putting some queries together and pulling back interesting data. In all of the examples below, we will be using the default Neo4J browser which youll still find at http://localhost:7474/. Heres were using the Cypher query language. This blog post wont go into too much detail about Cypher syntax itself, but feel free to look at the very rich Neo4J documentation. Also, Ill be using my own Twitter screen name \"JnBrymn\" as an example, so feel free to replace my screen name with your own and try the queries for yourself.",
      "First off, lets make sure the data weve ingested seems reasonable. The most obvious thing to do is to make sure were actually in the data set:",
      "MATCH (n {screen_name:\"JnBrymn\" })\nRETURN n",
      "Up pops an orange node representing me. And if I click on the node, I see a list of all my metadata.",
      "",
      "I wonder just how many users we have indexed now?",
      "MATCH (n)\nRETURN count(*)",
      "7098 users, not bad. How many are you following?",
      "MATCH (n {screen_name:\"JnBrymn\"})-[:FOLLOWS]->(o)\nRETURN count(*)",
      "371 – yep, that looks right. And check out how easy Cypher is – youre basically drawing ASCII art of the node connections. So its easy to ask the next obvious question: How many are following me? Here I just switch the direction of the relationship arrow:",
      "MATCH (n {screen_name:\"JnBrymn\"})<-[:FOLLOWS]-(o)\nRETURN count(*)",
      "Hmm… only 10 followers. Am I really that unpopular? (Checking Twitter now.) No, says Ive got 460 friends. Oh, thats right, if youll remember, were only collecting outbound FOLLOWS relationships from our seed users (labeled as SeedNode). The reason for this is because some people, Justin Beiber for example, are followed by millions of Twitter users! And we certainly dont want to keep track of that for now.",
      "But all this makes me think, of the seed users that I follow, who does not follow me back?",
      "MATCH (n {screen_name:\"JnBrymn\"})-[:FOLLOWS]->(o:SeedNode)\nWHERE NOT (o)-[:FOLLOWS]->(n)\nRETURN o.screen_name",
      "This returns a single name: mesirii. This is Michael Hunger, one of the Neo4J hot shots. If hes not following me back, then Im definitely not doing a good job of infiltrating the Neo4J community yet. No matter… I bet hes a @justinbeiber follower anyway… lets check:",
      "MATCH (n:SeedNode)-[:FOLLOWS]->(o {screen_name:\"justinbieber\"})\nRETURN n.screen_name",
      "Sadly… no one on our list follows Justin Bieber… I was sure I would have some good blackmail fodder there! (But hey, maybe youll discover some Beliebers in your own data set :P )",
      "Hmm… well if Im going to break into the Neo4J community, I need to find my likely vectors. Lets create a list of all people who follow me and order them by the number of Neo4J people that they follow. Maybe I can get introductions through these friends:",
      "MATCH (n:Neo)-[:FOLLOWS]->(m:SeedNode {screen_name:\"JnBrymn\"}),\n      (n)-[:FOLLOWS]->(o:Neo)\nRETURN count(*), n.screen_name\nORDER BY count(*) desc\nLIMIT 10",
      "This returns:",
      "count(*) |  n.screen_name\n---------+---------------\n13       |  wefreema\n11       |  technige",
      "Sweet, so my friends wefreema and technige look like my gatekeepers to the Neo4J community. The only thing left to determine is what people I need to connect to.",
      "MATCH (n:Neo)-[:FOLLOWS]->(o)\nRETURN count(*), o.screen_name\nORDER BY count(*) desc\nLIMIT 10",
      "This query enumerates the most popular people among the Neo4J community based upon who my Neo seed nodes are following. And the results of this query look like this:",
      "count(*) |  n.screen_name\n---------+---------------\n13       |  mesirii\n12       |  emileifrem\n12       |  jimwebber\n12       |  digitalstain\n11       |  apcj\n11       |  cleishm\n11       |  pandamonial\n11       |  iansrobinson\n11       |  p3rnilla\n11       |  neo4j",
      "As expected, plenty of these people are SeedNodes that I selected because I already knew them to be leaders in the community: mesirii, emileifrem, jimwebber, p3rnilla, neo4j. But who are these guys: digitalstain, apcj, cleishm, pandamonial, iansrobinson? After quickly looking them up on Twitter, I think weve discovered some new, key players in the Neo4J space.",
      "Conclusion",
      "This is only an intro to Neo4J. There are plenty of things that we could have talked about here: I could have gone into much more detail about the Cypher query syntax, I could have added indexes to speed up query times, and I could have put together some even crazier Cypher queries that make use of the broader Cypher syntax. But this is a good start. I think that youll agree: by looking at your own Twitter social graph, youll immediately think of questions that you want to ask and youll get a better understanding of what possibilities are out there.",
      "Want to learn more about Cypher? Well I might just be co-authoring a book on that very subject! Stay tuned.",
      "Update – Crowdsourcing a Collection of Key Community Figures",
      "Apparently some people are already using this post to search through their own communities of interest. Lets help each other out. If youre tracking a community, then comment below with the Twitter screen names of the key figures from the community. Ill edit the comments later to coalesce clean lists.",
      "",
      "Check out my LinkedIn Follow me on Twitter"
    ],
    "summary_t": ""
  },
  {
    "id": "c8868ebdd3c5b2ae5c12b21b34afbb2d",
    "url_s": "https://opensourceconnections.com/blog/2013/12/11/codds-relational-vision-has-nosql-come-full-circle/",
    "title": "Codd’s Relational Vision – Has NoSQL come full circle?",
    "content": [
      "Recently, I spoke at NoSQL Matters in Barcelona about database history. As somebody with a history background, I was pretty excited to dig into the past, beyond the hype and marketing fluff, and look specifically at what technical problems each generation of database solved and where they in-turn fell short.",
      "However, I got stuck at one moment in time I found utterly fascinating: the original development of relational databases. So much of the NoSQL movement feels like a rebellion against the \"old timey\" feeling relational databases. So I thought it would be fascinating to be a contrarian, to dig into what value relational databases have added to the world. Something everyone thinks is obvious but nobody really understands.",
      "It’s very easy and popular to criticize relational databases. What folks don’t seem to do is go back and appreciate how revolutionary relational databases were when they came out. We forget what problems they solved. We forget how earlier databases fell short, and how relational databases solved the problems of the first generation of databases. In short, relational databases were the noSomething, and I aimed to find out what that something was.",
      "And from that apply those lessons to todays NoSQL databases. Are todays databases repeating mistakes of the past? Or are they filling an important niche (or both?).",
      "The First Databases",
      "To appreciate how revolutionary the relational model is, we need to appreciate what the earliest, pre-relational databases looked like. These databases largely reflect what we would build if we were tasked by our boss to \"build a database\". Maybe, if we’re tracking movie rentals, in our ignorance we might start out with a line-by-line csv listing of customers and what movies they’ve rented. Maybe something silly that looks like this:",
      "Poor Mans Database",
      "We might also add an \"index\" to the back of the file to help us find specific records, in the same way we’d use an index in a book. Here the index is telling us that the \"doug\" record is 512 characters into the file while the \"rick\" record is 9212 characters into the file.",
      "Now the earliest designers of these data storage systems would have had to face the problem, \"How do I store movies as their own record?\". For example, we’re going to need to start storing how much a movie costs to rent and how many we have in stock. Should movies like \"Top Gun\" be treated as top-level records? Or should they be as parts-of (something owned by) the user records we already have? With movies as their own records, we don’t have to duplicate all the inventory and price data everywhere, but we’ll need to create another construct (an additional record type?) to specify the relationship.",
      "Poor Mans Database",
      "The databases that aggregate videos into users are considered to be following the \"hierarchical\" model. The databases that break out movies into their own records with the ability to link records fall into the \"navigational\" model.",
      "You can imagine, if we were interacting with this database, our first reaction would be to create a rather primitive interface – create something low-level that talked only in movies and users. Luckily, others saw past this and realized that patterns like the ones in our video rental and other data stores could be generalized–that we could abstract the notion of \"record\" and \"ownership\" into something much more powerful. Eventually this is exactly what Codasyl/DBTG (Database Task Group) did, creating a standard language for creating and interacting with navigational and hierarchical databases.",
      "So reflecting on our data model, you could create a record in one of these databases by using the RECORD command:",
      "Record Name is USER\n   Location Mode is Calc Using username\n   Duplicates are not allowed\n   username Type character 25\n   address Type character 50\n   phonenumber Type character 10",
      "And when we want to declare a relationship between two records, we can define a set that maps them in an owner-ownee relationship.",
      "Set Name is USER-VIDEOS\n   Order is Next\n   Retention is Mandatory\n   Owner is USER\n   Member is VIDEO",
      "This model reflects first building many databases and then attempting to generalize important abstractions. We can think of the early history of databases like our video rental example. Since the advent of storage media, folks needed to store all kinds of data. Eventually somebody did something laudable, reflected on all the patterns being used in data storage and management and created a generalized database that was codified in the DBTG language.",
      "Enter Codd",
      "Whereas the first database’s abstractions grew out of patterns learned building from the bottom-up, the relational model did just the opposite. The relational model creates an amazingly powerful abstraction rooted in predicate calculus, and then expects the implementation details to follow (which as we all know they did).",
      "When Codd wrote his paper, he criticized the DBTG databases of the day around the area of how the application interacted with the databases abstractions. Low-level abstractions leaked into user applications. Application logic became dependent on aspects of the databases: Specifically, he cites three criticisms:",
      "Access Dependencies: We often need to navigate from users -> videos to get at the videos. Application logic depends on how records are linked or aggregated. I must use one record type to get another record type.\n  Order Dependencies: Applications depend on how data is physically stored in the database. Notice the \"ORDER is NEXT\" line in the Set declaration above. This specifies storage based on insert order, so retrieval will in turn happen on index order.\n  Index Dependencies: When accessing indices, these databases required database indices be referred to explicitly by name.",
      "Codd proposed to get around these limitations by focusing on a specific abstraction: relations. A relation is simply a tuple of elements. The ith element of each tuple is a member of some set, known as that element’s domain. Perhaps a given element’s domain is the set of users, user ids, possible movies to rent, etc. So for our videos, a tuple might look like:",
      "(user=u, address=a, list of movies rented=l)",
      "Or in other words",
      "(doug, 1234 Bagby St, [Top Gun 3.99, Terminator 12.99])",
      "One can interpret this relation to \"mean\" any number of things. We might equate this to a statement that \"Doug rents Top Gun at 3.99 and Terminator at 12.99\" or \"Doug lives at 1234 Bagby St\".",
      "Adding to this, Codd proposes an idea known as normalization. Codd only touches on the basics of normalization in this first paper. Suffice it to say, one important goal of normalization is to create flatter and less-redundant relations, creating simpler sentences from our relations. In Codd’s first-order normalization, what he introduces in his paper, we’d rather say \"Doug rents Top Gun at 3.99\" and \"Doug rents Terminator at 12.99\" instead of including both movies in the sentence. In relation terms, this would look like:",
      "Users Relations\n(user=doug, address=1234 BagbySt)\n\nMovies-Rented Relations\n(user=doug, movie=Top Gun, price=3.99)\n(user=doug, movie=Terminator, price=12.99)",
      "With simpler and normalized relations, we can use Codds new JOIN operator that can derive relations from other, simpler and normalized relations. This is extremely powerful. In other words with JOIN we can construct knowledge from smaller units of knowledge. For example, what if our database contained these relations:",
      "Peoples Relations\n(name=Socrates, Occupation=Philosopher)\n\nOccupation-Characteristic Relations\n(Occupation=Philosopher, Characteristics=Drunk)",
      "We can do a JOIN on Occupation=Philosopher and learn that Socrates was a drunk! This is neat because, in my Artificial Intelligence class in college, this kind of \"reasoning\" is presented as something advanced. In fact there’s a whole language, Prolog, that’s whole job is to take assertions about the world and give you extra facts. I had to learn all that stuff in AI when mundane-old SQL was right under my nose giving me the tools to learn anything I want about the world my relations represented!",
      "So now that we’ve presented this model, let’s reflect on how it matches up against Codd’s criticisms:",
      "Access Dependencies: As relations, data is split out into independent relations. There’s no \"links\" between them to follow.\n  Order Dependencies: The order of a set of relations is not specified. Sets in math don’t have a notion of order. In practice, a relational query language, like SQL, allows you to sort by whatever criteria you want\n  Index Dependencies: What’s an index? This data model does not mention indices. Indices are, for better-or-worse, a knob that gets tuned at a place the application does not interact with the database.",
      "In short, Codd created a beautiful abstraction that turned out to be reasonable to implement. Instead of building on what had been done with databases, he went to the roots of predicate calculus and created a clean and beautiful way of interacting with data.",
      "It’s a pretty singular and daring achievement in fact to assert \"this is the abstraction we should use\" and have it be largely successful. There was not much anticipating Codd’s invention of relational databases. It wasn’t, as is common today, a minor innovation on previous innovations. Instead it was simply Codds vision. Pretty cool stuff!",
      "The wrench-in-the-spokes of-course came when we decided to build horizontally scalable systems \"webscale\" as all the kiddies say. Doing a JOIN over a distributed system turns out not to scale well. Because of this, we started to see old themes reintroduced, specifically the denormalized, hierarchical databases of yore in the form of many of today’s NoSQL stores.",
      "NoSQL – Have we come full circle?",
      "Yeah… in a way we’ve come full circle back to pre SQL days. Hierarchical data stores, once forgotten, have risen again. They certainly suffer from the same kinds of criticisms that Codd lays out. They leak lower-level details, access dependencies, order dependencies, and index dependencies up to the user application. Using Cassandra databases requires holding in high respect the primary row key, using that-and-only-that to access data. These are index and access dependencies. A database like HBase will impose order dependencies, storing your rows sorted based on the row key, allowing/requiring you to consider the order when your application scans keys.",
      "Most importantly, when using a NoSQL store, your opportunities for ad-hoc querying becomes much more limited. Whereas relational databases hold sacred the goal of allowing you to define more complex information from more primitive information, NoSQL databases require performing massive batch processing through map-reduce to create derived information.",
      "Moreover, the lack of normalization requires potentially large amounts of data duplication. If tracking users renting videos in a hierarchical data store, how does one update the inventory successfully? If users own videos, does this require a map reduce job to visit every user record and update the count. Will this occur atomically across all the records?",
      "In short, denormalization stinks!",
      "Ok Eeeyore",
      "Databases are hard",
      "But denormalization is extremely powerful.",
      "When we can denormalize we can do amazing things with horizontal scalability. We can simplify the task of atomically moving data around boxes. We can avoid having to worry about difficult distributed joins (all the data is in one place). Databases like Cassandra can be built that do beautiful things with distributed systems.",
      "My point is that it’s dangerous to just grab a database off the shelf because its fun and trendy. You might end up creating a lot of technical debt for yourself. Do you know you’re handling an extremely high volume of a single somewhat-well-defined \"thing\"? Then the hierarchical model might be an amazing bet for you. Do you have many different kinds of records that link together, but don’t necessarily contain parts of the whole? A relational data store might be the ideal solution.",
      "With Quepid, I started out using Redis as the primary database. Its been amazing for doing some simple storage of search query/document ratings. Once we started needing to add user accounts. With user accounts relating to sets of search problems, it became clear that Redis wasn’t good at this. My code started to look like a pseudo-relational database on top of Redis. So I’ve been working now (not a year from now) to transition to FoundationDB. FoundationDB will be great for my problem because I can mix-match different kinds of models in a single database, storing both hierarchical ratings for documents, and relational users.",
      "In conclusion, \"a foolish consistency is a hobgoblin of little minds\". Its important as a data architect to understand these tradeoffs in your bones. Its easy to get lost in hype and marketing material, but really appreciate what you’re gaining and measure if that gain is worth the potentially very real and painful cost.",
      "Do you need help choosing the right data store for your application? Contact us!. We can help you! Do you have feedback for this article? I’d love to hear what you have to say or critique and learn more about database history! So please leave a comment."
    ],
    "summary_t": ""
  },
  {
    "id": "bd4b28b15ad9867f2168d520c8c0ed1c",
    "url_s": "https://opensourceconnections.com/blog/2013/12/11/holiday-open-house-at-osc-come-share-in-the-fun/",
    "title": "Holiday Open House at OSC – Come Share in the Fun",
    "content": [
      "Hello Cville Tech Friends!",
      "You are in an exclusive group. Thats right, if you are reading this blog post, then you are one of the select few who are invited to the OpenSource Connections international headquarters for our 2nd annual Holiday Open House!",
      "When: Tuesday, December 17th from 5PM – 7PM",
      "Where: OpenSource Connections international Headquarters",
      "Why: Because we luuuuuuv Charlottesville and we luuuuuuuv tech.",
      "How: With beer and yummy food of course. I hear that well be featuring beer from 3-Notched Brewery. And if that doesnt work well probably drop by Sams and pick up a couple of cases of PBR.",
      "We hope you can make it! Here, add it to your calendar now!",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "e39a6de15e5c0acadb4bdb50fd8dd60b",
    "url_s": "https://opensourceconnections.com/blog/2013/12/19/subredis-keyspaces-in-redis/",
    "title": "Subredis – Keyspaces in Redis",
    "content": [
      "We’ve been using Redis extensively for Quepid, our search relevancy collaboration canvas. Redis is a convenient and efficient way to interact with in-memory data structures. In fact, we’ve been using it as our primary database.",
      "… and called it SubRedis!11",
      "One thing that’s stuck out as problematic for us is isolating and segmenting parts of our application within Redis. I want to give each part of Quepids backend a safe and reliable Redis sandbox, without running 10 instances of Redis on my box. For example, if the user database does a flushdb, I want it to be only resetting the set of users. Similarly, I don’t want to worry that keys within the user database will clash with keys with the query-ratings database. Isolation is a good thing!",
      "Introducing SubRedis",
      "So what I’ve done is create Subredis – a simple Python keyspace wrapper around Redis. In short, SubRedis lets you do this:",
      "r = redis.from_url(\"redis://localhost:6379\")\nsub = SubRedis(\"keyspacename\", r) #My very own Redis!\nsub.set(\"foo\", \"bar\")\nsub.get(\"foo\")\nsub.flushdb()",
      "Each SubRedis plays entirely in its own keyspace. Here for example, \"flushdb\" will only ever flush keys in \"keyspacename\" and the key \"foo\" will exist only here in this keyspace.",
      "Redis has traditionally been a very flat key-value store. However, SubRedis allows us to create hierarchy within Redis. For example, to get the user’s set of queries for Quepid, we can easily do:",
      "queryRedis = SubRedis(userId + \"-queries\", redis)",
      "And viola, each user has their own Redis instance ready to go and wholly owned by them. Moreover, theres no reason a SubRedis cannot in turn aggregate a SubRedis itself, achieving arbitrary levels of hiererachy:",
      "queryRedis = SubRedis(userId + \"-queries\", redis)\nratingsRedis = SubRedis(queryId + \"-ratings\", queryRedis)",
      "Pretty powerful stuff!",
      "More Details",
      "Subredis is a simple wrapper, all it does is prepend the name \"keyspacename\" to keys it receives. So a call in redis-cli to \"keys *\" before flushdb above would yield:",
      "keyspacename_foo",
      "SubRedis implements most of the methods of redispy’s StrictRedis with a few exceptions where it may not be possible to intercept the keys. Every data structure is supported. Transactions are supported via pipelines. We currently don’t support many of the admin methods for safety sake (ie bgsave etc) and Lua scripting is not feasible with this approach. As SubRedis uses the \"_\" delimiter, its not encouraged to create key names with \"_\" in them.",
      "Nevertheless, this has been extremely convenient when developing Quepid. Each component can trust in having its own isolated Redis instance, separated from other components.",
      "So if you’re a Redis user, give SubRedis a try! Simply pip install subredis and let me know what you think (and if you find bugs!). I hope to blog about the fun Python metaprogramming that made this relatively simple to perform in the future.",
      "And if you’re one of those people still wondering what data structure is right for your job, let us know! It may be Redis, or it could be any number of other things we specialize in (Cassandra, Solr, Hadoop, etc)."
    ],
    "summary_t": ""
  },
  {
    "id": "2460f412b0492413f259a85a008d1b72",
    "url_s": "https://opensourceconnections.com/blog/2014/01/02/presentation-building-client-side-search-applications/",
    "title": "Presentation: Building Client-Side Search Applications",
    "content": [
      "Last year I had the opportunity to speak at Lucene/Solr Revolution in Dublin, Ireland. My talk explores the benefits of client-side search applications, gives a case study of a project that Opensource Connections did for the Patent and Trademark Office, and walks through a tutorial on how to set up a search application with Spyglass.",
      "Building Client-side Search Applications with Solr  from lucenerevolution"
    ],
    "summary_t": ""
  },
  {
    "id": "11c869b966129699c586aa77c9054ab4",
    "url_s": "https://opensourceconnections.com/blog/2014/01/06/using-vagrant-with-angularjs/",
    "title": "Using Vagrant with AngularJS",
    "content": [
      "… and NodeJS, and Yeoman, and Compass, and …",
      "Vagrant is a super way to keep project dependencies isolated and explicit so you can easily jump between projects. AngularJS is an great framework that’s sort of an MVC (do people still say \"MVC\"?) for JavaScript development. I won’t do either project justice by going into any of their details here. Instead I’ll describe how to quickly bootstrap an AngularJS development environment so you can do a little more hacking and a little less installing.",
      "Here’s a preview:",
      "Install Vagrant if you haven’t already\n  Grab the two short files I’l gist\n  Do some console stuff to generate a Yeoman AngularJS project\n  Go to http://localhost:9000/",
      "Vagrant",
      "To start with, make a project folder and pull down this Vagrantfile:",
      "This starts a new Ubuntu VM and installs the a NodeJS puppet module (it’s a little irksome you can’t use puppet to install puppet modules, but I guess at some point the turtles have to end.) By the way, I had a little trouble with the particular version of Ubuntu and the PuppetLabs NodeJS module, which is why I’m using willdurand’s. NPM wasn’t getting installed.",
      "Note that I’m forwarding ports 9000 and 35729 so that grunt can serve and live-reload the app to the host. Another way to get that to happen is to start an ssh tunnel into the VM with something like: vagrant ssh -- -L 9000:localhost:9000 -L 35729:localhost:35729 -N",
      "Puppet",
      "Then, make a \"puppet\" folder and put this site.pp file in there:",
      "This installs git, bower, grunt-cli, gulp, and Compass. It also adds the NPM global bin directory to users’ $PATH. Edited on 2015-02-12: Yeoman was no longer working and I needed to update some required packages.",
      "Command Prompt",
      "Finally, use Yeoman to start an AngularJS project:",
      "I could’ve put the \"npm install\" stuff into puppet, but I was thinking I might use this for EmberJS development too. Also, since I’m forwarding grunt’s ports to the host I want to serve on 0.0.0.0 rather than localhost. The only other peculiarity I should note is that the VM I’m using doesn’t have any browsers installed so karma testing doesn’t work. To-Do!",
      "The thing that I hope sticks with you beyond bootstrapping Angular development is how easy it is to encapsulate and script all such bootstrapping. This helps projects get going so much quicker than even the most lucid README file."
    ],
    "summary_t": "Vagrant is a super way to keep project dependencies isolated and explicit so you can easily jump between projects. AngularJS is an great framework that’s sor..."
  },
  {
    "id": "b362e2d5ad69e7208a3c0899e8c1cf1e",
    "url_s": "https://opensourceconnections.com/blog/2014/01/20/build-your-own-custom-lucene-query-and-scorer/",
    "title": "Build your own Custom Lucene query and scorer",
    "content": [
      "Every now and then we’ll come across a search problem that can’t simply be solved with plain Solr relevancy. This usually means a customer knows exactly how documents should be scored. They may have little tolerance for close approximations of this scoring through Solr boosts, function queries, etc. They want a Lucene-based technology for text analysis and performant data structures, but they need to be extremely specific in how documents should be scored relative to each other.",
      "Well for those extremely specialized cases we can prescribe a little out-patient surgery to your Solr install – building your own Lucene Query.",
      "This is the Nuclear Option",
      "Before we dive in, a word of caution. Unless you just want the educational experience, building a custom Lucene Query should be the \"nuclear option\" for search relevancy. It’s very fiddly and there are many ins-and-outs. If you’re actually considering this to solve a real problem, you’ve already gone down the following paths:",
      "You’ve utilized Solr’s extensive set of query parsers & features including function queries, joins, etc. None of this solved your problem\n  You’ve exhausted the ecosystem of plugins that extend on the capabilities in (1). That didn’t work.\n  You’ve implemented your own query parser plugin that takes user input and generates existing Lucene queries to do this work. This still didn’t solve your problem.\n  You’ve thought carefully about your analyzers – massaging your data so that at index time and query time, text lines up exactly as it should to optimize the behavior of existing search scoring. This still didn’t get what you wanted.\n  You’ve implemented your own custom Similarity that modifies how Lucene calculates the traditional relevancy statistics – query norms, term frequency, etc.\n  You’ve tried to use Lucene’s CustomScoreQuery to wrap an existing Query and alter each documents score via a callback. This still wasn’t low-level enough for you, you needed even more control.",
      "If you’re still reading you either think this is going to be fun/educational (good for you!) or you’re one of the minority that must control exactly what happens with search. If you don’t know, you can of course contact us for professional services.",
      "Ok back to the action…",
      "Refresher – Lucene Searching 101",
      "Recall that to search in Lucene, we need to get a hold of an IndexSearcher. This IndexSearcher performs search over an IndexReader. Assuming we’ve created an index, with these classes we can perform searches like in this code:",
      "Directory dir = new RAMDirectory();\nIndexReader idxReader = new IndexReader(dir);\nidxSearcher idxSearcher = new IndexSearcher(idxReader)\nQuery q = new TermQuery(new Term(\"field\", \"value\"));\nidxSearcher.search(q);",
      "Let’s summarize the objects we’ve created:",
      "Directory – Lucene’s interface to a file system. This is pretty straight-forward. We won’t be diving in here.\n  IndexReader – Access to data structures in Lucene’s inverted index. If we want to look up a term, and visit every document it exists in, this is where we’d start. If we wanted to play with term vectors, offsets, or anything else stored in the index, we’d look here for that stuff as well.\n  IndexSearcher – wraps an IndexReader for the purpose of taking search queries and executing them.\n  Query – How we expect the searcher to perform the search, encompassing both scoring and which documents are returned. In this case, we’re searching for \"value\" in field \"field\". This is the bit we want to toy with",
      "In addition to these classes, we’ll mention a support class exists behind the scenes:",
      "Similarity – Defines rules/formulas for calculating norms at index time and query normalization.",
      "Now with this outline, let’s think about a custom Lucene Query we can implement to help us learn. How about a query that searches for terms backwards. If the document matches a term backwards (like ananab for banana), we’ll return a score of 5.0. If the document matches the forwards version, let’s still return the document, with a score of 1.0 instead. Well call this Query \"BackwardsTermQuery\".",
      "This example is hosted here on github.",
      "A tale of 3 classes – A Query, A Weight, and a Scorer",
      "Before we sling code, let’s talk about general architecture.",
      "A Lucene Query follows this general structure:",
      "A custom Query class, inheriting from Query\n  A custom Weight class, inheriting from Weight\n  A custom Scorer class inheriting from Scorer",
      "These three objects wrap each other. A Query creates a Weight, and a Weight in turn creates a Scorer.",
      "A Query is itself a very straight-forward class. One of its main responsibilities when passed to the IndexSearcher is to create a Weight instance. Other than that, there are additional responsibilities to Lucene and users of your Query to consider, that we’ll discuss in the \"Query\" section below.",
      "A Query creates a Weight. Why? Lucene needs a way to track IndexSearcher level statistics specific to each query while retaining the ability to reuse the query across multiple IndexSearchers. This is the role of the Weight class. When performing a search, IndexSearcher asks the Query to create a Weight instance. This instance becomes the container for holding high-level statistics for the Query scoped to this IndexSearcher (we’ll go over these steps more in the \"Weight\" section below). The IndexSearcher safely owns the Weight, and can abuse and dispose of it as needed. If later the Query gets reused by another IndexSearcher, a new Weight simply gets created.",
      "Once an IndexSearcher has a Weight, and has calculated any IndexSearcher level statistics, the IndexSearcher’s next task is to find matching documents and score them. To do this, the Weight in turn creates a Scorer. Just as the Weight is tied closely to an IndexSearcher, a Scorer is tied to an individual IndexReader. Now this may seem a little odd – in our code above the IndexSearcher always has exactly one IndexReader right? Not quite. See, a little hidden implementation detail is that IndexReaders may actually wrap other smaller IndexReaders – each tied to a different segment of the index. Therefore, an IndexSearcher needs to have the ability score documents across multiple, independent IndexReaders. How your scorer should iterate over matches and score documents is outlined in the \"Scorer\" section below.",
      "So to summarize, we can expand the last line from our example above…",
      "idxSearcher.search(q);",
      "… into this psuedocode:",
      "Weight w = q.createWeight(idxSearcher);\n// IndexSearcher level calculations for weight\nForeach IndexReader idxReader:\n    Scorer s = w.scorer(idxReader);\n    // collect matches and score them",
      "Now that we have the basic flow down, let’s pick apart the three classes in a little more detail for our custom implementation.",
      "Our Custom Query",
      "What should our custom Query implementation look like? Query implementations always have two audiences: (1) Lucene and (2) users of your Query implementation. For your users, expose whatever methods you require to modify how a searcher matches and scores with your query. Want to only return as a match 1/3 of the documents that match the query? Want to punish the score because the document length is longer than the query length? Add the appropriate modifier on the query that impacts the scorer’s behavior.",
      "For our BackwardsTermQuery, we don’t expose accessors to modify the behavior of the search. The user simply uses the constructor to specify the term and field to search. In our constructor, we will simply be reusing Lucene’s existing TermQuery for searching individual terms in a document.",
      "private TermQuery backwardsQuery;\nprivate TermQuery forwardsQuery;\n\npublic BackwardsTermQuery(String field, String term) {\n    // A wrapped TermQuery for the reverse string\n    Term backwardsTerm = new Term(field, new StringBuilder(term).reverse().toString());\n    backwardsQuery = new TermQuery(backwardsTerm);\n    // A wrapped TermQuery for the Forward\n    Term forwardsTerm = new Term(field, term);\n    forwardsQuery = new TermQuery(forwardsTerm);\n}",
      "Just as importantly, be sure your Query meets the expectation of Lucene. Most importantly, you MUST override the following.",
      "createWeight()\n  hashCode()\n  equals()",
      "The method createWeight() we’ve discussed. This is where you’ll create a weight instance for an IndexSearcher. Pass any parameters that will influence the scoring algorithm, as the Weight will in turn be creating a searcher.",
      "Even though they are not abstract methods, overriding the hashCode()/equals() methods is very important. These methods are used by Lucene/Solr to cache queries/results. If two queries are equal, there’s no reason to rerun the query. Running another instance of your query could result in seeing the results of your first query multiple times. You’ll see your search for \"peas\" work great, then you’ll search for \"bananas\" and see \"peas\" search results. Override equals() and hashCode() so that \"peas\" != bananas.",
      "Our BackwardsTermQuery implements createWeight() by creating a custom BackwardsWeight that we’ll cover below:",
      "@Override\npublic Weight createWeight(IndexSearcher searcher) throws IOException {\n    return new BackwardsWeight(searcher);\n}",
      "BackwardsTermQuery has a fairly boilerplate equals() and hashCode() that passes through to the wrapped TermQuerys. Be sure equals() includes all the boilerplate stuff such as the check for self-comparison, the use of the super equals operator, the class comparison, etc etc. By using Lucenes unit test suite, we can get a lot of good checks that our implementation of these is correct.",
      "@Override\npublic boolean equals(Object other) {\n    if (this == other) {\n        return true;\n    }\n    if (!super.equals(other)) {\n        return false;\n    }\n    if (getClass() != other .getClass()) {\n        return false;\n    }\n    BackwardsTermQuery otherQ = (BackwardsTermQuery)(other);\n    if (otherQ.getBoost() != getBoost()) {\n        return false;\n    }\n    return otherQ.backwardsQuery.equals(backwardsQuery) && otherQ.forwardsQuery.equals(forwardsQuery);\n}\n\n@Override\npublic int hashCode() {\n    return super.hashCode() + backwardsQuery.hashCode() + forwardsQuery.hashCode();\n}",
      "Our Custom Weight",
      "You may choose to use Weight simply as a mechanism to create Scorers (where the real meat of search scoring lives). However, your Custom Weight class must at least provide boilerplate implementations of the query normalization methods even if you largely ignore what is passed in:",
      "getValueForNormalization\n  normalize",
      "These methods participate in a little ritual that IndexSearcher puts your Weight through with the Similarity for query normalization. To summarize the query normalization code in the IndexSearcher:",
      "float v = weight.getValueForNormalization();\nfloat norm = getSimilarity().queryNorm(v);\nweight.normalize(norm, 1.0f);",
      "Great, what does this code do? Well a value is extracted from Weight. This value is then passed to a Similarity instance that \"normalizes\" that value. Weight then receives this normalized value back. In short, this is allowing IndexSearcher to give weight some information about how its \"value for normalization\" compares to the rest of the stuff being searched by this searcher.",
      "This is extremely high level, \"value for normalization\" could mean anything, but here it generally means \"what I think is my weight\" and what Weight receives back is what the searcher says \"no really here is your weight\". The details of what that means depend on the Similarity and Weight implementation. It’s expected that the Weight’s generated Scorer will use this normalized weight in scoring. You can chose to do whatever you want in your own Scorer including completely ignoring what’s passed to normalize().",
      "While our Weight isn’t factoring into the scoring calculation, for consistency sake, we’ll participate in the little ritual by overriding these methods:",
      "@Override\npublic float getValueForNormalization() throws IOException {\n    return backwardsWeight.getValueForNormalization() +\n            forwardsWeight.getValueForNormalization();\n}\n\n@Override\npublic void normalize(float norm, float topLevelBoost) {\n    backwardsWeight.normalize(norm, topLevelBoost);\n    forwardsWeight.normalize(norm, topLevelBoost);\n}",
      "Outside of these query normalization details, and implementing \"scorer\", little else happens in the Weight. However, you may perform whatever else that requires an IndexSearcher in the Weight constructor. In our implementation, we don’t perform any additional steps with IndexSearcher.",
      "The final and most important requirement of Weight is to create a Scorer. For BackwardsWeight we construct our custom BackwardsScorer, passing scorers created from each of the wrapped queries to work with.",
      "@Override\npublic Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,\n        boolean topScorer, Bits acceptDocs) throws IOException {\n    Scorer backwardsScorer = backwardsWeight.scorer(context, scoreDocsInOrder, topScorer, acceptDocs);\n    Scorer forwardsScorer = forwardsWeight.scorer(context, scoreDocsInOrder, topScorer, acceptDocs);\n    return new BackwardsScorer(this, context, backwardsScorer, forwardsScorer);\n}",
      "Our Custom Scorer",
      "The Scorer is the real meat of the search work. Responsible for identifying matches and providing scores for those matches, this is where the lion share of our customization will occur.",
      "It’s important to note that a Scorer is also a Lucene DocIdSetIterator. A DocIdSetIterator is a cursor into a set of documents in the index. It provides three important methods:",
      "docID() – what is the id of the current document? (this is an internal Lucene ID, not the Solr \"id\" field you might have in your index)\n  nextDoc() – advance to the next document\n  advance(target) – advance (seek) to the target",
      "One uses a DocIdSetIterator by first calling nextDoc() or advance() and then reading the docID to get the iterator’s current location. The value of the docIDs only increase as they are iterated over.",
      "By implementing this interface a Scorer acts as an iterator over matches in the index. A Scorer for the query \"field1:cat\" can be iterated over in this manner to return all the documents that match the cat query. In fact, if you recall from my article, this is exactly how the terms are stored in the search index.",
      "You can chose to either figure out how to correctly iterate through the documents in a search index, or you can use the other Lucene queries as building blocks. The latter is often the simplest. For example, if you wish to iterate over the set of documents containing two terms, simply use the scorer corresponding to a BooleanQuery for iteration purposes.",
      "The first method of our scorer to look at is docID(). It works by reporting the lowest docID() of our underlying scorers. This scorer can be thought of as being \"before\" the other in the index, and as we want to report numerically increasing docIDs, we always want to chose this value:",
      "@Override\npublic int docID() {\n    int backwordsDocId = backwardsScorer.docID();\n    int forwardsDocId = forwardsScorer.docID();\n    if (backwordsDocId <= forwardsDocId && backwordsDocId != NO_MORE_DOCS) {\n        currScore = BACKWARDS_SCORE;\n        return backwordsDocId;\n    } else if (forwardsDocId != NO_MORE_DOCS) {\n        currScore = FORWARDS_SCORE;\n        return forwardsDocId;\n    }\n    return NO_MORE_DOCS;\n}",
      "Similarly, we always want to advance the scorer with the lowest docID, moving it ahead. Then, we report our current position by returning docID() which as weve just seen will report the docID of the scorer that advanced the least in the nextDoc() operation.",
      "@Override\npublic int nextDoc() throws IOException {\n    int currDocId = docID();\n    // increment one or both\n    if (currDocId == backwardsScorer.docID()) {\n        backwardsScorer.nextDoc();\n    }\n    if (currDocId == forwardsScorer.docID()) {\n        forwardsScorer.nextDoc();\n    }\n    return docID();\n}",
      "In our advance() implementation, we allow each Scorer to advance. An advance() implementation promises to either land docID() exactly on or past target. Our call to docID() after we call advance will return either that one or both are on target, or it will return the lowest docID past target.",
      "@Override\npublic int advance(int target) throws IOException {\n    backwardsScorer.advance(target);\n    forwardsScorer.advance(target);\n    return docID();\n}",
      "What a Scorer adds on top of DocIdSetIterator is the \"score\" method. When score() is called, a score for the current document (the doc at docID) is expected to be returned. Using the full capabilities of the IndexReader, any number of information stored in the index can be consulted to arrive at a score either in score() or while iterating documents in nextDoc()/advance(). Given the docId, you’ll be able to access the term vector for that document (if available) to perform more sophisticated calculations.",
      "In our query, we’ll simply keep track as to whether the current docID is from the wrapped backwards term scorer, indicating a match on the backwards term, or the forwards scorer, indicating a match on the normal, unreversed term. Recall docID() is always called on advance/nextDoc. Youll notice we update currScore in docID, updating it every time the document advances.",
      "@Override\npublic float score() throws IOException {\n    return currScore;\n}",
      "A Note on Unit Testing",
      "Now that we have an implementation of a search query, we’ll want to test it! I highly recommend using Lucene’s test framework. Lucene will randomly inject different implementations of various support classes, index implementations, to throw your code off balance. Additionally, Lucene creates test implementations of classes such as IndexReader that work to check whether your Query correctly fulfills its contract. In my work, I’ve had numerous cases where tests would fail intermittently, pointing to places where my use of Lucene’s data structures subtly violated the expected contract.",
      "An example unit test is included in the github project associated with this blog post.",
      "Wrapping Up",
      "That’s a lot of stuff! And I didn’t even cover everything there is to know! As an exercise to the reader, you can explore the Scorer methods cost() and freq(), as well as the rewrite() method of Query used optionally for optimization.",
      "Additionally, I haven’t explored how most of the traditional search queries end up using a framework of Scorers/Weights that don’t actually inherit from Scorer or Weight known as \"SimScorer\" and \"SimWeight\". These support classes consult a Similarity instance to customize calculation certain search statistics such as tf, convert a payload to a boost, etc.",
      "In short theres a lot here! So tread carefully, there’s plenty of fiddly bits out there! But have fun! Creating a custom Lucene query is a great way to really understand how search works, and the last resort short in solving relevancy problems short of creating your own search engine.",
      "And if you have relevancy issues, contact us! If you dont know whether you do, our search relevancy product, Quepid – might be able to tell you!"
    ],
    "summary_t": ""
  },
  {
    "id": "4acba62bcb47b4754ddc2c7f9bfc3edd",
    "url_s": "https://opensourceconnections.com/blog/2007/03/30/business-process-management-and-workflow/",
    "title": "Business Process Management and Workflow",
    "content": [
      "Business Process Management (BPM) is increasingly appearing in technical articles and executive seminars, but most of the developers I talk to misinterpret BPM to be MBA jargon for workflow management (WFM). Thats not surprising since workflows handle tasks in a single application, and for the majority of organizations that is sufficient. However, BPM is a much larger concept and addresses things like enterprise orchestration rather than document approval states. Here is an excellent article describing the differences in more detail."
    ],
    "summary_t": ""
  },
  {
    "id": "9051418501fc919a09cce71a75992028",
    "url_s": "https://opensourceconnections.com/blog/2014/01/27/opensource-connections-2013-year-in-review/",
    "title": "OpenSource Connections 2013 Year in Review!",
    "content": [
      "Well 2013 was a busy year for OpenSource Connections! It was certainly another great year for technology leadership. In Fall alone, we gave nine talks at leading technology conferences on topics ranging from Cassandra to Solr to semantic search. We didnt slack with the grunt work either – completing dozens of search, discovery, and analytics projects big and small. We developed a new product, Quepid, to solve the problem of search relevancy testing. And of course, we contributed to/created a few open source projects as well.",
      "Learn more at our 2013 overview page . And if its been a while since youve been in touch, contact us! and let us know how we can help you in 2014.",
      "Ill leave you with a few videos from OSC folks sharing their insight at industry conferences. Enjoy!"
    ],
    "summary_t": ""
  },
  {
    "id": "c80075092c2a0f40f29592ae25c3a2ed",
    "url_s": "https://opensourceconnections.com/blog/2014/01/30/cut-through-the-clutter-use-clustering-to-suggest-facets/",
    "title": "Cut Through the Clutter: Use Clustering to Suggest Facets",
    "content": [
      "I have a concrete counter top in our kitchen that is rather different from most concrete counter tops because it is coloured with a thin surface layer of multiple shades of concrete dye all mixed together. It has a wonderful earthy look to it, but Ive discovered that it doesnt stand up very well to my wife swinging around heavy pots and pans!",
      "Scuffs on concrete countertop",
      "I tried a sanded grout with the brand name \"Mapei\" from Lowes Home Improvement, but it didnt really have the effect I wanted, leaving a very textured patch that doesnt take the concrete sealer that makes the counter look shiny.",
      "Patching the countertop with the sanded grout. Not so good.",
      "So I went onto Lowess website and searched for Mapei Unsanded Grout, and received back 30 results:",
      "Lowes search results for Mapei Unsanded Grout",
      "While on the face of it, 30 results for my query sounds like a great thing, when I scrolled down the page I was overwhelmed by all the choices! It seemed like I had 30 different choices to pick from. Go ahead, click the link to search Lowes.com and try it yourself!",
      "I scrolled up and down a couple of times and started realizing that I didnt have 30 choices. I actually had TWO choices: a \"Unsanded Powder Grout\" and a \"Premium Unsanded Powder Grout\", and they come in a variety of colors and sizes. So here I am, frantically scrolling up and down trying to build a mental model of all my choices, when I think \"ah, facets should help me narrow down my choices\". I knew I was doing just a bit of patching of the countertop, so I wanted the smallest amount of grout possible. And I knew I wanted a rich earthy red/brown color to match the existing concrete counter top. Unfortunately \"Size\" is not a facet option offered by Lowes.",
      "So I thought, instead of trying to build a mental model of all thirty results in my head, why dont I leverage clustering to see if I can pull out of the unstructured data some shared clusters that could act as facets? This blog post and the accompanying code are going to walk you through the steps I took. All the links assume you have Solr running on your localhost.",
      "1) Extracting the Data",
      "I wrote a very simple Ruby script that queries Lowes.com for \"mapei unsanded grout\", and parses out the results and stores them in a Solr index. Fire up Solr by running ./start.sh. Then run the script ruby download_lowes_data.rb. Warning, you may have to install some gems!",
      "2) View the Clusters",
      "I leveraged the Solr built in UI Solritas to expose the data about the clusters. Browse to http://localhost:8983/solr/browse to see the results of clustering the data:",
      "",
      "Its not very exciting at this point. Ive told Solr to cluster primarily on the title, and secondarily on the product detail bullets. And it has accurately identified that there are a number of grouts that come in the 10 lb. bag. And that we have some premium grouts. More useful clustering would be to seperate out those that are 10 lb., and those that are 25 lb. Although, one thing to note is that by clustering, we did successfully extract the root product title for two of the items, and dropped the color name portion. The products titled MAPEI 10 lb. Keracolor U Premium Unsanded Grout – Lt.Almond #49 and MAPEI 10 lb. Keracolor U Premium Unsanded Grout – Chocolate #07 both have the correct root title of Keracolor U Premium Unsanded Grout in the clustering list.",
      "3) Adding in Facets",
      "I had turned on faceting on for the Item Number and Model Number fields, and set the minimum facet count to display to 2 because I only care about item numbers or model numbers that are shared by multiple products. While each product does have a unique Model Number, what was interesting to discover was that there are 14 out of the 30 products share the same Item Number: 185276. It appears that Item Number is assigned by the manufacturer, not Lowes, and identifies a single product, regardless of size or color choice, while Model Number is the unique SKU for that product.",
      "",
      "I clicked the Item # 185276 facet and my clusters started looking much more interesting:",
      "!https://img.skitch.com/20120302-nq69pr7uqppe4886cumgiwsagb.png!",
      "I now have 5 clusters identified: Unsanded Powdered Grout, Keracolor U Premimum Unsanded Grout, MAPEI 10 Lbs, MAPEI 25 Lbs, and Help Contribute to LEED Certification of Projects. The first four clusters all look really useful as discriminators, as I can see that I have a set of products that are 10 lb., 25 lb., and either the Unsanded Powder Grout or the Premium Unsanded Grout. Even the fifth facet about LEED certification could be useful in helping me identify products that contribute towards LEED certification. One thing I noticed though was that the total unique number of clustered items didnt match the number of products returned by the faceted query. Turns out the clustering results are returned via an AJAX call to the /clustering handler, and it has a limit of 10 rows. So I bumped it up to 100 so that the results of clustering would be over all the documents returned, not just the top 10. Here is the configuration from solrconfig.xml:",
      "<requesthandler name=\"/clustering\" startup=\"lazy\" enable=\"${solr.clustering.enabled:false}\" class=\"solr.SearchHandler\">\n    <lst name=\"defaults\">\n        <bool name=\"clustering\">true</bool>\n        <str name=\"clustering.engine\">default</str>\n        <bool name=\"clustering.results\">true</bool>\n        <str name=\"carrot.title\">title_t</str>\n        <str name=\"carrot.url\">id</str>\n        <str name=\"carrot.snippet\">product_bullets_txt</str>\n        <bool name=\"carrot.produceSummary\">true</bool>\n        <bool name=\"carrot.outputSubClusters\">false</bool>\n        <str name=\"defType\">edismax</str>\n        <str name=\"qf\">text^0.5 product_bullets_txt^1.0 title_t^1.2 model_number_s^1.5 item_number_s^1.5 id^10.0</str>\n        <str name=\"q.alt\">*:*</str>\n        <str name=\"rows\">100</str>\n        <str name=\"fl\">*,score</str>\n    </lst>\n    <arr name=\"last-components\">\n        <str>clustering</str>\n    </arr>\n</requesthandler>",
      "4) Playing with Clustering Options",
      "The /clustering request handler makes it easy to play with the various clustering options. The clustering output is below the <result name=\"response\"></result> search results XML stanza, so scroll down! The basic faceted query that weve been doing is:",
      "http://localhost:8983/solr/clustering?&q=:&fq=item_number_s:%22Item+%23%3A+185276%22",
      "The primary source of data is the title, followed by the product bullets. If we disable the carrot parameter for summarizing the product bullets carrot.produceSummary=false, we get more clusters:",
      "http://localhost:8983/solr/clustering?&q=:&fq=item_number_s:%22Item+%23%3A+185276%22&carrot.produceSummary=false",
      "including clusters for Brick Paver and Glass and Clay Tiles. Its great that there are more clusters, but you can see they become less interesting unless you are specifically looking for those things.",
      "Another way of filtering down the volume of suggested clusters is to only cluster on the title field by blanking out the snippets field by setting carrot.snippet= to be blank:",
      "http://localhost:8983/solr/clustering?&q=:&fq=item_number_s:%22Item+%23%3A+185276%22&carrot.snippet= (notice the carrot.snippet= param to blank out snippets) removes the clusters based on the product bullets like Help Contribute to LEED Certification of Projects.",
      "Another way of changing what clusters are returned is to not filter what you cluster on by any specific item number. When you do a basic cluster on the full data set you get some great clusters on color:",
      "http://localhost:8983/solr/clustering?&q=*:*",
      "Lastly, you can play with different clustering engines, Solr comes with three of them, Lingo (the default), STC, and Kmeans options. Try both the full dataset and the faceted on item number dataset to see the different types of results from the clustering algorithms.",
      "STC seems to be very similar to the Lingo option, but does provide some clusters that have multiple labels. For example MAPEI 10 Lb and Premium Unsanded Grout, as well as a separate cluster of just MAPEI 10 lbs products. http://localhost:8983/solr/clustering?&q=:&fq=item_number_s:%22Item+%23%3A+185276%22&clustering.engine=stc http://localhost:8983/solr/clustering?&q=*:*&clustering.engine=stc",
      "KMeans returns clusters that are all over the place. Lots of labels, but I dont quite see the connection between the items that makes them all clustered. We have some clusters that are Tan, Chocolate, Cocoa, and three items associated with it, each one of the colors! http://localhost:8983/solr/clustering?&q=:&fq=item_number_s:%22Item+%23%3A+185276%22&clustering.engine=kmeans http://localhost:8983/solr/clustering?&q=*:*&clustering.engine=kmeans",
      "KMeans on this dataset seems to be the least useful.",
      "What does it mean?",
      "Well, first and foremost, it means that clustering can be a discovery tool for figuring out potential tags and facets for your documents. By facets, I mean potential ways of slicing through all the data. By tags, I mean a set of richer attributes about the products that is pulled out of the product descriptions.",
      "In this specific case, it really drove home that instead of 30 products with different sizes, colors, and types, that there are two products from a users perspective: MAPEI Unsanded Powdered Grout and MAPEI Keracolor U Unsanded Grout. These two products should have a set of drop downs for size and color. The simple bit of clustering we did suggested that size was something that could be extracted as potential tags.",
      "There are two ways to model these relationships. You could model the products in Solr either as two documents, one per product, and use dynamic fields to create multivalued fields for size and color. Or, if you need to index everything de-normalized the way Lowes has done it, then if you have identified some tags as distinct fields, then you can use field collapsing so that even though you may have 14 products all with the same name, but different sizes and colors you collapse on the item number, and then provide those data as distinguishing drop downs.",
      "Here is a sense of what field collapsing on item number will do, we have 17 documents. 16 are unique item numbers, plus the 17th is the collapsed version of the 14 products that all share item number 185276.",
      "http://localhost:8983/solr/browse?&q=%3A&wt=xml&group=true&group.field=item_number_s&group.main=true&rows=30",
      "Oh, and just so you dont think this is an easy problem to solve, Amazon, who normally is a wonderful example of search done right, has the same exact problem for Aqua Mix Grout Colorant",
      "Next Steps",
      "So after all that, my colleage Dan suggested that I needed an example of showing the payout of going through the effort of clustering. So I added a script that pulls back the clusters and stores them in a multi valued field tags_txt. tags_txt can now be treated as traditional facetsin the user interface. The two choices that seemed most interesting are clustering on all the products and just the products that share the Item Number 185276.",
      "Go ahead and run ruby insert_clustered_data.rb tags_smv is a multivalued string field that is perfect for faceting on, and should now have all the cluster labels stored. Reload the browse interface and you will now have a list of very reasonable facets to use derived by clustering:",
      "!https://img.skitch.com/20120316-kqp2d6q7ejh5xgmkj1fiy3prfg.png!",
      "Because we configured facet.mincount=2 in solrconfig.xml for the browse handler, any completely unique clusters are hidden from the list of facetable options, which reduces the amount of noise in the tags_smv listing.",
      "The Upshot of all this",
      "Compare the facet options from Lowes versus what I pulled out via clustering. The top 4 facets are very useful to someone trying to filter down the set of grout choices, and were pulled out of the content:",
      "",
      "Clustering can be a very useful tool, the trick is to figure out which clusters make sense, and which ones dont. And unfortunately, that is still something that appears to require human judgement!"
    ],
    "summary_t": ""
  },
  {
    "id": "5d930191a42a837b1e0209fd073962b2",
    "url_s": "https://opensourceconnections.com/blog/2014/02/13/what-is-cloud-meet-ocean/",
    "title": "What is \"Cloud meet Ocean\"?",
    "content": [
      "Cloud Meet Ocean from Matt Overstreet on Vimeo.",
      "Transcript:",
      "Whats the point of \"cloud meet ocean\"?",
      "We do a lot of large scale search systems. Often we have ourselves behind a \"firewall\" of the rest of the application stack. This means that all performance and scalability is, at least in part, dictated by the rest of our app stack.",
      "But, what if search could scale independently of the rest of our app?\nWhy waste our servers time concatenating strings of html for clients perfectly capable of doing that work on their own.",
      "Take this (example of html)",
      "<html>\n    <head>\n    <title>A new Page!</title>\n    </head>\n    <body>\n        <table>\n            <tr>\n                <td>row</td>\n                <td>row</td>\n                <td>row</td>\n            </tr><tr>\n                <td>your</td>\n                <td>boat</td>\n            </tr>\n        </table>\n    </body>\n</html>",
      "and reduce to this",
      "ditty: {[ \"row\", \"row\", \"row\"], [\"your\", \"boat\"]}",
      "Weve take to calling this \"cloud meet ocean.\" No matter how large your server cluster, a swarm of clients with a modern/fast javascript interpreter represent a massive cache of computational power.",
      "So, sure this applies to rendering html and similar issues. But were also excited about seeing where a super fast interpreted client/code thing and an incredible distribution network web/html can be applied to other classic distributed problems."
    ],
    "summary_t": "what if search could scale independently of the rest of our app? Why waste our servers time concatenating strings of html for clients perfectly capable of do..."
  },
  {
    "id": "ad8ba2c04a35040b4c578af9427d211c",
    "url_s": "https://opensourceconnections.com/blog/2014/02/14/come-learn-about-test-driven-search-relevancy-at-dc-solr-users-group/",
    "title": "Come Learn about Test Driven Search Relevancy at DC Solr Users Group!",
    "content": [
      "Is your search testing done in a spreadsheet? You need Quepid",
      "Do you struggle with troublesome search queries? Do you find it hard to balance the many conflicting requirements your search has to satisfy? Perhaps youre like many of our clients – you have a giant spreadsheet of queries with notes on which results are good, and which are poor. How do you translate that into a plan of action without breaking what currently works? How do you incorporate stakeholders in your search development and testing? How can you get to developers to build good search in coordination with non-developers deeply rooted in business and content expertise?",
      "We believe in a practice called \"Test Driven Relevancy\". And guess what – were speaking on this topic! Wed like to share how this can improve your ability to execute better search!",
      "So come see Doug Turnbull speak at the DC Area (NOVA) Apache Lucene/Solr Meetup on Wednesday February 26th to see how we apply this philosophy during customer engagements. Well be showcasing a new product now in Beta, Quepid that enables test-driven search relevancy. Quepid goes beyond \"search query spreadsheet\" – creating a search relevancy canvas that allows content experts and developers to work closely to paint a picture of what search should and can look like.",
      "So RSVP and bring us your search relevancy problems to discuss! If you cant attend contact us instead to chat."
    ],
    "summary_t": "Do you struggle with troublesome search queries? Do you find it hard to balance the many conflicting requirements your search has to satisfy? Perhaps you’re ..."
  },
  {
    "id": "6d8bdc06ae8478ace70f2fc7dfd53970",
    "url_s": "https://opensourceconnections.com/blog/2014/02/16/a-simple-promise-implementation-in-about-20-lines-of-javascript/",
    "title": "A Simple Promise Implementation in about 20 lines of Javascript",
    "content": [
      "Promises are a rather convenient way of creating readable asynchronous code. More importantly, implementing Promises is a great way to hone your ability to grok async code. So follow along, you might learn something just like I did!",
      "Whats a Promise?",
      "In short, promises clean up code full of callback chains. Instead of",
      "foo(function success1() {\n    bar(function success2() {\n        ...\n    });\n});",
      "Youd rather write more coherent code of the form:",
      "foo()\n.then(function success1() {\n})\n.then(function success2() {\n});",
      "The latter reads a bit better, especially as more behavior must be chained together. Specifically, we expect that when foos asynchronous bits are done, the success1 function will be called. As were chaining promises, we also expect that when success1s async bits are done, well call success2. Its a lot easier on the eyes than the large triangle of ugly that starts to form is the first example. Flat is better than nested of course!",
      "So how can we implement a Promise? How will foo, success1, and success2 create and return Promises? Moreover, how do we implement \"then\" so that it both acts as a method of the very first returned promise (ie the Promise instance foo returned) but also somehow connects with the Promise instances that might be created/returned in the yet-to-be called success1 and success2 callbacks? Brain hurt yet?",
      "Working backwards – Implementation of then",
      "The first thing to take apart is our implementation of \"then\". Assuming that \"foo\" has somehow newd a Promise, and returned it, what happens when we call then on this Promise? Lets sneak a peak:",
      "this.then = function(wrappedFn, wrappedThis) {\n  this.next = new Promise(wrappedFn, wrappedThis);\n  return this.next;\n};",
      "Interesting, then takes another callback (and an optional \"this\" to bind to the callback when called). With that callback, another promise is created and chained to this one. After this is called:",
      "foo()\n.then(function success1() {\n})",
      "The Promise returned by foo represents a linked list of size 2",
      "foo's Promise -> promise created by first then",
      "The final \"then\" is actually a method call on the Promise instance returned by the last then and simply adds another link to the list:",
      "foo's Promise -> promise created by first then -> promise created by second then",
      "A Promise wraps the function passed into it (ie success1 & success2), so our linked-list looks something like:",
      "foo's Promise -> promise created by first then -> promise created by second then\n                 (wraps success1)                 (wraps success2)",
      "Remember this is all asynchronous. Setting up this linked-list is done well before any of the wrapped functions are executed. To our linked list, success1 and success2 are entirely black boxes. All weve done setup a structure of related bits of work. Next well keep working backwards by seeing how we unravel this work queue.",
      "Fulfilling Promises",
      "So what happens when the async work completes? At some point, foo signals completion on its async work, and that completion needs to cause us to execute success1 – the next link in the chain. Somewhere in foo, where a handle to the promise still exists, foo signals completion, by calling \"promise.complete()\".",
      "var foo = function() {\n    var promise = ...\n    async.onevent(function() {\n        promise.complete();\n    });\n    return promise;\n}",
      "The job of promise.complete is going to be to trigger any callback work linked to the next Promise. Its job is to unravel the linked list weve built and call success1. This is exactly what we do, by telling our promise to \"complete\" which causes the next promise to \"run\".",
      "this.run = function() {\n  wrappedFn.promise = this; // a stupid trick, read below\n  wrappedFn.apply(wrappedThis);\n};\n\nthis.complete = function() {\n  if (this.next) {\n    this.next.run();\n  }\n};",
      "Making Promises",
      "A final, but crucial detail. How do we create promises? We cant simply new a Promise. If we did that, this new Promise would have know to connect it to existing Promise instance in the linked-list we setup before hand. No we need a stupid trick to intercept Promise creation and figure out if its already a link or if its actually a brand new not-yet seen Promise. Heres that stupid trick:",
      "Promise.create = function(func) {\nif (func.hasOwnProperty('promise')) {\n  return func.promise;\n} else {\n  return new Promise();\n}",
      "Did you notice earlier in \"apply\" how we set a \"promise\" property on the function before calling the wrapped function? This is intended to work in conjunction with \"Promise.create\". If the called function calls \"Promise.create\" we can check the passed in function to see if a promise already exists and reuse that instance. This requires functions calling Promise.create call with themselves as an argument to be able to reuse the correct promise, ie foo looks something like",
      "var foo = function() {\n    var promise = Promise.create(foo)\n    async.wait(function() {\n        promise.complete();\n    });\n    return promise;\n}",
      "This is of course unfortunate, and a bit hacky. Its prone to copy-paste errors as copy-pasters forget to repace \"foo\" above with their functions name. Im sure theres a more intelligent way to create promises that correctly reuse promises without monkeying with a property on the wrapped function. Please post and let me know if you have any ideas!",
      "And thats it! Works great. This is a fun little diversion, mostly for my own educational reasons. But Ive actually been using this as a very simple lightweight implementation in Quepid, our search relevancy workbench. Its worked well. If youre interested in the full source, check out this gist. And leave comments, let me know if you see something I missed!"
    ],
    "summary_t": ""
  },
  {
    "id": "a66f5ec1abf52e64c83a13fd55bcf173",
    "url_s": "https://opensourceconnections.com/blog/2014/03/09/improving-search-results-with-quepid-screencast/",
    "title": "Improving Search Results with Quepid Screencast",
    "content": [
      "Were very excited about our new product, Quepid. Once you have search up-and-running, you quickly realize users have high expectations of your search. If they search your online store for \"dress shoes\" and get a list full of dresses, theyll certainly be dissatisfied, limiting your sales and sending customers fleeing to competitors with a better search experience.",
      "Unfortunately, theres not a lot of tooling to help work towards your goals. Thats exactly why we built Quepid. Watch me go through a very simple example to demonstrate the power of Quepid as a search relevancy testing and development tool for iteratively improving search results:",
      "Be sure to get in touch with us to inquire about our Quepid products and services. Weve been delivering great search for clients through Quepid, and we hope you can be next!"
    ],
    "summary_t": "We’re very excited about our new product, Quepid. Once you have search up-and-running, you quickly realize users have high expectations of your search. If th..."
  },
  {
    "id": "2e672df97f242b171b0765d86a36f3ff",
    "url_s": "https://opensourceconnections.com/blog/2014/03/12/using-customscorequery-for-custom-solrlucene-scoring/",
    "title": "Using CustomScoreQuery For Custom Solr/Lucene Scoring",
    "content": [
      "This is a preview of a talk Ill be giving entitled Hacking Lucene for Custom Search Results at ApacheCon. Come join me April 7-9th in Denver!",
      "Lucene is the swiss army knife of fuzzy sorting!",
      "Previously, I guided you through implementing a custom Lucene query and scorer. Before I introduced you to that ultimate level of control, I listed the things you should try to improve your relevancy before getting that low-level. As a reminder, here’s a list of Doug Turnbull’s official list of stuff to try to improve your relevancy® ordered from easy-to-hard:",
      "You’ve utilized Solr’s extensive set of query parsers & features including function queries, joins, etc. None of this solved your problem\n  You’ve exhausted the ecosystem of plugins that extend on the capabilities in (1). That didn’t work.\n  You’ve implemented your own query parser plugin that takes user input and generates existing Lucene queries to do this work. This still didn’t solve your problem.\n  You’ve thought carefully about your analyzers – massaging your data so that at index time and query time, text lines up exactly as it should to optimize the behavior of existing search scoring. This still didn’t get what you wanted.\n  You’ve implemented your own custom Similarity that modifies how Lucene calculates the traditional relevancy statistics – query norms, term frequency, etc.\n  You’ve tried to use Lucene’s CustomScoreQuery to wrap an existing Query and alter each documents score via a callback. This still wasn’t low-level enough for you, you needed even more control.",
      "One item stands out on that list as a little low-level but not quite as bad as building a custom Lucene query: CustomScoreQuery. When you implement your own Lucene query, you’re taking control of two things:",
      "Matching – what documents should be included in the search results\n  Scoring – what score should be assigned to a document (and therefore what order should they appear in)",
      "Frequently you’ll find that existing Lucene queries will do fine with matching but you’d like to take control of just the scoring/ordering. That’s what CustomScoreQuery gives you – the ability to wrap another Lucene Query and rescore it.",
      "For example, let’s say you’re searching our favorite dataset – SciFi Stackexchange, A Q&A site dedicated to nerdy SciFi and Fantasy questions. The posts on the site are tagged by topic: \"star-trek\", \"star-wars\", etc. Lets say for whatever reason we want to search for a tag and order it by the number of tags such that questions with the most tags are sorted to the top.",
      "In this example, a simple TermQuery could be sufficient for matching. To identify the questions tagged Star Trek with Lucene, you’d simply run the following query:",
      "Term termToSearch = new Term(\"tag\", \"star-trek\");\nTermQuery starTrekQ = new TermQuery(termToSearch);\nsearcher.search(starTrekQ);",
      "If we examined the order of the results of this search, they’d come back in default TF-IDF order.",
      "With CustomScoreQuery, we can intercept the matching query and assign a new score to it thus altering the order.",
      "Step 1 Override CustomScoreQuery to create our own custom scored query class:",
      "(note this code can be found in this github repo)",
      "public class CountingQuery extends CustomScoreQuery {\n\n    public CountingQuery(Query subQuery) {\n        super(subQuery);\n    }\n\n    protected CustomScoreProvider getCustomScoreProvider(\n            AtomicReaderContext context) throws IOException {\n        return new CountingQueryScoreProvider(\"tag\", context);\n    }\n}",
      "Notice the code for \"getCustomScoreProvider\" this is where we’ll return an object that will provide the magic we need. It takes an AtomicReaderContext, which is a wrapper on an IndexReader. If you recall, this hooks us in to all the data structures available for scoring a document: Lucene’s inverted index, term vectors, etc.",
      "Step 2 Create CustomScoreProvider",
      "The real magic happens in CustomScoreProvider. This is where we’ll rescore the document. I’ll show you a boilerplate implementation before we dig in",
      "public class CountingQueryScoreProvider extends CustomScoreProvider {\n\n    String _field;\n\n    public CountingQueryScoreProvider(String field, AtomicReaderContext context) {\n        super(context);\n        _field = field;\n    }\n\n    public float customScore(int doc, float subQueryScore, float valSrcScores[]) throws IOException {\n        return (float)(1.0f);\n    }\n}",
      "This CustomScoreProvider rescores all documents by returning a 1.0 score for them, thus negating their default relevancy sort order.",
      "Step 3 Implement Rescoring",
      "With TermVectors on for our field, we can simply loop through and count the tokens in the field:",
      "public float customScore(int doc, float subQueryScore, float valSrcScores[]) throws IOException\n{\n    IndexReader r = context.reader();\n    Terms tv = r.getTermVector(doc, _field);\n    TermsEnum termsEnum = null;\n    termsEnum = tv.iterator(termsEnum);\n    int numTerms = 0;\n    while((termsEnum.next()) != null) {\n        numTerms++;\n    }\nreturn (float)(numTerms);\n}",
      "And there you have it, we’ve overridden the score of another query! If you’d like to see a full example, see my \"lucene-query-example\" repository that has this as well as my custom Lucene query examples.",
      "CustomScoreQuery vs a Full-Blown Custom Query",
      "Creating a CustomScoreQuery is a much easier thing to do than implementing a complete query. There are A LOT of ins-and-outs for implementing a full-blown Lucene query. So when creating a custom matching behavior isn’t important and you’re only rescoring another Lucene query, CustomScoreQuery is a clear winner. Considering how frequently Lucene based technologies are used for \"fuzzy\" analytics, I can see using CustomScoreQuery a lot when the regular tricks dont pan out.",
      "I hope you found that helpful! We focus a lot on improving search relevancy & quality, so if you feel like you need this level of work or any other Solr or Elasticsearch relevancy help, please contact us!"
    ],
    "summary_t": "Previously, I guided you through implementing a custom Lucene query and scorer. Before I introduced you to that ultimate level of control, I listed the thing..."
  },
  {
    "id": "11c954cc4097c336212f02af8187178b",
    "url_s": "https://opensourceconnections.com/blog/2006/07/25/we-made-it-to-oscon/",
    "title": "We made it to OSCON!",
    "content": [
      "After 8 hours of travel, we arrived into PDX last night at 10:50 PM. Headed down to the luggage claim and saw our pop up booth packed in a large plastic shipping case, codenamed \"Napolean\", come out.. Well, actually, what first came out was a red ratchet strap that theorectically was holding the heavy plastic case together! You can imagine the feeling of panic I felt contemplating two days of exhibiting with no booth!",
      "",
      "Fortunantly 5 minutes later \"Napolean\" showed up, sans ratchet strap."
    ],
    "summary_t": ""
  },
  {
    "id": "1a6db50064efdba4c7dd19d75ae341e0",
    "url_s": "https://opensourceconnections.com/blog/2007/04/03/erics-ruby-gems/",
    "title": "Eric’s Ruby Gems",
    "content": [
      "Following Mike Clark’s idea. Im curious to see what gems Ive got installed in 6 months!",
      "xativa:~ eric$ gem list|grep '^[a-zA-Z]'\nactionmailer (1.3.2, 1.3.1, 1.2.5, 1.2.4, 1.2.3, 1.2.1, 1.1.3, 1.0.1)\nactionpack (1.13.2, 1.13.1, 1.12.5, 1.12.4, 1.12.3, 1.12.1, 1.11.0, 1.9.1)\nactionwebservice (1.2.2, 1.2.1, 1.1.6, 1.1.5, 1.1.4, 1.1.2, 1.1.1, 0.9.3, 0.8.1)\nactiverecord (1.15.2, 1.15.1, 1.14.4, 1.14.3, 1.14.2, 1.14.1, 1.13.0, 1.11.1)\nactivesupport (1.4.1, 1.4.0, 1.3.1, 1.2.3, 1.1.1)\najax_scaffold_generator (3.1.5, 2.1.0)\natom (0.3)\nbuilder (2.0.0)\ncalendar_grid (1.0.2)\ncamping (1.5)\ncapistrano (1.4.0, 1.3.1, 1.2.0, 1.1.0)\ncgi_multipart_eof_fix (2.0.2)\ncheat (1.0.2)\nchronic (0.1.4)\ncolor-tools (1.3.0)\ndaemons (1.0.4, 0.4.4)\ndev-utils (1.0.1)\nextensions (0.6.0)\nezcrypto (0.7)\nfastercsv (1.1.0, 0.2.1)\nfastthread (0.6.3)\nflex_egenial_scaffold (0.0.1)\ngcalapi (0.1.0, 0.0.3)\ngem_plugin (0.2.2, 0.2.1)\ngooglecalendar (0.0.2)\nhoe (1.1.7)\nhpricot (0.4.43)\nicalendar (0.98)\nlibxml-ruby (0.3.8.4)\nmailfactory (1.2.3, 1.2.2)\nmarkaby (0.5)\nmechanize (0.5.4)\nmetaid (1.0)\nmime-types (1.15)\nmoney (1.7.1)\nmongrel (1.0.1, 0.3.13.3)\nmongrel_cluster (1.0.1.1, 0.2.1)\nneedle (1.3.0)\nnet-sftp (1.1.0)\nnet-ssh (1.0.10, 1.0.9)\npaginator (1.0.8)\npayment (1.0.1)\npdf-writer (1.1.3, 1.1.1)\nrails (1.2.2, 1.2.1, 1.1.6, 1.1.5, 1.1.4, 1.1.2, 1.1.1, 0.14.3, 0.13.1)\nrake (0.7.1, 0.6.2)\nrb-appscript (0.3.0)\nRedCloth (3.0.4)\nruby-web (1.1.1)\nrubyforge (0.4.0)\nrubygems-update (0.9.2, 0.9.0)\nrubyosa (0.1.0)\nrubyzip (0.9.1)\nruport (0.8.10, 0.8.1, 0.7.2, 0.7.1, 0.7.0, 0.6.1, 0.4.23, 0.4.19, 0.4.13)\nscruffy (0.2.2)\nSelenium (1.0.1)\nsentry (0.3.1)\nshipping (1.5.0, 1.3.0)\nsources (0.0.1)\nsqlite-ruby (2.2.2)\nsqlite3-ruby (1.2.1, 1.1.0)\nstreamlined_generator (0.0.3, 0.0.2)\ntattle (1.0.1)\ntransaction-simple (1.3.0)\ntzinfo (0.3.3)\nunicode (0.1)\nvpim (0.360)\nwebrick-webdav (1.0)"
    ],
    "summary_t": "Following Mike Clark’s idea. Im curious to see what gems Ive got installed in 6 months! xativa:~ eric$ gem list      grep ‘^[a-zA-Z]’ actionmaile..."
  },
  {
    "id": "35a1a3fef57d775b0862f4b621494e47",
    "url_s": "https://opensourceconnections.com/blog/2014/04/04/osc-improves-dukemedicine-search-relevancy-using-quepid/",
    "title": "OSC Improves DukeMedicine Search Relevancy using Quepid",
    "content": [
      "Visit www.dukemedicine.org to check out our search improvements!",
      "Recently we were brought in as part of the search team for DukeMedicine’s new portal to resolve crucial search quality issues. As beautiful and functional as the site is, it wasn’t until late in the project that the team realized that search had a serious problem. Search results were not at all relevant to the Duke’s patient searches. Duke’s patients use a single search box to find all kinds of content. They might need to search for \"dutch doctor\" to find a Dutch speaking doctor. Or they might search for a liver specialist near them by typing in \"liver doctor Durham\". Search results were not living up to anyone’s expectations.",
      "Even though everything was in the Solr index, users of the website were having a really difficult time finding anything they were looking for. You cant expect the average end user to know how to craft complex queries by specifying boosts manually for fields that they have no idea about. Users expect smart google-like search that carefully provide tuned search results by automatically searching across multiple fields.",
      "However, isnt it a real pain to craft a `one-size fits all search box that balances search relevancy across all use cases? It used to be. Enter Quepid!",
      "Day one on this project, Duke had provided us with a spreadsheet of search queries and what the expected results should be. The classic approach is to tweak search queries around to improve specific cases, but as you get deeper into the pile you start crossing your fingers that changes youre making arent affecting other searches. You could manually search every important entry in your spreadsheet. However, its simply not feasible to do this. Search testing falls by the wayside. If you do this enough, eventually your brain should get annoyed and say \"This can be done more efficiently, right?\" And thankfully, it can with Quepid.",
      "Quepid can turn confusing spreadsheets into an intuitive interface.",
      "On the DukeHealth project, we had a handful of high level concepts consisting of:",
      "Common Sense Criteria – A search for diabetes should bring up results for diabetes.\n  Search over Multiple Content Types – Searches had to be relevant across doctors, office locations, treatment information, etc.\n  Prioritization of Content – Site structure and hierarchy should affect order of search results. Searching for the broad term cancer should bring up the Cancer Center at Duke page as the top result rather than a lower-level cancer page.\n  Doctor Specifics – Doctors needed to be searchable by their specialties.\n  Phrase Matching – Searching for \"cancer treatments\" should yield more results for \"cancer\" than \"treatments\".\n  Synonyms – A query for \"afib\" should return matches on atrial fibrillation.\n  Languages – Queries for \"dutch doctor\" should return doctors that speak Dutch higher than others.\n  Locations – Searches should return results closest to a location even if they don’t match exactly.\n  Treatments – If no match is found for a specific treatment, a higher level result that includes the treatment should be returned.",
      "We were able to enter these as cases into Quepid and add multiple test queries for each. From there we could apply our geeky Solr expertise. We added synonyms and applied boosts to certain fields. We altered how the query is parsed and text analyzed. With each change we could immediately see how it affected all of our cases– instantly seeing how search changes help or hurt all searches. Sometimes we nailed it and everything stayed great, other times a change would fix one case and break others, and, because it happens, sometimes our change was way off and didnt even help what we were trying to fix. The key point is that Quepid gave us instant visual feedback on our changes. No time was wasted running queries manually to evaluate results.",
      "With the help of Quepid, we achieved fantastic relevancy in just a few weeks. Its a good feeling when you hear the user’s initial reaction to the search \"What the #$%# is this?!?\" changed to \"Wow! This is really great.\" With Quepid, we were able to show off how bad relevancy was using the old configuration and then demonstrate the improvements with our customizations. Our clients were able to visually grasp how our changes affected their system as we iterated on search quality.",
      "So congratulations to DukeMedicine for the recent launch. We were thrilled to be part of the team! And of course, if youre working with any Solr tuning projects, consider checking out Quepid because it will make your life a whole lot easier. If you want to make your life even easier, Contact Us! Bring in the experts for our Quepid relevancy health checks."
    ],
    "summary_t": "Recently we were brought in as part of the search team for DukeMedicine’s new portal to resolve crucial search quality issues. As beautiful and functional as..."
  },
  {
    "id": "93efff360fb0fd4cc88a393e256a2177",
    "url_s": "https://opensourceconnections.com/blog/2014/04/04/osc-search-relevancy-talks-coming-to-apachecon-next-week/",
    "title": "OSC Search Relevancy Talks coming to ApacheCon next week!",
    "content": [
      "See you in Denver!",
      "Im looking forward to seeing everyone at ApacheCon in Denver next week! Ill be giving two talks this year. They both focus heavily on search relevancy, an area that weve been working hard to highlight. Weve found that folks plug in search, Solr or Elasticsearch, and get themselves to a point where search seems to work. That is until somebody starts taking a close look at the quality of the results. The results for user searches arent returning expected results. Our Duke Medicine project is an example of this – getting Solr installed can be easy but you have to work to get relevant search. (And of course, shameless plug, thats when you contact us to help!)",
      "My first talk, Test Driven Relevancy, discusses the process of getting good search. We iterate on search the same way we iterate over software. We capture business requirements and develop tests to prove search is making progress. Thats why we built (another shameless plug) Quepid – to be our workbench on iterating on customer search requirements. Heres the synopsis:",
      "Getting good search results is hard; maintaining good relevancy is even harder. Fixing one problem can easily create many others. Without good tools to measure the impact of relevancy changes, theres no way to know if the \"fix\" that youve developed will cause relevancy problems with other queries. Ideally, much like we have unit tests for code to detect when bugs are introduced, we would like to create ways to measure changes in relevancy. This is exactly what weve done at OpenSource Connections. Weve developed a tool, Quepid, that allows us to work with content experts to define metrics for search quality. Once defined, we can instantly measure the impact of modifying our relevancy strategy, allowing us to iterate quickly on very difficult relevancy problems. Get an in depth look at the tools we use to not only search a relevancy problem – but to make sure it stays solved!",
      "My second talk, Hacking Lucene For Custom Search Results, discusses a bit more of the how – and at a very technical level! Recently while working for a client I had to pull out all the stops and completely rearchitect how Solr/Lucene was scoring to create a custom Lucene Query & Scorer. It was a daunting but fun task – and it shows how much control (and responsibility) one can take over how search works in the Lucene ecosystem. Heres the synopsis:",
      "Search is everywhere, and therefore so is Apache Lucene. While providing amazing out-of-the-box defaults, theres enough projects weird enough to require custom search scoring and ranking. In this talk, Ill walk through how to use Lucene to implement your custom scoring and search ranking. Well see how you can achieve both amazing power (and responsibility) over your search results. Well see the flexibility of Lucenes data structures and explore the pros/cons of custom Lucene scoring vs other methods of improving search relevancy.",
      "Please contact me if youd like to touch base at ApacheCon. Wed love to talk about your search relevancy problems. Were especially looking for freelancers and partners to help us with all this exciting work! Hope to see you there!"
    ],
    "summary_t": "I’m looking forward to seeing everyone at ApacheCon in Denver next week! I’ll be giving two talks this year. They both focus heavily on search relevancy, an ..."
  },
  {
    "id": "0416bc108c5f02275a766f275c94cb65",
    "url_s": "https://opensourceconnections.com/blog/2014/04/11/indexing-polygons-in-lucene-with-accuracy/",
    "title": "Indexing Polygons in Lucene with Accuracy",
    "content": [
      "Apache Lucene is a Java toolkit that provides a rich set of search capabilities like keyword search, query suggesters, relevancy, and faceting. It also has a spatial module for searching and sorting with geometric data using either a flat-plane model or a spherical model. For the most part, the capabilities therein are leveraged to varying degrees by Apache Solr and ElasticSearch–the two leading search servers based on Lucene.",
      "Advanced spatial",
      "The most basic and essential spatial capability is the ability to index points specified using latitude and longitude, and then be able to search for them based on a maximum distance from a given query point—often a users location, or the center of a maps viewport. Most information-retrieval systems (e.g. databases) support this.",
      "Most relational databases, such as PostGIS, are mature and have advanced spatial support. I loosely define advanced spatial as the ability to both index polygons and query for them by another polygon. Outside of relational databases, very few information-retrieval systems have advanced spatial support. Some notable members of the advanced spatial club are Lucene, Solr, Elasticsearch, MongoDB and Apache CouchDB. In this article we’ll be reviewing advanced spatial in Lucene.",
      "How it works…",
      "This article describes how polygon indexing & search works in some conceptual detail with pointers to specific classes. If you want to know literally how to use these features in Lucene, then first know you should be familiar with Lucene. Find a basic tutorial on that if you arent familiar with it. Then you should read the documentation for the Lucene-spatial API, and review the SpatialExample.java source which shows some basic usage. Next, read the API Javadocs referenced here within the article for relevant classes. If instead you want to know how to use it from Solr and ElasticSearch, this isnt the right article, not to mention that the new accurate indexing addition isnt yet hooked into those search servers yet, as of this writing.",
      "Grid indexing with PrefixTrees",
      "From Lucene 4 through 4.6, the only way to index polygons is to use Lucene-spatial’s PrefixTreeStrategy which is an abstract implementation of SpatialStrategy — the base abstraction for how to index and search spatially. It has two subclasses, TermQueryPrefixTreeStrategy and RecursivePrefixTreeStrategy (RPT for short) — I always recommend the latter. The indexing scheme is fundamentally based on a model in which the world is divided into grid squares (AKA cells), and is done so recursively to get almost any amount of accuracy required. Each cell, is indexed in Lucene with a byte string that has the parent cell’s byte string as a prefix — hence the name PrefixTree.",
      "The PrefixTreeStrategy uses a SpatialPrefixTree abstraction that decides how to divide the world into grid-squares and what the byte encoding looks like to represent each grid square. There are two implementations:",
      "geohash: This is strictly for latitude & longitude based coordinates. Each cell is divided into 32 smaller cells in an 8×4 or 4×8 alternating grid.\n  quad: Works with any configurable range of numbers. Each cell is divided into 4 smaller cells in a straight-forward 2×2 grid.",
      "We’ll be using a geohash based PrefixTree for the examples in this article. For further information about geohashes, to include a convenient table of geohash length to accuracy, go to Wikipedia.",
      "Indexing points",
      "Lets now see what the indexed bytes look like in the geohash SpatialPrefixTree grid for indexing a point. We’ll use Boston Massachusetts: latitude 42.358, longitude -71.059. If we go to level 5, then we have a grid cell that covers the entire area ± 2.4 kilometers from its center (about 19 square kilometers in this case):",
      "",
      "This will index as the following 5 \"terms\" in the underlying Lucene index:",
      "D, DR, DRT, DRT2, DRT2Y",
      "Terms are the atomic byte sequences that Lucene indexes on a per-field basis, and they map to lists of matching documents (i.e. records); in this case, a document representing Boston.",
      "If we attempt to index any other hypothetical point that also falls in the cell’s boundary it will also be indexed using the same terms, effectively coalescing them into the same for search purposes. Note that the original numeric literals are returned in search results since they are internally stored separately.",
      "To get more search accuracy we need to take the PrefixTree to more levels. This easily scales: there aren’t that many terms to index per point. If you want accuracy of about a meter then level 11 will do it, translating to 11 indexed terms. The search algorithms to efficiently search grid squares indexed in this way are interesting but it’s not the focus of this article.",
      "Indexing other shapes",
      "The following screen-captures depict the leaf cells indexed for a polygon of Massachusetts to a grid level of 6 (±0.61km from center):",
      "",
      "The biggest cells here are at level 4, and the smallest are at 6. A leaf cell is a cell that is indexed at no smaller resolution, either because it doesn’t cross the shape’s edge, or it does but the accuracy is capped (configurable). There are 5,208 leaf cells here. Furthermore, when the shape is indexed, all the parent (non-leaf) cells get indexed too which adds another 339 cells that go all the way up to coarsest cell \"D\". The current indexing scheme calls for leaf cells to be indexed both with and without a leaf marker — an additional special ‘+’ byte. The grand total number of terms to index is 10,755 terms. That’s a lot! Needless to say, we won’t list them here.",
      "Note: the extra indexed term per leaf will get removed in the future, once the search algorithms are adapted for this change. See LUCENE-4942.",
      "Theoretically, to get more accuracy we \"just\" need to go to lower grid levels, just as we do for points. But unless the shape is small relative to the desired accuracy, it doesn’t scale; it’s simply infeasible with this indexing approach to represent a region covering more than a few square kilometers to meter level accuracy. Each additional PrefixTree level multiplies the number of indexed terms compared to the previous level. The Massachusetts example went to level 6 and produced 5208 leaf cells, but if it had just gone to level 5 then there would have been only 463 leaf cells. With each additional level, there will be on average 32 / 2 = 16 times as many (for geohash) cells of the previous level. For a quad-tree it’s 4 / 2 = 2 times as many, but it also takes more than one added level of a quad tree to achieve the same desired accuracy, so it’s effectively the same no matter how many leaves are in the tree.",
      "Summary: unlike a point shape, which has a linear relationship between accuracy and the number of terms, polygon shapes are exponential to the power of the numbers of levels. This clearly doesn’t scale for high-accuracy requirements.",
      "Serialized shapes",
      "Lucene 4.7 introduced a new Lucene-spatial SpatialStrategy called SerializedDVStrategy (SDV for short). It serializes each documents shape vector geometry itself into a part of the Lucene Index called DocValues. At query time, candidate results are verified as true or false matches, by retrieving the geometry by doc-id on-demand, deserializing, and comparing to the query shape.",
      "As of this writing, there aren’t any adapters for ElasticSearch or Solr yet. The Solr feature request is tracked as SOLR-5728, and is probably going to arrive in Solr 4.9. The adapters will likely maintain in-memory caches of deserialized shapes to speed up execution.",
      "Using both strategies",
      "It’s critical to understand that SerializedDVStrategy is not a PrefixTree index; if used alone, SerializedDVStrategy will brute-force and deserialize compare all documents, O(N) complexity, and have terrible spatial performance.",
      "In order to minimize the number of documents that SerializedDVStrategy has to see, you should put a faster SpatialStrategy like RecursivePrefixTreeStrategy in front of it. And you can now safely dial-down the configurable accuracy of RPT to return more false-positives since SDV will filter them out. This is done with the distErrPct option, which is roughly the fraction of a shape’s approximate radius, and it defaults to 0.025 (2.5%). Given the Massachusetts polygon, RPT arrived at level 6. If distErrPct is changed to 0.1 (10%), my recommendation when used with SDV, the algorithm chose level 5 for Massachusetts, which had about 10% as many cells as level 6.",
      "Performance and accuracy — the holy grail",
      "What I’m most excited about is the prospect of further enhancing the PrefixTree technique such that it is able to differentiate matching documents between those that are a guaranteed matches versus those that need to be verified against the serialized geometry. Consider the following shape, a circle, acting as the query shape:",
      "",
      "If any grid cell that isn’t on the edge matches a document, then the PrefixTree guarantees it’s a match; there’s no point in checking such documents against SerializedDVStrategy (SDV). This insight should lead to a tremendous performance benefit since a small fraction of matching documents, often zero, will need to be checked against SDV. Performance and accuracy! This feature is being tracked with LUCENE-5579.",
      "One final note about accuracy — it is only as accurate as the underlying geometry. If you’ve got a polygon representation of an area with only 20 vertices, and another with 1000 for the same area, then more vertices are likely to be a more precise reflection of the border of the underlying area. Secondly, you might want to work in a projected 2D coordinate system instead of latitude & longitude. Lucene-spatial / Spatial4j / JTS doesn’t help in this regard, it’s up to your application to convert the coordinates with something such as Proj4j. The main shortcoming of this approach is that a point-radius (circle) query is going to be in 2D even if you might have wanted a geodesic (surface of a sphere) implementation that normally happens when you use latitudes & longitudes. But if you are projecting then the distorted difference between the two is generally fairly limited if the radius isn’t large and if the area isn’t near the poles.",
      "Acknowledgements",
      "Thanks to the Climate Corporation for supporting my recent improvements for accurate spatial queries!"
    ],
    "summary_t": ""
  },
  {
    "id": "5e749b7ba6a5c48266868638e08b5e08",
    "url_s": "https://opensourceconnections.com/blog/2014/04/24/correctly-using-camels-advicewith-in-unit-tests/",
    "title": "Correctly Using Camels AdviceWith in Unit Tests",
    "content": [
      "Ready to take your data from A to B through any terrain :)",
      "We care a lot about the stuff that goes around Solr and Elasticsearch in our clients infrastructure. One area that seems to always be being reinvented for-better-or-worse is the data ETL/data ingest path from data source X to the search engine. One tool weve enjoyed using for basic ETL these days is Apache Camel. Camel is an extremely feature-rich Java data integration framework for wiring up just about anything to anything else. And by anything I mean anything: file system, databases, HTTP, search engines, twitter, IRC, etc.",
      "One area I initially struggled with with Camel was exactly how to test my code. Lets say I have defined a simple Camel route like this:",
      "from(\"file:inbox\")\n.unmarshall(csv)  // parse as CSV\n.split() // now we're operating on individual CSV lines\n   .bean(\"customTransformation\")  // do some random operation on the CSV line\n   .to(\"solr://localhost:8983/solr/collection1/update\")",
      "Great! Now if youve gotten into Camel testing, you may know theres something called \"AdviceWith\". What is this interesting sounding thing? Well I think its a way of saying \"take these routes and muck with them\" – stub out this, intercept that and dont forward, etc. Exactly the kind of slicing and dicing Id like to do in my unit tests!",
      "I definitely recommend reading up on the docs, but heres the real step-by-step built around where youre probably going to get stuck (cause its where I got stuck!) getting AdviceWith to work for your tests.",
      "1. Use CamelTestSupport",
      "Ok most importantly, we need to actually define a test that uses CamelTestSupport. CamelTestSupport automatically creates and starts our camel context for us.",
      "public class ItGoesToSolrTest extends CamelTestSupport {\n    ...\n }",
      "(My usual disclaimers about my tendancy to write non-compiling code apply here :) )",
      "2. Specify the route builder were testing",
      "In our test, we need to tell CamelTestSupport where it can access its routes:",
      "@Override\nprotected RouteBuilder createRouteBuilder() {\n    return new MyProductionRouteBuilder();\n}",
      "3. Specify any beans wed like to register",
      "Its probably the case that youre using Java beans with Camel. If youre using the bean integration and referring to beans by name in your camel routes, youll need to register those names with an instance of your class.",
      "@Override\nprotected Context createJndiContext() throws Exception {\n    JndiContext context = new JndiContext();\n    context.bind(\"customTransformation\", new CustomTransformation());\n    return context;\n}",
      "4. Monkey with our production routes using advice with",
      "Second we need to actually use the AdviceWithRouteBuilder before each test:",
      "@Before\npublic void mockEndpoints() throws Exception {\n    AdviceWithRouteBuilder mockSolr = new AdviceWithRouteBuilder() {\n\n        @Override\n        public void configure() throws Exception {\n            // mock the for testing\n            interceptSendToEndpoint(\"solr://localhost:8983/solr/collection1/update\")\n                .skipSendToOriginalEndpoint()\n                .to(\"mock:catchSolrMessages\");\n        }\n    })\n    context.getRouteDefinition(1).\n        .adviceWith(context, mockSolr);\n }",
      "Theres a couple things to notice here:",
      "In configure we simply snag an endpoint (in this case Solr) and then we have complete freedom to do whatever we want. In this case, were rewiring it to a mock endpoint we can use for testing.\n  \n  \n    Notice how we get a route definition by index (in this case 1) to snag the route were testing and that wed like to monkey with. This is how Ive seen it in most Camel examples, and its hard to guess how Camel is going to assign some index to your route. A better way would be to give our route definition a name:\n\n    from(\"file:inbox\") .routeId(\"csvToSolrRoute\") .unmarshall(csv) // parse as CSV",
      "then we can refer to this name when retrieving our route:",
      "context.getRouteDefinition(\"csvToSolrRoute\").\n        .adviceWith(context, mockSolr);",
      "5. Tell CamelTestSupport you want to manually start/stop camel",
      "One problem you will run into with the normal tutorials is that CamelTestSupport may start routes before your mocks have taken hold. Thus your mocked routes wont be part of what CamelTestSupport has actually started. Youll be pulling your hair out wondering why Camel insists on attempting to forward documents to an actual Solr instance and not your test endpoint.",
      "To take matters into your own hands, luckily CamelTestSupport comes to the rescue with a simple method you need to override to communicate your intent to manually start/stop the camel context:",
      "@Override\npublic boolean isUseAdviceWith() {\n    return true;\n}",
      "Then in your test, youll need to be sure to do",
      "@Test\npublic void foo() {\n    context.start();\n    // tests!\n    context.stop();\n}",
      "6. Write a test!",
      "Now youre equipped to try out a real test!",
      "@Test\n public void testWithRealFile() {\n    MockEndpoint mockSolr = getMockEndpoint(\"mock:catchSolrMessages\");\n    File testCsv = getTestfile();\n\n    context.start();\n    mockSolr.expectedMessageCount(1);\n    FileUtils.copyFile(testCsv, \"inbox\");\n    mockSolr.assertIsSatisfied();\n    context.stop();\n }",
      "And thats just scratching the surface of Camels testing capabilities. Check out the camel docs for information on stimulating endpoints directly with the ProducerTemplate thus letting you avoid using real files – and all kinds of goodies.",
      "Anyway, hopefully my experiences with AdviceWith can help you get it up and running in your tests! Id love to hear about your experiences or any tips Im missing either in the comments or [via email][5].",
      "If youd love to utilize Solr or Elasticsearch for search and analytics, but cant figure out how to integrate them with your data infrastructure – contact us! Maybe theres a camel recipe we could cook up for you that could do just the trick."
    ],
    "summary_t": "Camel is an extremely feature-rich Java data integration framework for wiring up just about anything to anything else. And by anything I mean anything: file ..."
  },
  {
    "id": "2ea5eadda5929fa722cbdc021b09f0f2",
    "url_s": "https://opensourceconnections.com/blog/2014/04/24/improving-angular-dirty-checking-performance/",
    "title": "Improving Angular Dirty Checking Performance",
    "content": [
      "We build a lot of search applications, including our search relevancy workbench, Quepid or the US Patent and Trademarks Offices Global Patent Navigator Search. A big part of making clients happy is nailing the frontend – making search actually usable! In doing this we love Angular, but weve certainly had to guard against one performance anti-pattern – excessive dirty checking.",
      "Angular provides convenient two-way data binding. Its an extremely convenient way of building rich templates that somehow magically seem to know to update themselves in the DOM. I can easily write some declarative HTML that expresses the content Im trying to put on the screen:",
      "<h3>{{companyName}} Employees</h3>\n<ul>\n<li ng-repeat=\"employee in employees\">{{employee.fullname()}}</li>\n</ul>",
      "Instead of manually issuing a command to update a template, something magical in Angular knows somehow that the variables bound to the current $scope (in this case  and ) have changed – and therefore the associated snippets of HTML need to be updated.",
      "The Dirty (Checking) Secret of Two-Way Binding…",
      "How does Angular magically know how to update the content? It uses a technique called dirty checking. Angular tracks the previous values for these various expressions. If it notices a change, the corresponding Angular directive is given a chance to reflect those changes in the DOM. So if employee.fullname() changes, the interpolation directive (the fancy name for the double curly-braces syntactic sugar) has a chance to reflect this change in the DOM.",
      "All of this checking happens by methods on $scope. A $scopes digest cycle runs the dirty check, evaluating expressions on $scope, checking them against previous values, and overwriting the previous values with the just calculated ones. This digest cycle is triggered by angular directives. For example, ng-click is simply a directive that binds to the corresponding elements click event. On click, the directive evaluates an expression, perhaps calling a function or perhaps doing a simple assignment. For example, maybe the click caused the variable \"login\" to be set to \"true\". Once whatever data has been modified, the direct can then trigger the digest cycle (through $scope.$apply).",
      "On the receiving end of the two-way binding are bits of angular code (directives or otherwise) waiting to respond to changes. This is what you see in your angularized HTML with expressions such as ng-repeat=\"employee in employees\". Ultimately these directives register with $scope.$watch (these are the expressions being dirty checked). The job of the digest cycle is to identify all the expressions registered for watching (via $scope.watch), see if theres a change, and execute the registered callback. We might have $scope.$watch on the login flag from the previous paragraph. Our callback might update the DOM or transition to a new page.",
      "This is both Angulars secret sauce and Achilles heel. With most applications, its likely not a problem. With Quepid, however, we had a problem of repeated elements that could be clicked into. Once you were inside each element, you could interact with dozens of little widgets.",
      "From an implementation point-of-view, each of these widgets was hidden until they were clicked, something like:",
      "<div ng-show=\"isClicked\">\n        ...\n    </div>",
      "Unfortunately, \"hidden\" does not stop the dozens of little Angular widgets from registering with their scopes and being dirty-checked during the digest cycle. After enough of these repeated elements, the application would drag significantly. Using a profiler, we could clearly see the issue was in $scope.digest – Angulars digest cycle.",
      "So what can be done to deal with this problem? Let me walk you through some of the strategies that have worked for us.",
      "1. Use ng-if, not ng-show",
      "One of the best features to come out of Angular 1.2 is ng-if. ng-show works by applying \"display: none\" on or off of the element – thus hiding it but keeping it in the DOM. Unfortunately, by keeping it in the DOM the directives within are still watching for change and doing work. Ng-if on the other hand delays compiling the elements into the DOM until the condition is true. Effectively this is a way to lazy-load angular capabilities into the DOM, and thus avoid having hundreds of pointless dirty checks slowing down the application.",
      "The downside is, ng-if involves a small performance hit as it must fully compile the Angular template on-demand. This might be able to be mitigated with a combination of Ng-show and Ng-if. What if Ng-if was tied to a hover state, but paired with an Ng-click that was tied more closely to a click state?",
      "<div ng-if=\"hover\" ng-show=\"clicked\" ...>",
      "Perhaps future versions of Angular could combine some lazy-loading functionality from ng-if with the new snazzy shadow-DOM that somehow could do part of the work without registering all the little expressions that need to watch for changes on the DOM.",
      "2. Make sure what is being checked is cheap",
      "With frequent dirty checking, its inadvisable to place calls to complex functions into Angular expressions. Remember, this stuff will get called A LOT! An ng-click somewhere could trigger a lot of dirty checking. Thus Any lengthy calculation should be avoided. For example, sorting a big list and returning the top element would probably be a big no-no. Instead, have a way to check to see if the list has changed, then take some kind of action (see #3).",
      "3. For manual watches, use a single watch a version or a hash of the thing (not lots of little watches)",
      "Instead of",
      "$scope.$watch(\"employee.firstName\", function() {...})\n$scope.$watch(\"employee.lastName\", function() {...})",
      "Combine these into a single watch over a hash or other value that will change every time a state changes. An obvious solution is to give employee a hashing function that always returns a unique value:",
      "Employee.Hash = function() {\n    return this.firstName + this.lastName + this.Age ...;\n}",
      "Then elsewhere:",
      "$scope.$watch(\"employee.hash\", function() {\n\n});",
      "Hash functions can collide. They might also be expensive. So lately, Ive been using a version number that increments on every change for these problems:",
      "this.setFirstName = function(newFirstName) {\n    this.firstName = newFirstName\n    this.version++;\n}",
      "Its an implementation burden, but lets me write code that just checks if a single integer has changed:",
      "4. Dont use $scope.$watch for events",
      "When you start out with Angular, its tempting to pass events up and down the $scope hierarchy by setting status at one level and performing a $watch on it elsewhere. For example in a root scope we might set:",
      "$scope.userLogedIn = true",
      "And in child scopes, we might want to respond:",
      "$scope.$watch(\"userLoggedIn\", function() {\n});",
      "Alternatively, dont clutter up the digest cycle with event-like expressions. Use Angulars explicit event features like emit and broadcast.",
      "5. Avoid Dirty Checking Entirely by working straight with the DOM",
      "One of my favorite things about Angular is that I reserve the right to not use Angular if need be. Even in Angular world, I can simply use a directives link function to interact directly with the DOM as needed. For example, I could chose to simply insert a handlebars template into a div instead of worrying about getting angular-ese right:",
      "link: function(scope, element) {\n    button = element.find(\"button\");\n    button.onClick = function() {\n        // find our list!\n        employeeList = element.find(\"employeeList\");\n        employeeList.innerHtml = \"<li>No Employees, Only Ninjas!</li>\"\n    };\n};",
      "Anyway, those techniques have helped us tremendously in our work. I hope youll also find them useful. Please comment if you have anything youd add to this list! And of course if you have problems with your search UI or would simply like a better search application, contact us to see if we can help!"
    ],
    "summary_t": "We build a lot of search applications, including our search relevancy workbench, Quepid or the US Patent and Trademark’s Offices Global Patent Navigator Sear..."
  },
  {
    "id": "608b09ccb014d0f54cbf8d1fb6bd61a9",
    "url_s": "https://opensourceconnections.com/blog/2014/05/03/talking-search-and-discovery-to-exploit-data-at-enterprise-data-world-2014/",
    "title": "Talking Search and Discovery to Exploit Data at Enterprise Data World 2014",
    "content": [
      "As a search guy, I think of data as something to exploit in order to solve some business objective, with a focus on moving data along the axis of data becoming information in order to facilitate understanding.",
      "This week I had the opportunity to finally visit Austin, TX, a city Ive been hearing great things about, when I attended the Enterprise Data World conference.",
      "I was a bit of an oddball at the conference, since the focus was very much on the state of your data, and much less on the exploitation of your data to drive your business. However, the primary concern of the data folks is to make data as perfect as possible, in and off itself, with the idea that the rest of it comes out of that naturally. I see a list of mailing addresses and think \"Hey, can we figure out which cities are represented the most?\". Whereas a data person is focused more on \"here is a list of addresses, are they accurate and up to date?\". As someone who deals with messy data, I applaud folks who try to fix it, though it seems like a sissyphean task!",
      "My talk was on GPSN, a search engine for Chinese patents run by the USPTO, and while the audience was small, I had some wonderful questions at the end, and the discussion turned to eDiscovery, and what is preventing many of the concepts that have been so successful in eDiscovery from spreading to other areas of search. I didnt catch the name of the woman who brought up eDiscovery, but she had some great ideas, and I appreciated the discussion. My slides, for some reason, didnt show up on the website, so Ive posted the PDF of them on Slideshare:",
      "Searching Chinese Patents Presentation at Enterprise Data World  from OpenSource Connections",
      "In talking to other attendees, I saw a difference between the world I normally operate in and the world of most of the other attendees operate in is that most of my data is very messy and I am focusing on massaging that messy data to pull out some reasonable metadata that gives me more structure, and thus move my data along the axis to the \"information\" stage. Whereas the concerns of most of the attendees isnt to extract metadata, but instead to corral all the various forms of what is fairly structured data into one cohesive whole. They are often drowning in too much metadata! Hence topics like Master Data Management came up frequently. In search we typically just throw it all into a single index, and let the queries sort it out! The level of care and detail to curating all the data isnt normally part of the scope of work.",
      "A couple of other things jumped out at me. I attended the panel Enterprise Semantics: Can Smarter Data Really Add Value? to see if the Semantic Web has become any easier to use. I shuddered when one of the panelists, when asked about success stories, listed some companies who \"went all in on semantic web technologies\". There remains no reasonable path, in general, for folks to put their toe into semantic web, and then gradually adopt it. It remains an all or nothing proposition. There is amazing things we could do if we had better understanding of what people are asking for, but there isnt any reasonable path forward. And yes, Siri is great example of semantics being applied. But most companies are not Apple! I remain interested in seeing if some of the efforts like Schema.org lower the barrier to using semantic technologies.",
      "The other session that I really enjoyed was MDM: Master Data Management or Massive Data Mistake?. While the end of it was a marketing pitch for EnterpriseWeb, the focus of the talk was on the fact that the world is not a static place. And yet, many of the approaches that we have for dealing with complexity attempts to push the world into a more static place. As Jason Bloomberg (@TheEbizWizard) put it, we define contracts between systems. We come up with DTDs. Ideally everything is very flexible, but building something in a very flexible manner is expensive. Expensive to test. Expensive to code. Expensive to think about! His solution is something they call \"Extreme Late Binding\" where every time a system interacts with another system, you look up all the schemas for your data, at that immediate time, and then use that to figure out your execution path. Think the goal processing logic of Ant, applied to data movements instead of compiling code. Weve been doing some Apache Camel work, and while powerful, I can see how at a certain point, the massive number of interactions causes everything to break down. This \"extreme late binding\" makes me think of HATEOS (or Hypermedia driven system) and he did confirm that much of the lookup logic in their product is based on HATEOS principles. What struck me about his talk was pointing out that the world is not static, and we whiteboard out systems as if they are. Search brings some very powerful tools to deal with the messiness of data.",
      "Lastly I had a great conversation with a guy who does significant data work with McDonalds, and learned a lot about a company that I thought I knew about because I see the Golden Arches everywhere, but had no idea all that goes into making a hamburger and fries from a data perspective!",
      "I think I got a bit on my soapbox about how we need to be able to put a \"confidence\" ranking on our data. A frequent issue that we see is that we have 1 batch of data that has lots of noise, but a couple of golden nuggets. Another batch of data is fairly clean, but fairly boring. Right now, when we return results, unless you are really familiar with the data, you dont know that the hits from the noisy datset may be much more suspect then the hits from the boring system. We need to have a confidence ranking on our results that tells our users, this result may be perfect, or a complete red herring! To get to this level of sophistication, we all need to become data people. Business users who are data savvy will succeed over those who just accept a report at face value.",
      "Austin was a great city to host a conference, though I wish the hotel had been more centrally located. Really enjoyed me time there, and it was great to catch up with folks!"
    ],
    "summary_t": ""
  },
  {
    "id": "2b73fcf79bf0e533233736a95bbfb6a1",
    "url_s": "https://opensourceconnections.com/blog/2014/05/12/discovery-more-important-than-search-for-music-services/",
    "title": "Discovery more Important than Search for Music Services",
    "content": [
      "Recently I was choosing between several different music services. Rdio and Google Music were the front-runners. I assumed that the most important features that I would be comparing would be music availability or compression rates. However these elements factored much less into my decision than I expected. Instead I found that the process of music discovery, and whether or not the service facilitated natural ways of locating new music was actually the most important for my specific needs.",
      "The first product that I looked at was Rdio. This is a great music service and definitely the one with the most design sensibility overall. There web interface is smooth, and impressive. Outside of a \"top charts\" list, however, it can be very difficult to locate new bands. This presented a problem because one of the best things that a music service can do is to show me new music that I will fall in love with or to remind me about a band that I may have forgotten about years ago.",
      "",
      "In contrast, the Google Music interface is just adequate and sometimes buggy. But it has an \"Explore\" tab which list music by genre and sub-genre.",
      "",
      "This is useful for jumping straight to a playlist or discovering a band that you werent specifically looking for but that is an exact fit for your current mood. This sort of serendipitous path is really critical in music. Its easy to search for your ten favorite bands but finding a new album is much more about looking through the metaphorical record bins if you will.",
      "This is important to keep in mind when developing search interfaces. The mindset of a user who has a very specific query and knows exactly what they are looking for is very different from someone who only has a general inclination of the type of thing that they are interested in. The best interfaces come from combining both rock solid search with ambient findability into a seamless user experience.",
      "Have you seen any good examples of discovery interfaces recently? Let us know in the comments."
    ],
    "summary_t": "Recently I was choosing between several different music services… I found that the process of music discovery, and whether or not the service facilitated nat..."
  },
  {
    "id": "b70da9e89370d1eb3d348d2d188fa29d",
    "url_s": "https://opensourceconnections.com/blog/2007/04/04/does-the-virtual-world-equal-the-real-world/",
    "title": "Does the virtual world equal the real world?",
    "content": [
      "This is the question that the FBI is asking of Second Life as they investigate Second Lifes casinos. The three biggest casinos, according to the CNN article, make $1,500/month each. Seems like a small thing for the FBI to be investigating given the other crimes in the country, but is this actually an effort to start to establish some sort of case precedent for potential legal scraps in the future?",
      "Given that more and more businesses are moving into Second Life, without a way to establish and enforce contracts, the denizens of Second Life who enter into transactions must certainly heed the warning caveat emptor. As it currently stands, people who do things in Second Life are only buying, effectively, the right to certain bits and pieces of the Linden Labs server farm. With nothing but the loosest of guidelines to provide cover, Second Life can sometimes bring back the reminiscent feel of the Internet in the early 90s.",
      "Enter the FBI. While the real world crackdown on online gambling by the U.S. Justice Department certainly has the Feds busy, true established case law is very sparse on the subject. I spoke with a former counsel with Party Poker while we were exhibiting at the OReilly Open Source Convention (OSCON) last year, and she told me that because of the lack of established statutes and case law, the courts could come down either way on online gambling activities. Perhaps the Justice Department is looking to get a ruling on a similar set of circumstances in Second Life to help bolster the Internet gambling ban."
    ],
    "summary_t": ""
  },
  {
    "id": "a968a7d7671062dc422d674befb15d0b",
    "url_s": "https://opensourceconnections.com/blog/2014/05/24/crawling-with-nutch/",
    "title": "Crawling with Nutch",
    "content": [
      "Recently, I had a client using LucidWorks search engine who needed to integrate with the Nutch crawler. This sounds simple as both products have been around for a while and are officially integrated. Even better, there are some great \"getting started in x minutes\" tutorials already out there for both Nutch, Solr and LucidWorks. But there were a few gotchas that kept those tutorials from working for me out of the box. This blog post documents my process of getting Nutch up and running on a Ubuntu server.",
      "0) Install Java",
      "Included as step 0, as there is a good chance you already have the jdk installed. On Ubuntu, this is as simple as:",
      "apt-get install default-jdk\napt-get install default-jre\nexport JAVA-HOME=/path/to/jdk/folder",
      "1) Install Solr",
      "Ill be working off the LucidWorks build which is available free for download, but does require a license for beyond the trial use. Their install process is pretty well documented. I especially recommend their getting started guide if you are new to the search domain. If you are using a stand-alone Solr install, the nutch portion of this tutorial should be about the same, but your URLs for communicating with Solr will be slightly different.",
      "2) Install Nutch",
      "Nutch is an open-source project, and as such the active community ebbs and flows. In addition, some builds are more stable than others. Some documentation on the versions here:",
      "Nutch 1.x series: This uses Hadoop for the map/reduce phases. It will integrate with a pre-existing Hadoop install, but includes the necessary pieces if you dont.",
      "command referenced from the official nutch tutorial. Ill be using the 1.7 binary release, available here. To install:",
      "tar -zxvf apace-nutch-1.7.bin.tar.gz",
      "Nutch 2.x series: This uses Gora to abstract out the persistance layer; out of the box it appears to use HBase over Cassandra. At the time of writing, it is only available as a source download, which isnt ideal for a production environment.",
      "3) Set up your nutch-site.xml",
      "Nutch is highly configurable, but the out-of-the-box nutch-site.xml is empty. The default settings for the baked-in plugins are available in nutch-defaults.xml. Here are the settings I needed to add (and why):",
      "<property>\n <name>http.agent.name</name>\n <value>MyBot</value>\n <description>MUST NOT be empty. The advertised version will have Nutch appended.</description>\n</property>\n<property>\n <name>http.robots.agents</name>\n <value>MyBot,*</value>\n <description>The agent strings we'll look for in robots.txt files,\n comma-separated, in decreasing order of precedence. You should\n put the value of http.agent.name as the first agent name, and keep the\n default * at the end of the list. E.g.: BlurflDev,Blurfl,*. If you don't, your logfile will be full of warnings.\n </description>\n</property>\n<property>\n <name>fetcher.store.content</name>\n <value>true</value>\n <description>If true, fetcher will store content. Helpful on the getting-started stage, as you can recover failed steps, but may cause performance problems on larger crawls.</description>\n</property>\n\n<property>\n <name>fetcher.max.crawl.delay</name>\n <value>-1</value>\n <description>\n If the Crawl-Delay in robots.txt is set to greater than this value (in\n seconds) then the fetcher will skip this page, generating an error report. If set to -1 the fetcher will never skip such pages and will wait the amount of time retrieved from robots.txt Crawl-Delay, however long that might be.\n </description>\n</property>\n\n<!-- Applicable plugins-->\n <property>\n <name>plugin.includes</name>\n <value>protocol-http|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|metadata)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|indexer-solr|urlnormalizer-(pass|regex|basic)</value>\n<description> At the very least, I needed to add the parse-html, urlfilter-regex, and the indexer-solr.\n</description>\n </property>",
      "4) Set your regex_urlfilter.xml",
      "The regex_urlfilter.xml defines a set of include and exclude rules, which evaluates which urls from the crawldb will be fetched and indexed. The format of the rules is:",
      "[+|-][regex]",
      "This uses lazy evaluation so the first rule to match, top to bottom, will be applied. Make sure to put the most general rules last. Wildcards are generally expensive (especially on long urls) and uneccessary here. Evaluation is optimized to assume prefix paths. For example",
      "+^http://www.totally.fake/subdomain",
      "Will match both: http://www.totally.fake/subdomain http://www.totally.fake/subdomain/subsubdomain",
      "but not http://www.totally.fake",
      "For your very first crawl, it may be helpful to accept everything:",
      "+^.x",
      "Even for a first run, this has its drawbacks: if Nutch pulls something that it cant parse, youll get errors. Obviously for a production crawl, you will want to limit your crawl/index domains appropriately.",
      "5) Reconcile your solr and nutch schema mappings.",
      "Nutch actually includes a schema.xml with all the fields it requires out of the box at $NUTCH_HOME\\conf\\schema.xml. You could copy this directly to your Solr core directory, but I recommend adding these fields to an existing collection. Using LWS, this would be at:",
      "$LWS_HOME\\conf\\solr\\cores\\yourCollectionName\\conf\\schema.xml",
      "Nutch also has a solrindex_mapping.xml with the default fields filled in. The defaults in 1.7 were good enough that I had could pull data in to solr. However, users using a non-LWS Solr may need to also add a version field. In addition, if you need to index additional tags (like metadata), or just want to rename the fields in solr you will need to edit this accordingly. Metadata is indexed from an additional plugins, parse-metadata and index-metadata. Documentation for those plugins is available here.",
      "6) Plant your seeds",
      "Nutch is a seed-based crawler, which means you need to tell it where to start from. These take the format of a text-based list of urls, one url per line, that go in a file named seed.txt. I like apaches site for a first go.",
      "mkdir $NUTCH_HOME/urls\necho \"http://nutch.apache.org\" > $NUTCH_HOME/urls/seed.txt",
      "Remember, whatever seeds you pick, that they needs to match the regex you set in your regex_urlfilter.txt or nutch will filter them out before crawling",
      "7) Crawl",
      "At this point, everything should be set up for a test run. Most of the tutorials Ive run into are based on the old compiled command bin/nuch crawl . This is deprecated in 1.8 and beyond, to be replaced by the /bin/crawl script.",
      "To test, run the following from $NUTCH_HOME.",
      "./bin/crawl urls/ testCrawl/ http://127.0.0.1:8888/solr/yourcore 1",
      "There are more params you can add here, but you shouldnt need them to get started. Note that trailing 1 – this tells nutch to only crawl a single round. Since we set the regex-urlfilter to accept anything, it is important to set the number of rounds very low at this point.",
      "A side note here: many of the tutorials I have come across use the command",
      "./bin/nutch crawl <params>",
      "This command has been deprecated, and is disabled in version 1.8.",
      "8) Validate",
      "If that ran to completion, then you are ready to query Solr. From your browser, for a collection named test:",
      "http://127.0.0.1:12888/solr/test/select?q=*%3A*&wt=json&indent=true",
      "Should produce a single document – the nutch home page. Subsequent runs against the same crawldb should bring in pages referenced from the nutch home page, and on to the outside world.",
      "9) Debug",
      "There is a good chance that didnt work. Knowing how to debug your new tool is usually at least as important as how to set it up. This isnt a comprehensive guide, but Ill include the techniques I needed to get nutch off the ground.",
      "9a) Run the crawl script step-by-step",
      "The contents of the $NUTCH_HOME/bin directory are just bash scripts. It is educational to run through these steps once to understand what is going on, and this is what the nutch tutorial actually does. This does a few things: it lets you know where the failure occurred, recover from a failure, and skip any steps that dont apply to your crawl. I ultimately turned off both the dedup and invert link steps.",
      "9b) Look in both logs",
      "Nutch writts errors to $NUTCH_HOME/logs/hadoop.log.",
      "LWS writes its errors to $LWS_HOME/data/logs, although I found the log viewer from the sol admin to be a nicer display.",
      "9c) Look in the crawldb",
      "Nutch provides a tool called readdb, which will dump the crawl-db and its contents to a human-readable format. From the command line:",
      "$NUTCH_HOME/bin/nutch readdb testCrawl/crawldb -dump newPathToDump\nless newPathToDump/part-00000",
      "This is especially helpful for debugging fetch problems if your crawl completes without errors, but you still arent seeing any data in Solr.",
      "9d) A note about robots.",
      "Nutch is aggressively polite. This means that if a site has a robots.txt, nutch will obey the indications in that robots.txt. This will override your fetch rates, and potentially cause your fetches to fail as if the site were not reachable. There is also one network that I work from that blocks outbound bots altogether , which results in a \"abc.xyz.com/robots.txt was not reachable\" error. In general, politeness is the best policy, but this can be frustrating if you are trying to get a new system off the ground.",
      "10) Extended reading",
      "Lucidworks help documents\n  Apache Solr documents\n  Nutch configuration documentation\n  Nutch JIRA\n  Nutch Tutorial, c. 2009\n  Nutch Tutorial, c. 2011\n  Integrating Nutch with Lucidworks"
    ],
    "summary_t": ""
  },
  {
    "id": "5ea94b228a9b863e8cb9b6abc784b72d",
    "url_s": "https://opensourceconnections.com/blog/2014/05/27/drupal-devs-dont-undervalue-relevant-site-search/",
    "title": "Drupal Devs – Don’t Undervalue Relevant Site Search",
    "content": [
      "Bring your search problems to our booth at DrupalCon!",
      "Drupal developers: raise your hand if you’ve ever been in this situation. You’re ready to deploy your app. You’ve developed a beautiful site, leveraging Drupal to its max. You’ve plugged in site search through the Drupal \"Apache Solr Search\" plugin.",
      "But there’s trouble ahead. Just before you deploy you suddenly realize your search isn’t all it could be. The default Solr settings don’t quite return results you expect. Results arent relevant. Maybe your site is a news site and older news items are popping up before newer/more popular articles in search results? Maybe youve implemented a name search, and matches on non-names (the occupation Smith) crop up much higher than actual name matches? Whatever it is your users will certainly notice – and leave in droves.",
      "It turns out search is a huge part of our lives, and often a massively undervalued component of our sites. Users raised in the age of google expect highly relevant site search, or they’ll leave. Do you want take the risky route of ignoring search results quality, silently driving away frustrated users? Or do you want to be in the same class as Drupal sites like Duke Health – a recent Drupal Client of ours now with extremely smart & regularly maintained search?",
      "At OpenSource Connections, we have a long history of helping Drupal clients take their search relevancy from dud to money making. Many clients come to us with an extensive set of specific problems–failures to capture specific use cases. They want our help to try and figure out how to capture these use cases and thus improve relevancy.",
      "The problem is: how do we capture all of their use cases – all the requirements we hope to get out of search–multiple, overlapping, even conflicting requirements like incorporating variables like product profitability, document recency, and popularity? Most search developers fail at this – able to perform spot fixes on one or two relevancy problems – unable to get a holistic solution in place that captures all requirements.",
      "We have a product, Quepid, that can transform your search experience. Quepid captures search feedback & requirements in one place – in the form of tests. This lets search developers provably make progress over all your important use cases. We call this test driven search relevancy and its the core of our search relevancy practice. Let it be at the core of yours! You wouldn’t write code that you can’t test, why would you try the same with your search?",
      "Using Quepid to Tune an Ecommerce Sites Search",
      "With Quepid’s Test-Driven Relevancy approach search developers can stop and measure at every step. Did the tweak to improve the effectiveness of name search negatively impact our ability to return other search results? How good are we at all the requirements we’re trying to meet? Are we making forward progress, or did the latest tweak break important features?",
      "Armed with this information search developers can quickly get at the root of your relevancy problems. Quepid takes snapshots of your search at important milestones, allowing it to diff what’s currently being worked on vs what’s deployed. This lets search developers check in on your search, gathering intelligence and performing health checks on how your search is doing.",
      "When this product is coupled with our search services, it gives us a competitive edge other search consultancies. With Quepid, we have an ability to check in on your search, efficiently find troublesome patterns, and resolve issues quickly – while provably testing against preexisting use cases. Most importantly you’ll be able to say very directly whether our search services have been helpful or not – something not easy to get from other search companies.",
      "So, are you stuck with poor search relevancy? Feel like you’re going in circles trying to make your search relevant?",
      "We hope you’ll find us at DrupalCon to discuss your search needs and how Quepid products & services can help you make search the star of your Drupal app! Youve already built something amazing with Drupal, don’t fail your users by ignoring search relevancy!"
    ],
    "summary_t": "Drupal developers: raise your hand if you’ve ever been in this situation. You’re ready to deploy your app. You’ve developed a beautiful site, leveraging Drup..."
  },
  {
    "id": "2755e70e40eb74c312fd0f918a637bc8",
    "url_s": "https://opensourceconnections.com/blog/2014/05/29/sponsoring-drupalcon-na-2014/",
    "title": "Sponsoring DrupalCon NA 2014!",
    "content": [
      "See everyone in Austin!",
      "More-and-more were noticing the impact of Drupal in our clients businesses. Theyre able to deliver quality content and apps fast with Drupal. Thats why were excited to be sponsoring DrupalCon North America in Austin next week! We hope you’ll find our booth!",
      "This year we’ll be discussing one of the most undervalued elements of any Drupal installation: site search. Its easy to get going, but can be challenging to master. Undervalue site search, and users will silently leave in droves. Hone and tune it, and youll delight users with relevant results. (This of course where we come in!).",
      "There’s plenty to think about when implementing a solid site search for Drupal. Just like regular software, you need to think about what are my requirements? What use cases do I need to capture? Is popularity/hotness/recency important? What about fuzzy matching and domain-specific synonyms?",
      "These are all tricky issues, and just like software development it’s important to capture your use cases in one place – preferably in a place where you can test and iterate over what’s important. That’s why we’ve introduced Quepid. A test-driven search relevancy product that’s been a hit for our Drupal clients both as a tuning product and a means to deliver search services.",
      "Anyway, if you’re struggling with search for Drupal, please come find our booth! Come by and see our colleagues with your issues – Matt Overstreet, Drupal site search relevancy ninja, Daniel Beach, frontend search designer/developer extrordinare, and John Woodell, the most interesting search businessman of the world. We look forward to chatting with you!"
    ],
    "summary_t": "This year we’ll be discussing one of the most undervalued elements of any Drupal installation: site search. Its easy to get going, but can be challenging to ..."
  },
  {
    "id": "6d76fb27c9cad409b0a08655d71aedba",
    "url_s": "https://opensourceconnections.com/blog/2014/05/30/links-i-shared-at-new-york-city-solrlucene-meetup/",
    "title": "Links I shared at New York City Solr/Lucene Meetup",
    "content": [
      "Thank you everyone who came to my war stories talk on Wednesday. Really enjoyed the conversation, thanks to everyone for asking so many questions!",
      "Here are some of the links:",
      "Solr Security Proxy that you can whitelist/blacklist parameters by Alex Dergachev: Git Hub Project\n  Spyglass Solr Search Widget Toolkit by Daniel Beach: Spyglass\n  Example of GPSN w/ a complex Lucene query: http://gpsn.uspto.gov/#/search/q=%5Ba%20TO%20b&sort=score%20desc\n  I showed off the OCR tool that outputs bounding box information about words on a tiff image: Tessaract OCR\n  Quepid is our platform for talking to the business about relevancy.",
      "Building a Lightweight Discovery Interface for Chinas [email protected] Solr/Lucene Meetup  from OpenSource Connections",
      "Thanks everyone, and my colleague Doug is going to be doing a talk on Test Driven Relevancy in two months, hes a great speaker and youll learn all about how to deal with those difficult business stakeholders!"
    ],
    "summary_t": ""
  },
  {
    "id": "46ae41482aafc4fa9da5247f5e88ac94",
    "url_s": "https://opensourceconnections.com/blog/2014/06/10/what-is-search-relevancy/",
    "title": "What is Search Relevance?",
    "content": [
      "Have you ever tried a sites search and been underwhelmed with the accuracy of the results? Do you find yourself feeling frustrated and leaving when the search doesnt return what you’re looking for? Even worse – do you find yourself just assuming what you’re looking for must not exist on that site – only to find the item on that exact same site through other channels?",
      "If so, you’ve just experienced bad search relevancy. It’s something we all experience daily – a frustration for users and lost opportunity for the sites attempting to serve us.",
      "\"One of those old-timey-push-mower thingies!?!\" he said to the confused sales associate",
      "What is this called? How would you search for one? How well would the search do?",
      "I remember a time I was searching for a manual push mower online. You know the kind that you’ll probably associate with suburban America in the 1950s. Not gas powered, just the kind you manually push and the cylindrical blade accelerates to cut grass. I thought it’d be an environmentally friendly way to cut grass and get exercise.",
      "Searching on several ecommerce sites with \"manual push mower\" and \"old time lawn mower\" and lots of other similar search queries did not surface one of these lawnmowers. I was very frustrated. Part of me began to assume that they no longer exist – despite the fact that I could have sworn I had seen someone cutting their grass with such a lawn mower last week!",
      "Suffice it to say all those sites lost out on a sale as I eventually gave up and went to a home improvement store. I described what I was looking for to an sales associate. I said \"you know one of those old-timey push mowers on leave-it-to-beaver\". Sure enough he pointed me in the correct direction to the right item. It turned out I didnt know the correct terminology. This lawnmower is known as a \"Reel Mower\". Thank goodness for that sales associate! The associate had enough smarts to take what I was searching for and figure out what I had meant in a way none of the search engines were able to.",
      "Search relevancy is the practice of turning a search engine into a helpful sales associate. In the same way the associate understood what I meant when looking for a lawn mower in the store, relevant search can do the same for an online store.",
      "What this means is that bad search is bad service. Poor relevancy is the modern equivalent of a lazy sales associate that seems unwilling or unable to help. You’re likely to be frustrated and not return to that online store if the site cant or doesnt appear to want to help you.",
      "Good search relevancy, on the other hand, keeps users on the site. They’re delighted by what comes up and they want to come back for more. If you care about keeping and retaining users and customers, little can be more important than important how your site interacts with users through site search. Dont disappoint with bad service. Delight with amazing, prompt, and relevant service.",
      "The Art and Science of Relevancy",
      "The trick to relevancy is that search engines, like Solr and Elasticsearch, are simply sophisticated text matching systems. They can tell you when the search word matches a word in the document but they aren’t nearly as smart or adaptable as a human sales associate. Once a match is determined a search engine can use statistics about the relative frequency of that word to give a search result a relevancy score.",
      "Outside of this core \"engine\" a lot of search relevancy is about the development required to either jury-rig text to allow fuzzy matching or correctly boosting/weighting on the right factors. A developer working on search relevancy focusses on the following areas as the \"first line of defense\":",
      "Text Analysis: the act of \"normalizing\" text from both a search query and a search result to allow fuzzy matching. For example, one step known as stemming can turn many forms of the same word \"shopped\", \"shopping\", and \"shopper\" all to a more normal form – \"shop\" to allow all forms to match.\n  Query Time Weights and Boosts: Reweighting the importance of various fields based on search requirements. For example deciding a title field is more important than other fields.\n  Phrase/Position Matching: Requiring or boosting on the appearance of the entire query or parts of a query as a phrase or based on the position of the words",
      "Outside of this initial \"first line of defense\" that satisfies a lot of use cases, you can quickly get into more advanced areas to get more out of your search. These include:",
      "Tags and ontologies – understanding the query and the document text in terms of specific concepts instead of simply matching terms. Often considered a \"concept\" search.\n  Natural Language Processing – understanding the grammatical structure of text in the query and search result to allow deeper understanding and matchnig\n  Statistical Processes – understanding statistically the relationship between different words. For example creating code that can detect that \"Spatula\" and \"Frying Eggs\" have some level of association.\n  Click Tracking – Given enough logs of user behavior with search, post process user behavior to attempt to determine which result is statistically most likely to be the best result for a query.\n  Search Engine Plugins – Plugins that modify the built-in scoring or text analysis behavior to create a custom relevancy algorithm\n  Genetic Algorithms – Given enough data about good search quality, determine the correct values for various weights and boosts that produce the optimal results using a genetic/evolutionary process",
      "That’s a lot of approaches! And it’s an intimidating landscape if you don’t know where/how to start. The good news is that you can get a decent solution for most results with the basic features that your search engine provides. But search relevancy is a constant effort. Google is still working on their search engine and doesn’t show signs of stopping anytime soon.",
      "Creating a Search Relevancy Practice",
      "It looks intimidating, but dont fret. Step one in working on your relevancy is to figure out a good process for sandboxing these ideas. There’s a vast menu of options of approaches to improving search, but how will you know whether one has made an improvement or is making things worse? How do you test search relevancy to make sure you’re not going backwards in quality?",
      "We advocate an approach known as Test Driven Search Relevancy. Using a tool like our product, Quepid, that can evaluate important search queries against a list of known good/bad documents we can make statements about the progress of our search.",
      "In fact, this kind of practice is more important than normal software automated testing. Why? Your search developers often don’t know good search. They need business stakeholders to help craft search goals and use cases. Correctness can’t easily be defined by the developers, it takes collaboration. It takes putting the search developer in the same room as your equivalent of the \"sales associate\" to create use cases & tests – to help figure out what a customer really means when they put \"manual push mower thingy \" in the search box.",
      "How will you capture this information from stakeholders? Can you put a dozen sales associates in a room to keep trying search and giving you feedback?",
      "No you’ll need to centrally store that feedback in one place. In a way that whatever mundane or crazy idea you want to try to improve relevancy, you can instantly get the feedback of dozens of skilled sales associates without them in the room.",
      "What form should this feedback take? Preferably this would be in the form of identifying which results should come back for which search queries based on the expert judgement of your equivelant to a sales associate – some kind of expert in the content you serve. In the industry these curated testing lists are known as \"judgement lists\". They can help guide your search efforts by keeping the knowledge of your best associates at the fingertips of search developers.",
      "Once this feedback is captured in a testing tool, you can apply the skill of your best sales associates to all your users’ important queries a hundred times a day and get a reliable score knowing whether search results meet up with expectations of all stakeholders.",
      "Whether you build a tool for your own use or use our tool Quepid, this practice should be the central cycle to your search relevancy work. As new search use cases arise, as new problems are solved, armed with this evolving feedback, developers can keep answering the question about whether new solutions are causing old problems to crop up again. Many ideas sound terrific until they’re met with the brutal reality of testing. Perhaps the new gizmo actually makes search worse. You want to know that immediately, not when the new relevancy algorithm is pushed to production and users leave in droves!",
      "So have fun with your search – try all the interesting approaches. But test early and often. Dont get stuck not understanding how or why things work, instead focus on improving quality step-by-step armed with solid tests that guide and reinforce vital use cases.",
      "Stop Ignoring Relevancy",
      "Its become more-and-more critical that search relevancy be a front-and-center concern. Users require good service. In the absence of a sales associate or librarian, all youve got to build a relationship with a client is search. Do you want to leave them in the lurch? Or turn their engagement into sales? In an age of google, smart search is the norm. Dont be the exception.",
      "Of course if you need help with your search, Get in touch and well be glad to discuss. And check out Quepid as a test-driven relevancy sandbox!"
    ],
    "summary_t": "Have you ever tried a site’s search and been underwhelmed with the accuracy of the results? Do you find yourself feeling frustrated and leaving when the sear..."
  },
  {
    "id": "3586001871b688fe74ac831ecf9a1da7",
    "url_s": "https://opensourceconnections.com/blog/2014/06/30/rds-is-expensive-a-cautionary-aws-tale/",
    "title": "RDS is expensive – a cautionary AWS tale",
    "content": [
      "I wanted to share with the world a cautionary story related to my by @softwaredoug that reminded me that while Amazon AWS is amazing, its also best used in situations where your needs are extremely variable. Its the natural gas powerplant versus coal powerplant of hosting providers.",
      "I looked at our bill out of curiosity. I was using AWS RDS (Relational Database Service) for the Quepid database. It took up a large chunk of the AWS bill:\n\n  RDS $721.32\nEC2 $688.17\nRoute53 $3.01\n\n  I had been using RDS for all kinds of stuff – a testing scratch database, my dev site database, etc. Plus I happily clicked \"next\" -> \"next\" etc when setting this up. Which of course massively over provisions for what I need.\n\n  (So naturally I face-palmed myself when I saw that billing data! Head down in shame, etc etc)\n\n  Ive since scaled RDS usage back so that it only hosts the Quepid production data with all the appropriate/default backup and availability bells & whistles turned on. Its provisioned more appropriately. The cost should be much more reasonable.\n\n  Just a cautionary tale before dorking around with AWS services. It makes me want to consider other cloud/hosting options. It makes me pine for the days of very cheap shared LAMP hosting like these far cheaper prices: http://www.pair.com/services/web_hosting/",
      "We are big fans of AWS, and recommend it frequently to our clients, but its good to remember that for basic constant loads, there are many other cheaper solutions out there."
    ],
    "summary_t": "I wanted to share with the world a cautionary story related to my by @softwaredoug that reminded me that while Amazon AWS is amazing, it’s also best used in ..."
  },
  {
    "id": "097835b62f184a76656148789636e7c8",
    "url_s": "https://opensourceconnections.com/blog/2014/07/11/quepid-athena-release/",
    "title": "Quepid : Athena Release",
    "content": [
      "As the newest full time developer working on Opensource Connections search relevancy tool, Quepid, Im happy to announce that our newest release, codenamed \"Athena\", is now live. This release is the first in a series named after Greek figures in mythology that aims to add powerful new features for our tool.",
      "Athena",
      "Athena (In the Vatican)",
      "So what does the Athena release of Quepid contain? We focused on user feedback to guide our development efforts, so most of our efforts went into squashing bugs and adding minor, but useful, features. Several users wanted more initial guidance in their usage of Quepid, so now a helpful wizard guides you through the initial setup of usage. An oft requested feature was the ability to delete cases, and that was implemented quickly. We also added some functionality focusing on microinteractions, such as the ability to hover over certain objects to get a popup with their functionality. We fixed several bugs, and one of them relating to our frontend took our team on a deep journey into the innards of Angular (specifically, digging through Angular source code to discover how ng-repeat actually tracks objects submitted to it…). Quepid should now run more quickly and feel quite a bit more responsive!",
      "Boreas and Beyond…",
      "So whats next for Quepid? In our next release, codenamed \"Boreas\", we want to help people better understand the significance of the search results returned, and why those search results were returned. This will most likely include visualizations that better show the exact factors causing results to return for a query.",
      "After Boreas, we plan to add even more features, respond to more recent user feedback, and improve our UX. Much of this will focus on improving Quepids ability to handle very large (1k+) sets of queries. Our user exerience in this area has stayed the same since Quepids prototype stage when it was designed as an internal tool, and OSC never needed to use more than 100 queries. Quepid definitely works with a large number of queries, but redesigning Quepids interactions and microinteractions to be more pleasant and efficient in those use cases is a priority for us.",
      "If youve tried Quepid out and have some ideas or suggestions, Doug (the mastermind) and myself (his righthand lackey) are always looking for feedback, whether its tweeted, emailed, or written on the back of a twenty dollar bill and mailed to… just kidding."
    ],
    "summary_t": "As the newest full time developer working on Opensource Connection’s search relevancy tool, Quepid, I’m happy to announce that our newest release, codenamed ..."
  },
  {
    "id": "ee0e009482ac650b4fc5cc283adf8701",
    "url_s": "https://opensourceconnections.com/blog/2007/04/09/figuring-out-the-local-timezone-in-ruby-on-osx/",
    "title": "Figuring out the local timezone in Ruby on OSX",
    "content": [
      "I am starting to hit some of the odd corners of Ruby. For example, I want to know what the local timezone of my Mac is. I can run Time.now and parse the output, but unfortunately I get back EDT, and the TZinfo library cant parse that because its not a standard timezone, throwing an TZInfo::InvalidTimezoneIdentifier.",
      "I did discover however that on OSX the file /etc/localtime is sym-linked to the correct timezone file! When I did ls -l /etc/localtime I got back /usr/share/zoneinfo/US/Eastern, which gives me something to parse that TZInfo will understand.",
      "Below is my unit test that demonstrates my progress in hacking to a solution! Hopefully this will help anyone else.",
      "My solution is very brittle, but should work for now. Anyone have a better suggestion, please speak up!!!",
      "def test_getting_local_timezone\nt = Time.now\nassert_equal `EDT, t.strftime(`%Z)\n\nassert_raise(TZInfo::InvalidTimezoneIdentifier) { Timezone.get(`EDT)}\n\nresult = %x[ls -l /etc/localtime]\nputs result\n\nassert_match '/etc/localtime -> /usr/share/zoneinfo/US/Eastern', result, 'Make sure we get back US/Eastern. This only works when you are in US/Eastern Timezone!'\n\nstring = 'lrwxr-xr-x 1 root wheel 30 Jan 17 20:38 /etc/localtime -> /usr/share/zoneinfo/US/Eastern'\nsplit = string.split('/')\nassert_equal 8, split.size\ntimezone = '#{split[split.size-2]}/#{split.last}'\nassert_equal 'US/Eastern', timezone\n\nend"
    ],
    "summary_t": ""
  },
  {
    "id": "c3eae14c508523ca3e8f08bbd78c6017",
    "url_s": "https://opensourceconnections.com/blog/2014/07/13/reindexing-collections-with-solrs-cursor-support/",
    "title": "Reindexing Collections with Solrs Cursor Support",
    "content": [
      "When a Solr schema changes, us Solr devs know whats next – a large reindex of all of our data to capture any changes to index-time analysis. When we deliver solutions to our customers, we frequently need to build this in as a feature. Many cases, we cant easily access the source system to reindex. Perhaps the original data is not easily available, having taken a circuitous route through the Sahara to get to Solr. Perhaps the sys admins dont want us to run a nasty SQL query with 15 joins to pull in all the data.",
      "How does your data flow into Solr?",
      "In these cases, we might save docs in an easy-to-digest, already denormalized form, perhaps in the form of solr XML or JSON files on disk, so that we can kick off a reindex job by simply replaying these prebuilt documents through Solr. It mostly works. Of course now weve got new problems. Now were maintaining and keeping in sync a large set of pre-chewed data in a third scratch space.",
      "So why not just use Solr as a source for its own data? We could, for example store all of our fields, then upon a need to reindex: (1) create a new collection with the new schema & configuration (2) read data from the original Solr collection and one-by-one (3) write the read data to the new collection. Once done, we could blow away the old collection, and viola everything is in place!",
      "Step (1) is fairly straightforward, a matter of using the collections API or modifying solr.xml to suit our needs. Step (3) is just a matter of a simple translation from whats read to Solr back to what we write to Solr.",
      "The \"Deep Paging\" Problem",
      "What about step(2)? Well Solr has \"start\" and \"rows\" params, cant we just set rows=100 and increment start by 100 everytime to page through all the documents? Successive queries of q=:&start=0&rows=100 then q=:&start=100&rows=100 etc?",
      "Turns out Solr has typically been very slow at scanning through a set of search results. Solr gets progressively slower as you go deeper and deeper into the search results. How slow? Well I have numbers to show you! Writing my own tests (comparable to hossman’s in his blog post) show for a fairly small data set (70,000 documents) just pulling back just the id field. Pulling back 100 fields at a time, we see a gradual degradation of each subsequent query to pull back the next set of documents. As we get farther down the result set, we plateau around 200ms per query. This quickly adds up – the whole process takes about 120 seconds.",
      "Deep Paging Problem: using start, each progressive page retrieval gets progressively worse",
      "Why does this happen? Search engines are built around the idea of sorting on a relevancy score. Its based on the assumption that you’ll likely only care about the ten or so most relevant results. The underlying data structures therefore are built to keep track of some top N documents. When the data structure fills up, we can safely discard anything that can’t compete with what’s already in the top N. A runner that finishes a marathon in 3 hours, clearly isn’t going to be in the top 10 if the current top 10 finisher finished in 1hour 30 minutes.",
      "Digging one step deeper, as Solr (really Lucene) iterates through matching documents for a query, they get scored. These docs get picked up they get collected by a Lucene class knows as a \"Collector\". What is Collector’s job? Its job is to store the match for later (or perhaps throw it away) based on the kind of query the user is executing.",
      "With searches where relevancy is the primary sort value, a TopDocsCollector collects matching documents. TopDocsCollector uses a priority queue. The queue is sized based on how many results need to be evaluated. In the case of a Solr query of rows=100 and start=10000, this means 10100 documents need to be collected before a definitive judgement on what are the ranked 10000-10100 documents. Imagine a marathon with 11000 participants. We can’t tell you who placed 10,000th without knowing something about the first 9,999 racers.",
      "Giving Lucene Hints",
      "However, we can make a huge improvement if we know the time of the 9,999th racer. We can scan over the times of all the racers and quickly determine whether racers beat the 9,999th racer and decide to ignore those. We don’t need to store them or anything, we can safely discard the fastest runners. Lucene exposes this strategy in IndexSearcher’s searchAfter method that lets you provide an \"after\" value to the IndexSearcher.",
      "This value (one of the search documents produced by a previous query) gives Lucene the \"value\" of the 9999th document (or time of the 9999th racer). The after value might wrap a score of actual field value (in the case of sorting on a field) that can help Lucene pick up where it left off. When searching, Lucene does what it usually does – iterating through docs that match query, scoring them, and then forwarding them to a collector. In this case, Lucene forwards to a paging collector. This collector is the smarts that knows to throw away the 1-9999th runners (this runner is faster than this 9999th runner I know about, throw them away!). By looking at the after doc it has access to, this collector can more intelligently save the data it needs to save (the 10000-10100th runners).",
      "This strategy means we only store 100 documents. Not 10100 like we would in a typical search. Just like that we can more quickly page through search results!",
      "So how would we take advantage of this functionality in Solr? Well there’s no obvious after parameter to send Solr that says \"look I’ve already seen runners that finished before time X, just give me the next 100\". Or in other words \"heres where I left off in the search results, you can safely ignore anything that compares before this marker\".",
      "Solr Cursors",
      "Well actually, now there is! Solr’s new cursor support effectively emulates Lucene’s after parameter when searching. How does this work? After every search, Solr serializes out a marker back to the client in the response. This marker may take the form of encoding a score, or it might represent a pointer to a place in the part of Lucene’s FieldCache that presorts fields for segments. When Solr gets this marker again (in the form of cursorMark) it can use Lucene’s field cache to reconstruct the after document. This marker is sent back on subsequent requests instead of a \"start\" parameter to help Solr pickup where it left off.",
      "(I’m not going to cover the exact details on how to use this, but if you’re interested hossman’s original blog article as well as the Solr Reference Guide’s page on cursors are great resources.)",
      "You can see the performance improvements here. Paging through 70,000 results takes about 12 seconds total (compared to about ten times as long for non-cursor). An interesting note is that with more documents, the difference gets relatively much larger. Theres not a linear relationship between these times. Cursor time is not always ten times better than using start. With very large document sets, the factor between the two is much much larger. At small sets of documents in my testing (< 10K) the difference between using start and the cursor feature were negligible.",
      "Cursor maintains a steady QTime during paging while start gets progressively worse",
      "Nice! So the new cursor support helps us tremendously with iterating through the documents. Instead of an extremely inefficient process of building up a giant priority queue every time during our reindex, we can only collect what we need to on each step. Its certainly a grep step in the right direction to help us not need an extra bit of storage to save off documents for reindexing.",
      "Other Parts of the Problem",
      "The new cursor support solves a large part of the problem of reindexing from Solr back to Solr. However, its not a panacea for solving reindexing. I still have a lot of reservations to this approach:",
      "Your search infrastructure needs to be able to support an index size twice the current index size\n  Your search infrastructure can handle the load of iterating through every result",
      "I also have some other open questions to this sort of solution:",
      "How much extra does it cost for Solr to dump out lots of fields and large fields? Here we’re only outputting \"id\", so its not entirely reasonable. I have done this with more than id, but would love to get a handle on performance as a function of number and size of fields being reindex?\n  How much faster would the javabin codec be then using XML or JSON? I suspect somewhat faster, but how much exactly?\n  How does performance of Solr paging compare to other database solutions? Are those databases more stateful in their paging strategies. Solr/Lucene still iterates and attempts to score every document, even if it discards most of them, perhaps there’s a better strategy?\n  When cursors miss updates, the new documents may get sorted before the current cursor and we may miss them. Perhaps this isn’t the best solution for rapidly updating indexes. Or perhaps we need to flag documents as they’re updated? Or maybe we could maintain a bloom filter of doc ids that could give us hints as to whether or not docs have been reindexed?\n  How would we cope with copyFields? We may retrieve them in the result, but we wouldnt want to write them back. Wed have to be sure to only store non copyField fields.\n  How does performance/usability compare to Elasticsearchs scan and scroll features.",
      "And it also causes me to think of some other possibilities that might be further improvements:",
      "Would it be possible to build a Solr component that could read from one collection and write to another – thus avoiding the network traffic?\n  Could we achieve the same thing by forcing an atomic update of a document? Perhaps by simply updating a counter in the document, it would trigger the read-modify-write behavior in an atomic update and force the document to be reindexed?",
      "Final Thoughts",
      "Still, for reasonably sized and fairly static data sets, this solution seems to work fairly well. We’ve had a couple occasions outside of reindexing where enumerating the whole index is necessary. One area that comes to mind is our use of term vectors in our semantic indexing work. For a lot of advanced machine learning work, you want to get all the term vectors of every Solr document. This traditionally involved paging through every document. Maybe cursors can make this more efficient and allow you to use external tools like Mahout in a more real-time capacity? Another blog post perhaps :)",
      "Do you have any thoughts on reindexing? Have you used the new cursor support – what do you think? Are you someone struggling on how to connect your data infrastructure to search in your backend? If so, contact us, we may be able to help!"
    ],
    "summary_t": ""
  },
  {
    "id": "352a5dacc4c980b64c9ee9abd9378c11",
    "url_s": "https://opensourceconnections.com/blog/2014/07/15/improving-the-camel-solr-component/",
    "title": "Improving The Camel Solr Component",
    "content": [
      "Weve been using Apache Camel a fair amount recently as our ingestion pipeline of choice. It presents a fairly nice DSL for wiring together different data sources, performing transformations, and finally sending data to Solr. Using the normal Solr component, you can write code that looks like this:",
      "from(\"file://foo?fileName=input.csv\")\n    .unmarshall().csv()\n    .split(body())\n        .to(\"bean:convertToSolrDoc\")\n    .setHeader(SolrConstants.OPERATION, SolrConstants.INSERT)\n    .to(\"solr://localhost:8983/solr/collection1\")",
      "This code defines a camel route. This route takes a csv file, splits it line-by-line into individual records. For each record (after the split) it transforms the csv record into a SolrInputDocument. The Solr document is then sent to Solr for insertion.",
      "Help us make the camel-solr component better!",
      "Being able to tie together data and features from different systems is Camel’s strong point. Perhaps you want to build a distributed pipeline and tie in leader-election so that only one of these routes runs at a time. Well you can! Just tie in some features from the Zookeeper component.",
      "Built-In Solr Component",
      "The Solr component has most of the basic features you’d expect. It wraps SolrJ and performs fairly standard indexing operations. However as we get more experience with Camel, we’ve been finding more-and-more that we’re missing many much-needed features.",
      "For example, the Solr component:",
      "Has no built in support for Solr Cloud. More often our clients expect to be able to run SolrCloud, and would like to leverage SolrJ’s CloudSolrServer for optimal document routing.\n  Does not support communication to Solr over https, only http. Even in trusted backend environments, many of our clients feel the need to put sensitive data behind https. So this becomes another must.\n  While you can write to Solr, there’s no way to read through Solr search results. In Camel terminology, while there’s a Camel producer there’s no consumer. This is a very important nice-to-have for work such as reindexing from Solr back to Solr",
      "Building a Better Solr Component",
      "So what have we done? Well instead of lots of one-offs, we’ve decided to make dramatic improvements to the Camel Solr component! You can find our improvements here ready for production use (specifically this pull request)! What’s been done out of our wish list above?",
      "We now support Solr over https (by specifying the URI solrs://) and SolrCloud (by specifying solrCloud in the URI and passing a zkHost and collectionName parameter). For example, if I wanted to reimplement the example above to go to Solr Cloud I would simply do:",
      "from(\"file://foo?fileName=input.csv\")\n    .unmarshall().csv()\n    .split(body())\n        .to(\"bean:convertToSolrDoc\")\n    .setHeader(SolrConstants.OPERATION, SolrConstants.INSERT)\n    .to(\"solr://localhost:8983/solr?zkHost=localhost:8123/solr&collectionName=collection1\")",
      "Getting all this to work wasn’t terribly difficult, but we wanted to ensure that all of the camel tests passed regardless of whether Solr was being used over http, https, or in cloud mode. So much of the challenging work was getting tests to run against Solr in each of these modes. We learned a lot about how to get the embedded Jetty Solr server to work in https.",
      "We’re particularly grateful for the MiniSolrCloudCluster support that was recently added to Solr 4.8. This addition allowed us to sidestep a large amount of work we had started to create an embedded Jetty Solr server that was running in Solr Cloud server. Instead, we were able to bring up a MiniSolrCloudCluster for our tests with only minimal fuss. In fact if you need a usage example other than the existing tests, check out this file in our test code.",
      "Naturally, we’d like these changes to flow back to the Camel project. So we’ve attached our pull request onto this Jira issue. We hope you’ll help us out by voting for this issue. In any case, please try out our component, and please feel free to send us bugs/feedback/pull requests! And stay tuned to continued improvements!",
      "And of course, if you need any help tying together a reasonable to connect your systems to Solr, contact us and leverage our expertise with Camel and under frameworks that help make your data searchable!"
    ],
    "summary_t": "We’ve decided to make dramatic improvements to the Apache Camel Solr component! You can find our improvements here ready for production use (specifically thi..."
  },
  {
    "id": "87863fa31aaa7e419085f7ddf4c6b38c",
    "url_s": "https://opensourceconnections.com/blog/2014/07/24/using-quepid-to-improve-relevancy-of-advance-auto-intranet-search/",
    "title": "Using Quepid to Improve Relevancy of Advance Auto Parts Intranet Search",
    "content": [
      "",
      "Recently, Advance Auto Parts contacted OSC to improve the search relevancy of their intranet application, Starting Line. Starting Line serves as the knowledge base for every store employee, so having relevant internal search results helps keeps employees connected with resources and company news.",
      "Through our two day Quepid relevancy assessment, we helped bring together content experts and developers. Advance’s field communications team identified several problems with their Solr search results:",
      "Large number of search results, even for targeted queries Search queries for common forms returned spurious results simply because many items mentioned \"form\" somewhere in their text\n  Important components outside of text matching, such as how recent the document was published or the popularity of a specific document, were not incorporated into the results",
      "To begin our collaboration with the Advance authoring team, we constructed important use cases, and then used Quepid to describe the use cases in the form of executable tests. We then took a snapshot of how well the current search algorithm met up to these expectations, so later we could visually compare our progress over the rest of the two day assessment.",
      "We used Quepid to explore the impact of several changes, including:",
      "Phrase Boosting so that, for example, if a user searches for \"automatic transmission\", documents with the full phrase \"automatic transmission\" get boosted above documents containing just \"automatic\" or \"transmission\"\n  Understanding how to correctly apply weights to fields\n  Understanding how to allow each matched field to have a vote in the score (dismax usually lets the highest matched field count in the score)\n  Strategies for applying a boost for date and popularity",
      "As we tried each algorithm change, the use cases we captured earlier allowed us to easily test the impact of those changes, both positive and negative. More importantly, at every step Quepid gave us insight into why the relevancy algorithm produced the results we were seeing. Using snapshots, Quepid let us easily compare our final results with where we began, letting the customer judge our progress for themselves. To put it in the customer’s words:",
      "As a content editor for our intranet, I’m very concerned when my work doesn’t show up in our Solr search results like it should. Until Quepid, getting to the root cause was rarely worth the effort. Investigation attempts would fizzle out in a confused haze of screen shots and waiting for re-indexes. But with Quepid, we are now working together like speed chess players, tweaking config settings, defining success and swapping out dozens of test queries in a flash – refresh and repeat. And, we don’t even have to worry that our Dev environment doesn’t exactly match Production, because we have a Production sandbox! Will Carter Field Communications, Advance Auto Parts",
      "Do you want a highly relevant search like Advance Auto Parts? Do relevancy problems plague your search and you’re wondering how to start tuning? Contact Us! Whether it’s a quick tune-up like Advance or a long-term project with many requirements, OSC can use Quepid to make your search relevant."
    ],
    "summary_t": "Recently, Advance Auto Parts contacted OSC to improve the search relevancy of their intranet application, Starting Line. Starting Line serves as the knowledg..."
  },
  {
    "id": "149109f40de4398c517f2beb8a0589d5",
    "url_s": "https://opensourceconnections.com/blog/2014/08/18/introducing-splainer-the-open-source-search-sandbox-that-tells-you-why/",
    "title": "Introducing Splainer – The Open Source Search Sandbox That Tells You Why",
    "content": [
      "One piece of feedback that has consistently come with our Quepid search testing tool is the need to understand \"why\" search results come back the order they do. In plain English, what factors influence search the most? Why does my search engine think a document about \"water bottles\" is more relevant than \"baby bottles\" for a search about \"milk bottles\"?",
      "Indeed this is the entire art and science of search relevancy. Its not magic gnomes inside a box that understand all about baby bottles. No its heavily tuned heuristics that Solr and Elasticsearch use out of the box (in the form of Lucene’s scoring systems) based on decades of information retrieval research that rests on the foundation of dumb string matching.",
      "How do we tune this insanity? Well luckily you can retrieve the explain information – the debug information that Lucene gives you telling you exactly how each document was scored the way it was. Armed with that information, you can alter how you query your search engine (reweight this field, boost on that field, etc). Unfortunately, the explain is full of unhelpful search nerd trivia (do you know what coord means?) not to mention deeply nested and often redundant information. Good luck if you want to parse this with your eyes. Just give up… your done. Luckily, there are parsers out there. Copy and paste your explain info, and get something a little more sane to deal with.",
      "But even with nice parsers, we continue to face two problems:",
      "Collaboration: At OpenSource Connections, we believe that collaboration with non-techies is the secret ingredient of search relevancy. We need to arm business analysts and content experts with a human readable version of the explain information so they can inform the search tuning process.\n  \n  \n    Usability: I want to paste a Solr URL, full of query paramaters and all, and go! Then, once I see more helpful explain information, I want to tweak (and tweak and tweak) until I get the search results I want. Much like some of my favorite regex tools. Get out of the way and let me tune!",
      "I’m proud to announce that we’ve taken our first big steps in these directions with Splainer. Splainer open-sources the core sandbox behind Quepid including new features that tell you why a search result is appearing where it is. At your fingertips in Splainer are three levels of explanation data:",
      "Hot Matches immediately tell you which matches most influence your search results. Often many matches occurr when searching, but due to the machinations of how these matches factor into other search operations, it can be hard to determine which matches matter. We figure that out for you!\n  \n  \n    Summarized Explain summarizes the relevancy calculation in more human readable terms. This takes things one level deeper. If this were math homework, \"hot matches\" would be the answer and the \"summarized explain\" would be showing your work. If you want to know exactly whats going on, look at the summarized explain.\n  \n  \n    Finally, the ugly stuff is still there if you want it – the raw explain pulled straight from Lucene. (yuk) for when you really just absolutely need to see your eyes bleed.",
      "But wait theres more! Modify how you query Solr in the application. A key part of our sandbox is to be able to work with your Solr instance directly from the browser. In fact, the entire application is driven 100% through HTML and Javascript (no backend involved). If your browser can see it, then Splainer can see it! Simply paste your Solr URL from your browser and into Splainer – and it will work! Start tuning to your hearts content!",
      "As I said the entire project contributes components of Quepid to the open source community under an Apache license! And this is just the beginning. We’ve already got Elasticsearch support in the works. And we want to keep working on making the explain information even less search geeky. As a sandbox, Splainer has access to the queries being executed. We should be able to tie the two together to definitively say \"this happened because you boosted on this field!\".",
      "We hope youll give it a spin and let us know how it can be improved. We welcome your bugs, feedback, and pull requests. And if you want to try the Splainer experience over multiple queries, with diffing, results grading, a develoment history, and more – give Quepid a spin for free!"
    ],
    "summary_t": ""
  },
  {
    "id": "62515eb08342ee38d6c41eef11c0b795",
    "url_s": "https://opensourceconnections.com/blog/2014/08/28/september-chock-full-of-talks-dougtember/",
    "title": "September Chock Full of Talks (Dougtember?)",
    "content": [
      "I somehow managed to line up a speaking gig for every week in September! I hope youll join me on this insane marathon. Ill be talking about topics key to what we care about at OSC: search as a datastructure, search relevancy, and search/big data at performance and scale. Dont hesitate to email me if youd like to chat about any of these topics before or at the conference!",
      "Heres what Ive got lined up:",
      "Modern Python Concurrency",
      "PyCHO (Python Charlottesville Users Group) September 2nd, 6:30 PM, Center for Open Science, Charlottesville VA",
      "Are you choosing the right model for your analytic and data-processing workload? Is it going to scale? Do you have the right concurrency model? Andrew Montalenti and I will be talking about Python and Concurrency! Im going to setting the stage talking about the primitives the OS exposes, as well as the current state of Python concurrency – including libraries such as gevent and twisted. Andrew will introduce everyone to the new asyncio library that hes used to great effect on his Apache Storm library, streamparse.",
      "Test Driven Relevancy",
      "NYC Solr/Lucene Meetup September 10th, 6:30 PM, XO Group Inc, New York, New York",
      "Do you struggle to maintain search relevancy over time? This is the talk Ive given before on the ideas behind our product Quepid. This time Ill have Splainer to demo too! Heres the blurb:",
      "Getting good search results is hard; maintaining good relevancy is even harder. Fixing one problem can easily create many others. Without good tools to measure the impact of relevancy changes, theres no way to know if the \"fix\" that youve developed will cause relevancy problems with other queries. Ideally, much like we have unit tests for code to detect when bugs are introduced, we would like to create ways to measure changes in relevancy. This is exactly what weve done at OpenSource Connections. Weve developed a series of tools and practices that allow us to work with content experts to define metrics for search quality. Once defined, we can instantly measure the impact of modifying our relevancy strategy, allowing us to iterate quickly on very difficult relevancy problems. Get an in depth look at the tools we utilize when we not only need to solve a relevancy problem, we need to make sure it stays solved over the products life.",
      "Hacking Lucene for Custom Search Results",
      "DC Solr/Lucene Meetup September 18th, 6:00 PM, Comcast Labs",
      "Stuck on a problem that might need very specific search ranking? This is my talk on working with custom search scoring and ranking. If youve ever been stuck on a tough search relevancy problem and think you need a custom solution, this talks for you!",
      "Heres the blurb:",
      "Search is everywhere, and therefore so is Apache Lucene. While providing amazing out-of-the-box defaults, theres enough projects weird enough to require custom search scoring and ranking. In this talk, Ill walk through how to use Lucene to implement your custom scoring and search ranking. Well see how you can achieve both amazing power (and responsibility) over your search results. Well see the flexibility of Lucenes data structures and explore the pros/cons of custom Lucene scoring vs other methods of improving search relevancy.",
      "Elasticsearch Night!",
      "Open Source Staunton, September 25th, 6:00 PM LightCastle Technology Complex, downtown Staunton VA",
      "New to search? Want to understand why everyones so excited about Elasticsearch? My colleague, Daniel Beach, and I will be giving two short presentations introducing everyone to Elasticsearch! This is a chance to come out and learn about both the fundamentals of search engines as a technology while getting started with Elasticsearch. If you want to begin using Elasticsearch for a new project, or have any questions about why search engines are so great, come out to this talk!",
      "I hope youll come out to find me for these talks. Please feel free to email me if theres any search problems youd like to chat about!",
      "Cheers!"
    ],
    "summary_t": ""
  },
  {
    "id": "aa44c96078934a1521c7e296d076dc16",
    "url_s": "https://opensourceconnections.com/blog/2014/09/17/cassandra-summit-2014/",
    "title": "Recap of Cassandra Summit 2014",
    "content": [
      "OpenSource Connections was well represented in San Francisco at this years Cassandra Summit 2014. We had Chris Bradford, Eric Pugh, and Matt Overstreet in attendance for the training, sessions, and networking events. John Berryman represented OSC with a presentation discussing CQL 3 and maps to the Cassandra internal data structure.",
      "The Opening Keynote covered the release of Cassandra 2.1 with a wonderful set of features and fixes. Aaron Morton from The Last Pickle held a talk in the Grand Ballroom outlining specific features and changes (with their associated JIRA numbers) between 2.0 and 2.1. This helped underscore many of the improvements discussed throughout the keynote.",
      "Cassandra and User Authentication",
      "Two back to back talks centered on Cassandra’s use within authentication systems. First up Disney discussed their new C* powered OAuth system. This service is used internally across many properties and applications. The calling system requests a token and provides a expiration time along with it. Once generated, the token is set in Cassandra with a TTL equal to the requested expiration. Tokens are automatically expired the token after the appropriate period. Disney also took the time to predict the read and write paths across the cluster. This guarantees that for every read at least one node in the cluster will have the most recent data.",
      "Next Stormpath presented a method for clustered session storage with Cassandra and Apache Shiro. Apache Shiro is a java security framework which encompasses authentication, authorization, cryptography, and session storage. In Stormpath’s sample implementation TTLs are used to expire sessions in a similar manner to the Disney talk, although less OAuth focused. One method specifically called out was touch() which could be used by rich client side applications to reach out and let the server side know the session is still active.",
      "In both cases Cassandra’s cross datacenter replication was mentioned as an additional benefit when working with authentication and session data. It was worth noting that using TTLs in this way requires examining the gc_grace period on the session related column families. Once a TTL is tripped a tombstone is generated in the system. These are not removed until after gc_grace period. Lowering this value on those tables will keep the SSTable’s smaller as compaction is run.",
      "Cassandra at Scale",
      "Apple dropped some massive scale numbers on the summit this year. They are currently running over 75,000 Cassandra nodes. With a single cluster spanning over 1,000 nodes. Cassandra is running in the million of operations per second range. The talk that followed these impressive digits delved into specific JIRA issues discovered while running these sized clusters. Each issue discussed would not appear to have a significant impact, but from the perspective of a large cluster had far reaching implications.",
      "Hardware and network partition repairs can be costly depending on the number of nodes and amount of data. Node repairs with extremely large datasets may cause unnecessary duplication of data across the cluster (far exceeding the required replication factor). This lead to the ability to perform partial repairs where token ranges may be specified to prevent excess data from being copied. In turn this reduces both repair time and the amount of data needed to perform said repair.",
      "Features like Lightweight Transactions can incur serious performance penalties when run across a larger cluster. Cassandra’s implementation of paxos requires four round trips. Apple’s teams worked to reduce the time needed to perform these transactions by cutting down on the number of locks during the paxos determination.",
      "CQL3 & Cassandra Internals",
      "John Berryman had an enlightening presentation exposing the format of the internal Cassandra data store. He began with visualizing the Cassandra data model. This lead in to the a discussion of the read and write paths along with their related caches. A comparison of simple rows and SQL to CQL followed. Next he approached how composite keys, sets, lists, maps and how they translate to wide rows. Wrapping up the presentation John demonstrated how to use the Cassandra CLI. This tool may be used to show how Cassandra maps the CQL statements to it’s storage engine.",
      "Sponsor Hall",
      "Instaclustr, who recently received funding from Datastax, is a managed hosting service of C* in AWS with a dedicated ops team and monitoring. They handle provisioning, scaling, and maintaining your C* cluster in an Amazon VPC environment.\n  Databricks / Spark - Provided an awesome USB key to attendees containing multiple spark tutorials and a sample dataset. Spark has been making waves in the big data community. It has been integrated with DataStax Enterprise 4.5 for analytics workloads."
    ],
    "summary_t": ""
  },
  {
    "id": "724d720cc2ed5e2ec1f12d80cf3d65f0",
    "url_s": "https://opensourceconnections.com/blog/2007/04/11/google-calendar-api-accepts-bad-data/",
    "title": "Google Calendar API accepts bad data!",
    "content": [
      "If youve been reading my posts lately, youve noticed lots of posts about parsing working with Time and Dates in Ruby. This has been because my hobby project has been to write a Ruby tool for synchronizing my Apple iCal calendar to my Google Calendar.",
      "This project is published on RubyForge as Novo, and is finally nearing a 0.1 release.",
      "As Ive been working on flushing out a lot of the bugs, Ive learned quite a bit about the GData API. And one of the things Ive noticed is that you can create bad data in their system via the REST API pretty easily. My code had a bug where it wasnt converting Times properly when inserting events into gCal, so I managed to put the same event in twice, once without start and end times, and once with! Unfortunately I cant delete the bad entry through the Google Calendar interface. It shows up in searches, but not in the listing:",
      "",
      "Because of a couple of bad calendar entries that I created during development, Ive add an ignore option so that the bad entries can be permanently ignored.",
      "As I get closer to the Novo 0.1 release Ill post more about the tool"
    ],
    "summary_t": ""
  },
  {
    "id": "61731d92f972fe6dd1d6e085ab0158ac",
    "url_s": "https://opensourceconnections.com/blog/2014/09/19/the-semantic-web-up-and-coming-impressions-of-semantics-2014/",
    "title": "The Semantic Web up and coming – impressions of SEMANTiCS 2014",
    "content": [
      "When you hear someone say about a technology that it only works in theory, it is too labour-intensive and `it is not industry-ready, chances are that they are talking about semantic web technologies. As my experience has been different in a semantic search project for OpenSource Connections, I went to the SEMANTiCS 2014 conference in Leipzig, Germany, earlier this month, to learn more about the state of the art of semantic technologies and Linked Data. SEMANTiCS was preceded by three days of workshops and community events, including the 2nd DBpedia community meeting.",
      "Right from the start, the conference left no doubt that semantic web technologies are now mature enough to be deployed to real-world applications in many industries. There was a general impression that we are now at a point where semantic solutions are really taking off.",
      "In the opening keynote talk, Phil Archer, W3C, gave many examples to demonstrate how widespread the application of semantic web technologies and Linked Data already is today. Among others, they are used by governmental organisations, including the E.U., libraries, publishers and the media industries in general, health care, the finance industry, e-commerce and the automotive industry. This success has been made possible by the development of solid and widely used standards (RDF, SPARQL, SKOS, …) Furthermore, we now have a mature technology stack that allows us to build semantic applications at enterprise level. Finally, Phil noted a growth in the number of system integrator companies that train their consultants in semantic web technologies. And indeed, there were a lot of consultants and developers new to semantic web technologies and who had come to the conference eager to learn.",
      "Case studies",
      "The SEMANTiCS conference featured case studies of applied semantic web and Linked Data technologies from a broad variety of industries. According to the organisers, more than half of the 45 talks were from industry, which is another indication that the Semantic Web has spread beyond academia.",
      "Sofia Angeltou described the use of Linked Data at the BBC. She explained that annotating and linking data has been a very efficient way to share content across editorial departments and a lightweight approach to data integration. The use of Linked Data as a successful integration strategy within and beyond an organisation, while leaving existing systems mostly unchanged, was a recurring theme at the conference. This alone could be an argument for the use of semantic web and Linked Data technologies. Another interesting aspect was that the BBC takes an agile approach to expanding their ontologies. They have an established process to grow the ontologies on demand and they focus on specific editorial topics, such as the 2012 Olympics and the Scottish independence referendum, to introduce the use of ontologies. This process seems to facilitate the introduction of semantic web technologies, especially in large organisations.",
      "Other talks included case studies of Healthdirect Australia, an impressive platform to provide health-related content and (self-) services; \"ZEIT ONLINE\", semantic support for content management of the online edition of one of Germanys most important weekly newspapers; and JURION, a semantically supported legal knowledge management and search solution by Wolters Kluwer, Germany.",
      "I particularly liked the case study of Oerlikon Metcos knowledge portal because it demonstrated that semantic technologies are relevant not only for companies that primarily create, publish or manage content or knowledge. Oerlikon Metco is a leading manufacturer of surface solutions (coatings, sprays, etc.) who have built a platform that provides a centralised portal to all internal technical information using MS SharePoint and semantic technologies. The portal has a great positive impact on the accessibility and diffusion of information across sub-divisions of the company and on response times to customer requests.",
      "Semantic web technology and beyond",
      "A number of conference talks gave insight into the available semantic web technology stack. Semantic technology has reached maturity and is used in many applications today. There also were a lot of poster sessions and research papers which showed that we can expect many more exciting semantic solutions in the future.",
      "One of the most popular databases for Semantic Web applications is the Virtuoso server by OpenLink Software, which also has an Open Source edition. Orri Erling gave an impressive keynote talk, in which he explained the challenges to implement a very fast SPARQL-enabled database. According to his experience, the main source of slow queries are not-optimal query plans. It is hard to optimise query plans, given the schemaless nature of triple stores. On the other hand, Virtuoso, when used as an SQL database, is magnitudes faster than a MySQL database (Star Schema Benchmark). In order to make Virtuoso even faster when used as a SPARQL database, the developers are trying to discover and explore structures in triple store data and then apply optimisations that are known to work for SQL tables.",
      "Virtuoso is also part of the Linked Data Stack, which was presented at the conference. This data stack has been compiled by the E.U. funded LOD2 project. The components of this stack cover probably all aspects of managing Linked Data, including authoring, storage, linking, search and quality assurance. They are downloadable as Debian packages and Virtual Machines. The project is currently approaching its final release. After completion, the components will continue to be available under the name Linked Data Stack at this location.",
      "I especially liked Andreas Blumauers talk. Although he had a great tool to show – Semantic WebCompanys PoolParty Semantic Suite – he reminded us that technology is not everything and he shared his insights in the planning of semantic web projects and semantic data management. He suggested to start with a simple SKOS-based taxonomy and only later turn this into an elaborated ontology and publish linkable data. This provides a better acceptance of semantic web technologies within an organisation and allows them to better recognise their requirements and build up required skills along the way.",
      "My impression at the conference was that it seems obvious to almost everyone working with semantic web technologies that data quality and long-term data management by domain experts are crucial to building successful applications. As a search technology consultant, this particularly pleased me and is very different from usual experience as search tends to be seen merely as a technology – an approach which can make search applications fail.",
      "Next years SEMANTiCS conference will take place in Vienna from 15th to 17th September, but semantic web technologies and Linked Data will probably come to you before!"
    ],
    "summary_t": ""
  },
  {
    "id": "22519e1bbf4f2d265241c5570c051b3d",
    "url_s": "https://opensourceconnections.com/blog/2014/09/26/solving-data-variety-with-postgress-nosql-extensions/",
    "title": "Solving data \"variety\" with Postgress NoSQL Extensions",
    "content": [
      "Raise your hand if you’ve heard the three \"Vs\" of Big Data?",
      "Velocity – your query/updates are exceptionally fast or large. Your processing the entire twitter feed.\n  Volume – you store a massive amount of data at rest. You’ve crawled the web and are storing the entire web in a database.\n  Variety – The structure of records varies dramatically. You need to store information about people, but on rare occasion you’d like to about their pets, their names, sometimes their pet’s vet names, sometimes their mother’s maiden name.",
      "The three \"Vs\" are a big part of our job helping clients understand whether or not they need NoSQL. Many clients have problems that go like:",
      "Help us with variety! We don’t really have volume/velocity scaling concerns, but our data model is getting weird. We think we should migrate from our relational solution to NoSQL because of our dynamic variety from record-to-record.",
      "Switching to NoSQL for a little extra \"variety\" and not scaling focussed volume/velocity concerns has always felt wrongheaded to me. We present these clients with pretty daunting tradeoffs for migrating to NoSQL. Many of these tradeoffs involve forcing hierarchical, denormalized data models, that solve specific targeted query patterns at massive volume/velocity scale. Many clients don’t quite realize how much powerful ad-hoc query capability they’re losing by leaving SQL.",
      "But how can we possibly have the best of both worlds? Well luckily for us Postgres is working on a very handy solution. In the upcoming release (9.4) Postgres is getting first-class JSON document support. What does this mean? Lets take it for a test spin:",
      "JSON CRUD",
      "First, go off and install postgres 9.4 beta 2 somewhere you don’t care about (I have a vagrant box).",
      "Second, fire up psql or whatever you do your postgresing in, and lets start cracking!",
      "Create a table that looks like any other SQL table",
      "create table person (\n      id serial,\n      first_name varchar(40),\n      last_name varchar(40),\n      extra_details jsonb\n    );",
      "Except notice that little \"jsonb\" column type? That’s our handy-dandy binary JSON column type. What can we do with this table? Well we can insert a few records:",
      "From the mundane",
      "insert into person (first_name, last_name, extra_details)\n    VALUES ('doug', 'turnbull', '{\"age\": 34}');\n    insert into person (first_name, last_name, extra_details)\n    VALUES ('eric', 'pugh', '{\"age\": 37}');",
      "to the schemaless:",
      "insert into person (first_name, last_name, extra_details)\nVALUES ('daniel', 'beach',\n        '{\"age\": 37, \"cats\": {\"names\": [\"itsy\", \"bitsy\", \"cottontail\"]}}');",
      "and hey look at my JSON!",
      "select * from person;\n id | first_name | last_name |                          extra_details\n----+------------+-----------+-----------------------------------------------------------------\n  1 | doug       | turnbull  | {\"age\": 34}\n  2 | eric       | pugh      | {\"age\": 37}\n  3 | daniel     | beach     | {\"age\": 37, \"cats\": {\"names\": [\"itsy\", \"bitsy\", \"cottontail\"]}}",
      "Great, we can store and retrieve arbitrary JSON in a SQL column. We’ve solved our \"variety\" problem – right?",
      "But how do we actually find things in our JSON?",
      "Well one thing thats left – how can we query JSON documents? Postgres nicely provides some operators to apply to JSON data. First, there’s basic queries against elements within the JSON document using the -> operator:",
      "select * from person WHERE extra_details->'age' = '37';\n id | first_name | last_name |                           extra_details\n----+------------+-----------+-------------------------------------------------------------------\n  2 | eric       | pugh      | {\"age\": 37}\n  3 | daniel     | beach     | {\"age\": 37, \"cats\": {\"names\": [\"itsy\", \"bitsy\", \"cottontail\"]}}",
      "or select attributes within the JSON fields:",
      "select id, extra_details->'age' as how_old\nfrom person\nwhere extra_details->'age' = '37';\n id | how_old\n----+---------\n  2 | 37\n  3 | 37",
      "An important operator to know from the list is the containment operator. This operator asks the JSON data is this operand on the right a subset of the JSON data on the left? Or in other words, does the intersection of JSON documents A and B == document B?",
      "For example, lets find all the people with cats named \"itsy\" and \"bitsy\"",
      "select *\nfrom person\nwhere extra_details @> '{\"cats\": {\"names\": [\"itsy\", \"bitsy\"]}}';",
      "Yeah but indexes?",
      "You bet. We’re in luck, because JSON columns support postgres’s GIN (inverted index) indexes. The subset query above can be sped up by simply creating a GIN index on the entire column (indexing the entire document):",
      "CREATE INDEX alldetails ON person USING gin (extra_details);",
      "Or on just a specific property in the JSON:",
      "CREATE INDEX age ON person USING gin ((extra_details->'age'));",
      "Schemaless FTW – right? well maybe.",
      "This is probably some of the coolest stuff I played with all month. It certainly hits the nail on the head for many client’s need for broader, unpredictable variety in their data models without leaving the ad-hoc capabilities of their SQL databases. If every person in our database has a collection of extra data that is going to be so different we cant easily anticipate creating new tables for that data, then this is a great solution.",
      "But word to the wise: schemaless is a bit of a misnomer. You usually care somewhere about \"schema\" concerns. When we say \"schemaless\" we really just mean \"we’ve pushed the problem to you, the consumer, to validate data\". Certainly its great when validation can be lighter and less opinionated. Or when validation is very application specific and possibly even more opinionated than whether an input string matches a given primitive type like string, integer, etc.",
      "Sometimes people say \"schemaless\" and mean \"stop making me think about a relational data model\". Its an open question that we’ll discover with these new Postgres features – is it ok to just have a database with a single table and a single JSON column that we query? Its likely equivalent to the question \"should I switch to MongoDB\"? Well its probably ok up until we experience the painful reasons relational databases were created in the first place – because normalization is actually a good thing(R) that models entity relationships much more richly than hiererarchical representations like JSON.",
      "I’m just glad to now be able to straddle both worlds, and make informed engineering decisions that use the best of relational and non-relational.",
      "Are you contemplating NoSQL technologies? Concerned that your relational database can’t meet today’s needs? Want a balanced opinion on what the best tech for your data problem is? Well contact us, and ask about one of our Data Architecture Assessments!"
    ],
    "summary_t": ""
  },
  {
    "id": "ce36f24a56310a339f86ab3d52cbd9a3",
    "url_s": "https://opensourceconnections.com/blog/2014/09/27/lets-stop-saying-nosql/",
    "title": "Lets Stop Saying \"NoSQL\"",
    "content": [
      "NoSQL to describe a database makes about as much sense as \"NoSedan\" to describe a car",
      "I say the word \"NoSQL\" a lot. When I say NoSQL, I tend to talk about denormalized and hierarchical document/row-based data stores like Cassandra, Mongo, Couch, or HBase. But its a terrible way to use that term. Because there are also graph databases that feel even more normalized than traditional relational databases. Then there are non-relational data stores that have been around for a long time like MarkLogic or BerkelyDB. And where do search engines fit into this? They’re not relational, and have also existed for a long time before anyone cared to use the term \"NoSQL\".",
      "More and more I’m annoyed at myself when I use the term \"NoSQL\". Because theres a voice in my head saying that every use of the term needs to probably have a footnote that says \"document database\" or \"big table\" inspired database. The set of things called \"NoSQL\" is far too broad to have meaning. I’d rather call something a search engine or a massively distributed hash table than a \"NoSQL database\".",
      "Another problem with the term NoSQL is the subtly implied migration path. As if its the \"next\" thing after SQL. As if to imply \"Oh you’re using MySQL? Hey the 90s called, they want their database back\". More than one client has expressed a desire to \"migrate to NoSQL\" to not be \"left behind\". If instead of asking \"does it make sense to migrate to NoSQL?\" they asked – \"does it make sense to migrate to a massively distributed, very wide hash table with only PUT and GET?\" They might say \"err… no probably not\". Or they might say \"yes! we’ve been slowly pushing our MySQL database to be exactly that, and its painful!\"",
      "The point is there is a menu of options from relational to search engines to document databases to simpler key-value stores, and I hate to sound like a consultant but it almost always \"depends\" on your problem. But lets try creating better verbiage around these problems. The question \"Do we need NoSQL?\" is rather hard to answer. The question \"I have problem X, do we need NoSQL?\" gets better. But even better \"I have problem X, what kind of database makes sense for me?\" will help you even further. As a \"NoSQL\" consultant, I might tell you that relational databases are awesome. Or I might point out how cool Datastax Enterprise is for analytics problems at scale. Who knows? As always \"it depends\".",
      "The use of the term \"NoSQL\" will be about as hard to avoid in our marketing/client communications as \"Big Data\" – its the very rough language we’ve landed on to talk about some general ideas to the non-technical crowd. But maybe we can bring some awareness that while the term \"NoSQL\" might start a conversation, it should almost always disappear after we get a little deeper into your data problem."
    ],
    "summary_t": "I say the word NoSQL a lot. When I say NoSQL, I tend to talk about denormalized and hierarchical document/row-based data stores like Cassandra, Mongo, Couch,..."
  },
  {
    "id": "98f01dc30a51c55638b1d98de7e0b8f8",
    "url_s": "https://opensourceconnections.com/blog/2014/10/08/when-click-scoring-can-hurt-search-relevance-a-roadmap-to-better-signals-processing-in-search/",
    "title": "When click scoring can hurt search relevance – towards better signals processing in search",
    "content": [
      "Have you heard of \"click scoring\" or \"click tracking\"? In the context of search click scoring is the method whereby you collect statistics on where users click in their search results, then use that information to prefer that search result for the queried text.",
      "Consider, Virginia Decoded. A set of Virginia’s state laws For example, if the user searches for \"Virginia Tech\" these are the search results that come back:",
      "Searchs for \"Virginia Tech\" dont look so great",
      "What would happen if we relied on simply click scoring as a measure of the user’s behavior? There’s a good chance that the first result would be reinforced as a good search result for \"Virginia Tech\". Which, in the absence of other data, seems like the best possible search result for Virginia Tech. So click scoring helps, right?",
      "Well it turns out that \"Virginia Tech\" is only one way to say this school’s name. As a proud hokie, I know the right thing to search for:",
      "We get better results when searching for Virginia Polytechnic",
      "Wow, the search results are much better! Clearly laws about Virginia Tech mentioned in the title are likely much more relevant to my search.",
      "Your next thought might (and should be) – would click tracking help or hurt in this instance? Well certainly a lot depends on the implementation. We would see many users go through something like the following click log of behavior, which our click tracker will then reinforce:",
      "Query: Virginia Tech\n  Click: First search result",
      "In the case of the search for \"Virginia Tech\", the most relevant search results aren’t even on the page. Instead of helping, we’re reinforcing something fundamentally irrelevant and wrong in our relevancy implementation. It builds inertia behind what’s already there on the search results page, damaging search relevancy.",
      "Let’s look at another case, one where the search results are already looking pretty good:",
      "Query: Virginia Polytechnic\n  Click: One of many results",
      "Here, we already surface any number of potentially relevant documents to the user. Click tracking can help us optimize the correct placement of the top search results. For example if the 3rd result is the dominant search result, we can boost accordingly, putting it in the #1 spot.",
      "Yeah but do we even have this data?",
      "If you build a bad search, and nobody clicks on it – does it make a sound? :)",
      "To detect these patterns, we need to be generating enough search traffic for the data to be statistically significant. Does your search application fall into that category? Consider an internal search application that searches laws for a small law firm. Search traffic is relatively low. Every day the problems and intent is different. There’s always new cases to work on, and its unlikely that a search from last week will even be seen again for years. Second, it may also be unlikely for the right answer (the best result) for a search, say \"car tax law\" to be the same from case-to-case.",
      "For this law firm, does it even matter what gets clicked on? Does this feedback help relevancy? Probably not. Every search is an outlier, stuck in the vagaries and oddities of the current problem and the current person doing the search. What is clicked could only interfere with relevancy. The next time a term is searched, the context will be different. The right answer not quite the same.",
      "Now let’s say we’ve convinced a lot of law firms to use our cloud-based legal search application. It’s cheaper, but in exchange, we’re going to use your click tracking data to make everyone’s search more relevant. We’ve got the entire body of laws available. We’ve captured a lot of lawyers and paralegals doing search. Now does click tracking help?",
      "Perhaps! It’s still unclear. In some cases, where traffic is significant and the right answer obvious its a clear win. If our searches are highly clustered around a small set of concepts – if \"car tax\" is searched frequently, click tracking could help us optimize this search results. However if the diversity of legal searches is far greater, if everyone’s case is different, and both queries differ a lot and right answers to queries differ enough, click tracking becomes less of a clear win.",
      "Another important component is your audience. It’s likely that when you’re sitting down doing really intense legal research, you’re willing to page through a couple of pages of search results. So this could be good for click tracking – if the right answer for \"car tax\" is always page 3, item 2.",
      "Or it could have no impact on search results. If the \"right answer\" differs among 30 correct search results depending on if we our user is searching for \"car tax\" because their client is late paying taxes or \"car tax\" because their client wants to get a tax break next year for their hybrid.",
      "Better Relevancy before click tracking",
      "The utility of click tracking rests on already having close to a good relevancy solution. If you’re first page of results gives you good results, then click tracking can help clarify user intent of what’s on the page. Poor relevancy, and the clarification will be misguided – like choosing the least-worst option of those presented. It’s like a good chef giving you a menu that looks like (1) steakums, (2) kraft mac & cheese (3) microwavable fish sticks. If everyone chooses fish sticks, does that mean the chef is pleasing their customer with fish sticks?",
      "No that chef still needs to serve good food. And we still need to build good search results. We need to do important work to make search relevant. Things like:",
      "Proper text analysis, stemming, lemmatization\n  Proper field weighting, titles are important to match on. Body text less so.\n  Weighting of phrases– \"Virginia Tech\" in a phrase over just \"Virginia\" and \"Tech\"\n  Recognize \"Virginia Tech\" and \"Virginia Polytechnic\" are synonyms or better, represent that these strings correspond to the entity (id 12345) which has text representations \"Virginia Tech\", \"Virginia Polytechnic\", \"VT\", \"VPI\", etc -But don’t confuse VT for Vermont!",
      "More importantly, you need a process for iterating and improving these search results. A Test-Driven approach for provably making progress on these issues.",
      "All of this has to be built before we can start contemplating the degree to which click tracking, or other signals processing methods matter.",
      "Better Signals, Not just clicks",
      "Fundamentally, another problem is trying to ascertain when clicks correspond to user success or failure. Sometimes, we might be able to ascertain that a user backtracked and searched for another term to clarify their intent just with clicks. Consider the following query log representing a user session:",
      "Query: Virginia Tech\n  Click: First search result\n  Query: Virginia Polytechnic\n  Click: One of many results",
      "If this happens enough in user sessions, a click tracker should hopefully determine that Virginia Polytechnic is a clarification on Virginia Tech. If our click tracker does this, the next question we need to ask is how often does this happen? It could be more likely that this happens:",
      "Query: Virginia Tech\n  Click: First search result",
      "If we have enough statistically significant traffic, we may determine that a small subset of users (hokies like me) know to clarify their search with a different term. More likely we’ll see that Virginia Tech results in the first search result 95% of the time, with an odd 5% changing their mind and searching for Virginia Polytechnic. Those weirdo hokies have no idea what theyre talking about – so well stick with the obvious 95% of the users that know what theyre doing.",
      "Most importantly – what are we not capturing here? We aren’t making a determination on whether or not the user was ultimately successful with a result. Did the 5% of hokie users turn out to be more succesful with the search app than the 95% of other searches? And how do we measure this? Did they spend time with that result? Did they add an item to their cart? Was there a conversion to something we want (and they would want).",
      "And nothing here captures whether the user failed in their attempt. What’s an anti-conversion look like? The user ends up on a document another way? More likely, in today’s world of impatient users, it might be you never see them again. If measuring failure is the absence of a user, then its extremely challenging to measure, or more importantly to engage that user ever again.",
      "A better signal processing and monitoring framework reinforces more than clicks – it builds upon statistically significant user success. It defines metrics for success custom to your application and user base. It determines when and if a signal is statistically significant to have meaning in search results scoring, and it optimizes the statistically measurable use cases.",
      "Perhaps more importantly, it’s a monitoring solution to prevent users from fleeing search. It’s an opportunity to flag where more in-the-trenches relevancy work can help. Where search grunt work like realizing that \"Virginia Tech\" and \"Virginia Polytechnic\" are the same thing. Or that searches for \"Tyler O’Connell\" turns into a search for \"tyler o connell\" not \"tyler oconnell\" because of how text is tokenized by a search engine. A way to file bugs back to your search team for areas where their expertise with Solr, Elasticsearch, Lucene, etc could be valuable.",
      "Is \"click tracking\" by itself useful? Maybe. But more importantly is search success and failure monitoring. Followed by a contemplation on how to make this monitoring data useful to search for your application. Its not an easy button. Its another variable to tune, another dimension to factor into search along with your relevancy engineering work.",
      "Would you like to take advantage of our expertise building contextual, relevant search applications? Interested in determining whether click or signals tracking is appropriate to your application? Contemplating LucidWorks Fusion or Datastax Enterprise to combine search and signals analytics? Contact us! We’d love to help."
    ],
    "summary_t": ""
  },
  {
    "id": "84d098c73c226d8dfbe7a67712a7a061",
    "url_s": "https://opensourceconnections.com/blog/2014/10/15/datastax-award/",
    "title": "Datastax Engagement Award",
    "content": [
      "OpenSource Connections Awarded\nDataStax Engagement Award",
      "CHARLOTTESVILLE, Virginia, October 15, 2014 – OpenSource Connections (OSC), a search, discovery and analytics consultancy, today announced it is the proud recipient of the DataStax Partner Network Engagement Award from DataStax, the company that delivers Apache Cassandra™ to the enterprise. The award was presented at the company’s Cassandra Summit Partner reception event on September 11 at the Westin St. Francis in San Francisco.",
      "The award represents DataStax’s recognition of partners such as OSC for their excellence and commitment to jointly delivering DataStax Enterprise solutions for the Fortune 1000.",
      "\"OpenSource Connections is a valuable partner that has proven its commitment to the DataStax Partner Network,\" said Kevin Pardue, Regional Channel Director, DataStax. \"OSC provides DataStax customers with outstanding proficiency in search and discovery to power the online applications that transform business.\"",
      "\"We’re proud and honored to be recognized for our commitment to bringing leading-edge search, discovery, and analytics tools to our commercial and government clients,\" said Eric Pugh, Founder and Principal, OSC. \"These companies have complex data that can often hide very valuable information. Cassandra brings that information to light and DSE enables customers to take full advantage of Cassandra, with the ability to do real-time analytics and full-text search at scale, without needing to hire a group of dedicated developers and administrators.\"",
      "OpenSource Connections builds custom search, discovery, and analytics platforms for companies in markets like government, health, financial services, and more. By focusing on short-to-medium term engagements staffed by industry experts, OSC is able to deliver innovative and leading-edge solutions, and the firm’s use of collaborative development allows clients to build the expertise they need.",
      "About Cassandra Summit",
      "The fifth annual Cassandra Summit was September 10-11, 2014 in San Francisco. With more than 3,000 attendees at the sold-out Summit and at meet-ups around the world, including Tel Aviv, Cape Town and London, the event was the largest gathering of Cassandra users ever. Leading enterprises including Sony, ING, Target, Google, Credit-Suisse, Microsoft and Instagram highlighted how they have transformed their businesses, exceeded customer demands, and accelerated their growth using Cassandra, the world’s fastest and most scalable distributed database management system.",
      "About DataStax",
      "DataStax delivers Apache Cassandra™, the leading distributed database technology, to the enterprise. Apache Cassandra™ is built to be agile, always-on, and predictably scalable to any size.\nWith more than 400 customers in over 50 countries, DataStax is the database technology and transactional backbone of choice for the world’s most innovative companies such as Netflix, Intuit, and eBay. Based in Santa Clara, Calif., DataStax is backed by industry-leading investors including Comcast Ventures, Crosslink Capital, Lightspeed Venture Partners, Kleiner Perkins Caufield & Byers, Meritech Capital, Premji Invest and Scale Venture Partners. For more information, visit datastax.com</a> or follow us @DataStax.",
      "About OpenSource Connections",
      "OSC is a boutique search, discovery, and analytics consulting firm headquartered in Charlottesville, VA. OSC works closely with clients to identify and develop solutions to enable business success. OSC experts are authors, invited speakers, and contributors to the big data ecosystem. Visit OpenSource Connections, or follow @o19s."
    ],
    "summary_t": "OpenSource Connections (OSC), a search, discovery and analytics consultancy, today announced it is the proud recipient of the DataStax Partner Network Engage..."
  },
  {
    "id": "969a4fba7c7859cecc72bdef3a07bc1f",
    "url_s": "https://opensourceconnections.com/blog/2014/11/25/playing-with-thoth/",
    "title": "Playing with Thoth",
    "content": [
      "At LuceneRevolution last week, one of the sessions that got me really excited was about Thoth, presented by Damiano Braga and Praneet Mhatre. It was very nicely done, especially considering a 30 minute timeslot! Thoth is a new Solr monitoring solution open sourced by Trulia.",
      "Ho hum I can hear you saying, yet another logging solution. Well, the part that got me excited was this line from the program guide:",
      "Damiano and Praneet will also summarize their application of machine learning algorithms and its results to the process of query analysis and pattern recognition.",
      "Thoth not only collects the data about your Solr queries, including both the query and how long it took, but then can actually use that data, via magic machine learning, to make smarter decisions. Specifically, Trulia wanted to split up incoming queries into fast and slow queries, and send those queries to specific clusters of Solr servers. Now, we can debate if its a search architecture smell to need to have separate clusters for slow versus fast queries, versus one large pool, however if it works for Trulia, then the machine learning to build the classifier is cool.",
      "Data collection happens very simply. As requests come in to what they call the intercepted instance, the query and duration are sent to another Solr server:",
      "3 Collecting Query Data",
      "The data is used to extract the training dataset of slow and fast queries, which in turn is used by a simple Machine Learning tool (that I dont recall from the presentation) that is used to classify incoming queries and send them to the correct cluster. The Thoth project has published its various modules on GitHub. The Thoth ML module is going to be available in the next little while according to Damiano, and then youll be able to actually see this use of machine learning in anger. When that is published, hopefully youll be able to duplicate what Trulia did with this setup (slightly annotated!):",
      "6 Complete Cycle Architecture",
      "You can try out Thoth very easily via the Thoth Demo project.",
      "So I do have one nitpick: the use of ActiveMQ as the mechanism to collect the query data. If you are already using ActiveMQ, then fine, but otherwise its a fairly significant thing to add to your stack. Since the source of the data is Solr, and the target location for the data is Solr, I would have just added a component that would send that data directly into the target Thoth queue. The data is pretty tiny, and I dont think it would add any more latency than using ActiveMQ. Not as robust, but one big moving piece less to deal with. Weve done this for powering an analytics interface, instead of writing out to a logging tool like LogStash, we just insert the data directly from our API into the target Solr, and it works great. Id love to see some clear documentation on how to put in your own connector.",
      "Overall, Im really excited to see this tool. Especially the special sauce around the machine learning. If you arent already invested in a monitoring tool for Solr, and like the Solr specific analytics, then take a look at Thoth.",
      "The source of the diagrams, and a good summary presentation done by the developers is available here."
    ],
    "summary_t": ""
  },
  {
    "id": "e95e9822237bf8d597e3ccc109174b31",
    "url_s": "https://opensourceconnections.com/blog/2014/11/25/two-search-conferences-in-two-weeks-was-too-informative/",
    "title": "Two Search Conferences in Two Weeks Was Too Informative",
    "content": [
      "This year I experienced the conference equivalent of a lunar eclipse: two search conferences in two weeks located two hours away from my home town of Charlottesville, Virginia! Enterprise Search Summit (ESS) and LuceneRevolution (LR) share many similarities. Both have changed their names in the last year, Enterprise Search Summit expanding its focus to be Enterprise Search & Discovery Summit , and LuceneRevolution billing itself as the Solr/Lucene Revolution! Ironically, both still use their original domain names. Both are overlapping more in the focus on open source search, with Solr and ElasticSearch being frequent topics of conversation at ESS.",
      "However, they both continue to have different audiences. ESS is focused as the business user, and how to provide great knowledge management via crawling the enterprises internal resources is the most common topic of discussion. LR is really focused at the people building applications that are powered by search, with the expectation that you are really going to customize your search technology. It is a very geeky audience ;-).",
      "1) Death of the commercial search engine has been greatly exaggerated.",
      "A couple of years ago, as a Solr guy, I was riding high on the proof that open source was the best solution to search. We had toppled the Autonomy Idol, FAST had switched to the slow lane of being Sharepoint search, and Endeca was rapidly becoming \"remember Endeca? Good for e-commerce search\". Since then, Ive come to miss the integrated enterprise search engine, because while the core technologies of open source search based on Lucene are fantastic, glueing everything around that technology is a pain. Connectors. Security Model. Homemade Tuning Tools. Were now seeing the rejuvenation of enterprise search engine product, with LucidWorks, ElasticSearch, SearchBlox, and Attivio all built on Lucene, as well as large ecosystem of component providers (like us with our Quepid and Splainer relevancy tuning tools!). This feels like a step backwards, but instead acknowledges that the hardest part of open source is the integration between projects into a single platform. One of the comments that I heard at ESS was from a business user who said:",
      "We need to be able to modify the documents shown, tweak rankings, and otherwise optimize the data for our users, but without it becoming a full scale IT project.",
      "I couldnt recommend them Solr, without a bundle of custom work upfront. I think that we will see more commercial options that are targeted to specific verticals, and bundle in more then just a search engine. For example, a search engine for the medical field that understands what MeSH and PubChem are out of the box would be worth buying versus building. The era of one size fits all commercial products is over, but there is plenty of room for new more narrowly focused offerings.",
      "I first drafted this conclusion last week, and since then, two blog posts touching on this have come out that are worth reading, that I think validate my statement!",
      "The third wave of Search Tech by Karl Hampson\n  More than an API – the real third wave of search technology by Charlie Hull",
      "2) Aggregations! Aggregations! Aggregations!",
      "While facets may be the original aggregation tool, it seems like today all the excitement is about counting things. Field collapsing (aka grouping) was introduced in Mid 2011 as part of Solr 3.3. Since then it seems like what everyone is trying to do is turn a search engine into a calculating engine. The line between a database, a search engine, and a Map/Reduce engine is getting blurrier and blurrier. Today, if my job is to figure out how to analyze log messages, I would have a raft of options: Splunk, Cassandra, Solr, ElasticSearch, or Hadoop. This is all part of the influence of the NoSQL movement on search, but I look forward to when we go back to arguing about if Stemming or Lemmatization is a better solution ;)",
      "3) Two approaches for dealing with complex rules around text parsing.",
      "I am seeing two approaches for dealing with really complex relevancy rules for parsing free text. One is the approach Ill call \"Get inside Lucenes head and understand everything\" and the other is \"The world is complex, I give up\".",
      "In the \"Get inside Lucenes head and understand everything\" camp was a presentation by Ramzi Alqrainy on Arabic Content with Solr. In there, he really focused on understanding the rules of Arabic language, and tweaking the various knobs and dials in Solr to make it work. Another example is the way that Ernst and Young askss its user to rate the results so that it can boost them. Again, but directly trying to directly influence how the search engine works. We have a customer doing Trademark Search that has the same set of very specific rules. The challenge is that the more very specific rules you have, the interactions between them become more complex and harder to understand. And you are the rules engine.",
      "So as a reaction to that is the \"The world is complex, I give up\" crowd. By that, I mean they attempt to build a model of a search relevancy problem, based on signals, and then use that model to influence the results. The interesting thing is that there are well known techniques for measure how well your model works, but you may not know exactly why any individual result was returned! Because the model is built off of inputs, you are two degrees separated from the specific rules. However, this black box approach has been proved to work well. Andrew Fast gave a really wonderful presentation that advocated for this appraoch. My only beef with his presentation is that he made building the model seem like a simple process! The presentation on Thoth, a Solr monitoring solution, touched on this approach as well. Thoth includes a model for figuring out which incoming queries are fast and which are slow, but without trying to parse out the parts of the query. The Thoth ML module isnt public yet, but the use of a simple classifier for queries is.",
      "",
      "It was two very busy weeks, dare I saw too exhausting even? However it was great to catch up with the community, and see where search is going."
    ],
    "summary_t": ""
  },
  {
    "id": "11059969ddf45d123f3c6708578ae5c4",
    "url_s": "https://opensourceconnections.com/blog/2014/11/26/stepwise-date-boosting-in-solr/",
    "title": "Stepwise Date Boosting in Solr",
    "content": [
      "When you want to boost on recency of content (ie more recently published documents before older ones), the Solr function query documentation gives you a basic date boost:",
      "boost=recip(ms(NOW,mydatefield),3.16e-11,1,1)",
      "This will give you a nicely curving multiplicative downboost where the latest documents come first (multiplying the relevancy score by X for stuff \"NOW\"), slowly sloping off into the past (multiplying the relevancy score by some decreasing value < X as content moves into the past).",
      "Let’s say instead of a nice curving boost, you’d just like to downboost anything that happens before some date in the past. Say you set a date, 10 years ago, anything older than 10 years ago should get significant downboost. Anything newer should not be impacted.",
      "Well you can do this with Solr function queries!",
      "Let’s start by thinking about how you might naturally think about this problem in code",
      "if mydatefield > dateInPast\n    return 1.0\nelse\n    return 0.8 // some downboost multiplier",
      "Unfortunately, we’re hamstrung by the fact that Solr function queries don’t have a greater than or less than operator. Function queries do have an if function that tests if the value is non-zero. Both negative and positive values are considered true. Can we use some of the other things available in function queries to get the effect we desire?",
      "First, let’s calculate a date in the past to compare against. Well call this \"marker\" as well it marks something. Here we’ll use 10 years ago (315569259747 is 10 years in milliseconds).",
      "// 10 years ago\nmarker = sub(ms(NOW),315569259747)",
      "This translates to English as NOW (as milliseconds since epoch) minus 10 years in milliseconds.",
      "Now we just need to figure out a way to do a comparison. First let’s observe that if we take our date field and subtract it from our marker we’ll either get a positive value (happens after our marker) or a negative value (happens before our marker). In other words",
      "2010/01/01  -  2004/0101    == (some postive number)\n2001/01/01  -  2004/0101    == (some negative number)",
      "If we take the min of this subtraction and 0, we’re either left with 0, which indicates that the subtraction was positive (and thus our mydatefield was to the future of the marker). Or we’re left with a negative number (our mydatefield was to the past of the marker).",
      "This dovetails nicely into Solr’s if. Once we do this subtraction, we can use the 0 (the falsy value) to know that mydatefield is to the future of marker. So we can put a multiplicative boost that shows no impact (1) in the false parameter, and down boost when there’s a true parameter (negative values are still truthy).",
      "Putting that together, you have something like:",
      "boost=if(min(0, mydatefield-marker),0.8,1.0)",
      "Expressing (mydatefield – marker) as a function query:",
      "sub(ms(mydatefield),sub(ms(NOW),315569259747))",
      "Putting it all together:",
      "if(min(0,sub(ms(mydatefield),sub(ms(NOW),315569259747))),0.8,1)",
      "And viola, you have a multiplicative down boost of 0.8 for anything older than 10 years!",
      "Oddly enough when I was writing this post I realized that 10 years ago from NOW (ie sub(ms(mydatefield),sub(ms(NOW),315569259747))) was when I met and started dating my wife. Nowadays, she frequently edits my blog posts against my will. So Ill dedicate this post to her and all the great stuff she does for our family!",
      "And as always, please to contact us to discuss how you can utilize our expertise in improving the relevancy of your Solr search results!"
    ],
    "summary_t": ""
  },
  {
    "id": "6ca9dc013228fa03966d4dcccb46a013",
    "url_s": "https://opensourceconnections.com/blog/2014/12/02/apache-sentry-so-close-and-yet-nothing/",
    "title": "Apache Sentry.   So close, and yet nothing.",
    "content": [
      "Security, its always been the bug a boo of Solr. There is a wide sense that security isnt a concern of the Solr community, and that isnt quite accurate. How to secure Solr is pretty simple. Its just that there isnt any one \"blessed\" approach that is wrapped into the codebase as each organizations needs are different.",
      "Before I continue, I want to mention that when you say security, it actually encompasses three areas:",
      "Securing documents and collections using roles.\n  Securing the server from a web perspective\n  Securing the data at rest and in transit.",
      "Apache Sentry, currently undergoing incubation, was announced as a tool for securing Solr. I attended the nicely done session by Gregory Chanan at LuceneRevolution last week. What convinced me to go was the program description:",
      "Sentry augments Solr with support for Kerberos authentication as well as collection and document-level access control. This session will cover the ACL models and features of Sentry’s security mechanisms, implementation details on Sentry’s integration with Solr, and performance measurements in order to characterize the impact of integrating Sentry with Solr.",
      "I was really excited about Sentry! Finally, something that combines the early binding and late binding approaches for documents into a single packaged solution. I hoped that it would give a nice layer around the Solr admin interface to lock it down. Provide some features around hardening Solr, like maybe running a check to see if the enableRemoteStreaming option was enabled and showing it as a vulnerability in dashboard. And lastly, how about giving me some nice options to ensure that no jokers pass rows=10000000 to Solr! Or even, rows=100&start=1000000.",
      "Instead, Sentry turns out to be just another stalking horse for selling Cloudera distributions. Ostensibly it provides Kerberos integration for Solr, but during the presentation it was highlighted that the hard work of enabling Kerberos (key tabs and the rest) remain your responsibility, unless of course you use CDH. The problems that it does solve, namely document and collection filtering capabilities are the easiest of the the three areas of security that I highlighted above.",
      "I think part of my dis-illusionment was that when I heard Apache Sentry I thought, great, a best of breed general purpose security project! Great. But instead, its very specific to Hadoop. Maybe if it had been called Apache Elephant Enclosure I would have realized that it was specific to Hadoop, and integrating things into the Hadoop ecosystem.",
      "If you are looking to lock down your Solr, here are some options:",
      "https://github.com/dergachev/solr-security-proxy\n  https://github.com/o19s/solr_nginx"
    ],
    "summary_t": ""
  },
  {
    "id": "6b1d75a70babf1919bc67cb965436b62",
    "url_s": "https://opensourceconnections.com/blog/2007/04/12/producing-html-output-from-nunit/",
    "title": "Producing HTML output from NUnit?",
    "content": [
      "As part of our build process we run NUnit and NCover over some C# code. We are currently focusing on getting our code coverage up from the measly ~18% that the project is at.",
      "One of the developers asked me to make it easier to find out which test failed versus parsing through 100s of lines of raw build output. We already have NCover producing nice HTML reports of the code coverage metrics, so I figured I could do the same with the NUnit XML file as well.",
      "Much to my dismay, it appears that you are expected to use the NUnit GUI application to parse the failure results! The only existing conversions to HTML that I could find was this blog posting on Automating NUnit and MSTest Builds. I tried out the conversion from XML to text output, and that was better, but very ugly.",
      "Daniel Irvine commenting on the new NUnit 2.4 release also mentions that the XSLT transformation of the raw data is something hes looking for as well!",
      "If anyone has any suggestions, please let me know. I hope I havent talked myself into creating the XSLT transformation tool for NUnit!!"
    ],
    "summary_t": ""
  },
  {
    "id": "3c0c0aa768d31205d47182e8d5944099",
    "url_s": "https://opensourceconnections.com/blog/2014/12/08/title-search-when-relevancy-is-only-skin-deep/",
    "title": "Title Search: when relevancy is only skin deep",
    "content": [
      "How do users judge that articles, books, and blog posts are relevant to what they’re searching for? What about you? If you’re searching for an article on ‘Socrates’, what might be relevant to you?",
      "A lot of search relevancy work with Solr or Elasticsearch focusses on getting really deep into written prose–the type that occurs in the body text of articles and books. To extract whether a document is relevant, we study complex scoring models, latent semantic indexing, natural language processing and other techniques that can extract meaning and metadata from written prose.",
      "Those are powerful techniques. But do you know what often really matters? Often its the simple things. Sometimes its just something as simple as whether our search term, \"Socrates\", is mentioned prominently in the article’s title. Titles speak volumes about curated, published content.",
      "Titles are:",
      "very thoughtfully written by authors,\n  the most concise description of publication \"aboutness\",\n  written to intentionally to snag the target audience, and\n  appeal to our extremely low attention span",
      "So having a very good title search helps tremendously with search relevance.",
      "Great! Problem solved. Search over the ‘title’ field and we’re done, right?",
      "Not quite.",
      "In fact, much of my most challenging relevancy work has turned out to be focussed heavily on understanding and working with title text. In this article, I want to share my thoughts on what has worked well when working with title fields across a number of relevancy projects using Solr and Elasticsearch.",
      "Term Frequency Doesn’t Matter, but Phrase Matching Does",
      "Which article on Socrates is more relevant?",
      "Socrates on Socrates",
      "vs",
      "Socrates, a brief biography",
      "They’re both probably just about the same. Both articles about Socrates. Compare your intuition about these titles with your intuition on these snippets of body text.",
      "Plato was pretty cool. Plato liked to party a lot. Sometimes with Socrates.",
      "vs",
      "Socrates was kind of grumpy. Socrates didn’t like Plato",
      "You can see in these snippets, how often our search term (lets say \"Socrates\") is mentioned matters much more in our intuition when judging relevance. We want a computer to see that the term \"Socrates\" is mentioned more frequently in the second snippet, and therefore rank this document highly when returning search results. The other document mentions \"Socrates\" only once, in passing, so our notion is often that this document should not rank highly in the search results.",
      "The measure for how often a term (ie Socrates) is mentioned in a document is known as \"term frequency\" or TF for short. Term frequency is a fundamental part of most relevancy ranking algorithms. Lucene-based search engines like Solr and Elasticsearch use the term frequency in their default scoring formula.",
      "Much of the information-retrieval science that goes into search engines, relies on our intuitions about written prose like the snippets above. Which is why term frequency is a key component of relevancy scoring. What about title search? Is term frequency as important?",
      "In the case of title search, aboutness hasn’t correlated to term frequency in my relevancy tuning experiences. Instead, simply a mention of a term in a title seems enough to convince us that the title is likely about what we’re searching for. Titles don’t go on and on verbosely about a subject like written prose does, they are concise and to the point. When a term is mentioned more than once, such as our article \"Socrates on Socrates\" its a fleeting flourish, not meaningful to judging results. Mentioning Socrates twice doesnt convince us this article is any more about \"Socrates\" than an article simply entitled \"Socrates\".",
      "Phrase Matching",
      "Phrase matching, taking the users multi-word query (\"Socrates on Socrates\") and attempting to find that exact phrase in a title, matters tremendously. We want to support users searching for exact titles or parts of titles.",
      "To support this form of querying, the search index needs to have a feature known as term positions enabled. This tells a search engine like Solr or Elasticsearch where individual terms in a field (\"Socrates on Socrates\") are with relation to each other. You can imagine them as getting encoded as Socrates{0,2}, on{1} – denoting \"Socrates\" occurs in positions 0 and 2 and \"on\" occurs in position 1. (check out how this really looks here).",
      "Once each term is indexed with its associated position, the search engine can match up the positions of the terms in the user’s query with the positions of terms in the field (in our case \"title\"). A search for phrases or exact titles like \"Socrates on Socrates\" can use position information to rank phrase matches more highly than simple term matches.",
      "Phrase Matching without Term Frequencies",
      "As we noted in our last section, our ideal title search disables term frequencies in scoring but uses phrase matching. Unfortunately, neither Solr or Elasticsearch have the ability to enable positions while disabling term frequencies. The options for Elasticsearch are documented here, they allow configuring a field with:",
      "freqs (doc numbers and term frequencies),\n  positions (doc numbers, term frequencies and positions)",
      "Similarly, in Solr’s schema, you can",
      "omitPositions\n  omitTermFreqsAndPositions",
      "What we need is an \"omitTermFreqs\" while keeping positions on. This doesn’t appear to be a feature, and indeed as my colleague Peter Dixon-Moses pointed out seems to be tied to how Lucene’s underlying index API is organized.",
      "Solutions?",
      "An often cited solution is to write a custom Lucene Similarity plugin. Lucene’s Similarity class defines rules for how exactly TF or other ranking statistics are calculated from data stored in the index. You’d still enable term frequencies when indexing your fields, but when it came to using them you’d hijack the calculation and return 1.0 instead. You can write some really basic Java code and return whatever you want for these statistics.",
      "public class NoTfSimilarity extends DefaultSimilarity {\n\n    public float tf(float freq) {\n        return 1.0f;\n    }\n}",
      "(then similarly enable this similarity for your field in Solr or Elasticsearch)",
      "Not everyone likes Java plugins. I try to avoid them unless I have a clear need I can’t solve with the external API. So another solution I use is simply to define two fields:",
      "notf_title (A title field configured with both term frequencies and positions disabled)\n  phrase_title (A title field with freqs/positions enabled, for phrase matching)",
      "Solr’s edismax handler let’s me specify fields for normal single term query matching/scoring and other fields for just phrase matching. With Solr, I could do:",
      "q=\"Socrates on Socrates\"\nqf=notf_title\npf=phrase_title",
      "Elasticsearch’s Query DSL lets me explicitly state",
      "{\n    \"query\": {\n        \"match_phrase\": {\n            \"phrase_title\": \"Socrates on Socrates\"\n        }\n        \"match\": {\n             \"notf_title\": : \"Socrates on Socrates\"\n        }\n    }\n}",
      "By working with Solr or Elasticsearch’s query DSLs and applying the appropriate boosts and weights to both queries, we can tune how much influence we want from term frequencies vs phrases.",
      "Know Your Pantheon – IDF, Norms, and Keepword Curation with Titles",
      "With much less text, often focussed on a specific subject, title fields don’t follow the same statistical patterns of free text \"body\" fields. We saw this above when we discussed term frequency. Let’s explore another consequence of this. Let’s say you searched for \"Who was Plato\" in our philosophy articles. To your chagrin, you get the following set of results:",
      "Search: Who was Plato?\nResults:\n– Who was Thales?\n– Who was Socrates?\n– Who was Aristotle?\n– … (50 results later)\n– Who is Doug Turnbull?\n– Plato: The Biography",
      "What went wrong? Why is the Plato result so low in our search results?",
      "You would think that \"Plato\" is pretty specific. It turns out for our title field, however, that a search for \"Plato\" is less meaningful in relevancy ranking than a search for \"Who\". This is because of something called inverse document frequency (IDF) and how it tends to work in odd ways with title fields.",
      "What is inverse document frequency? Whereas term frequency tells us about how frequent a term is in a specific document, another statistic, \"Inverse Document Frequency\" (IDF) tells us about how rare a term is over all of the documents.",
      "IDF matters because it helps us measure how relatively important a search term is in a document relative to other matching documents in the corpus. For example, if Socrates is mentioned in two documents, then we have a notion that one of those articles contain roughly 50% of the available mentions of Socrates. If we introduce more Socrates articles – say two more documents, suddenly our original document represents a lower proportion of all the Socrates – 1/4 or 25% of them. Suddenly our original document isn’t scored as highly.",
      "More importantly for us, when our query contains multiple search terms \"Who was Plato\", the relative IDF (again, read rareness) of each term plays a role. In our hypothetical \"body\" field, Plato is very rare in written prose compared to \"who\", so matches on \"Plato\" dominates the ranking. English prose tends to follow some common statistical patterns. However, with our title field, it turns out \"Plato\" is a very common (thousands of articles written about Plato) and \"Who\" is exceedingly rare (very few articles with \"Who\" in the title). Again, we’re not seeing a normal sample of English prose in our title field – the founding intuitions behind these stats arent quite maintained. In other words, titles are a statistical outlier, and we need to revisit some assumptions.",
      "How do we fix this? Build a Pantheon.",
      "Is IDF useless for title fields? No, not quite, but it requires us to carefully curate a list of terms that are likely to get written about in our domain to craft a meaningful IDF. What do I mean by this? Well in the context of Philosophy, proper names such as \"Plato\" and \"Aristotle\" are obviously in. Important also to capture slightly more obscure philosophers like \"Erasmus Darwin\". We’d also include other important topics like \"epistemology\" – all important philosophy jargon, topics, and other nouns that articles would be written about in our corpus.",
      "I call this list of concepts our \"Pantheon\". A list of subjects in our domain, professionally curated by domain experts. Actually building such a list can be time consuming – but for many fields, much of the work is done for you. For example in medicine, there’s the MeSH vocabulary that attempts to cover everything that could be written about in medicine. For other domains, it may be possible to create such a list yourself manually or use NLP techniques.",
      "We can use our pantheon along with a KeepWordsFilter to create yet another search field to use in our search. We can create a \"keep words\" list that contains the terms in our pantheon. Only terms in our list make it into the search index. We can call this field pantheon_title. For example, when the following title is analyzed to go into the index:",
      "Who was Socrates",
      "we will strip out all terms other than the ones in our pantheon:",
      "Socrates",
      "Similarly the title",
      "Socrates and Plato on Metaphysics",
      "Can be boiled down to these three members of our pantheon:",
      "Socrates Plato Metaphysics",
      "Now IDF can be used on this new field to help us. IDF now tells us about how commonly written about the topics in our high value terms list. Socrates is going to be a very common term in our new title field, with many pantheon_title fields mentioning \"Socrates\". However, Erasmus Darwin, not so much. The search engine can use IDF to know how rare (and therefore how highly ranked) titles that match Erasmus Darwin should be.",
      "Additionally, we’ve filtered out the junk words. If I search for \"Who is Socrates\", we’ll get something closer to desired behavior, as we’ll be searching a field that has stripped out all the words that have little to do with philosophy. For free text \"junk\" words (so called stop words) tend to be ok to leave in as they are so common they have very little impact on the scoring. However, here we need to remove them precisely because the search engine doesn’t have enough data to know they are junk through IDF.",
      "With this new field, we can rely heavily on it in our title search solution. Giving it a strong preference over plain-text title matches. For Solr, this would mean calibrating the weights to give more emphasis to pantheon_title, only relying on notf_title as a last-resort match:",
      "qf=pantheon_title^100 notf_title\npf=title",
      "With Elasticsearch, we could use the query DSL’s multimatch to boost appropriately:",
      "{\n    \"query\": {\n        \"match_phrase\": {\n            \"phrase_title\": \"Socrates on Socrates\"\n        }\n        \"multi_match\": {\n            \"query\" : \"Socrates on Socrates\"\n           \"fields\":  [\"notf_title\", \"pantheon_title^100\"]\n        }\n    }\n}",
      "Normal caveats about selecting correct dismax boosting apply.",
      "Getting No More Specific Than The Query",
      "Oh great, looks like our search engine for philosophy articles has taken on a new set of documents from the International Journal of Hyperdomal Metaphysics. Unfortunately, after indexing these document we’ve noticed our search for Socrates has gone askew. Suddenly we’re getting very esoteric titles that happen to mention Socrates, not general titles just about Socrates:",
      "Search: Socrates\nResults:",
      "What is the relationship between Socrates and Plato when it comes to the differences in opinion on hyperdomal metaphysics?\n  …\n  Socrates Bio",
      "Typically in my experience, when users search for \"Socrates\" they want the general result – a bio or general article that covers the search term. Only when users get more specific do they want to dive into the more specific subjects. All things being equal, you don’t want to be any more specific than a user’s query.",
      "This behavior is related to what are known in Lucene-based search engines as \"norms\". Norms bias search results to shorter pieces of text. It can be thought of as a bias towards density. Consider these two pieces of prose about Socrates",
      "Socrates was a philosopher. Everybody loves Socrates.",
      "vs",
      "Socrates partied all the time. He partied with Plato.\nHe also partied with ...\n...\nSocrates was the coolest philosopher ever.",
      "These two pieces of prose both contain two mentions of our search term \"Socrates\". Search engines tend to bias search results towards documents where a larger percentage of the text contains the search term. In other words, bias search results towards shorter fields. In our example, the first snippet would be scored more highly than the second.",
      "With our title field, and more importantly our pantheon_title field, we can use norms to a related effect. If our search term is simply \"Socrates\", shorter versions of pantheon_title that just contain \"Socrates\" come first. Additional important concepts in our pantheon_title field cause the document to be punished somewhat in relevancy ranking. We can see this if we take apart our titles:",
      "title: What is the relationship between Socrates and Plato when it comes to the differences in opinion on hyperdomal metaphysics? pantheon_title: Socrates, Plato, hyperdomal metaphysics",
      "title: Socrates Bio pantheon_title: Socrates",
      "With norms on, we’ll bias towards the shorter matching pantheon_title field. We’ll get exactly as specific as the user’s search query, no more, no less. We end up with a saner ranking:",
      "Socrates on Socrates\n  Socrates Bio\n  …\n  What is the relationship between Socrates and Plato when it comes to the differences in opinion on hyperdomal metaphysics?",
      "Only by the user adding \"Plato\" and/or \"hyperdomal metaphysics\" to their search query will they pull up the more specific title into our search results.",
      "Luckily for us, Solr and Elasticsearch enable norms by default. However, its an often cited optimization to disable norms. So if you find yourself in a sticky situation where your title results are far more specific than you’d like, be sure to enable the setting on your field. For Solr this is the omit_norms setting. Elasticsearch exposes this setting through the mappings API.",
      "These are just the building blocks",
      "I’ve given you the broad, sweeping pieces of a good title search. There’s a lot of extra puzzles that need to be worked out.",
      "Working with your pantheon can be tricky – consider a pantheon full of multi-term phrases. Will the search engine treat them the same as single terms? What about nested concepts, like these two:",
      "hyperdomal metaphysics\n  metaphysics",
      "How do these get treated? Do we need more than just a KeepWordsFilter to capture these? Should we craft field norms to consider these redundant concepts, and not punish for related concepts? What about synonyms between these concepts? How will those factor into norms, IDF, and term frequency?",
      "Here’s another puzzle: if instead of a vocabulary, we have an ontology – a set of connected concepts – could we use the notion that multiple concepts are related? Could we use the fact that we have nothing on \"hyperdomal metaphysics\" to surface results on the parent concept of \"metaphysics\"?",
      "And this is only one important piece of a good relevancy solution. What about that prose in the body text? What about layering in other positive signals of relevance, like tracking and using user usage metrics?",
      "In short, relevancy is hard work! But at OpenSource Connections, we love it! Contact us if you want this level of detail applied to your search. And check out Quepid, our relevancy regression tester and its little sister Splainer if you need help with your search tuning!"
    ],
    "summary_t": ""
  },
  {
    "id": "902ef1c900fddf963e5c4e6af1b030ca",
    "url_s": "https://opensourceconnections.com/blog/2014/12/09/quepid-write-tests-against-your-search-results/",
    "title": "Quepid: Write Tests Against Your Search Results",
    "content": [
      "Quepid: Relevancy Solved",
      "Quepid is our \"Test Driven Search Relevancy\" workbench product actively used by several clients. What do we mean by test-driven relevancy? We want to give you the ability to iterate quickly when creating a search solution. Sometimes the correctness of search results is fuzzy – based on how users or domain experts grade search results. Quepid has supported this since day one.",
      "But you know what? Many times we just want to just make an assertion about our search results. If I search for \"dress shoes\" I want to assert that every search result is from the \"shoes\" department, and not dresses. I want to tweak my relevancy settings boosts and other parameters and be able to visually see what passes and what fails – driving closer and closer to all searches passing my important criteria.",
      "I want to iterate like a speed-chess player. I want to express exactly what a passing or failing search is in a workbench that can tell me exactly what the search engine is thinking.",
      "That’s Quepid.",
      "Quepid now lets write your own Javascript tests against your search results. Observe as we write a really basic test for this query in our Cables-R-Us fake ecommerce website:",
      "writing a test for Quepid asserting theres search results and the 1st item has Cysco in the title.",
      "See the documentation to see whats possible.",
      "And that’s not all! Did your test fail? Be sure to check the friendly explain parsing that let’s you know what search matches mattered the most in relevancy ranking:",
      "It Broke! Check out the explain to see what matched to drive search closer to expectations",
      "So check out Quepid and start testing your search today! And contact us if youd like to talk to us about how we can use Quepid to build better search for you! (heres a sample of the kind of work we do)."
    ],
    "summary_t": "Quepid is our \"Test Driven Search Relevancy\" workbench product actively used by several clients. What do we mean by test-driven relevancy? We want to give yo..."
  },
  {
    "id": "ae26481a509c44ccb44e26289091c2da",
    "url_s": "https://opensourceconnections.com/blog/2015/01/08/using-solr-cloud-for-robustness-but-returning-json-format/",
    "title": "Using SolrJ CloudSolrServer and retrieving JSON",
    "content": [
      "On a project I’m working on we want to return the Solr JSON formatted results to our rich front end application based on the Spyglass framework.   However, in order to support High Availalbity (HA), we are using SolrCloud.",
      "So what happens when a Solr node goes down?  Well, if every node is in the load balancer, then we can have our incoming queries be directed to any Solr node, and intenrally Solr will collect all the results and then return them in the JSON format.  However, this means that every node needs to be in the load balancer, and that adds an additional configuration step as we bring up and down nodes.",
      "Instead, since we have ZooKeeper tracking the state of the cluster, then we want to take advantage of all the support that the CloudServer provides.    However then we stubbed our toe, by default the response is a set of Java objects, and we don’t want to convert those back to JSON objects and return them.  Also, since SpyGlass is expecting the results in the Solr JSON format, we don’t want to change that.",
      "So how can we combine SolrJ’s robust connection with the JSON output?   It’s easy, with the awkwardly named NoOpResponseParser.java.",
      "CloudSolrServer server = new CloudSolrServer(\"localhost:2181\");\nserver.setDefaultCollection(\"collection1\");\nserver.connect();\n\nSolrQuery query = new SolrQuery(\"*:*\");\nquery.setRows(1);\nQueryRequest req = new QueryRequest(query);\n\nNoOpResponseParser dontMessWithSolr = new NoOpResponseParser();\ndontMessWithSolr.setWriterType(\"json\");\nserver.setParser(dontMessWithSolr);\nNamedList<Object> resp = server.request(req);\nString jsonResponse = (String) resp.get(\"response\");\nSystem.out.println(jsonResponse);\nAssert.assertTrue(\"Verify that we get back some JSON\",\njsonResponse.startsWith(\"{\\\"responseHeader\"));",
      "NoOpResponseParser returns the full JSON output and stores it under the key \"response\".   You can do this for any of other output formats, like PHP, Ruby that you want to mess with as well."
    ],
    "summary_t": "SolrCloud gives you HA capabilities for your Solr setup, but currently only the SolrJ client supports SolrCloud natively, and it returns Java objects.   Here..."
  },
  {
    "id": "24b5994bfc198343116888ef674235c8",
    "url_s": "https://opensourceconnections.com/blog/2015/01/14/solrcloud-healthcheck-command/",
    "title": "Using Solr's new Healthcheck command",
    "content": [
      "SolrCloud is great, up until you start seeing lots of ZooKeeper exceptions and you think to yourself, \"wtf?\".",
      "Thats when you discover that you should have been monitoring your various servers proactively, but since you now have a number of Solr processes, and many nodes, that is a pain.",
      "Well, as part of the new SolrCLI command line interface to Solr, we have a new tool, HealthCheck to play with:\n\thttps://github.com/apache/lucene-solr/blob/a847deabd09e9110957d22ec3a294c64ffd6e159/solr/core/src/java/org/apache/solr/util/SolrCLI.java#L871",
      "You pass in your collection you are interested in checking, as well as the ZooKeeper URL, and it helpfully interrogates the cluster:",
      "Goes through and reports health of each replica, including useful thinks like the uptime, memory consumption, number of documents.\n  If the shard doesn’t have a leader, then it marks the replica status as DOWN, otherwise if it can’t reach the replica, then ERROR, or hopefully, ACTIVE.\n  Finally, it rolls, per shard, all of the replicas state to a single response that is healthy, degraded, down, or my nemesis, no_leader.",
      "./bin/solr healthcheck -c collection1 -z localhost:2181",
      "Which outputs a whole bunch of JSON.",
      "{\n  \"collection\":\"collection1\",\n  \"status\":\"healthy\",\n  \"numDocs\":14,\n  \"numShards\":2,\n  \"shards\":[\n    {\n      \"shard\":\"shard1\",\n      \"status\":\"healthy\",\n      \"replicas\":[\n        {\n          \"name\":\"core_node2\",\n          \"url\":\"http://192.22.22.1:8983/solr/collection1_shard1_replica1/\",\n          \"numDocs\":5,\n          \"status\":\"active\",\n          \"uptime\":\"0 days, 1 hours, 9 minutes, 6 seconds\",\n          \"memory\":\"68.2 MB (%7.5) of 310 MB\"},\n        {\n          \"name\":\"core_node3\",\n          \"url\":\"http://192.22.22.1:8985/solr/collection1_shard1_replica2/\",\n          \"numDocs\":5,\n          \"status\":\"active\",\n          \"uptime\":\"0 days, 1 hours, 9 minutes, 7 seconds\",\n          \"memory\":\"64.8 MB (%7.1) of 243 MB\",\n          \"leader\":true}]},\n    {\n      \"shard\":\"shard2\",\n      \"status\":\"healthy\",\n      \"replicas\":[\n        {\n          \"name\":\"core_node1\",\n          \"url\":\"http://192.22.22.1:8983/solr/collection1_shard2_replica1/\",\n          \"numDocs\":9,\n          \"status\":\"active\",\n          \"uptime\":\"0 days, 1 hours, 9 minutes, 7 seconds\",\n          \"memory\":\"68.5 MB (%7.5) of 310 MB\"},\n        {\n          \"name\":\"core_node4\",\n          \"url\":\"http://192.22.22.1:8985/solr/collection1_shard2_replica2/\",\n          \"numDocs\":9,\n          \"status\":\"active\",\n          \"uptime\":\"0 days, 1 hours, 9 minutes, 7 seconds\",\n          \"memory\":\"64.9 MB (%7.1) of 243 MB\",\n          \"leader\":true}]}]}",
      "This is still better then looking at clusterstate.json directly, however if I combine it with jq, I can get this:",
      "sansabastian:bin epugh$ ./solr healthcheck -c collection1 -z localhost:2181 | jq .status\nINFO  - 2015-01-13 17:26:32.247; org.apache.solr.util.SolrCLI$HealthcheckTool; Running healthcheck for collection\n\"healthy\"",
      "Now one thing I noticed was that with a downloaded version of Solr 4.10.3, running ./solr healthcheck would give me classpath issues.   I ended up putting this little shell script together to unpack solr.war and run the healthcheck Java class directly:",
      "COLLECTION_NAME=collection1\nZKHOST=172.31.64.31:2181\n\njava -cp WEB-INF/lib/*:solr-4.10.3/example/lib/ext/* org.apache.solr.util.SolrCLI healthcheck -v -collection ${COLLECTION_NAME} -zkHost ${ZKHOST}",
      "This is all great, but does mean I need to be able to access ZooKeeper and my cluster from wherever I am running from.  Someday, hopefully this capability is boiled into the Solr Admin.\nI know, patches welcome ;-)."
    ],
    "summary_t": "Eyes hurt from trying to read through clusterstate.json?  ./bin/solr healthcheck to the rescue!"
  },
  {
    "id": "b98248d50656784c59c8808eeaf3297d",
    "url_s": "https://opensourceconnections.com/blog/2015/01/16/ad-hoc-solr-monitoring/",
    "title": "Ad-hoc Solr Monitoring",
    "content": [
      "What is the solution when you’re doing a 24 hour load test, you haven’t already set up monitoring and you need to easily see the Physical Memory and JVM Memory Utilization?",
      "Hack it together!",
      "We were recently in the position where a Performance Load test was going to start at 10pm one night and end at 10pm the next night. We wanted to see the effect of a long running load test on the stability of the server, but who wants to stay up late hitting refresh?",
      "Add a dash of creativity to a terminal command wrapped in an infinite loop and a Chrome plugin and you have a quick, easy, and cheap Solr monitoring solution!",
      "##The plugin\nA quick Google search pointed me to the plugin Easy Auto Refresh. You simply click the extension icon in the Omnibox, enter in the number of seconds you want between refreshes and hit start. It will go on until you click stop.",
      "##The terminal command\nI was doing this on a Mac and surprisingly I didn’t see a quick and easy way of doing this with Automator. But another quick Google search led to this terminal hack.",
      "while [ 1 ];do vardate=$(date +%d\\-%m\\-%Y_%H.%M.%S); screencapture -t jpg -x /path/to/save/$vardate.jpg; sleep 600; done",
      "This sets up an inifinite loop, store the filename consisting of the date and current time of the screenshot in a variable, calls the command, sets the filetype, then the path, and then how often to take the screenshot in seconds.",
      "##Putting it all together.\nI decided to grab a screenshot every ten minutes. I set the plugin to reload every 594 seconds (I had to choose a few seconds before the screenshot went off to make sure I didn’t grab a screenshot of a page refresh. Then run the terminal command to grab a screenshot every 600 seconds (10 minutes).",
      "I started a few minutes before the performance test started and left it going over night.",
      "When I checked around 9am we had a great \"flipbook\" of screenshots of the Solr admin panel showing memory utilization.   Having the date show up in the toolbar provides timing context to each image for later correlation with other monitoring tools.",
      "##Wrapping it up\nAfter getting the screenshots you can use one of several tools to make a movie or what I did was just made an animated .gif.",
      "",
      "We can see several things.",
      "1) The memory immediately starts gradually increasing and maxes out around 2am. We found out from the Performance Analysis Results that Solr didn’t start throwing errors 500/502/504 until around 4am.",
      "2) We can see the JVM garbage collection going off and unfortunately see some large spikes within those 10 minute intervals.",
      "3) We can see the temperature gradually drop in Charlottesville through the course of the evening and we see that my Crashplan did in fact go off around 3:17am and my data is safe! :)",
      "##Summary\nThere are some good monitoring solutions out there but if something hasn’t been setup already and you’re in a hurry to do some testing, this is a fun little hack to get some Solr monitoring!"
    ],
    "summary_t": "Hacking together Solr monitoring using Easy Auto Refresh (Chrome Plugin) and the command line"
  },
  {
    "id": "c7bfc8ba7df314b2e516133499e4270b",
    "url_s": "https://opensourceconnections.com/blog/2015/01/22/first-look-at-visualops/",
    "title": "A First Look at VisualOps",
    "content": [
      "A couple of months ago I gave VisualOps a try with a very specific goal in mind: create a diagram for an AWS environment I’ve been working on. Unfortunately I wasn’t quite able to achieve that goal. Recently they implemented my biggest requests so I hope to give it another shot soon.",
      "First let me outline what I had trouble with:",
      "For some reason it wasn’t obvious to me how to delete objects. Once someone from their team (see below) pointed out how it’s done I felt kinda dumb.\n  It’s meant for features available on public AWS. If you work in AWS GovCloud, which has different endpoints for the various APIs, you won’t be able to do end-to-end coordination.\n  The serialized format they use isn’t CloudFormation. For some reason I assumed it would be.",
      "Now what I liked:",
      "The rest of the interface was intuitive to work with and doesn’t require installation of anything.\n  They’re able to interrogate your account and reverse-engineer what you have there.\n  Changes you make in the interface can be published so that they affect your live environment.",
      "Lastly, the biggest thing I appreciated was their focus on acquiring happy customers. They definitely understand the importance of that for a young company. Someone emailed me immediately after I sent a help request, and afterward he plugged me into their Trello board so that I could see what features they have in the works (I had made a few suggestions – a couple were already up there and a couple he added after our discussion.)",
      "Overall I’m very excited about this product, especially because I hate making architecture diagrams and I really hate keeping them in sync!"
    ],
    "summary_t": "VisualOps looks to be a great time-saver for managing AWS architecture, and it scratches an itch I’ve been having for quite a while."
  },
  {
    "id": "c8f7b7352fb3c84e0ffc69cb085f33a8",
    "url_s": "https://opensourceconnections.com/blog/2015/01/28/hey-lanyrd-and-speakerrate-come-crawl-us/",
    "title": "Hey Lanyrd and SpeakerRate, come crawl us!",
    "content": [
      "When it comes time to share data, as a developer, my mind shifts to fun topics like building API’s, providing data in multiple formats, putting data into public S3 buckets, or even providing information in various RDF formats to support SPARQL end points.",
      "But there is a simpler way, and all it requires is the ability to write some HTML!",
      "Microdata is a standard for embedding meta data into existing HTML web pages that was sponsored by the WHATWG.   The biggest consumer of microdata is Google, which uses it to simplify it’s understanding of content that it crawls.   You can see microdata in action when you look for a movie in your town:",
      "",
      "The movie time information is all pulled from the HTML web pages of various theaters.",
      "So this brings me to one of my pet peeves of going to conferences, updating my information in Lanryd, often meaning that I am the first one to set up the event, and then, if I’m speaking, doing the same thing in SpeakerRate.",
      "So, in the spirit of making information sharing easier, I went ahead and added some microdata to the event information that we show on our website so that in the shiny future, the information about what events we’ll be at will already be indexed!",
      "The best resource I found for getting started was to read this StackOverflow question.",
      "Here is a simple example of the itemscope, itemtype, and itemprop tags being mixed into an event listing:",
      "<span itemscope itemtype=\"http://schema.org/Event\">\n\t<h1 class=\"title\" itemprop=\"name\">StrataConf 2015</h1>\n\t<span itemprop=\"location\" itemscope itemtype=\"http://schema.org/Place\">  \n\t\t<span itemprop=\"name\">San Jose, CA</span>\n\t</span>\n\t<div class=\"event-url\">\n\t\t<a href=\"http://strataconf.com/\" itemprop=\"url\">http://strataconf.com/</a>\n\t</div>\n</span>",
      "There are a number of tools for helping you validate your microdata, and get a sense of what will be extracted.   Click here to see Structured Data Linter validate a page.   The extracted contact might look in a search engine like:",
      "",
      "Google also provides both their testing tool: https://developers.google.com/webmasters/structured-data/testing-tool/ as well as a nice tool for visually marking up existing HTML content with the microdata tags: https://www.google.com/webmasters/markup-helper",
      "One disappointment I had is that I couldn’t get the people attending/speaking at a conference added to the data structure.   I tried to add both a attendee item property as well as a performer property.   According to the documentation at http://schema.org/attendee, an attendee is a valid property on an http://schema.org/Event.   It may be that the extraction libraries I was testing don’t really respect that however.  So right now they are just listed separately.",
      "If you know how to do it, please do let me know!"
    ],
    "summary_t": "Sharing common knowledge is hard, microdata (and schema.org) makes it easier."
  },
  {
    "id": "2dd4c026f98566f6d023292fdeb16eb1",
    "url_s": "https://opensourceconnections.com/blog/2015/02/02/do-you-really-need-a-nosql-database/",
    "title": "Do you really need a \"NoSQL\" Database?",
    "content": [
      "Visit me at Strata! (logo (C) OReilly Media)",
      "I’m fortunate enough to have been selected to speak at Strata 2015 in a few weeks on one of my favorite topics, database history. My talk is about the tradeoffs between going with a highly denormalized NoSQL database vs a normalized relational database. In the talk, we get to explore what the creator of the relational database, Edgar Codd, saw as the major problem with databases of his time. Could his criticisms apply to pre-relational databases to today’s mileau of non-relational databases?",
      "Supposedly, relational databases aren’t appropriate for today’s \"web scale\" problems. Instead, we’re supposed to look at NoSQL technologies to solve problems. It’s not at all that simple. Companies like Stackoverflow and Facebook famously use relational databases to store enormous amounts of data. If that’s the case, then what problem is really getting solved? These companies are also heavy users of non-relational technologies. Why? For what use cases?",
      "In truth – outside of the trendiness of one solution over another, different problems simply call for different solutions. Peek a layer deeper and we can see that in fact NoSQL is a fairly vacuous term, as is the SQL vs NoSQL divide. There’s simply families of databases with different features and affordances. Some of these come in the form of distributed vs non-distributed. Sharded vs non-sharded. Query language vs no query language. Tabular, wide row, or document structured. Strictly linearizable consistency vs last writer wins vs something in between. Which set of features in appropriate for your problem?",
      "One trend, however, that we’ll see come out of database history in my talk and in the \"NoSQL\" realm is a tradeoff of normalization vs denormalized. The latter grants a higher degree of availability by breaking up rich data relationships at the cost of losing data consistentcy features. As it turns out, without normalization, moving to many NoSQL databases feels a bit like going from programming in Ruby to C. Certainly great power and control comes with being able to manipulate every part of the distributed database. You can control, per query, for example, what consistency options should be used. Many problems call for targeted solutions that need fine-grain control. But with great power comes responsibility. Suddenly you’re even more in the drivers seat with fewer protections to make sure your data stays consistent. Do you have the skills to take this level of control of your data? Do you really need to be programming in C? Or will a relational database suit you perfectly fine?",
      "It’s a fun topic and if you’re confused on how to solve your problem, come see my talk, hopefully we can chat about some ideas. Feel free to find me at Strata or email us to see how OpenSource Connections can be your trusted NoSQL advisor for selecting the correct database."
    ],
    "summary_t": "I’m fortunate enough to have been selected to speak at Strata 2015 in San Jose on one of my favorite topics, database history. My talk is about the tradeoffs..."
  },
  {
    "id": "d77fa8392caac05a901f19725aba416d",
    "url_s": "https://opensourceconnections.com/blog/2015/02/04/its-log-its-log-its-big-its-hyper-its-good/",
    "title": "It's Log! It's Log! It's Big, It's Hyper, It's Good!",
    "content": [
      "What rolls down stairs alone or in pairs?(thanks OpenClipArt)",
      "Have you heard about the HyperLogLog data structure? It sounds something out of science fiction. Something Lt. Cmdr Data certainly would use in conjunction with the deflector dish to communicate with 2-dimensional aliens.",
      "It’s actually quite simple. A HyperLogLog is a substitute for counting things. Instead of storing everything we want to count (like every IP address that visits our site) we can use a HyperLogLog to closely approximate our counting while storing significantly less data.",
      "Don’t Worry, I’m Only Using your SSN For Educational Purposes!",
      "To understand how a HyperLogLog works, let’s solve a problem that doesn’t involve computers, cause computers are annoying. Let’s say we have a whiteboard and we have to count how many individuals visit our home. If our neighbor visits 5 times, they count as one unique visitor. If a friend comes over once, that’s another unique visitor. So far, we have 2 unique visitors.",
      "To use a HyperLogLog to do this, first we need a way to map people to numbers. Well here’s a stupid one – let’s just track everyone by their unhashed social security number. (there’s reasons that may not be optimal, but this is educational).",
      "Let’s further assume we can write two numbers on our whiteboard. These two numbers will represent our HyperLogLog. As someone enters our home, regardless if it’s their first time or not, they tell us their Social Security Number. We then go to count them in our HyperLogLog. If it begins with 0-4 they get counted on the left number. If it begins with 5-9 then they get assigned the right number. We’ll call these numbers the buckets.",
      "Our whiteboard starts out looking like this:",
      "0-4\n5-9\n\n\n\n\n0\n0",
      "Now pay attention. Here’s the trick – and what makes a HyperLogLog magical. When we take someones social security number and check its bucket, we need to then decide whether to update that person’s bucket or leave the original number there.",
      "The trick is that each bucket here is a high-score in a really stupid competition. Sort of like how many cheerios we can fit in our mouth at once. In this case, it’s a leaderboard for the thrilling game how many 0s are at the end of our visitor’s social security number. If our visitor has a 0 at the end of their social security number, well thats pretty unique. They deserve credit! If nobody has come along yet with more than one 0, then that visitor is currently winning their bucket(bracket?). In other words, if the social security number is 4XXXXXX50, then update the first bucket with a 1 to reflect the high score:",
      "0-4\n5-9\n\n\n\n\n1\n0",
      "Odds are the 0-4 bucket will keep that high score through many visitors until some even more special person comes along with a social-security number with two zeros at the end! Well isn’t that person special. If their social security number is 3XXXXX500 suddenly we have a new winner! We update the bucket’s high score accordingly:",
      "0-4\n5-9\n\n\n\n\n2\n0",
      "Wow. We must have seen quite a few people to have found someone with two 0s at the end of their social security number. In fact, if we do the math, we must have seen roughly 10^2 people for one person to show up with two 00s at the end of their social security number. In other words 10^{high score} people for that bucket!",
      "Wait, what did we just do there? Did we use this data structure to provide an approximate count of something? Yes – in fact that’s exactly what we did! And that’s the magic! (see it’s not that hard). A HyperLogLog is a stupid leaderboard that we can reverse-engineer the likelihood of acheiving said score. We can use the high score to approximate how many players have played the game.",
      "We have only been counting within the little competition happening in our 0-4 bucket. If we have a couple more visitors with SSNs that begin with 5-9, they’ll play out our stupid little game in their own 5-9 league. Ultimately, after all the competition is over (ie when we want to count) lets say we end up with this on our whiteboard:",
      "0-4\n5-9\n\n\n\n\n2\n1",
      "Having two high scores (for two silly competition leagues) lets us get more finely grained than just 10, 100, 1000 etc. In this case, our approximate count turns out to be 10^{high score bucket 1} + 10^{high score bucket 2} or 10^2 + 10^1 or 110!",
      "Adding Back In The Computers",
      "In our example, we’ve been using decimal numbers – as that’s what human beings know and love. Computers, however, live-and-breathe binary. So the main difference between what we’re doing is that while in both cases we count 0s, the counting approximation for a computer is actually done by powers of two. Its much easier for us to come across a binary number that ends in a 0. It has a probability of 1 in 2. So our leaderboard gets higher a bit faster, also resulting is much finer grain approximations.",
      "Another clear difference between this and real applications is the number of buckets. Many applications increase the accuracy of the approximation by increasing the number of buckets pretty significantly, creating a more sensitive approximation of the count.",
      "If you’re counting something like IP addresses, or other identifier, you’ll want to make sure you take care to hash that number into something with a reasonably random distribution before using a HyperLogLog. Social Security Numbers probably work decently for people, but in a real implementation calculating a well distributed hash will only help the accuracy of the data structure.",
      "If you want to play with an example, here’s an educational example I’ve cooked up in C. Enjoy! For further reading read the HyperLogLog Paper and also learn about Redis’s Implementation. You can also see a neat application tracking spammers in this article of ours.",
      "And of course, don’t hesitate to get in touch to discuss your analytics needs!"
    ],
    "summary_t": "Have you heard about the HyperLogLog data structure? It sounds something out of science fiction. Something Lt. Cmdr Data certainly would use in conjunction w..."
  },
  {
    "id": "4a4f5cb67328943253dbc9dcc97212ec",
    "url_s": "https://opensourceconnections.com/blog/2015/02/06/a-vagrant-conversation/",
    "title": "A Vagrant Conversation",
    "content": [
      "Vagrant is great… most of the time",
      "Vagrant is a powerful tool for setting up development environments quickly. But just like any other tool, there are use cases where vagrant works well, and use cases where it doesn’t quite fit. I (Jonathan/@64BitsPerMinute) had a conversation with Eric Pugh(@dep4b) and Chris Bradford(@bradfordcp) about how we used Vagrant on our last project involving fraud detection. It went a little like this…",
      "The Conversation",
      "@dep4b: Morning @64BitsPerMinute, @bradfordcp, thanks for joining me on this conversation!  We’re going to revolutionize the world of blogging by having a conversation and posting it! It’s like podcasting, only better, it’s textcasting.",
      "So this project was my first real \"in anger\" experience with Vagrant. I’ve tried it once or twice, and always been disappointed. So close, and yet it didn’t work. But on this project, I think it was critical to our success, and I just wish we hadn’t rejiggered it so many times over the course of the project!",
      "@64BitsPerMinute: @dep4b, well, in real life, requirements can change, so you have to adapt! But what’s going on?",
      "@dep4b: Well, should we share the path of vagrant over the project, and then talk about the pros and cons at each point?   Do you remember all the changes that we did?  Just to remind folks, this was a project where we stored a large set of data into DSE [DataStax Enterprise], and we leveraged all the features of DSE: Cassandra, Solr, and Spark.    The rest of it was just a simple set of APIS and then a very rich set of frontend JavaScript gui components.   So what was our first jab at getting things set up?  Didn’t we use Vagrant basically from day one?   Well, actually, I think I used a locally installed copy of DSE that I already had on day 1!",
      "@64BitsPerMinute: When we were still prototyping for the first two weeks, we actually were using local Cassandra nodes! :O",
      "@dep4b: Right! And then @pfreez did something better.",
      "@64BitsPerMinute: At this point in the project, it was just two people working on the prototype, so @pfreez started us off with a simple ubuntu-based vagrant setup.",
      "@dep4b: Was it using DSE, or was it using open source Cassandra?",
      "@64BitsPerMinute:  Well, at first we were using vanilla Cassandra. We realized that we would eventually need the search capabilities of solr, and the computational power of spark later on down the road, so we made the switch to DSE for its relative convenience.",
      "@dep4b: So yeah, I remember we had two nodes.   And since they were on vagrant, we had them each on a separate local IP address.   192.x.x.30 and 192.x.x.31, I remember cause I kept typing them in when connecting via cqlsh. \ncqlsh 192.168.0.33",
      "So what I really remember was @pfreez saying \"oh, it’s just fine\" when I ran vagrant up, and me thinking, gee, it’s been like 30 minutes, and I’m downloading the contents of the internet!  Our vagrant image, it started with a standard blank ubuntu box right?",
      "@64BitsPerMinute: Yeah, unfortunately it did. There are a lot of packages and dependencies to install to get a good development environment set up!",
      "@dep4b: There were definitely some annoyances in building the environment from scratch, not just the time to download java and all the dependencies.  Remember we saw that DataStax’s public repo either died, or was super slow for a day?   That hurt.   And how many times did you forget to set your DSE_USER and DSE_PASS credentials as environment variables to log into the DataStax repo?  I always remembered about 10 minutes into the process!",
      "@64BitsPerMinute: Yeah, we were seeing 100mb packages each take 20+minutes to download. And setting up passwords just to download stuff was getting annoying.",
      "@dep4b: Guess what I still have in my .bash_profile?",
      "export DSE_USER=epugh_opensourceconnections.com\nexport DSE_PASS=mypassword",
      "@64BitsPerMinute: lol",
      "@dep4b: I think my last complaint was that it also sometimes, for no good reason just tanked.   On the other hand, reading through the .vagrantfile was very good for me to learn how to install DSE, since every step was there, from the prerequisites to the network configuration aspects.",
      "@64BitsPerMinute:  At that point, both OSC and our client had added more developers to the project, so things needed to change. At one point, the client’s devops tried to make an entire ISO for their production servers, and then give it to the devs, but obviously throwing around a 40-60+ gig .iso wasn’t a convenient solution. So eventually we figured out a new way to go, and between our two teams, we created a compromise between these two solutions.",
      "@dep4b: What was it?",
      "@64BitsPerMinute: Well, we made the smallest base box possible that was able to support any type of node.",
      "@dep4b: What’s a base box again?   That is the \"image\"?",
      "@64BitsPerMinute: Yup! So if the image isn’t a raw image, but instead has all the main programs you’ll be using already installed, calling ‘vagrant up’ and ‘vagrant destroy’ only takes a few seconds, once you’ve downloaded the base box. And if you use a compact and slim OS ( we switched to CentOS for this reason), even you initial download can be small! Ours was only 2.5 gigs!",
      "@dep4b: Right, I remember running that import command once ??.   Also, we made some other changes right?  Instead of two C* nodes, one of which ALSO ran Solr, we had three seperate nodes running, one for C*, one for Solr, and one for Spark?",
      "@dep4b: There were some other nice things that we did as well.   Like, instead of having to remember IP addresses, we put those IP addresses into our /etc/hosts file, and gave them shortcut names.",
      "192.168.33.11 dev-dse.application.vm dev-dse\n192.168.33.12 dev-spark.application.vm dev-spark\n192.168.33.13 dev-solr.application.vm dev-solr\n192.168.33.14 dev-web.application.vm dev-web",
      "@64BitsPerMinute: Yeah, and for the vagrant vms we used the vagrant-hosts plugin for copying over those dns resolutions into each vagrant instance easily.",
      "@dep4b: Then I could do cqlsh dev-dse and jump right to that server.   It felt to me a bit clunky to be modifying my hosts file, but after doing it once it was more natural.   It would have been nice though if the \"vagrant up\" process had done that for me!\nThe other nice thing we got was a working OpsCenter as well.   Our initial setup never quite got OpsCenter working, and it was very nice to have that tool as we moved to working with Solr and Spark.",
      "So what didn’t work out perfectly, as I remember some issues still?   Sometimes vagrant up failed.  My debugging process was first to do vagrant destroy and then vagrant up.   If that didn’t work then I typically just rebooted my laptop!",
      "@64BitsPerMinute: I never had to do that on Arch Linux :P Though I did have to use the sudo vboxreload command every time I booted from scratch to make sure the kernel modules were properly loaded.",
      "@dep4b: I still heart my Mac ;-)  I also wasn’t good about suspending my vm’s either.  I also remember that we started using Talky.io heavily on the project, and the combination of running 3 2 GB VM’s and Talky would put all 4 of my processors at 100% CPU load!",
      "@bradfordcp: Toward the end our Spark box grew to 4GB. That box wasn’t started every day :D",
      "@64BitsPerMinute: Oh yeah, suspend is pretty critical if you don’t want your vms to end up in weird states.",
      "@dep4b: So @64BitsPerMinute, what else did we learn? I felt like the week you spent with the sys admin guy to validate the Cent OS image was pretty painful.  We needed some UI code cranked out, and it was a week of you messing with vagrant images.    On the other hand, once it did work, it worked for all of us.  And up until that point, I think we might all have had various flavors of DSE and vagrant running…..",
      "@64BitsPerMinute: Yeah, he was working on the scripts that customized each of our 4 nodes based on a single box image. There’s a lot of little configuration steps that you have to remember to add into scripts when you do more complicated multi node setups.",
      "@dep4b: So was having separated out nodes good or bad, from a DSE deploy perspective?   I remember that we also had the API tier deployed in a node, but as a developer I preferred to run that locally, so I just never spun that node up.",
      "@64BitsPerMinute: While having multiple nodes does lead to increased disk usage (about 5 gigs per node for me), the ease of being able to select which parts of the system are running is awesome for the developer. For the final deploy, the isolation of instances was great for catching otherwise hard to find bugs.",
      "@dep4b: so in the future, what would you add on that we didn’t get too in this project?",
      "@64BitsPerMinute: Well, our scripts weren’t 100% finished, because vbox reload never quite worked properly. This was fine for my personal workflow, because I stuck to vagrant up and vagrant suspend, and only occasionally using vagrant destroy when I really messed things up. But with more time I would have definitely put some more work into making sure the reload feature was working.",
      "@dep4b: And jonathan, if you had to summarized some lessons learned, what would they be?  Would you follow the same path?   Go straight to the images being built as VM’s?",
      "@64BitsPerMinute: I would stick to the same general workflow, but alter the timings a bit. Starting with vagrant using stock images with init scripts to install packages means that starting your vagrant can take a long time, but as the project changes rapidly in the prototype stage, you can easily alter what’s installed, and have the team synchronized in an otherwise chaotic environment. Once the dependencies have settled down, it’s definitely a good idea to make a custom base box. The initial download time of the base box can be a pain, but the vastly increased speed of creating and destroying boxes definitely leads to enhanced productivity and less downtime.",
      "@dep4b: yeah, and we were passing the basebox around via a VPN connection as well, a S3 bucket would have made it super speedy.   We never had to update the base box at all either, how would we have handled that? @bradfordcp, would vagrant up still taken care of that?",
      "@bradfordcp: Changing the base box is fine, we would update the image referenced in the Vagrantfile after the base box was updated and a new version was deployed. If you’re using Hashicorp’s Atlas system a new version for a box may be pushed. Then it is just a matter of running vagrant box update on your local machine. See the Vagrant Docs for more info.",
      "Vagrant providers were a little annoying while working on this project. Personally I use both the vmware and virtualbox providers. Not all boxes work with both providers. In this project’s case the base boxes were virtualbox only which required the usage of –provider=virtualbox every time I ran vagrant up. Depending on your setup it may be worth having boxes for multiple providers.",
      "@64BitsPerMinute: Well, this was a great post-mortem for all the ups and downs of vagrant on this project. Thanks for the chat, everyone!"
    ],
    "summary_t": "Eric, Jonathan, and Chris have a conversation about Vagrant"
  },
  {
    "id": "f19f49b1b8cb61b17f6f88991e05da22",
    "url_s": "https://opensourceconnections.com/blog/2007/04/16/upgrading-to-subversion-14-on-a-fedora-core-4-server/",
    "title": "Upgrading to Subversion 1.4 on a Fedora Core 4 Server",
    "content": [
      "Yesterday Eric asked me to upgrade our Subversion version from 1.2 to 1.4 in order to take advantage of the new XML responses needed by CruiseControl.rb. My first clue that this wasnt going to be easy was that the stock yum repositories only carry version 1.2. After a little digging I found an RPM that had version 1.4, but that carried with it a stack of other RPM dependencies that I decided not to try to untangle (been there, hated that.)",
      "CollabNet Subversion",
      "My next step was to try the recommended enterprise configuration from CollabNet. I really didnt realize what I was getting myself into, but the result was a 1.4 version of Subversion bundled with its own Apache server. Now, Ive been tweaking our trac-Apache-Subversion deployment quite a bit to make project creation quick and properly integrated. The thought of throwing in another Apache server into the mix didnt seem manageable, and recreating what I did with our current Apache server looked like a serious undertaking. So, \"rpm –erase\" and back to the drawing board.",
      "subversion-1.4.3.tar.bz2",
      "\"When in doubt, build from the source.\" Building Subversion from the sources is pretty much what Ive come to expect from the ./configure, make, make install cycle. I made sure I also downloaded the subversion-deps tarball recommended in the release notes, and followed it to the letter. The result was a broken svn client.",
      "svn undefined symbol: SSL_load_error_strings",
      "The Subversion server seemed to be working well behind Apache and trac, but Erics CC.rb builds were failing with an SSL error. I verified the error from the command-line and did some more Googling. A few hits down I ran across this post and this email thread which helped me greatly. The only thing standing in my way was a stupid bit of automation I failed to account for.",
      "Could not read status line: Secure connection truncated",
      "I banged my head against this one for much longer. Google failed me, rebuilds failed me. I had a thought that maybe I forgot to restart Apache after one of the rebuilds, and I noticed that it was complaining that the svn modules had already been loaded so they wouldnt be loaded again. I had seen this in an earlier restart and just thought, \"Okay, Ill clean that up once I get this Subversion thing to work.\" However, if I had bothered to look at the httpd.conf Id have seen that my \"make install\" had added the correct LoadModule line pointing to the new module, but the old LoadModule was left in place pointing to the old module. The old LoadModule happened before the new one, so I was loading an incompatible module. Once I pulled out the old module there was joy in Mudville once again."
    ],
    "summary_t": ""
  },
  {
    "id": "864cc63ad4d091f83d59fa30c6bd5349",
    "url_s": "https://opensourceconnections.com/blog/2015/02/12/experiments-in-visualizing-lucene-index-structure/",
    "title": "Experiments in Visualizing Lucene Index Structures",
    "content": [
      "Bearded Wookie is here!",
      "A while ago someone asked me for a entity diagram for the Solr \"database\".   My first, and rather snarky, plan was to provide a single table called collection with a set of columns.    Just draw it up real quickly.",
      "Then I thought to myself, wait, this person really does want to understand what is in the index.   And you know, I’ve had that same challenge.  On every consulting engagement where Solr was already in use.   Reading schema.xml kinda sucks, and the dynamic schema browser doesn’t let me see the big picture.",
      "So I went ahead and did some experiments with exposing Lucene structure via bubble charts and tree maps (yes, I wanted to play with D3!).",
      "The experiments, code named bearded wookie are posted at http://www.opensourceconnections.com/bearded-wookie, and you can try the visualizations out with your own Solr setups."
    ],
    "summary_t": "Someone asked me for a ER diagram for Solr.   This is what I came up with."
  },
  {
    "id": "81bffce9ea197a6f56be49cef030ff65",
    "url_s": "https://opensourceconnections.com/blog/2015/02/16/solr-cassandra-database-migrations-with-trireme/",
    "title": "Solving Cassandra Database Migrations with Trireme",
    "content": [
      "Recently we have been working on a project with a distributed team of developers. Each developer had a local DataStax Enterprise cluster running via Vagrant. Over time we kept running into problems with various developers’ clusters being out of sync with the latest schema. One early solution involved tearing down and rebuilding the keyspace after every pull. This worked, but lead to delays between building the keyspace, loading data, and having indexed content to work with. This led to problems during deployments as certain changes were looking for columns that no longer existed. Communication was flowing, but there were disjoints in the schema conversation.",
      "In response to this we developed a framework for managing our Solr and Cassandra schemas. For Cassandra, changes to the schema are in migration files within the repo. A tool is used execute new changes against the cluster. The Solr structure has a single schema file for each core. This file is synced to the cluster with a reload and reindex command to update the underlying indexes.",
      "Today we are releasing Trireme, a polished version of the tool and framework born from the project. Trireme is a migration framework targetting Apache Cassandra and DataStax Enterprise Cassandra & Solr. It handles setting up your project’s Cassandra keyspace and Solr cores and keeps them in sync over time. Trireme exposes tasks for developers that automate the generation new Solr core configuration files and Cassandra migrations.",
      "How does Trireme work?",
      "Trireme keeps a collection of Cassandra migrations that have run against the cluster in a table that it creates along with the keyspace. Upon running the migrate command this table is compared with the list of migrations on disk. Any missing migrations are run and after successful execution, recorded in the migrations table.",
      "Solr support is similar. All Solr core configuration files are stored within a named directory. The files are uploaded to Solr along with a reload command for the core. This keeps the schema in sync over time as it develops. Each command for Solr may run for all cores or optionally against a particular core.",
      "Trireme workflow integration",
      "Adding the Invoke commands to your workflow is simple. Did you just pull in your colleagues changes? Run the migrate commands to update your cluster. Have you cloned the repo and need to get your cluster up to speed quicky? Run the load_schema command to load the latest schema instead of all the migration files. Have you made changes to the Solr schema and want to test them out? Push the updated schema.xml file with the Solr migration command and watch as the new structure is pushed to the cluster. Trireme exposes a collection of tasks to satisfy these needs and more.",
      "Onward to the future!",
      "All core features are in and the package is available via PyPI. The project is fully functioning and stable. There are multiple features planned. If you would like to see a new feature or are having a problem add an issue on GitHub."
    ],
    "summary_t": "Chris discusses OSC’s new migration framework Trireme for Apache Cassandra"
  },
  {
    "id": "80ba7122668d442609da5eb75e194c0f",
    "url_s": "https://opensourceconnections.com/blog/2015/03/19/elasticsearch-cross-field-search-is-a-lie/",
    "title": "Elasticsearch Cross Field Search Is A Lie",
    "content": [
      "At OpenSource Connections, We Do What We Must, Because We Can </br>",
      "In Elasticsearch, searching across multiple fields can be confusing to beginners. This is a tough first step in creating a relevant search solution, so it’s important to get this right. In particular, it can be hard to wrap your head around multi_match’s cross field search and where exactly it fits into a querying strategy.",
      "The idea behind cross field search is that it attempts to do more sensible searching across fairly disjoint fields – disjoint fields we might consider parts of a whole. The example often given is first and last name fields. We tend to consider these two fields a decomposition of a name. When we search for \"Thomas Tucker\", \"Thomas, William\", or \"Frank Oz\" we typically just want to search for these terms ( [thomas], [tucker], [thomas], [william], etc) in either of the name fields.",
      "With normal dismax type queries (\"best fields\" or \"most fields\"), we sort of do this. However, we take each term to each field in isolation. The field with the best score dominates the resulting score. This is often termed \"field-centric\" search, and is the default mode of operation for most query strategies.",
      "Let’s walk through an example to understand field-centric search, and why it might be a problem for our use case.",
      "A query such as:",
      "'query': {\n    'multi_match': { \n        'query': 'tucker thomas',\n        'fields': ['first_name', 'last_name']\n    }\n}",
      "Will be interpreted by Elasticsearch as the following Lucene query:",
      "(first_name:thomas first_name:tucker) | (last_name:thomas last_name:tucker)",
      "We know that by default, multi_match does a best fields search. This is precisely what we see here – each field scored for each field independently with the best field score taken. When we take apart the scoring, we can see why the best fields strategy might be a problem here. \"Thomas\" is a common first name, however \"Tucker\" is a particularly rare first name. That means \"Tucker\" will receive a very low document frequency, and thus will be scored very highly by TF*IDF. Now let’s look at the last_name field. Thomas and Tucker are fairly common last names, thus they don’t receive a particularly high score. In best field search, the best fields score becomes the final score. Thus the score for \"Tucker\" in first_name will win (it’s the best field). The resulting score is the score for a first_name that matches \"Tucker\". We’ll now have search results that look like",
      "Tucker Fredrickson\n  Tucker Turnbull\n  Tucker Smith",
      "This is the destructive winner-takes-all pattern in dismax-type search we’ve blogged about before.",
      "Switching to \"most fields\" helps some. Most fields sums, it doesn’t take a max of the two field’s scores. We’ll add the miniscule score from the last name queries for these terms. This will act as a tie breaker for the first name search:",
      "Tucker Thomas\n  Tucker Turnbull\n  Tucker Smith",
      "Close, but no cigar. The problem is, in most use cases, we probably want \"last name\" matches to be more important than first name matches. In our fairly unintelligent name search, we’d prefer to see results that look like:",
      "Tucker Thomas\n  Bob Thomas\n  Tim Thomas",
      "How can we solve this problem?",
      "At the root of the problem is that our search is focused on the individual fields with their own particular odd statistics. Tucker is a fairly rare first name, but common last name. Bob is a very common first name but rare last name. We’d like to combine what we know about both fields. However, because of the stark differences in term distribution, document frequency, and field length, field scores between our name fields are not portable. Indeed this is true for all fields. Field scores live in their own little scoring universes. Scoring the fields together is not simple.",
      "So the problem can be restated as creating a sane way to score the fields together. We’d like to think in terms of the document frequency and other statistics of the two fields together, as if they were merged together, not how they behave in isolation. One common technique that people use to do this is to create what’s known as an all field. In Elasticsearch, this would involve using copy_to to append fields together into a larger field (perhaps full_name) via the mapping API.",
      "Indeed, this is a strategy that’s actually codified in Solr with the text field often baked into the default configuration. The advantage of the all field approach is it does the best job of accurately combining the statistics of the two fields together. It does this by literally combining two fields into one. Obviously, this causes the index statistics to truly reflect the combination of fields. Unfortunately, there’s a fairly big downside. These all fields create a great deal of duplication and bloat. The relevancy use case better be very valuable indeed to create so much overhead.",
      "Elasticsearch, however, has an alternative solution that attempts to blend fields together at query time. This is where cross_field comes in. Cross field search attempts to gather term statistics on the two fields together at query time to attempt to score the two fields as if they were one. For statistics like term frequency, Elasticsearch can sum the term frequency in each field as its scored. However, other statistics are not as simple. Document frequency is global to the index, and therefore a bit harder to compute when considering individual terms at query time. We can’t take the sum of the term’s document frequency at query time, as it will double count cases where both fields have the term. The best cross field can reasonably do is to take the maximum of the two fields’ document frequency and hope this comes close. For example if the names in our search engine look like:",
      "first_name\n        last_name\n      \n      \n        doc 1\n        douglas\n        turnbull\n      \n      \n        doc 2\n        thomas\n        turnbull\n      \n      \n        doc 3\n        douglas\n        thomas",
      "Here, the document frequency of \"thomas\" in blended, cross-field search will be \"1\" (the max document frequency of first_name and last_name). In reality, if the fields are appended together in an all field, the document frequency will be \"2\". The upside of cross-field search is that we’ve eliminated bloat and overhead from an all field in our index. The downside is that by taking a maximum of document frequency, we may undercount how common a term truly is between the two fields (and therefore overcount it as rare in scoring).",
      "So certainly, \"cross field\" search is a bit of a lie. But it comes with a number of advantages that give it a great deal of value. In particular, we mentioned in our use case that perhaps last name should be considered more valuable than first name. With cross field search, we can continue to leverage the field weighting available to us in multi_match. So a reasonable solution to our name search problem might take the shape of",
      "'query': {\n    'multi_match': { \n        'query': 'tucker thomas',\n        'fields': ['first_name', 'last_name^10']\n        'type': 'cross_fields'\n    }\n},",
      "Unfortunately, this search strategy would not always be a complete replacement for all fields. If you want to truly blend multiple fields into a whole scorable unit, then all fields is your best bet. The all field will give you deep control over how the field is turned into tokens (and therefore scored) in a way that requires far less head scratching.",
      "However, if you want a really good approximation, then cross_fields does a reasonably good job. I’m also hopeful (hey Elastic!) that work can be done to more accurately compute document frequency at query time. Could one, perhaps, create some way of efficiently diffing the postings between two fields for a term at query time? In fact, perhaps simply running an OR query for the term against both fields might be one (possibly pretty slow!) technique of computing the true document frequency. (Oh and we haven’t even talked about field norms!) Hopefully there’s work being done in this area!",
      "If you’re stuck on a problematic Elasticsearch or Solr relevance problem, be sure to contact us! Don’t hesitate to get in touch to let us know if you have any thoughts, feedback or criticisms of this article!",
      "photo from deviant art user Pandaxninja"
    ],
    "summary_t": "In Elasticsearch, searching across multiple fields can be confusing to beginners. This is a tough first step in creating a relevant search solution, so it’s ..."
  },
  {
    "id": "2e5a64491724e8bf4543451ec1b71b01",
    "url_s": "https://opensourceconnections.com/blog/2015/03/26/going-cross-origin-with-solr/",
    "title": "Going Cross-Origin with Solr",
    "content": [
      "It is becoming more common to connect directly with a Solr cluster from rich client side applications. Performing a search directly against the cluster will require either JSONP or Cross-origin Resource Sharing (CORS). Here we discuss a few methods for connecting with a search resource with CORS.",
      "Let’s assume we have an angular app running on app.o19s.com and a Solr cluster available at search.o19s.com. We attempt to use Angular’s $http service to perform a query against the http://search.o19s.com/solr/core/select endpoint. No results are displaying in the application. When we test the Solr URL in another tab the results are displayed. The web inspector displays an XMLHttpRequest error",
      "ACCESS DENIED",
      "What’s going on?",
      "In this case our browser performed some checks prior to the request being sent and decided it needed to verify our application has permission to access the search.o19s.com resource. It then sends a separate \"preflight\" request first asking if we have access. At this point our Solr cluster doesn’t know how to handle the request and doesn’t respond appropriately. This signals the browser that we do not have permission and the original request is never sent.",
      "How does CORS work?",
      "When an XMLHttpRequest is performed our browser checks the protocol, domain, and port of the request verifying they match that of our current page. In our example the protocol and port match, but not the domain. This triggers the cross-origin preflight check.",
      "The preflight request is sent by the browser to the resource being accessed. This request is sent with the OPTIONS HTTP method and an Origin header. The Origin header provides the origin that is attempting to access the resource, in our case the header will read Origin: http://app.o19s.com/.",
      "At this point the server should respond with a pair of headers, Access-Control-Allow-Origin and Access-Control-Allow-Methods. The Access-Control-Allow-Origin header indicates which origins are permitted access. The browser will take this list and match the origin domain, protocol, and port to see if it is permitted access. The second header, Access-Control-Allow-Methods, lists all supported HTTP methods. Now the browser checks the XMLHttpRequest’s method verifying that it matches the Access-Control-Allow-Methods.",
      "Enabling CORS at the reverse proxy",
      "When running Solr publicly it is recommended to place Solr behind a reverse proxy. We have talked about this previously with IIS, but other web servers like Apache and nginx also offer similar functionality. If your configuration includes a reverse proxy CORS support may be enabled on this level. Enable CORS has a list of configuration entries for many common web servers.",
      "Apache\n  nginx\n  IIS7",
      "Note this list is not complete, check out Enable CORS if your server is not in this list or continue reading for another approach.",
      "Enabling CORS within the Solr application server",
      "In some environments enabling CORS at the reverse proxy level is not possible. This can be for a variety of reasons, in this case we can move the CORS configuration to the app server level. Solr ships with the Jetty servlet engine. Jetty hosts the Solr WAR and handles requests to the application. We will configure Jetty to serve the appropriate CORS headers when requested, this requires no change to the reverse proxy.",
      "Verify the server/webapps/solr.war file is extracted to server/solr-webapp/webapp. If not this may be accomplished with the following commands\n\n     # Server is the directory where Solr resides\n cd server\n unzip webapps/solr.war -d solr-webapp/webapp\n    \n  \n  Download the appropriate libraries (links below direct to search.maven.org’s mirrors)\n    \n      jetty-servlets-8.1.14.v20131031.jar\n      jetty-util-8.1.14.v20131031.jar\n    \n  \n  Copy the downloaded JAR files to the server/solr-webapp/webapp/WEB-INF/lib directory\n  \n    Edit the server/solr-webapp/webapp/WEB-INF/web.xml file to include the following filter right after the <web-app> line. On a clean Solr 5.0.0 download this entry will start at line 24.\n\n     <filter>\n   <filter-name>cross-origin</filter-name>\n   <filter-class>org.eclipse.jetty.servlets.CrossOriginFilter</filter-class>\n   <init-param>\n     <param-name>allowedOrigins</param-name>\n     <param-value>*</param-value>\n   </init-param>\n   <init-param>\n     <param-name>allowedMethods</param-name>\n     <param-value>GET,POST,OPTIONS,DELETE,PUT,HEAD</param-value>\n   </init-param>\n   <init-param>\n     <param-name>allowedHeaders</param-name>\n     <param-value>origin, content-type, accept</param-value>\n   </init-param>\n </filter>\n\n <filter-mapping>\n   <filter-name>cross-origin</filter-name>\n   <url-pattern>/*</url-pattern>\n </filter-mapping>\n    \n\n    Note that the allowedOrigins value here is * which permits access from all origins. Consider setting this value to the domain where your application is hosted (ex: app.o19s.com). The allowedMethods param may also be trimmed to just the methods needed by your application (ex: GET,POST).\n  \n  Restart Solr\n  \n    Test the CORS changes have been applied with cURL\n\n     curl -I -X \"OPTIONS\" \"http://search.o19s.com/\" -H \"Origin: http://app.o19s.com\"\n HTTP/1.1 200 OK\n ...\n Access-Control-Allow-Origin: http://app.o19s.com",
      "With the correct value in the responses’s Access-Control-Allow-Origin header XMLHttpRequests will no longer be blocked by the browser. If you’re stuck on a problematic Solr deployment problem, be sure to get in touch! Don’t hesitate to get in touch to let us know if you have any thoughts, feedback or criticisms of this article!"
    ],
    "summary_t": ""
  },
  {
    "id": "a48795653427de8d524f39481b58986e",
    "url_s": "https://opensourceconnections.com/blog/2015/03/26/visualizing-file-ingestion-punchcard-graph/",
    "title": "Visualizing Solr file ingestion with a punchcard graph in R",
    "content": [
      "##Putting the data together\nRecently we’ve had to analyze the size of files being ingested into a Solr index. Performance testing had been done several times and we were seeing some really great response times with zero errors and other times we were seeing really high response times with hundreds of 504 Server errors.",
      "We knew new files were being ingested during this time but we weren’t sure of the file sizes or number of files coming in while we were pounding the server with 1800 requests per minute.",
      "So what does a good data analyst turn to? R!",
      "First we had to get the data. New files were being uploaded to an incoming AWS S3 bucket, going to the ingestion server, then being sent to an AWS S3 done bucket. Getting the files in the done bucket was a cinch with the AWS CLI.",
      "aws s3 ls s3://your-bucket-name-goes-here --output text >> ~/Desktop/s3.txt",
      "This gives a space delimited text file that is easily converted to a .csv.",
      "At that point it’s just a matter of loading the data into an R dataframe.",
      "library(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"grid\")",
      "First read in the s3.csv file, unless you added headers outside of R you’ll need to add those yourself using the below colnames function.",
      "s3_files <- read.csv(file=\"s3.csv\", head=FALSE)\ncolnames(s3_files) <- c(\"date\", \"time\", \"size\", \"filename\")",
      "This will give us the hour the file was ingested and put it into a new column in the data frame.",
      "s3_files$time <- hms(s3_files$time) \ns3_files$hour <- factor(hour(s3_files$time))",
      "This creates a new data frame with a column for the sum of the file sizes by the hour and puts that into a new data frame called break.down.hour.date.",
      "break.down.hour.date <- s3_files %>%\n  select(-time) %>%\n  group_by(date, hour) %>%\n  summarise(size=sum(size))",
      "This is the ggplot2 code to create a GitHub style punchcard graph. It’s really a scatterplot with the x axis the hour of the day a file came in and the y axis is the date. The size of the dot is based on the sum of the file size.",
      "hour.date.graph <- ggplot(break.down.hour.date, aes(x=break.down.hour.date$hour, y=break.down.hour.date$date, size=size, color=size))\nhour.date.graph + geom_point() + \n  ggtitle(\"Hourly breakdown of files from Jan 05 - Mar 04\") +\n  xlab(\"Hour of day\") +\n  ylab(\"Date\") +\n  guides(size=FALSE) + \n  scale_size(range=c(5, 15)) +\n  guides(color=FALSE) +\n  theme(axis.title=element_text(size=16),\n        plot.title=element_text(size=18, face=\"bold\"),\n        panel.grid.major = element_line(colour=\"grey\"),\n        panel.grid.minor = element_line(colour=\"grey\"),\n        plot.background = element_rect(fill=\"grey90\"),\n        plot.margin = unit(c(4, 4, 10, 4), \"mm\"),\n        panel.background = element_rect(fill=\"grey90\"))",
      "And below is the final output!",
      "",
      "##Analysis\nOn the above graph we can see a few things. Most of the large files come in between 12pm and 5pm which is during peak time when the most users will be on the site. This was a good indicater that we needed to find a way to move ingestion to earlier or later in the day.",
      "We also see that the vast majority of the incoming files are smaller in size which won’t present a problem for search and site performance.",
      "And we see that there are some days were no files are ingested. This was actually known from the start because no files are sent to be ingested on the weekend.",
      "##Summary\nMaybe my Google-Fu is a little weak but I couldn’t find a lot of documentation on how to do a punchcard style graph in R or ggplot. But it wasn’t difficult to just use a scatterplot and set the size of the point to the sum of the file size by hour.",
      "This type of plot can be very beneficial when you need to see a breakdown of what is happening by hour during over days or weeks."
    ],
    "summary_t": "Recently we’ve had to analyze the size of files being ingested into a Solr index. Performance testing had been done several times and we were seeing some rea..."
  },
  {
    "id": "5a06ebe597f8f3147b54124230d28ee1",
    "url_s": "https://opensourceconnections.com/blog/2015/03/30/building-the-worlds-smallest-cassandra-cluster/",
    "title": "Building the world's smallest Cassandra cluster",
    "content": [
      "A few weeks ago I received my first Intel Edison in the mail. After a bit of tinkering I was able to bootstrap a small Cassandra cluster on this little SoC. I tweeted a picture showing it up and running with OpsCenter connected in the background.",
      "I think I've made the world's smallest Apache #Cassandra cluster with an Intel Edison. @DataStax @cassandra pic.twitter.com/BPjjoY2T2a— Christopher Bradford (@bradfordcp) February 15, 2015",
      "",
      "I will ouline how to bootstrap the Intel Edison and install a functioning Cassandra cluster. Note that even though you can run Cassandra on the Edison, that doesn’t necessarily mean you should.",
      "What is an Intel Edison?",
      "Image via SparkFun",
      "The Intel Edison is a development platform for embedded applications boasting impressive specs:",
      "Dual core Intel Atom SoC (500MHz)\n  1GB LPDDR3 RAM\n  802.11 a/b/g/n Wifi\n  Bluetooth 4.0 LE",
      "Components",
      "In this project we’re going to need to provide power, serial console access, and some additional storage capacity. These features can be stacked on to the Edison through blocks. If you’re framiliar with Arduino shields, the concept is the same.",
      "Intel Edison\n  SparkFun Block for Intel® Edison - Console\n  SparkFun Block for Intel® Edison - microSD\n  Intel Edison Hardware Pack\n  SanDisk Ultra 32GB UHI-I/Class 10 Micro SDHC Memory Card",
      "At the time of publication a single node in our cluster costs approximately $119 USD.",
      "Node Assembly",
      "Assembled Node",
      "After all of the components have arrived it is time to start assembling the node. Each node is comprised of 3 layers (Edison => Console Block => MicroSD Block) which are connected via 70-pin connectors on the top and bottom of the blocks. The order of the Console and MicroSD blocks are not important, but the Edison must be placed on the top. In our assembly we have placed the MicroSD block on the bottom as fewer components are exposed in this configuration.",
      "Begin assembly by attaching two standoffs from the hardware pack onto the top of the console board with the small phillips head bolts. This will expose two posts that will support the Edison.\n  Attach 4 standoffs to the bottom of the board with nuts\n  Connect the Edison to the Console block and affix it to the standoffs with two nuts.\n  Next add the MicroSD block securing it with 4 additional standoffs.",
      "Bootstrap the Edison",
      "With the node assembled it is time to start setting up the software side. Intel provides a step by step getting started guide. For our setup we should configure the password and connect the device to our wifi network.",
      "Connect to the Edison via the USB serial device. Note that on your device the value A402YSYU may be different\n\n      screen /dev/tty.usbserial-A402YSYU 115200 –L\n    \n  \n  Press enter twice and a login prompt should appear\n  Type root and press enter\n  Run the configure_edison program. This will prompt you for a name for the device, a new root password and walkthrough the wifi setup process.\n  \n    Prepare the MicroSD card. We first unmount the card (which is auto-mounted at /media/sdcard). Next we run fdisk and repartition the card. Finally we create the filesystem.\n\n     umount /media/sdcard\n fdisk /dev/mmcblk1\n d                         # Delete the current partition (There is usually only 1 if any)\n n                         # Create a new partition\n p                         # Mark the partition as primary\n 1                         # Assign it the number 1\n <enter>                   # Select the first block on the device\n <enter>                   # Select the last block on the device\n w                         # Write the change to the disk and exit\n mkfs.ext4 /dev/mmcblk1p1\n    \n  \n  \n    Setup the Cassandra data directory\n\n     mount /dev/mmcblk1p1 /media/sdcard\n mkdir /media/sdcard/cassandra\n    \n  \n  Type exit and press enter. Future setup will be performed over SSH.",
      "Install Java",
      "Cassandra is written in java and requires the JRE to run. It is recommended that we use Oracle’s JRE version 7.",
      "Download the jre-7u75-linux-i586.tar.gz from the Oracle download page. This requires accepting a license agreement before downloading the file.\n  Copy the file to the Edison via SFTP\n  \n    Extract the file\n\n      tar xvzpf jre-7u75-linux-i586.tar.gz\n    \n  \n  \n    Set the JAVA_HOME environment variable to point at the extracted directory\n\n      export JAVA_HOME=/home/root/jre1.7.0_75\n    \n\n    It may be worthwhile to place this in your .bash_profile to prevent having to run this every time we start Cassandra.",
      "Install getopt",
      "Cassandra’s start script utilizes the getopt command. The Yocto Linux distribution running on the Edison does not ship with this utility. This package may be found through Yocto’s Recipe reporting system. From here there is a link to the source code which we will pull down and compile on the Edison.",
      "Download the util-linux package from kernel.org\n\n     wget http://kernel.org/pub/linux/utils/util-linux/v2.25/util-linux-2.25.2.tar.xz\n    \n  \n  \n    Extract the package\n\n     tar xvf util-linux-2.25.2.tar.xz\n    \n  \n  \n    Build the source\n\n     cd util-linux\n ./configure\n make getopt\n    \n  \n  \n    Install the built binary\n\n     mkdir -p /usr/local/bin\n cp getopt /usr/local/bin",
      "Install & Configure Apache Cassandra",
      "Download Cassandra 2.1.3\n\n     wget http://mirrors.gigenet.com/apache/cassandra/2.1.3/apache-cassandra-2.1.3-bin.tar.gz\n    \n  \n  \n    Extract the package\n\n     tar xvzpf apache-cassandra-2.1.3-bin.tar.gz\n    \n  \n  \n    Configure Cassandra - be sure to edit the values listed below instead of replacing the file\n\n    cassandra.yaml\n\n     cluster_name: \"Edison Cluster\"\n listen_address: <insert ip here>\n rpc_address: <insert ip here>\n    \n seed_provider:\n     - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n       parameters:\n           - seeds: \"<insert ip here>\"\n data_file_directories:\n     - /media/sdcard/cassandra/data\n commitlog_directory: /media/sdcard/cassandra/commitlog\n saved_caches_directory: /media/sdcard/cassandra/saved_caches\n    \n\n    cassandra-env.sh\n\n     MAX_HEAP_SIZE=\"512M\"\n HEAP_NEWSIZE=\"200M\"\n    \n\n    logback.xml\n\n     <!-- Comment out the following line to prevent spamming stdout -->\n <!--<appender-ref ref=\"STDOUT\" />-->\n    \n\n    /etc/hosts - The hostname must be in /etc/hosts or DNS\n\n     <insert ip here> <hostname>\n    \n  \n  \n    Start Cassandra\n\n     /home/root/apache-cassandra-2.1.3/bin/cassandra",
      "Intel Edison Micro-Cluster Benchmarks",
      "Cluster Status",
      "nodetool -h 192.168.1.50 status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address       Load       Tokens  Owns (effective)  Host ID                               Rack\nUN  192.168.1.50  70.54 KB   256     100.0%            2395b793-2d8b-4c31-931c-64fa94097cc5  rack1",
      "Cassandra Stress Writes",
      "./cassandra-stress write n=100000 -node 192.168.1.50\nCreated keyspaces. Sleeping 1s for propagation.\nWarming up WRITE with 50000 iterations...\nINFO  18:36:16 Using data-center name 'datacenter1' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)\nConnected to cluster: Edison Cluster\nDatatacenter: datacenter1; Host: /192.168.1.50; Rack: rack1\nINFO  18:36:16 New Cassandra host /192.168.1.50:9042 added\nSleeping 2s...\nRunning WRITE with 200 threads for 100000 iteration\n...\nResults:\nop rate                   : 1922\npartition rate            : 1922\nrow rate                  : 1922\nlatency mean              : 104.0\nlatency median            : 87.9\nlatency 95th percentile   : 176.9\nlatency 99th percentile   : 381.7\nlatency 99.9th percentile : 696.5\nlatency max               : 1855.3\ntotal gc count            : 4\ntotal gc mb               : 569\ntotal gc time (s)         : 2\navg gc time(ms)           : 474\nstdev gc time(ms)         : 19\nTotal operation time      : 00:00:52\nEND",
      "Cassandra Stress Reads",
      "./cassandra-stress read n=100000 -node 192.168.1.50\nWarming up READ with 50000 iterations...\nFailed to connect over JMX; not collecting these stats\nINFO  18:41:53 Using data-center name 'datacenter1' for DCAwareRoundRobinPolicy (if this is incorrect, please provide the correct datacenter name with DCAwareRoundRobinPolicy constructor)\nConnected to cluster: Edison Cluster\nINFO  18:41:53 New Cassandra host /192.168.1.50:9042 added\nDatatacenter: datacenter1; Host: /192.168.1.50; Rack: rack1\nSleeping 2s...\n...\n         id, total ops , adj row/s,    op/s,    pk/s,   row/s\n  4 threads, 100000    ,       238,     238,     238,     238\n  8 threads, 100000    ,        -0,     399,     399,     399\n 16 threads, 100000    ,       588,     588,     588,     588\n 24 threads, 100000    ,       725,     722,     722,     722\n 36 threads, 100000    ,        -0,     873,     873,     873\n 54 threads, 100000    ,        -0,    1096,    1096,    1096\n 81 threads, 100000    ,        -0,    1247,    1247,    1247\n121 threads, 100000    ,        -0,    1376,    1376,    1376\n181 threads, 100000    ,        -0,    1444,    1444,    1444\n271 threads, 99688     ,        -0,    1532,    1532,    1532\n406 threads, 100000    ,        -0,    1515,    1515,    1515\n609 threads, 100000    ,        -0,    1500,    1500,    1500\n913 threads, 100000    ,      1509,    1501,    1501,    1501\nEND",
      "RF Performance",
      "Overall we’re seeing ~1,900 writes per second and ~1,500 reads per second. That’s respectable for a tiny SoC storing the data on a MicroSD card and communicating over wifi. Bringing up our Access Point’s (AP) interface we can take a look at the wireless performance of our nodes. The node used in these stress tests was connected via 802.11 g/n from approximately 10 feet away from the AP. Looking at the graphs we see the node connected at 75mpbs and some issues with retried frames (both in and out). A second node was brought online which didn’t have these issues. It connected at 150mbps via 802.11 a/n and didn’t have as many retries in either direction.",
      "The Edison is a quite capable SoC. It handled running Cassandra with no problem. The configuration may be futher tuned to match the hardware. Remember even though you can run Cassandra on the Edison, that doesn’t necessarily mean you should. If you’re stuck trying to tune and configure Cassandra, be sure to contact us! Don’t hesitate to get in touch to let us know if you have any thoughts, feedback or criticisms of this article!"
    ],
    "summary_t": "Running a big data platform on small hardware"
  },
  {
    "id": "2dd8451640e1e9e71a9619fcb0be58fe",
    "url_s": "https://opensourceconnections.com/blog/2007/04/18/a-web-20-bubble/",
    "title": "A Web 2.0 Bubble?",
    "content": [
      "Reuters reported this morning that the number of contributors to many Web 2.0 sites such as YouTube and Flickr is much smaller than the number of people who come to look at the content generated. With the glaring exception of Wikipedia, the average tends to run around 500 viewers for each contributor. At first glance, this might seem like a shock given that Web 2.0 applications are supposed to make it easier for the user to participate and contribute, but further examination shows that maybe its not surprising after all.",
      "First, let us look at the ease with which users can add content. The original \"category killer\" was the blog, where within minutes, people would be able to set up their own commentary site and become the next Bill Kristol or Fred Barnes. While creation of blogs continues at a rapid pace, Technorati reports that there are 1.5 million posts daily to a population of approximately 57 million blogs, or about a 2% contribution rate. That is for a medium which requires no additional equipment besides a computer. Once you add in digital equipment necessary to create the media for many Web 2.0 sites, it is not surprising that the contribution rate drops.",
      "Secondly, it seems that human nature is a contributing factor to this phenomenon. Garret Hardins",
      "Tragedy of the Commons succinctly explains what is happening here. Any time you have a contribution to the public good (as the site owners would argue these sites are), many more people will take advantage of the public good than will contribute to it. These \"free riders\" are happy to take part and take advantage of the common good, but few will pony up resources to keep it going for everyone else. That is why public radio has to have fundraising drives all of the time and why street performers only have the scantest of change in their tip jars.",
      "Does this mean that the Web 2.0 sites should pack up and go home? No. Aside from the most altruistic of creators, the purpose of these sites is profit. Profit is generated on these sites almost exclusively from advertising revenue. What draws advertising revenue? Visits to the site. So, even if these sites have an incredibly skewed proportion of contributors to viewers, they still have viewers, and the contributors are free staff, meaning that the cost of content generation is zero. One of the biggest variable costs in a fixed media such as a newspaper, the staff needed to create the content, is eliminated in Web 2.0 sites. As Flickr and YouTube have proven, it does not take many content creators to drive an enormous amount of traffic."
    ],
    "summary_t": ""
  },
  {
    "id": "14b6c115cc0d49d0a7a2ba23c4fcaef3",
    "url_s": "https://opensourceconnections.com/blog/2015/04/02/bringing-the-search-relevancy-message-to-cio-review-magazine/",
    "title": "Bringing The Search Relevancy Message to CIO Review Magazine",
    "content": [
      "I’m happy to be taking the search relevance mantra to the CIO crowd in my article in CIO Review Magazine (p65). In the article, I highlight that search is how everyday users expect to interact with Big Data. So much of the focus in Big Data has been analytics and I think this is unfortunate. Everyday human beings want to interact with data in human terms – not through SQL, map-reduce, or dashboards. They want to type in a few search terms and get back results relevant to them and their goals. Nevertheless, the current mantra from data science and \"Big Data\" startups is \"analytics! analytics! analytics!\". The emphasis is on selling the data structure dujour that can solve all your analytics problems in a single silver-bullet NoSQL Hadoopy platform.",
      "Compared to this latest trend in \"NoSQL analytics\", it’s interesting how long-lived Lucene has been. Lucene-based search like Elasticsearch and Solr has been quietly crept into all sorts of applications. Both for analytics and search. Recently, it seems the trend even in search circles has been heavily focused on analytics, with compelling features like Elasticsearch’s aggregations, and columnar-like DocValues gaining a lot of visibility in this area.",
      "The quieter but perhaps more important story, however, is how the search bar has crept into nearly every application. Look at your phone. So many applications have search at their core. OpenTable and Yelp for finding restaurants near me. Spotify and iTunes for music. Youtube for video. We don’t even think about how often we’re using search. We’re not conscious of how each of these are unique and diverse user experiences over data focussed on search. We just search! We certainly, however, think about when it doesn’t return results we want!",
      "So as you develop your applications, and you think about what \"Big Data\" actually means, ask yourself should a focus on relevant search be part of the solution? Are users going to want to see data or talk to data? Ask for things in natural-language terms? If so, think about how tuning the relevance of your search results might be a better place to spend your smart folk’s time! It could certainly pay off.",
      "Anyway, I’d love to hear feedback on the article. If you’d like to discuss, please contact me!"
    ],
    "summary_t": ""
  },
  {
    "id": "128129fd288b1a4701c4bb6b9730792c",
    "url_s": "https://opensourceconnections.com/blog/2015/04/09/osc-is-taming-search/",
    "title": "We're Writing The Book on Search Relevance!",
    "content": [
      "The search bar has crept into your application. Do you know how to extract value out of it? Or is it a dangling afterthought?",
      "Buy now with a discount! Use offer code turnbullmu to get 39% off all formats!",
      "Search is hard. Yet it’s increasingly what users expect. Your users want intelligent search that understands your content and responds to their vernacular.  To you this can feel like a mystical endeavor. Building a search engine that communicates on human terms to return relevant results can feel more like trying to pass the Turing Test than programming.",
      "We’re happy to announce Relevant Search to help solve these problems. Taming search captures OSC’s collective decades of relevance tuning and monitoring experience. Turning the search bar into a revenue generator instead of a cost center is what we do every day. Until now, there’s been no book that captures this day-to-day engineering of working in the trenches with Solr or Elasticsearch to solve these problems.",
      "This is exactly what we do in Relevant Search. How can you leverage the internals of the search engine to deliver a relevant search solution? How do you collaborate with content experts and measure the effectiveness of your search solution? How do you integrate external systems like machine learning and knowledge bases like ontologies to enhance search relevance?",
      "We invite you to check out the book and participate in the discussion. Hopefully you’ll be able to leverage our experience, and avoid painful lessons that so many have gone through. And as always, if you’d like to enlist our expertise to solve your relevance problems, please get in touch!"
    ],
    "summary_t": "We’re writing the book on Search Relevance! Relevant Search from Manning Publications turns your search fom a dangling afterthought to a core part of your ap..."
  },
  {
    "id": "3f88b61d13db8152748f31cf6be03e7f",
    "url_s": "https://opensourceconnections.com/blog/2015/04/13/search-books-day-at-manning/",
    "title": "Search Books Day at Manning!",
    "content": [
      "Buy our book and many others today! Use offer code dotd041315au to get 50% off all formats!",
      "Today only get 50% off Relevant Search – the search relevance book we’re writing. You can also pick up Solr In Action and Elasticsearch In Action with this deal.",
      "The most important book is of course Relevant Search :). Once you plugin your data to Solr or Elasticsearch, you’re far from what your users expect from a search user experience! Taming Search teaches you how to optimize search results for your content, users, and their vernacular. It explores topics such as Lucene Internals, to appropriate use of search engine features, to machine learning – all to grow beyond dull, basic text matching that satisfies no one to deep search that focusses on a specific user experience.",
      "So be sure to use Manning’s Deal-Of-The-Day code dotd041315au to save on these great titles!"
    ],
    "summary_t": "Save big on great search books! 50% off Relevant Search, Elasticsearch In Action, and Solr In Action. Get’em while they’re hot!"
  },
  {
    "id": "e01c3eb11b375e2c69797fdbe352becb",
    "url_s": "https://opensourceconnections.com/blog/2015/04/23/excerpts-from-taming-search/",
    "title": "Excerpts From Relevant Search",
    "content": [
      "The Search Relevance Book!",
      "As many of you know, we’re writing Relevant Search to address the crisis of low quality, untuned,  and irrelevant search applications. If you truly want to do more to help your users, check out these free excerpts from chapter one to learn how to deliver your users what they want from your search bar:",
      "Search is Eating The World, where we argue about the vastly undervalued role the search bar plays in today’s world.\n  Solr and Elasticsearch—Sharpening Your Search Results by Improving Relevance, where we layout a high-level framework for solving relevance problems.",
      "Don’t be afraid to contact us if you need help solving a tough relevance problem! And remember – if your users can’t find it, it doesn’t exist!"
    ],
    "summary_t": "As many of you know, we’re writing Relevant Search to address the crisis of low quality, untuned,  and irrelevant search applications. If you truly want to d..."
  },
  {
    "id": "c626372b8ada96b14739d71c949e274c",
    "url_s": "https://opensourceconnections.com/blog/2015/04/30/debugging-solr-5-in-intellij/",
    "title": "Debugging Solr 5 in Intellij",
    "content": [
      "I recently had to debug Solr 5 to help answer some client questions. With Solr 5, there’s been several fundamental changes to the Lucene/Solr codebase. My previous methods of debugging Solr didn’t work anymore, so I had to figure out anew how to debug Solr 5.",
      "If you try to work with a recent Solr/Lucene codebase you’ll immediately hit a few snags:",
      "Java 7 is deprecated, time to upgrade to Java 8!*\n  Solr is run through the handy ./bin/solr command. No more java -jar start.jar",
      "*turns out you don’t need java 8 for Solr 5, but it has a bug where ant tasks dont run without it!",
      "Install Java 8",
      "Java 7 is EOL, Solr is deprecating Java 7 in the next release. So you’ll of course need to get Java 8 on your machine. I leave this as an exercise to the reader. I prefer OpenJDK unless explicitly told not to by the project. It’s simpler to install and manage on *nixs that I usually work on.",
      "Clone lucene-solr",
      "I always work directly with the Github mirror of the Lucene/Solr project. Using git clone, I create a lucene-solr directory with the latest source code (you may wish to just debug a specific tag):",
      "$ git clone [email protected]:apache/lucene-solr.git\ncd lucene-solr",
      "Note this checks out the trunk, examine the tags to play with a specific 5x+ branch.",
      "Side-by-side Java 7/Java 8",
      "If you want to keep your default Java 7, but use Java 8 just for Lucene/Solr, use direnv. This tool lets you define environment variables such as PATH and JAVA_HOME on a directory-by-directory basis.",
      "With direnv installed, you can define a .envrc file in the lucene-solr directory. Create a .envrc. Here’s what mine looks like which makes Java 8 default for anything under the lucene-solr directory:",
      "export PATH=/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/:/home/doug/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games",
      "You’ll also need to tell direnv to allow per-directory environment variables with direnv allow:",
      "$~/workspace/lucene-solr> direnv allow",
      "Now run java -version and verify you’re using Java 8:",
      "java -version\n$~/workspace/lucene-solr> java -version\nopenjdk version \"1.8.0_45-internal\"\nOpenJDK Runtime Environment (build 1.8.0_45-internal-b14)\nOpenJDK 64-Bit Server VM (build 25.45-b02, mixed mode)",
      "Great! Now you’re ready to get started with Solr 5 development.",
      "Build Solr",
      "With Java 8 working in this directory, you should be able to run the various ant tasks to build Solr 5 and setup IntelliJ project files. Let’s build Solr:",
      "$~/workspace/lucene-solr> cd solr\n$~/workspace/lucene-solr/solr> ant server",
      "You’ll notice that ant example is deprecated. This was the old way of building a default Jetty infrastructure. Now Solr is just a binary that gets run without worrying about Jetty containers.",
      "Run Solr with local configs",
      "Now under solr, you should have a ./bin directory. This is where the new Solr executable lives. Run ./bin/solr --help for information on all the command line options.",
      "For purely local, non SolrCloud configurations, you’ll just want to run this command pointed at a specific Solr home directory. A directory with a solr.xml at its root, and various collections underneath. To do that, simply pass the -s command with a directory as such:",
      "$~/workspace/lucene-solr/solr> ./bin/solr -f -s /home/doug/workspace/statedecoded/solr_home/",
      "(-f runs in the foreground, not as a daemon)",
      "You’ll quickly hit errors as many deprecated features in your Solr 4 configs are no longer supported. So go mucking headlong into your solr.xml, schema, and solrconfig to clean those up.",
      "Loading Source in IntelliJ",
      "Getting Intellij to work is fairly straightforward. At the root Lucene directory, generate the IntelliJ project files using an ant task:",
      "$~/workspace/lucene-solr/> ant idea",
      "Open IntelliJ, and go to File -> Open. Browse to the lucene-solr directory. The directory should have an Intellij icon, indicating the project files exist.",
      "Once opened, be sure to go to File -> Project Structure and set Java 8 as the default SDK.",
      "Debugging Solr in Intellij",
      "The new /bin/solr lets you pass an -a argument to append Java arguments to Solr. You can use this to add remote debugging configuration like in this example:",
      "$~/workspace/lucene-solr/solr>./bin/solr -f -a \"-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=4044\" -s /home/doug/workspace/statedecoded/solr_home/",
      "Run this command. It will pause waiting for a debugger to attach (suspend=y). In IntelliJ, create a debug configuration in Run -> Edit Configurations. Hit the + button and select Remote. Give this configuration a name, and set the port number to the port number passed in the address command above (here it’d be 4044).",
      "Now start the debugging session, and viola! You should be able to set breakpoints and work in the Solr/Lucene codebase.",
      "Hope this little tutorial was helpful, please get in touch if you see anything that could be improved."
    ],
    "summary_t": ""
  },
  {
    "id": "7c71fab6cd6fca4f5df7f4d649f8b7bd",
    "url_s": "https://opensourceconnections.com/blog/2015/05/15/relevance-data-modeling/",
    "title": "Data Modeling For Search Relevance -- Signals and Semantics",
    "content": [
      "Mold your fields to capture their searchable essence",
      "Perhaps the biggest relevance mistake you can make is to take content, straight from it’s source, and plop it directly into Elasticsearch or Solr. Trying to search fields that directly reflect attributes from your data’s source indexed with default settings won’t give you great results.",
      "Why is this? Unmodified relevance scores from field searches likely don’t correspond to meaningful business or domain ranking criteria. You need to get at criteria like \"does a searched for actor star in this movie?\", \"is this restaurant nearby?\", \"is this product well reviewed?\". Yet data from APIs, databases, and filesystems are often subdivided into attributes ill equiped  to answer these questions with default search settings.",
      "You almost always have to manipulate your data to answer these questions with a search engine. Luckily, with open source search, you are in control! You can to craft relevance scores to reflect needed criteria numerically. You control what terms end up in fields to form the basis of a relevance score. You deeply manipulate Lucene to control scoring and searching! When you understand this, you begin to express relevance in terms of quantifiable business criteria instead of surrendering to default scoring and field structure. You see your fields as built to be searched and scored to measure important relevance signals – numbers that correspond meaningfully to valuable criteria such as \"the likeliness the user is searching this type of restaurant\", \"the proximity of the user to the restaurant\", \"how well rated the restaurant is\", or \"how much is the body text about the search terms\" – or whatever your users deem relevant.",
      "What do I mean? Why can’t a search engine just figure out how to rank results by this criteria automatically? Let’s take text fields as an example. Recall the default text relevance score in Solr or Elasticsearch is TF*IDF. This means a strong bias towards fields that mention more of the search terms (TF or term frequency) multiplied by how rare those terms are in the corpus (IDF or inverse document frequency). TF*IDF has proven a reasonable, general-purpose measure of relevance for searching text.",
      "However, the specifics of your use case may mean default TF*IDF is way off the mark for measuring the criteria you need. We saw this when we dug into title search in a previous article. Odd terms like \"who\" turned out to be strikingly rare in title fields. This greatly disrupted the results ranking, causing searches like \"Who is Socrates\" to return search results like \"Who is Plato\" ranked much higher than simple, on-topic, titles like \"Socrates\". The rareness (IDF) of terms like \"who\" wasn’t helpful in creating a signal associated with \"the article’s title describes topics in the user’s search terms\".  Therefore, we had to eliminate many of these strikingly rare terms. This and a number of other measures turned title search from a rather dumb search into a smarter experience closely resembling perusing shelves in a book store to find a good book on \"Socrates\".",
      "In other words, we improved the quality of the information associated with the title field’s relevance score. We turned the relevance score into a signal that more precisely measures when \"the article’s title describes topics in the user’s search terms\" . This is criteria more meaningfully tied to what the user intends, and how our business wants to construct ranking rules.",
      "The process of turning relevance scores into smarter, domain specific signals that quantifiably measure important criteria to you and your data is known as relevance data modeling. When relevance data modeling, we deeply manipulate fields to be scored more precisely to measure criteria. It’s what differentiates high quality ranking solutions from those that don’t really try that hard. This is a key idea in Relevant Search and fundamental to how OSC approaches relevance problems.",
      "When you think about all the ways you can manipulate fields in a search engine, you see how Lucene-based solutions can really enable relevance data modeling to capture important signals. The strength of open source search solutions is how deeply you can manipulate how terms makeup a field and how they’re scored through features such as:",
      "Analysis to control the composition of  terms in an index. Enabling domain-specific synonyms, stopwords, indexing of bigrams, or whatever tokens you can derive from your content!\n  Disabling/enabling scoring features such as fieldNorms (tied to field length), term frequency, idf, etc to control various aspects of text-based relevance\n  copyFields to copy finer-grained fields into a larger field to generate a more general relevance signal, when users don’t care to differentiate between arbitrarily subdivided fields like an article’s \"abstract\" and \"body\"\n  Modifying the scoring or similarity through Lucene plugins to directly control relevance scoring\n  Leveraging value fields such as user ratings, profitability, sales\n  Using geographical information",
      "Indeed, once you realize that fields are NOT storage and retrieval mechanisms, but rather they’re containers to enable scoring, you can begin to use them and the features above to build truly robust and precise relevance signals. You need not take fields from your source system at face value. Instead, you can manipulate them and their scoring to the nth degree to perfect the measurement they deliver at search time. Don’t fall into the trap of NOT mastering the tools at your disposal, instead understand that fields exist to be manipulated – truly modelled – to create a relevance signal that satisfies important ranking criteria to your users. As their precision increases, these signals have meaning – and enable real \"semantic\" search that expresses ranking in terms both the user and business can associate with meaningful information.",
      "This may seem abstract. The examples of relevance data modeling can be found throughout our blog as we reflect on how fields are composed and scored. It’s a skill that permeates our relevance philosophy – to the point that we’re writing a book that features it highly.",
      "If you’d like to get beyond your own basic search solution, to build something more robust. Please contact us! We’d love to help you with your search problems!",
      "Image ‘Clay Puppy’ by Eden, Janine and Jim"
    ],
    "summary_t": "Perhaps the biggest relevance mistake you can make is to take content, straight from it’s source, and plop it directly into Elasticsearch or Solr unmodified...."
  },
  {
    "id": "cdd825db6f4538a592cd8f61b47ff58b",
    "url_s": "https://opensourceconnections.com/blog/2015/05/27/deep-dive-into-elasticsearch-cross-field-search/",
    "title": "Deep Dive into Elasticsearch Cross Field Search",
    "content": [
      "Crossing streams -- problem. Crossing fields? No problem!",
      "Elasticsearch’s cross-fields queries are a great feature. They let you blend multiple fields scores together on a search-term by search-term basis to give you the best score for that term. I covered the motivation for cross-field queries in a previous blog post. In this blog post I want to dive a layer deeper. How exactly do cross field multi_match queries work? How can you tune their scoring behavior?",
      "Why Cross Fields?",
      "Let’s recap why cross fields are so powerful. Their power is heavily related to another common pattern in both Solr and Elasticsearch – custom all fields. Both of the solutions solve similar problems, so an appreciation for both helps you get to the bottom of working with all fields.",
      "What is this \"all fields\" pattern? Let’s examine briefly the problem they solve. Consider a case where academic articles are broken up into fields that reflect the sections in that document: abstract, introduction, methodology, and conclusion sections. What’s might be your starting point is a multi_match query over those fields",
      "GET academia/article/_search\n{ \"query\": {\n     \"multi_match\": {\n        \"query\": \"click tracking\",\n        \"fields\": [\"abstract\", \"introduction\", \"methodology\", \"conclusion\"]\n}}}",
      "This ends up doing a field-by-field search of the whole query string and taking the highest scoring field. This is a multi_match \"best fields\" query. In Lucene query syntax, in expands to these field-by-field (or field-centric) searches:",
      "abstract:(click tracking) | introduction:(click tracking) ...",
      "These field-centric searches fail for a number of reasons. Many of which are written about in this amazing book :-p. However, the most important reason for our purposes is that each of these individual field TF*IDF scores don’t truly reflect criteria users care about. They don’t map to signals users think about when ranking. Its often not the data model users expect to be searching against. They have their own mental model of the text that has little do to with how a parser or your database organizes data.",
      "However, if you can build fields that reflect the user’s mental model of the content’s structure, TF*IDF scores against these fields can correspond to user notions of relevance. One way to do this is to combine fields together into a single scorable unit (an all field). In other words, your users don’t care about a relevance score that measures the signal \"this methodology section is about click tracking\" more likely they want something more general – a signal measuring \"this articles is truly about ‘click tracking’\".",
      "Measuring Term Rareness",
      "Behind the scenes there are mechanical reasons field-centric scores doesn’t measure the more general signal. One reason is an inverse document frequency (IDF – roughly 1 / document frequency) that doesn’t map to users expectations. Remember IDF is intended to map to user’s notions of term rareness, a higher IDF (lower document frequency) means the term is rare. Unfortunately, IDF very often fails to accurately measure rareness as users expect field-by-field. For example, if the term ‘click’ is fairly common throughout the corpus but for some reason rare for the abstract field, you could see surprising results. The user could see articles shoot straight to the top simply due to click matching abstract matching – not because the article is truly about \"click tracking\". To the field-by-field search, click is seen as a rare in the abstract field, thus the abstract:click match is far too much of a special snowflake to be ignored! Unfortunately, users don’t quite see this as important.",
      "This is the nuts-and-bolts motivation for combining fields together into a custom all field. A field that more closely maps to our user’s notion of the structure of the corpus means the IDF for \"click\" will more closely reflect our user’s expectations of \"click\"s rareness. In an all field, \"click\"s idf is computed across a combination of the articles parts, not for individual fileds. For that reason, its not rare to create an all field like all_text below, built up by copying other fields (using Elasticsearch’s copy_to feature):",
      "{\n\"mappings\": {\n   \"article\": {                      \n      \"properties\": {\n         \"all_text\": {\n            \"type\": \"string\",\n            \"analyzer\": \"english\",                                                \n          },\n          \"abstract\": {\n            \"type\": \"string\",\n            \"analyzer\": \"english\",     \n            \"copy_to\": \"all_text\"\n          },\n          \"introduction\": {\n            \"type\": \"string\",\n            \"analyzer\": \"english\",\n            \"copy_to\": \"all_text\"\n          }\n          ...",
      "As it captures all the text, searching all_text more reliably produces meaningful TF*IDF scores that correspond to most of our users simpler view of these documents –  a blob of all the article’s text. Therefore the TF*IDF score is more likely to be more closely atuned to criteria users care about. The score corresponds to the general signal \"this document is about click tracking\" not some combination of per-field TF*IDF scores possibly skewed to terms where one term happens to be particularly rare.",
      "Cross Fields",
      "The downside to this approach is that it involves duplicating every text field in the index. Moreover, its static. You must build it at index time. You can’t decide, ad-hoc, to create an \"all field\" out of only two text fields after the index is built. Cross fields compensates for many of the failings of all search. It allows you search on a term-by-term basis with a blended query that attempts to fix the document frequency math that mangles field-centric searches.",
      "Let’s remember how we issue cross field queries. Recall, they’re a particular type of multi_match query:",
      "GET academia/article/_search\n{ \"query\": {\n     \"multi_match\": {\n        \"query\": \"click tracking\",\n        \"type\": \"cross_fields\",\n        \"fields\": [\"abstract\", \"introduction\", \"methodology\", \"conclusion\"]\n}}}",
      "Which is also explained to us as the following \"parsed\" psuedo-Lucene query:",
      "blended(abstract:click introduction:click ...)   blended(abstract:tracking introduction:tracking ...)",
      "Last time we simply mentioned how cross-fields worked at a high-level. The query calculates a new document frequency (remember IDF = 1/ document frequency) to use when querying. If the document frequency for abstract:click was 7 and the document frequency for introduction:click was 20, then the larger of the two was taken. This way, we side step the issue of low document frequencies for particular fields (like abstract:click above) driving up documents and creating unintuitive search results. Instead, we work with document frequencies closer to the user’s notion of those terms rareness in the entire corpus – the combination of all fields.",
      "There’s a bit more to the story. If you really want to leverage cross fields at the next level down, you’ve got to understand how they work under the hood. The mechanics of cross fields along with the knobs-and-dials available to you can help you leverage this great Elasticsearch feature even further to exactly get the search results you want.",
      "Cross Fields Under The Hood",
      "With all of that context, let’s get into how cross fields work. This is great feature, and you’ll need some context to know how to tune cross field queries to your requirements!",
      "Elasticsearch is open source, and you can see exactly how Crossfields work by examining the source code. Cross fields are implemented as a custom Lucene query known as BlendedTermQuery. So if you’d like to follow along at home, poke around in that file. It’s after all a lot of fun to unwind how these features work! Hopefully I’m not the only crazy person that reads Lucene code on the beach :).",
      "You might remember from previous blog post that custom Lucene queries work by creating custom weights and scorer. These delegate classes in turn allow you to modify the matching and scoring behavior anyway you’d like. BlendedTermQuery is different. Its a facade. Instead, the main action for BlendedTermQuery happens in its rewrite method – a method whereby Lucene asks a query to return an equivalent, possibly more efficient version of itself. BlendedTermQuery takes this opportunity to rewrite itself into a dismax query. If you follow BlendedTermQuery’s rewrite and blend methods, you get the following general steps:",
      "Gather term statistics from every search term\n  Blend together document frequencies between the fields (other global term stats like total term freq are also blended together)\n  Monkey-patch a TermQuery for each term search (ie abstract:click) with the blended doc frequency\n  Return a Dismax query of each term’s field search.",
      "That sounds like a lot of Lucene-ese. The bottom line is instead of acting like an honest query, Blended term query patches itself, turning this query:",
      "GET academia/article/_search\n{ \"query\": {\n     \"multi_match\": {\n        \"query\": \"click tracking\",\n        \"type\": \"cross_fields\",\n        \"fields\": [\"abstract\", \"introduction\", \"methodology\", \"conclusion\"]}}}",
      "into something more like the following psuedo-Query DSL, with a patched document frequency:",
      "\"dis_max\": [\n   {\n     \"term\": {\n       \"abstract\" : \"click\"\n       \"patched_df\": 20\n     }\n   },\n   {\n     \"term\": {\n       \"description\" : \"click\"\n       \"patched_df\": 20\n     }\n   }\n   ...",
      "Dismax also looks like the following in Lucene query syntax (| reflects the dismax behavior)",
      "abstract:click(patchedDF=21) | introduction:click(patchedDF=20)",
      "This means a couple of different things:",
      "First, there’s no real blended term query in the sense that there’s something that iterates the inverted index and scores docs. BlendedTermQuery just transforms itself into a Dismax query over a bunch of term queries. In other words, there’s actually a set of real honest-to-goodness term queries here, with the only difference being that each has a patched document frequency. Other factors for each field like term frequency and norms (bias based on length) continue to play a role in the scoring of each individual term search. Only IDF has just been adjusted to map to user’s notions of how rare a term is over all fields-not just one field.",
      "Second, the monkey patching of document frequency (steps 2 and 3 above) doesn’t actually give every term query the same document frequency. If you examine BlendedTermQuery, instead of assigning each term query the same document frequency, a slight bias is given towards the field with the most mentions of the term. In other words, description gets a document frequency of 20 while abstract gets a 21, as shown in this table:",
      "Field\n      Original DF\n      Patched DF\n    \n  \n  \n    \n      abstract\n      7\n      21\n    \n    \n      description\n      20\n      20",
      "If you notice, the document frequencies have effectively been very very slightly inverted! Now the lower DF for description will bias the resulting score towards that field. Why do this? Well if this term happens more documents in one field, one could argue its more likely that field is generally more relevant to the search term. If you searched for \"Doug Turnbull\" and the name occurred very commonly in a name field, but rarely in a text body field, you may want to bias search results where the search term is more common – NOT where its most rare as document frequency would. \"Doug Turnbull\" clearly belongs in the name bucket and not the other!",
      "Cross Fields Is Just Dismax!",
      "Finally, at the end of the day you need not be intimidated by Cross Fields. Understand that",
      "blended(abstract:click introduction:click ...)",
      "really is just psuedo-code for \"monkey-patch the IDF, then turn it into a big dismax query\":",
      "abstract:click(patchedDF=21) | introduction:click(patchedDF=20)",
      "If you know dismax, you can figure this out! Dismax (the | operator) picks the maximum field score of the underlying queries. So now that document frequency has been monkey patched, TF and field norms are the main determiners of the field score. The result of the dismax operation is likely going to be the shortest field with the most mentions of the term. If it happens to be abstract:click, then that score is taken for the term \"click\".",
      "Just like in other forms of dismax, you can tune the query! You can use tie_breaker to add in scores from the other fields that didn’t win the dismax competition (winner + tie_breaker * (sum of other fields)), such as:",
      "abstract:click + tie_breaker * ( introduction_click ... )",
      "Setting tie_breaker to 1 has the additional effect of turning the query into a boolean query (each clause is an \"OR\"). Additionally, you can use field-boosts to give different fields boosts. For example, we could give abstract a higher boost (5 here), aand set the tie_breaker to 0.5:",
      "GET academia/article/_search\n{ \"query\": {\n       \"multi_match\": {\n            \"query\": \"click tracking\",\n            \"type\": \"cross_fields\",\n            \"fields\": [\"abstract^5\", \"introduction\", \"methodology\", \"conclusion\"]\n            \"tie_breaker\": 0.5,\n}}}",
      "The bottom line is, once you understand that this query is just dismax with a more intuitive document frequency, tuning becomes an exercise in using the tools you already know and love when tuning dismax. You can boost, use a tie_breaker, and use all the other features you know and love.",
      "The result is search with quite a few nice properties. You get the ideal scole for ecah search-term, with consistent per-term document frequency that reflects what users expect, and scoring you can control with tie_breaker and boosts.",
      "For some context, you could compare this to Solr’s (e)dismax query parser. Avid readers of this blog know that Solr’s dismax suffers from winner takes all problems. Solr’s dismax behaves similarly to the above, except that document frequency is NOT monkey patched. Meaning that document frequency has an unduly strong influence on the final score – usually creating odd field \"winners\" in the dismax calculation that don’t jive with what users expect. For example, that match of \"doug turnbull\" in the description field could shoot straight to the top due to low document frequency – despite a name field with many \"dougs\" perhaps being a more appropriatte match.",
      "In other words – I wish Solr would examine Cross Field search and incorporate it as a feature! Looking forward to the cross-fields query parser :)",
      "Well that’s about it for today’s deep dive! As always, get in touch if there’s any way we can help you with your Solr or Elasticsearch problems. We love turning dull search into semantic search!",
      "Cake image by Flickr User poppet with a camera"
    ],
    "summary_t": "Elasticsearch cross-fields are a great feature. They let you blend multiple fields scores together on a search-term by search-term basis. I covered the motiv..."
  },
  {
    "id": "a9b53498b10afc5283b31f737e16c07f",
    "url_s": "https://opensourceconnections.com/blog/2015/06/03/hawt-makes-camel-hotter/",
    "title": "Hawt.io makes Camel hotter",
    "content": [
      "Playing with Hawt",
      "Watch the startup logs for a pop culture reference ;-)",
      "Background",
      "We’ve been using Apache Camel on a number of projects over the past two years, and it’s worked very well for the fairly simplistic use cases that we’ve needed, primarily taking incoming data files and reprocessing them into a format suitable for searching, and then indexing them into Solr.",
      "However, one of the things we’ve noticed is the lack of visibility into what Camel is doing.  We can monitor things by looking at the incoming/processing/done queues, but getting hard numbers on performance etc is hard.  And looking at JMX output is not very fun!",
      "So I’ve been taking a spin around Hawt.io.   This GUI console seems pretty targeted at the enterprise integration market, despite it being positioned as a general purpose management console for your JVM based applications.",
      "My first test",
      "The first test was to see if Hawt would work fine with an already existing Camel process.  In this case, we monitor S3 bucket, and grab the files that land in it, convert them from XML to Java POJO’s, send them to Solr, and then move the completed file back to another S3 \"Done\" bucket.   I took the war file that we had, and dropped it into the same Tomcat hosting Hawt.   Wow!  It worked!   You can see the four different routes that we require loaded in the web app, along with some of the end points:",
      "Here is a visualisation of the steps for the route that populates Solr from a file made up of many XML documents:",
      "",
      "But my world is larger then 1 server!",
      "Ever since the cloud came along, I can’t imagine only having one copy of an application running.   Most of our ingestion processes consist of a pool of servers processing workloads.   So, what happens when I have a couple of Camel processes running?   I started up a separate Tomcat on port 8081, and dropped in an \"agent\" called Jolokia into that Tomcat. I started both Tomcats, and then clicked the \"Connect\" button.   You then pass in the credentials to connect to the other tomcat, called server8081 in the screenshot below:",
      "",
      "After connecting I see the routes for a sample Camel webapp I had deployed on server8081:",
      "",
      "So this is okay, because if I have a cluster of 2 or 3 servers running Camel deployed, I can swap between them.   However, if I need to scale out, then how can I monitor my Camel cluster across all the servers?  I have to go to each one to check on it, and aggregate metrics manually!  Now, I think folks might be saying \"This is what Fabric8 and Kubernetes are all about\", to which I answer… \"sigh…   Yet another layer.\".",
      "Is my Camel process dead?",
      "In one of the projects, we are shipping 250 MB csv files around (and yes, I know it should be zipped!).   The download and upload can take a while, and while that is happening \"nothing\" is going on…   Additionally I was seeing some failures in the upload process, but didn’t have a good sense of what was going on.  However, the \"inflight exchanges\", which in English means \"messages moving around\" gives some great insight to what is going on:",
      "",
      "You can see that the upload process is frequently failing, 24 times so far, but that it does periodically complete, 4 times.  This also gives me a nice sanity check, I expect the ingestFile value to match the uploadDone value, at least, in the success Completed #.  And that despite the number of failures that uploadDone is incurring, the overall systems is limping along.",
      "Other Notes",
      "The Tomcat plugin gives some nice metrics and server load utilization, but nothing that other tools don’t provide as well.",
      "There is some sort of general purpose dashboarding capability that lets you tailor what you see, but I couldn’t quite grok how to get it working.",
      "Final Thoughts…",
      "So Hawt.io, in a trend that I’ve seen in other open source projects, is clearly meant to be just one cog in a much bigger stack that the sponsors of the project, JBoss in this case, want you to adopt..   It so clearly is meant to be a gateway into Fabric8, as much of the capabilities, like the dashboard, I suspect come to life when you use those other projects.",
      "It’s also rather limited in what it supports.  They have clearly gone deep in Camel, ActiveMQ, and some other closely related projects, but there doesn’t seem to be any plugins being developed by folks outside of the immediate Hawt team.  Hawt is also going through a big v1 to v2 upgrade, which may also be delaying broader adoption.",
      "Don’t count on Hawt.io being the broad based administrative console you were hoping for.   Today at least it isn’t quite:",
      "a modular web console for managing your Java stuff",
      "However, since Camel’s existing Web Console isn’t very good, and is now deprecated, then that iself is a huge argument in favor of Hawt.io for monitoring your Camel setup."
    ],
    "summary_t": "Apache Camel is very powerful, but once you have a couple of routes, keeping track of what they are doing gets to be harder.  Plus, you want to know what mes..."
  },
  {
    "id": "cfb3602901dd78755db04fecbc273181",
    "url_s": "https://opensourceconnections.com/blog/2007/04/26/parsing-ruby-for-porting/",
    "title": "Parsing Ruby for Porting",
    "content": [
      "Ive recently started a research project to port a Ruby workflow engine (OpenWFEru) to Python. I thought that if I could write a basic Ruby parser I could at least form the skeleton of my project (since package, module, and class notations are similar between the languages, and mapping the Abstract Syntax Tree to actual code would be a breeze.)",
      "I did a little Googling to find a Ruby grammar that I could work from, and there seemed to be a little activity around using Antlr. Antlr does support generating Python parsers, so I downloaded it and the Ruby grammar and gave it a shot.",
      "When I ran Antlr against the grammar I got a bunch of warnings about lexical non-determinism. That didnt surprise me given Rubys complexity, but a parser and a lexer was generated and I hoped that it would suffice. However, my hopes were immediately dashed when I fed actual Ruby code into the lexer:",
      "error: exception caught while lexing: unexpected char: `#",
      "It didnt understand comments!\nI really only wanted to spend a little time on this portion of the project (whats commonly referred to as a \"spike\") so thats as far as I took it. The Antlr docs spoke of comment ambiguity in the Python parsers, but I didnt have time to dig deeper.\nAlong the way I Googled to see how the Python community handles its parser generator needs, and ran across this excellent analysis. YAPPS seemed to be the clear winner in that case, but in order to use a different parser I would probably need to translate the only Ruby grammar I found into a different syntax.",
      "So for now Ill be using the parser generator between my ears and lexing with my eyes. If anyones had better success at parsing Ruby, please leave a comment below."
    ],
    "summary_t": ""
  },
  {
    "id": "64714be04f72d7c04ed25bc19d14a77b",
    "url_s": "https://opensourceconnections.com/blog/2015/06/16/joe-lawson-joins-osc/",
    "title": "Joe Lawson Joins OpenSource Connections",
    "content": [
      "Joining OpenSource Connections",
      "Hello world! My name is Joseph (Joe) Lawson and I’m the newest team member to OpenSource Connections (OSC). If you’re unfamiliar, people don’t join OSC just for another job and it’s no different for me. My passion lies in making people’s computing experience as pain-free as possible, sharing information and making that information more accessible. OSC is the go-to company for information retrieval via search. By drawing upon my experience in making efficient data systems and combining it with OSC’s expertise in search I will be able to make a greater impact on people’s lives than if I was working behind the scenes at one company.",
      "Experience",
      "I have practiced system and software engineering for over ten years. My projects have ranged from small specialized government web applications catering to individual needs on up to large sites with million dollar stakeholders that take on tens of millions of visitors each month. While delivering reliable systems for my colleagues and clients I’ve also engaged in \"getting it done\".  This practice ranged from chasing down obscure show stopper bugs (and fixing them) to enabling multiple production deploys a day with a robust software development lifecycle centered around continuous integration. My goal is to make your project better. I’m not shy about asking questions to figure out the right direction or choice of directions for each client.",
      "OSC is search.",
      "That’s right and I love search! I believe that everyone can benefit from finding the right data faster and want to help everyone experience the joy of a properly tuned machine and pipeline. OSC lets me learn the breadth of the greatest search engines out there and help others run their search systems reliably.",
      "I’ve designed and run multi-staged Elasticsearch clusters on Amazon Web Services which ingested at peak tens of thousands records per second and accumulated over a billion records each month. While designing this dynamically scaling logfile ingestion platform, I wrote the third party Logstash plugin logstash-kafka which was eventually included in Logstash 1.5.0.",
      "Anything else?",
      "I have contributed in multiple languages to a multitude of open source software projects such as gor, Moto, Jenkins’s Amazon EC2 Plugin, Logstash and many more. Some highlights of recent contributions include:",
      "PR 116 allows gor’s HTTP worker queue to scale up and down dynamically eliminating replay throughput bottlenecks.\n  PR 235 implements in Moto some of the more advanced features of CloudFormation template processing such as the Fn::GetAtt command which can get attributes of any other resource created in the template.\n  PR 224 implemented Amazon Web Services’ tagging API into Moto. All of it.",
      "I know scalable applications and how to contribute to software projects in impactful ways and am looking forward to unleashing this knowledge for you at OpenSource Connections! Please feel free to get in touch to discuss how I can help you."
    ],
    "summary_t": "Joe Lawson is an experienced DevOps hacker and the newest member of OpenSource Connections. Find out who is this guy and how can he help you?"
  },
  {
    "id": "5acd88ff6513aadff5b1e487b9ca2d54",
    "url_s": "https://opensourceconnections.com/blog/2015/06/17/biosolr-stop-reinventing-the-wheel-in-life-sciences-search/",
    "title": "BioSolr -- Stop Reinventing The Wheel in Life Science Search",
    "content": [
      "I was privileged to be hosted in Cambridge UK by our buddies at Flax two weeks ago. During my time over in the UK, I got a chance to see some of the interesting work they’re doing. I was also very grateful to get to speak to the London Lucene/Solr Meetup about Search Relevancy, Quepid, and my book Relevant Search.",
      "One project that caught my eye in particular was BioSolr. BioSolr is being developed by Flax in conjunction with the European Bioinformatics Institute (EBI). We’ve done a great deal of work in life sciences with search, and we frequently find organizations solving the same sorts of problems over and over and over. For this reason, I was really excited to compare notes with Flax and EBI about common themes and challenges encountered in life science search. What Flax and EBI are hoping to do is implement a set of features that can be hopefully integrated back into the Solr mainline that help enable many of life science search use cases out there. And here in the US at OSC we’re looking to find ways to support their efforts!",
      "One topic of particular interest to me is integrating external ranking signals seamlessly with normal relevancy ranking (Solr’s XJOIN). Organizations face this problem in many ways. For life sciences, this could be anything from specific software to rank chemical structures and proteins, to information that might require real-time image similarity. The point is it can be hard to integrate every important content feature as part of an search index. So if you can’t bend a search engine to do the ranking you want, why not rely on systems that might do this work more precisely to get specific signals, and integrate those with the search engines other ranking abilities?",
      "Another place where XJOIN could be interesting is integrating recommendations and search. Everyone (inside and outside of life sciences) is trying to figure out how to integrate user signals with content-based relevance. You don’t need a big proprietary platform to do this! You can have one system that can handle recommendations–systems explicitly built to recommend content to users based on user behavior– and another system explicitly built for traditional search. The two can be used in tandem, with a search engine ultimately serving up the results with carefully tuned ranking that balances features of the content and signals about what users like.",
      "Another topic that I hear over and over in life sciences search is managing ontologies and taxonomies. Life Sciences is full of taxonomies such as NIH’s MeSH that describe a large catalog of medical concepts. BioSolr is working to find useful ways of integrating Solr and taxonomies and ontologies. I had a long conversation with EBI and Flax’s team about some tricks for doing similarities between taxonomies. There’s a lot of fascinating work to do to find the best way of incorporating these aids into search. I’m glad Flax and EBI are working on getting this shared problem to share a common solution.",
      "I’m eager to keep up with the BioSolr project and peck out features I think will be useful for our clients. But you should check it out! They need your support and momentum to continue to accumulate life science inspired search features for eventual inclusion in the Solr mainline. Right now the project is just a bunch of plugins – but with your help it can be the open source project where we work together these problems instead of siloed into our own organizations!",
      "If BioSolr interests you, and you’d like Solr consultation to see if it can apply to your use case do feel free to contact us in the US or Flax in Europe! There’s great work to leverage here – all us smart search folks CAN work together to stop reinventing the life sciences search wheel!"
    ],
    "summary_t": "BioSolr is being developed by Flax in conjunction with the European BioInformatics Institute. We’ve done a great deal of work in life sciences with search, a..."
  },
  {
    "id": "9519c45b7e6e03f7443d1fad7912e9fb",
    "url_s": "https://opensourceconnections.com/blog/2015/06/21/what-should-I-build-my-search-application-in/",
    "title": "Which JavaScript framework should I build my search application in?",
    "content": [
      "People are always asking me \"What should I build my rich Search interface in?\", and right now it’s pretty easy, it’s either Angular or EmberJS if you want to go by market share.  I lean towards EmberJS if you are building a single monolithic rich application and want all the power, and Angular if I think I am going to be pulling in other libraries, integrating into an existing application, and/or come from a jQuery background.",
      "However Dojo was once the answer!  Dojo Toolkit was one of the first MVC based JavaScript library, back in 2005.  It established rapid dominance and interest between 2007 and 2009, as you can see from this chart from Google Trends:",
      "",
      "However, as you can see post 2009, interest in Dojo fell off as newer contenders, primarily Angular and EmberJS, built on the strengths of Dojo and dealt with some of it’s weaknesses.   This can be seen in this trend comparing Dojo with the two most popular contenders for MVC based Javascript frameworks: Angular and EmberJS.",
      "",
      "Today, most Front End Engineers learn AngularJS or Ember, as those are the most widely deployed and general purpose frameworks.",
      "Other frameworks, such as Dojo (still!) and Sencha Touch are much more specific in the use cases they target.  Dojo for example is really applicable to only the most complex web applications, and Sencha Touch targets mobile device centric web applications.   Therefore, they have many fewer developers who know them.",
      "However, the good news is that Dojo, Angular, and EmberJS are all MVC based frameworks.    Developers who understand the constructs behind Model, View, Controller, and how they fit together into a single framework are able to move between these frameworks much quicker, than say a developer who has only worked with a pure library like jQuery.   And all good front end engineers expect to move among these frameworks, and learn new ones.",
      "Indeed, right now there is a robust argument that Angular 1.0, which is the dominant solution, is about to lose its place to EmberJS due to the radical differences between AngularJS 1.0 and 2.0 putting off many front end developers.   Which means that the title of \"most popular framework\" that Angular took from Dojo, may soon be taken by EmberJS!"
    ],
    "summary_t": "Angular or EmberJS.   Not long ago, the answer was Dojo!"
  },
  {
    "id": "852ed31b2fda83743809a5cf817d840b",
    "url_s": "https://opensourceconnections.com/blog/2015/06/25/why-cassandra/",
    "title": "Why Cassandra?",
    "content": [
      "On Tuesday, I had the opportunity of attending Cassandra Day in DC, a one-day conference put on by Datastax.",
      "As an application developer and designer, my role is building applications on top of search technologies like Elasticsearch and Solr. Going into the day my impression was that the big data space which Cassandra inhabits is pretty saturated with competitors that all use nearly identical buzzwords and claims. Being relatively unfamiliar with Cassandra, my main objective was to leave with an answer to the question \"What makes Cassandra unique?\"",
      "In case you are in the same boat, I wanted to share a few of my takeaways from the talks that I attended.",
      "So what makes Cassandra different?",
      "Cassandra is a distributed datastore, with a built-in coordinator. This means that requests are intelligently forwarded to the correct node.\n  \n  \n    It is generally very fast, and especially shines with write heavy workflows.\n  \n  \n    It scales linearly. If you double the nodes, you’ll double your throughput.\n  \n  \n    Embraces eventual consistency.\n\n    \n      \n        You can manage the speed versus stability trade offs by setting the consistency levels of your data. Two popular strategies are:\n\n        \n          \n            One - The transaction is acknowledged as written when it is added to the commit log and one replica.\n          \n          \n            Quorum - The data must be written to a quorum (51%) of the replicas before it is acknowledged.\n          \n        \n      \n      \n        In general, Cassandra uses a last-in wins strategy when there conflicts between two nodes try to write to the same row.\n      \n    \n  \n  \n    Masterless replication across data centers means that your data is always accessible.",
      "",
      "A few use cases for Cassandra:",
      "Transactional data such as user data\n  \n  \n    Fraud detection\n  \n  \n    Recommendation data\n  \n  \n    Log data",
      "If you want to add analytics to your data you can either augment the data or use Cassandra as your source of truth for another system, such as Solr or Elasticsearch.",
      "By adding Spark to your stack you add batch and streaming processing to the data in Cassandra\n  \n  \n    Solr is available as part of DSE. Data is replicated from your Cassandra cluster into a Lucene index. This may be queried directly via HTTP or through Cassandra’s CQL language.",
      "",
      "Another point that came up is that while Cassandra is usually touted in relation to big data, it is just as equally qualified to be used with smaller data sets. The distributed nature and great write speeds are just as important when you are dealing with gigabytes of data as they are with petabytes of data. Chris Bradford has demonstrated this on an almost cartoonish scale by building a fully functional Cassandra cluster on a pair of Intel Edisons. Your data needs are probably somewhere in between that and Netflix, but the point still applies.",
      "Overall, the event was a good opportunity to connect with people in the data community and learn about Cassandra. If you’re interested in learning more you might want to check out Planet Cassandra or attend one of the free webinars from Datastax.",
      "I am looking forward to leveraging Cassandra on one of my future projects.",
      "– @danielbeach, with input from @bradfordcp"
    ],
    "summary_t": "Takeaways from Cassandra Day DC"
  },
  {
    "id": "2576f0e22620772aae594ff55b31e4b0",
    "url_s": "https://opensourceconnections.com/blog/2015/06/29/improving-on-git-flow-code-reviews/",
    "title": "Improving on Git Flow: Code Reviews",
    "content": [
      "Something amazing happened today on our Quepid project. We did a code review.",
      "Not some formal thing whereby we sat together in a conference room with someone presenting hundreds of changes trying to get them approved as we all silently nodded. We of course, don’t work that way. We prefer to use git flow. Developers on our distributed team work on issues or features in a branch, issuing a pull request once they are satisfied with their changes.",
      "For a few months, I’d become the Linus Torvalds of this whole process for Quepid. As pull requests came in, I peered over them from on high. I wanted to understand everything. So I dug into these pull requests with great enthusiasm commenting freely as I went along. Commenting too when I saw something from fellow developers have confused me or left me concerned. Not one to hide my opinion, I spoke every time I saw something.",
      "The Internet absolutely sucks for anything that even has the hint of possible conflict. So even though I’m far nicer than Linus I still have my tastes, opinions, and a strong sense of ownership over Quepid. Commenting on other’s work with curt comments on the Internet is a magnet for conflict. It became easy for others to take my thoughts more personally than I expected. At one point Eric Pugh commented that there seemed to be a lot of conflict in the pull requests. I thought – I don’t think so! We’re just having a frank conversation about this code!",
      "Yet fellow developers would joke – \"sheesh don’t give Doug those short pull requests. He really digs in! He’s so picky!\" Moreover, I was often increasingly holding back on my thoughts. Minor suggestions without any particular action item, stray observations, or purely educational points were being left out. My comments too sometimes came to a lack of understanding. Simply perusing the code in the pull request left me with questions and didn’t help me truly understand the intricacies of the problem solved. So many times my questions, complaints, and concerns were out of ignorance.",
      "Today, I decided to do something different to take the conflict out of the conversation and help me really understand the problems being solved. Something I’d done dozens of times in non-distributed teams that didn’t use git flow. Receiving a pull request, I had the completely unoriginal idea to ask my colleague – \"do you want to just jump on a 15 minute hangout to go over this PR?\". In other words \"do a code review!?!\"",
      "What followed was by orders of magnitude both the best code review and best pull request review I’ve ever done. In previous lives, code reviews were giant endeavours that happened far too late to get value. Pull requests were just the right size for a good review. Yet pull requests are a thing to talk about on the Internet – essentially a solicitation for criticism. Its easy even on a distributed team where everyone knows each other to misinterpret critique for annoyed criticism. By chatting in person, the team understands that my questions come out of appreciation for their work and out of our shared sense of care for the product – not a \"get off my lawn\" tone that can often be heard when reading comments from the Internet.",
      "During our code review we walked through the code. We TALKED about the code. Which means we were both much more engaged than the scanning that fills in for most pull request reviews. We BOTH felt like we owned the code, including any problems. Our mutual understanding for the work done and the code landscape ran deep. During this time we together managed to:",
      "Discover 2 minor bugs in the implementation\n  Identify patterns I liked in their code, encouraging them in the future\n  Demonstrate to the developer an important Angular testing technique which will be useful to them in the future\n  Teach me more about the considerations that they thought about when implementing the pull request – giving me a deeper appreciation for the code and problem\n  Make longer term plans for code outside the current issue, including our preferences around things like database model objects and controller responsibilities, laying the groundwork for future similar work",
      "As we talked, we commented in the pull request with takeaways from our conversation. Instead of comments being the start of a conversation predicated on conflict, they were the result of two minds meeting from different points of view at a shared solution.",
      "In other words, pull requests by themselves are a barely acceptable code review gruel. Pull requests without in person code review might even be a bad smell for me from now on. Reviewing just the pull requests can create bad feelings very easily. They don’t work particularly well either. Code reviews done in person trigger numerous important conversations that don’t happen without being engaged in code. More importantly, when there is an area for conflict or a possible bug, you can work on it together instead of throwing the problem back-and-forth over the fence possibly blaming and complaining.",
      "Code reviews puts the code ownership in the hands of the team. Pull requests can be an invitation for the blame game. I can’t say I’ll do a code review on every pull requests, but I certainly will for anything non-trivial."
    ],
    "summary_t": "Something amazing happened today on our Quepid project. We did a code review. Instead of trying to extract value from reviewing pull requests in isolation, w..."
  },
  {
    "id": "41ba428203d3ae6baddeb5b538f277af",
    "url_s": "https://opensourceconnections.com/blog/2007/04/28/citcon-2007-code-metrics/",
    "title": "CITcon 2007: Code Metrics",
    "content": [
      "Notes from the Code Metrics session:",
      "What kind of code metrics do you collect?",
      "Complexity\n  Coverage\n  Dependencies\n  Documentation\n  Code Style\n  \n    of unit tests, line of code\n  \n  Code Duplicates\n  Volatility\n  Velocity",
      "Great first metric is volatility or churn: the number of commits in a specific area contracted to complexity. Volatility comes from version control data, complexity is produced by a script. Gives you a solid feel of where potential bugs are, typically where there are lots of commits in a complex area as measured by your complexity tool.",
      "\"Cyclomatic Complexity\" apparently has lots of research behind it proving that high complexity leads to bugs, and that this is really good spot to look at in reducing code.",
      "Metrics are much more valuable when you have them in a historical context. Hackystat (a favored tool of mine!) gets a plug. Sabre has developed a home grown system that parses reports and adds them to Mysql… They could use HackyStat for similar function.",
      "Demo/introduction of Panopticode. Visualizes metrics with the goal of making the use of metrics simple and ubiquitous.",
      "I think Ill be looking very closely at Panopticode, and how visualizations can help make metrics more relevant to management and teams, versus just some charts that are never acted on."
    ],
    "summary_t": "Notes from the Code Metrics session: What kind of code metrics do you collect? * Complexity * Coverage * Dependencies * Documentation * Code Style * # of uni..."
  },
  {
    "id": "9ad2ed8ea657cbe6ccb63d64f44efa20",
    "url_s": "https://opensourceconnections.com/blog/2015/07/02/log-every-solr-doc/",
    "title": "Log Every Document Added To Solr",
    "content": [
      "I often want to intercept the complete Solr updates sent to Solr in a format I can use offline. Clients have complex ingestion systems. I shouldn’t need to have the full ingestion apparatus to do some Solr work. With documents offline, I can script something simple and stupid that throws documents at Solr to test my search relevancy work without having the full system at hand to populate Solr.",
      "Add Update Processor Chain to Solr Config",
      "Solr lets you hook in some code to run prior to indexing documents. The list of hooks run is known as the UpdateRequestProcessorChain. This code can of course be Java. For simpler tasks, however, you can simply use one of the Java scripting engines available to Solr. In the update chain, you can do all sorts of crazy things to Solr documents. Here we’ll simply set up some logging through some Javascript scripting to capture the full contents of every document added. To do that, first lets walk through configuring an update processor chain.",
      "First, setup the update handler to use a specific update chain, add the update.chain property as so:",
      "<requestHandler name=\"/update\">\n  <lst name=\"defaults\">\n     <str name=\"update.chain\">script</str>\n  </lst>\n</requestHandler>",
      "You need to actually define \"script\". Update chains can have a bunch of steps in processor blocks, here we’ll just add one step at the very beginning of the update chain, pointing it at a Javascript file log-solr-docs.js.",
      "<updateRequestProcessorChain name=\"script\">\n  <processor class=\"solr.StatelessScriptUpdateProcessorFactory\">\n    <str name=\"script\">log-solr-docs.js</str>\n    <str name=\"engine\">js</str>\n  </processor>\n  <processor class=\"solr.RunUpdateProcessorFactory\" />\n</updateRequestProcessorChain>",
      "As an aside, you may have seen Solr’s LogUpdateProcessor in this context. Important to note that LogUpdateProcessor simply logs an id. Here we’re after the full doc!",
      "Add Javascript Logging",
      "So now we simply need to define log-solr-docs.js to log some Solr docs! This file expects several callbacks (all apparently mandatory, hence the empty functions). Below is the Javascript code you’ll need. Place this code in log-solr-docs.js in the same directory as your solr config.",
      "Examining the code, you can see it simply iterates through the Solr document, building a corresponding Javascript object (jDoc). Finally, we log the JSON object to the log.",
      "function logDoc(doc) {\n    if (logger.isTraceEnabled()) {\n            var fieldNames = doc.getFieldNames()\n            var jDoc={};\n            for (var namesIter = fieldNames.iterator(); namesIter.hasNext();) {\n                var fieldName = namesIter.next();\n                jDoc[fieldName] = []\n                var fieldValues = doc.getFieldValues(fieldName)\n                for (var valIter = fieldValues.iterator(); valIter.hasNext();) {\n                        fieldValue = valIter.next();\n                        jDoc[fieldName].push(String(fieldValue.toString()))\n                }\n            }\n            logger.trace(\"NEWDOC: \" + JSON.stringify(jDoc));\n     }\n}\n\n\nfunction processAdd(cmd) {\n  doc = cmd.solrDoc;  // org.apache.solr.common.SolrInputDocument\n  logDoc(doc);\n}\n\nfunction processDelete(cmd) {\n  // no-op\n}\n\nfunction processMergeIndexes(cmd) {\n  // no-op\n}\n\nfunction processCommit(cmd) {\n  // no-op\n}\n\nfunction processRollback(cmd) {\n  // no-op\n}\n\nfunction finish() {\n  // no-op\n}",
      "Turn on Logging",
      "You’ll notice the code above outputs the JSON string to a logger at the TRACE logging level. The simplest thing to do to turn this on is to set TRACE in the Solr admin UI for the script handler. These settings are temporary, they go away when you restart Solr:",
      "",
      "And now you should start seeing every document logged to Solr! With this in place, now you can quickly identify every solr document being added. Of course for large indices, this become untenable. But for an ad-hoc way to grab all the Solr docs its quite handy!",
      "And of course, if you have a tricky Solr problem, be sure to Contact Us!"
    ],
    "summary_t": "I often want to intercept the Solr docs in a format I can use offline. Clients have complex ingestion systems. I shouldn’t need to have the full ingestion ap..."
  },
  {
    "id": "1531724e7a26a9811bee1edac183cf9c",
    "url_s": "https://opensourceconnections.com/blog/2015/07/06/chapters-4-and-5-now-available-for-relevant-search/",
    "title": "\"Relevant Search\" Chapters 4 and 5 Now Available!",
    "content": [
      "We’re pleased to announce that Chapters 4 and 5 are available for early access for Relevant Search! Please read and give us feedback. This is early access for a reason: we want to hear what you think!",
      "Buy our book Use offer code turnbullmu to get 38% off all formats!",
      "What do we have in store?",
      "Chapter 4 teaches you to carefully control how your text and data is analyzed into tokens. This process is key for building a search index that can be used to intuitively find articles, people, ideas, locations, and just about anything your users hope to find.  You need to learn to shape the resulting tokens that go into building the inverted index precisely use case! This of course applies to text. But more importantly, the chapter teaches that features of just about anything can be expressed to a search engine as searchable tokens. Heck, we did this recently in a pretty naive image search – turning pixels and colors into tokens! Any entity users might be looking for – places, ideas, even melodies – can all be tokenized and searched! Check out the chapter to learn more.",
      "Chapter 5 teaches how to compose searches over multiple search criteria or fields. Often developers turn to Elasticsearch’s multi_match query or Solr’s edismax, listing every field they think is important to search over. But do you truly know the behavior of these queries? Or does it remain mystical to you? In this chapter, we teach you that in order to combine the influence of multiple fields. You’ll need to understanding two key techniques. First you’ll learn how to manipulate relevance scores into becoming true signals. Instead of numbers, you treat them as measurements of specific user or business ranking criteria like \"the user searched for the article’s author\" or \"the restaurant is close by\". Second, you’ll mastering control over ranking function – how multi field queries like dismax and multi_match combine scores together. You’ll see how signals are combined or selected to combine useful ranking signals.",
      "So check out these chapters. We’re eager to hear your feedback! Get in Touch!"
    ],
    "summary_t": ""
  },
  {
    "id": "4f257fba0f92bc8298efb87a9e686fcb",
    "url_s": "https://opensourceconnections.com/blog/2015/07/07/vlds-insights-recap/",
    "title": "Recap of VLDS Insights Conference",
    "content": [
      "VLDS Insights Conference Recap",
      "Yesterday I had the privilege of attending the VLDS Insights conference.   VLDS is the Virginia Longitudinal Data System, which provides researchers and policy makers anonymized data collected from Virginia’s schools and workforce training initiatives.",
      "As we do quite a bit in the analytics space, even in our search engine work, I went because I was curious to learn more about this very large dataset, and how they disseminate this very valuable data set without violating the very valid privacy concerns around school data.   I learned about different approaches used by the researchers working with the data, and how the data was hashed to provide a \"double blind\" data set.",
      "Learning about the VR ROI Project.",
      "I sat in on a fascinating talk about how do you measure the return on investment for some of these large programs.   My first challenge was to figure out what the acronym VR meant!  Turns out VR means Vocational Rehabilitation.",
      "Started in 2010, and was meant to: test applicability of valid methodologically rigorous process for assessing ROI at state agency level.",
      "The speaker also stated that this was:",
      "Not your grand father’s ROI.",
      "Focused on program level ROI, wrapping in all of the costs.   Look at impact of VR from the very beginning, so from the applicant perspective, versus being based just on the people who graduate the program.   Able to provide an ROI on each individual person, not just groups of people.",
      "This translates into having a LOT of depth in your ROI calculations.  For example, when you look at \"Training\", it turns out on a quarterly earning short run, training is bad.  But on a longer term basis, it’s very valuable.  Which makes sense, because while the person is in training, they are not earning as much.  While this might be \"blindingly obvious\" conclusion, for someone embarking on training, they need to plan for reduced income.",
      "Being able to calculate ROI allows you to make a elevator pitch like this:",
      "80% of VR applicants in 2000 earned more as a result of VR services.    For every $1,000 spent by DARS, the average (median) consumer earned $7,1000 more over 10 years.   The top 10% earned $45,100 (or more) over the same period.",
      "This is empirical knowledge about the value of the programs, not just intuitive knowledge.",
      "Using VLDS to Predict 8th Grade Outcomes for Virginia’s Preschoolers",
      "I initially went to this session because my colleague @softwaredoug did a simple Student Dropout Predictor using Elasticsearch and a public data (Source available at http://github.com/o19s/student-dropout-predictor).",
      "I was curious to learn more about what a Research Scientist, working with these large data sets, actually does.  It was interesting hearing, yet again, that much of being a Research Scientist is actually being a Research Data Janitor, since significant effort is put into cleaning up the data.   The older VLDS datasets have lower quality then the newer ones, and the underlying data models have evolved over the years since data was collected.",
      "I did get the feedback that working with the VLDS datasets is relatively cumbersome, and that is potentially by design.  You have to be an accredited researcher.  You need to be fairly specific about what data within the full universe of information that VLDS contains that you want to use in your research.  There doesn’t exist an easy search engine style interface that lets you quickly click around and pick some facets like schools in southwest Virginia and schools in northern Virginia, and then compare information like dropout rates.  Probably because computed data points like \"drop out rate\" don’t exist in VLDS, just the raw underlying data."
    ],
    "summary_t": "Trying to answer hard policy questions like the impact of Pre-K attendance on 8th grade graduation rates?  VLDS is your friend."
  },
  {
    "id": "90ac50136619b75ac478c4bccc479674",
    "url_s": "https://opensourceconnections.com/blog/2015/07/08/visualizing-your-data-with-openlayers/",
    "title": "Visualizing your Location Data with OpenLayers",
    "content": [
      "Do you have location data in your Solr index? Are you making use of it?  A typical thing that comes to mind\nwhen thinking about search results is a list of records with various fields that you paginate through.  If you\nhave location data, why not plot those results on a map for a more interesting visualization of your search results?",
      "OpenLayers",
      "For this example we’re using OpenLayers for the map visualization.  If you have lat/lon data\nin your Solr index, there isn’t much overhead to getting a visualization up and running.  For simplicity’s sake, we’ll\nalso be using the angular-openlayers-directive.  If you’re not using\nAngular, not to worry, you just might need to get your hands a little bit more dirty interfacing directly with the OpenLayers API.",
      "Pointing Things Out",
      "In the interest of brevity, this article makes the assumption you’re already querying Solr and have data available to use (If that’s not the case,\ncheck out the source code linked below for a full fledged example!)  Let’s get started with what the view code looks like using the angular-openlayers-directive.",
      "<openlayers height=\"480px\" width=\"100%\" ol-center=\"center\">\n    <ol-marker \n      ng-repeat=\"marker in markers\"\n      ol-marker-properties=\"marker\">\n    </ol-marker>\n  </openlayers>",
      "So not too shabby. We open with the openlayers tag to create a map using the default OpenStreetMap tile set.  Within that, we’re using\nng-repeat to display all of the marker data we have.  ol-marker-properties tells the marker directive to look in the marker object for data.",
      "What properties does the marker object need?",
      "lat: Latitude\nlon: Longitude\n(optional) label: {\n  message: \"Your message for the marker\",\n  show: true/false\n  showOnMouseOver: true/false\n  showOnMouseClick: true/false\n}",
      "At this point, if you’ve setup a list of markers, you’ll have your data being rendered on the map.",
      "A visualization of Historical markers related to Oregon. (Vaguely outling the Oregon Trail)",
      "Heating Things Up",
      "OpenLayers also provides support for heatmap visualizations.  Unfortunately we can’t get away with the simple data structure we used for the markers.\nTo get heatmaps up and running, we need to pass in location data that OpenLayers can make sense of.  For this example we’re using GeoJSON.",
      "Below is a simple JS function demonstrating how to build out a GeoJSON object given a list of coordinate data:",
      "function toGeoJson(data) {\n      var geojson = {\n        type: 'FeatureCollection',\n        features: []\n      };\n\n      angular.forEach(data, function(value, key) {\n        var coordData = value.coords.split(',');\n        for(var i = 0; i < coordData.length; i++) {\n          coordData[i] = parseFloat(coordData[i]);\n        }\n        coordData.reverse();\n\n        var marker = {\n          type: 'Feature',\n          geometry: {\n            type: 'Point',\n            coordinates: coordData\n          },\n          properties: {\n            name: value.name,\n            description: value.description\n          }\n        };\n\n        geojson.features.push(marker);\n      });\n\n      return geojson;\n    };",
      "Heatmaps require their own layer. In your controller logic prepare the layer as detailed below:",
      "$scope.layer = {\n    opacity: 0.5,\n    type: 'Heatmap',\n    source: {\n      type: 'GeoJSON',\n      radius: 5,\n      geojson: {\n        object: yourGeoJsonData \n      }\n    }\n  };",
      "Once your layer is prepared, you can display it using the following view code:",
      "<openlayers height=\"480px\" width=\"100%\" ol-center=\"center\">\n    <ol-layer ol-layer-properties=\"layer\"></ol-layer>\n  </openlayers>",
      "A heatmap demonstrating the clustering of Civil War historical markers in the US",
      "A Live Example",
      "This was a quick overview of getting your location data rendered in OpenLayers.  If you want to see this code in action, check out HMDB Markers.\nThis project utilizes data from the Historical Marker database to demonstrate pairing search with marker and heatmap views.  The full source is available on GitHub at solr-spatial-viz",
      "Thanks for reading!"
    ],
    "summary_t": "Make the most out of your location data by using OpenLayers to provide a visualization."
  },
  {
    "id": "6b82c3bea8e45fa47f495fa549679e88",
    "url_s": "https://opensourceconnections.com/blog/2015/07/10/copy-git-logs/",
    "title": "Copy & Paste your git status logs like a pro",
    "content": [
      "It’s time to fill out your timesheet, again. You’ve put in a full week of work but remembering everything you’ve accomplished can be difficult when you’re jumping between projects.",
      "What if you could just quickly copy your git commits for the week and be done?",
      "TL;DR",
      "git log --pretty=\"%ad - %s\" --date=short --author=username --since=1.weeks --all --no-merges | pbcopy",
      "Git log to the rescue",
      "Running git log from the command line while within a git directory will give us a history of the commits for our current branch.",
      "Our goal is to get a short list of commit messages that we can paste directly into a status update or time tracking software.",
      "So to narrow down the commit messages to only the commits that we have made we can use the --author=username flag.",
      "Then, to create a list of commmit messages with their authored date we specify a format of --pretty=\"%ad - %s\" --date=short.",
      "We’re getting closer. But it looks like there are a number of merge commits that clutter up the list. We can remove those by adding --no-merges.",
      "To narrow down the list to just the last week of work we’ll specify --since=1.weeks.",
      "And lastly we can copy the whole thing to the clipboard with | pbcopy.",
      "Here are a few variations that I’ve found useful:",
      "alias thisWeek='git log --pretty=\"%ad - %s\" --date=short --author=username --since=1.weeks --all --no-merges | pbcopy'\nalias today='git log --pretty=\"%s.\" --author=username --since=midnight --all --no-merges | pbcopy'\nalias yesterday='git log --pretty=\"%s.\" --author=username --since=3.days --until=yesterday --all --no-merges | pbcopy'",
      "You can add these aliases by editing your ~/.bash_profile file and then updating your terminal by source ~/.bash_profile.",
      "Now all you need to do is type today in the command line, and your statuses will be copied to your clipboard.",
      "–@danielbeach"
    ],
    "summary_t": "What if you could just quickly copy your git commits into your timsheet for the week and be done?"
  },
  {
    "id": "71514a02545f9dfd0c517a7f01bb66ce",
    "url_s": "https://opensourceconnections.com/blog/2015/07/15/quepid-v0.2.0-released/",
    "title": "Introducing Quepid v0.2.0 (Organizations, Custom Scorers, and More!)",
    "content": [
      "A more robust dashboard view, that scales to hundreds of queries per case tested.",
      "The release of Quepid v0.2.0 (July 3, 2015) added several new features as well as enhanced some existing features. The Release Notes below provide a quick look to whet your appetite. Individual posts detailing the how Organizations and Custom Scorers work are coming soon!",
      "Quepid v0.2.0, Release Notes:  New Functionality and Enhancements",
      "NEW!  Organizations",
      "Users may now define an Organization or Organizations for use within Quepid and associate other users as members of those Organizations. A user may then share Cases with the Organization, which are then available to all members of that Organization. A user may also share custom scorers to be used by all members of that Organization.",
      "NEW!   Custom Scorers",
      "Quepid now allows for the creation of Custom Scorers, in addition to the standard Quepid Default Scorer (1-10 scale). Therefore Users can now establish the scoring algorithm that works best for them.  Scorers can be assigned at the Case level (for all queries within that case) or at the Query Level. Custom scorers can also be shared with other users for all members in an Organization.",
      "NEW!  All Cases Dashboard",
      "Quepid now has a Case Dashboard, which displays all Cases for a user (both owned and shared through an Organization). A summary score for all queries within a Case has now been added to the Case Dashboard.",
      "Scoring Enhancements",
      "Quepid now allows a user to score all of the results within a query to the same rating.\n  \n  \n    A user can set a Default Scorer for use with all cases (for users who always want the same scorer).",
      "Single Case Dashboard Enhancements",
      "Total number of results is now available at the top of each query.\n  \n  \n    Query results have a rank number added on the UI.\n  \n  \n    Quepid has been modified to allow easy navigation through a larger number of queries within each case. To allow this we have added pagination through cases.\n  \n  \n    A Quepid user can now move a query with rankings from one case to another.\n  \n  \n    Within a Case you can sort on Name, Score and Errors both Ascending and Descending, in addition to the Manual Sort.\n  \n  \n    The Errors sort now places queries with missing judgements at the top of the errors list.\n  \n  \n    Quepid now allows you to add multiple queries at once to an existing case.  A user can copy a list from a spreadsheet column and paste it in, to easily add multiple at one time.\n  \n  \n    Collapse Multiple Queries within a Case at once with the Collapse All Queries link.",
      "###Other Enhancements",
      "Queries with missing judgements are highlighted by a more visible \"!\".\n  \n  \n    Both the Case Dropdown and User Dropdown now close once a user makes a selection from either menu.\n  \n  \n    In the Developer Settings, Quepid will prepend http to the request handler URL if not specified."
    ],
    "summary_t": "The release of Quepid v0.2.0 (July 3, 2015) added several new features as well as enhanced some existing features. The Release Notes below provide a quick lo..."
  },
  {
    "id": "41027842825a95f118910cafdafd1f62",
    "url_s": "https://opensourceconnections.com/blog/2015/07/24/splainer-a-solr-developers-best-friend/",
    "title": "Splainer: A Solr Developers Best Friend",
    "content": [
      "Do you know about Splainer? It’s our handy-dandy, free and open source tool for developers working with Solr search results. In addition to Quepid, it’s become my favorite go to tool for ad-hoc tweaking a specific Solr query. Let’s face it: nobody likes working with Solr in their browser’s URL bar. It’s a royal pain. Further, when you’re left decoding ugly gobbly gook that looks like this, how can anyone make any progress on search?",
      "<response>\n  <lst name=\"responseHeader\">\n  <int name=\"status\">0</int>\n  <int name=\"QTime\">1</int>\n  <lst name=\"params\">\n    <str name=\"bq\">title:deer</str>\n    <str name=\"q\">catch_line:deer hunt from boat</str>\n    <str name=\"defType\">edismax</str>\n</lst>\n</lst>\n  <result name=\"response\" numFound=\"7379\" start=\"0\">\n  <doc>\n    <str name=\"id\">l_15358</str>\n    <str name=\"catch_line\">Deer enclosures prohibited; exceptions; penalty.</str>\n    <str name=\"text\">",
      "This is a headache! You’d rather paste your Solr URL into some prettier, easy to decode tool that can explain what’s happening to you. In short you’d rather work like whoever this badass ninja is:",
      "",
      "Once you get your Solr URL into Splainer, you can tweak the params sent to Solr to your hearts delight. Splainer eases your search work, letting you tweak and iterate on an ongoing basis. Instead of working painfully in your browser’s URL bar, trying to read the XML tea leaves, you can work in Splainer to get search where it needs to be!",
      "Well I’m here to tell you that Splainer is now better than ever. I want to highlight two handy features we’ve added recently to make your search life much easier.",
      "Explain Other & Compare",
      "One question you always ask yourself with search is \"where did that important document go!?!?\". Why isn’t that document on deer hunting coming up when the user searches for deer hunting!? Why is it scored so much lower than this other odd document? Now Splainer can help you answer those questions through its explain other capabilities:",
      "",
      "Notice how you simply apply a simple field-based search to explore your documents. The explanations are given in terms of the primary query – letting you know why a given document isn’t ranked as highly for the main query. This is a crucial debugging capability, and it sure has saved us a great deal of time!",
      "Share Splain’d Results Through URLs",
      "Did you know that in the state of virginia there’s laws on deer hunting from watercraft? Do you know when bear hunting season is? Hopefully you see what I did there :)! URLs are extremely powerful. With Solr, you can directly share search results simply by copy/pasting URLs to your colleagues. Now you can do the same with Splainer, simply by using the URL in the URL bar:",
      "",
      "This is of course a tremendous collaboration feature. Now you can have conversations such as:",
      "Tim: Did you see the bug with the bear hunting? Check it out here\n\n  Sue: Sure did, I’ll work on solving that\n\n  …Sometime later…\n\n  Sue: Hey Tim, what do you think of this solution\n\n  Tim: Great!",
      "Splainer has enabled so much great search relevance work for us. Along with Quepid, we love developing tremendous tools to help solve tough client search problems. We hope you’ll find great value in Splainer and we hope to here your feedback. And as always, let us know if we can help you with a tough search problem!"
    ],
    "summary_t": "Do you know about Splainer? It’s our handy-dandy, free and open source tool for working with Solr search results. It’s become my favorite go to tool for twea..."
  },
  {
    "id": "3b471da9b7f1467337c3bccfed81cae0",
    "url_s": "https://opensourceconnections.com/blog/2015/07/27/advanced-aws-cli-jmespath-query/",
    "title": "Advanced AWS CLI JMESPath Query Tricks",
    "content": [
      "JMESPath Query in the AWS CLI",
      "Introduction",
      "The Amazon Command Line Interface (AWS CLI) is a great tool for exploring and querying your Amazon Web Services (AWS) infrastructure and AWS provides the AWS Command Line Interface Documentation to give you a good idea of how to use the tool but some of the nuances of the advanced options are left up to the user to discover. This post will focus on the --query command which allows you to filter your command results based on JMESPath query expressions. Using query expressions operations allows operators to skip some, if not all, of the painful exercises of extracting and manipulating the JSON with custom software.",
      "If you haven’t explored it prior, take a moment to familiarize yourself with the methods by which you can Control Command Output from the AWS Command Line Interface. For this article specifically, we are going to focus on the techniques highlighted in the section How to Filter the Output with the --query Option.",
      "Looking up Amazon Machine Images",
      "Nearly everyone using AWS at one time or another has had to find an Amazon Machine Image (AMI) ID and code it into their software or launch an instance using it. The most painful part of using these images is their IDs are very unfriendly for people to remember. Furthermore, the image id changes every time an update is rolled out leaving you with an outdated and perhaps soon to be deleted AMI. Let’s take a look at how the AWS CLI, and a few pipe commands, can filter out the extra info so you find the latest and greatest AMI.",
      "If you just run a simple query to see what AMIs Amazon owns you will be quickly overwhelmed. Below we attempt to see how many AMIs are actually offered by AWS themselves. The default output is json so we switch to --output text which allows IMAGES results to return all on one line. In a moment, when we are trying to select JSON within the results, we will use the default --output json.",
      "$ aws ec2 describe-images --owner amazon --output text | grep -c ami\n977",
      "977 AMIs! Well there is quite the selection. For some introductory filtering just get the last AMI listed.",
      "What if I just wanted the name and AMI id of all the images? AWS CLI has a query toggle (--query) to filter results. Note that --query is a JMESPath expression. Below, in the --query toggle, Images[] is a MultiSelect List. The documentation explains, \" A multi-select-list with N expressions will result in a list of length N\". So we are going to get all Images[] returned owned by AWS. The period (.) between Images[] and [ImageId,Name] signifies that we want JMESPath to perform a SubExpression evaluation. A subexpression takes the result of the expression on the left and then evaluates the expression on the right. So the full query says, we want all Images that have the values ImageId and Name. Let’s see what the first five results of this query results in.",
      "$ aws ec2 describe-images --owner amazon --query 'Images[].[ImageId,Name]' --output text | grep -m5 \"ami-\"\nami-0048c968    .NET Beanstalk Cfn Container v2.0.2.1 on Windows 2012\nami-005daf69    ElasticBeanstalk-Tomcat6-64bit-20110322-2041\nami-0078da69    amzn-ami-pv-2012.03.2.x86_64-s3\nami-00c17768    aws-elasticbeanstalk-amzn-2014.09.0.x86_64-php55-gpu-201409291824\nami-013aca6a    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121\n\n[Errno 32] Broken pipe",
      "So, ignoring the error, our first five results have some elasticbeanstalk AMIs which isn’t too surprising considering that AWS Elastic Beanstalk is one of the original offerings by AWS.",
      "You may be asking yourself, \"Hey Joe just evaluated two expressions, ImageId and Name, but how did he know which to work with?\" Sometimes you have to RTFM. AWS CLI uses botocore and in the botocore documentation the response syntax is given for a botocore ec2 describe_images function call. Each key in the response structure can be evaluated with the SubExpression to get its value. Note that botocore is returning a Pythonic CamelCase form of what the EC2 DescribeImages API XML response returns. You can use the AWS API and botocore API references to figure out what is available and then chop away to what your script may need. Yay!",
      "Just to confirm out suspicions, try querying the tomcat7java6 Elastic Beanstalk AMI.",
      "$ aws ec2 describe-images --image-ids ami-013aca6a\n{\n    \"Images\": [\n        {\n            \"State\": \"available\",\n            \"Architecture\": \"x86_64\",\n            \"OwnerId\": \"102837901569\",\n            \"KernelId\": \"aki-919dcaf8\",\n            \"RootDeviceName\": \"/dev/sda1\",\n            \"RootDeviceType\": \"ebs\",\n            \"CreationDate\": \"2015-06-15T21:35:00.000Z\",\n            \"ImageId\": \"ami-013aca6a\",\n            \"ImageLocation\": \"amazon/aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121\",\n            \"VirtualizationType\": \"paravirtual\",\n            \"Hypervisor\": \"xen\",\n            \"Public\": true,\n            \"BlockDeviceMappings\": [\n                {\n                    \"Ebs\": {\n                        \"Encrypted\": false,\n                        \"VolumeSize\": 8,\n                        \"DeleteOnTermination\": true,\n                        \"SnapshotId\": \"snap-521f631c\",\n                        \"VolumeType\": \"standard\"\n                    },\n                    \"DeviceName\": \"/dev/sda1\"\n                }\n            ],\n            \"ImageType\": \"machine\",\n            \"Name\": \"aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121\",\n            \"ImageOwnerAlias\": \"amazon\"\n        }\n    ]\n}",
      "As you can see, both Name and ImageID are both present. This concludes a quick overview of JMESPath queries for AWS CLI.",
      "Advanced JMESPath Queries",
      "As powerful as the simple JMESPath queries are, there is a lot more expression left in the specification! If you recall earlier, the MultiSelect List can take an optional expression within the bracket. There are a number of \"bracket-specifier\" expressions that allow you to access an array of results. Index enumeration, ie Images[0], returns the result at the specified position within a list. A slice-expression uses a start and end index along with an optional stepping number (the default step value is 1). For example, Images[0:5:2] returns three elements, 0, 2 and 4, within the array between the indexes 0 to 5 and a stepping value of 2. Finally, there are Filter Expressions which allow for the comparison operators ==, !=, <, <=, >, and >=. Filter expressions always start with a question mark (?).",
      "JMESPath also includes Functional Expressions. When combining Filter Expressions with Functional Expressions you really start to feel the power. There are a number of Built-in Functions, all Functional Expressions, which allow you to filter down what you see. These functions generally operate on the various Data Types offered by JMESPath, ie number, string, boolean, array, object, null. Built-in Functions for the string type include contains, ends_with, join, length, reverse, sort, sort_by, and starts_with. You can chain expressions together using Pipe Expressions and Or Expressions. Being that it can take a bit to get how to put these queries together, I wanted to step through some advanced examples. Once you get the hang of them, you can really make efficient scripts and queries for your infrastructure.",
      "Previously we used a grep expression to just get the first five results of a query. Let’s replace that grep limit with a range expression.",
      "$ aws ec2 describe-images --owner amazon --query 'Images[0:5].[ImageId,Name]' --output text\naki-0251b36b    None\naki-0a4aa863    None\naki-12f0127b    None\naki-1a946e73    None\naki-1c669375    vmlinuz-2.6.21.7-2.ec2.v1.3.fc8xen.manifest.xml",
      "Hrm, not quite what was expected. Notice that previously we grepped for the string \"ami-\". If you are curious, the prefix aki refers to an Amazon Kernel Image which all older generation paravirtual (PV) guest machines made use of. When you launched a PV AMI, you were actually choosing a kernel image and a boot disk image. Because these are both disks, AWS stores them all in the images API. If you look where we described ami-013aca6a you can see that it is also associated with \"KernelId\": \"aki-919dcaf8\". You can read more about this topic in the EC2 Documentation under the section Linux AMI Virtualization Types.",
      "Okay, lets try to eliminate the second part of our grep expression using a JMESPath query. The starts_with function looks like a good candidate. So we want to get the first five images whose ImageId starts with ami-. We will use the pipe, |, to chain our Images filters together. Because we need to take the first five that start with ami- we must make [?starts_with(ImageId, ami-) == true] the first filter in the chain.",
      "$ aws ec2 describe-images --owner amazon --query 'Images[?starts_with(ImageId, `ami-`) == `true`]|[0:5].[ImageId,Name]' --output text\nami-0048c968    .NET Beanstalk Cfn Container v2.0.2.1 on Windows 2012\nami-005daf69    ElasticBeanstalk-Tomcat6-64bit-20110322-2041\nami-0078da69    amzn-ami-pv-2012.03.2.x86_64-s3\nami-00c17768    aws-elasticbeanstalk-amzn-2014.09.0.x86_64-php55-gpu-201409291824\nami-013aca6a    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121",
      "As you can see the result is the exact same as the piped expression we did the in introduction but it uses pure JMESPath expressions. Neat!",
      "Okay so we got results coming back that give us AMIs only but what if we want to find all AMIs which start with the name aws-elasticbeanstalk? This should be simple enough, we’ll just query for all images that start with the Name aws-elasticbeanstalk. Right?",
      "$ aws ec2 describe-images --owner amazon --query 'Images[?starts_with(Name, `aws-elasticbeanstalk`) == `true`][0:5].[ImageId,Name]' --output text\n\nIn function starts_with(), invalid type for value: None, expected one of: ['string'], received: \"null\"",
      "Oh no! We’ve run into an edge case with using the starts_with function! starts_with expects only strings to evaluate but it appears that some Name values are null which causes an exception. Because starts_with demands to only get strings we must first find all Images whose names are not equal to null. Comparison Operators and Pipe Expressions to the rescue!",
      "$ aws ec2 describe-images --owner amazon --query 'Images[?Name!=`null`]|[?starts_with(Name, `aws-elasticbeanstalk`) == `true`]|[0:5].[ImageId,Name]' --output text\nami-00c17768    aws-elasticbeanstalk-amzn-2014.09.0.x86_64-php55-gpu-201409291824\nami-013aca6a    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121\nami-033aca68    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-hvm-201506152121\nami-08302e60    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-python26-pv-201505182010\nami-08566660    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-python26-hvm-201504032050",
      "Fantastic! We now can discover all the Elastic Beanstalk AMIs using just JMESPath queries.",
      "What if we just want tomcat7 AMIs? Let’s try out a contains built in function.",
      "$ aws ec2 describe-images --owner amazon --query 'Images[?Name!=`null`]|[?starts_with(Name, `aws-elasticbeanstalk`) == `true`]|[?contains(Name, `tomcat7`) == `true`]|[0:5].[ImageId,Name]' --output text\nami-013aca6a    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121\nami-033aca68    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-hvm-201506152121\nami-143b257c    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201505181913\nami-248dff4c    aws-elasticbeanstalk-amzn-2014.09.1.i386-tomcat7java7-pv-201501140057\nami-34c1775c    aws-elasticbeanstalk-amzn-2014.09.0.x86_64-tomcat7java7-gpu-201409291824",
      "Spot on. Okay we are starting to really get the hang of this. Let’s try one final example which is really to illustrate that sometimes a little Unix pipe command is your friend.",
      "What if you want to retrieve only the latest tomcat7java6-pv AMI? You know there is a CreationDate field but JMESPath doesn’t support date sorting, only string and numbers, bummer. Don’t fret yet! GNU’s sort has your back. Okay, we want to get all the Elastic Beanstalk AMIs which run Tomcat7 and Java 6 on a paravirtual platform but really only want the latest build. Let see how far JMESPath gets us.",
      "$ aws ec2 describe-images --owner amazon --query 'Images[?Name!=`null`]|[?starts_with(Name, `aws-elasticbeanstalk`) == `true`]|[?contains(Name, `tomcat7java6-pv`) == `true`].[CreationDate,ImageId,Name]' --output text\n2015-06-15T21:35:00.000Z        ami-013aca6a    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121\n2015-05-18T19:18:26.000Z        ami-143b257c    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201505181913\n2015-04-20T17:45:22.000Z        ami-58959230    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201504201742\n2015-04-03T20:07:00.000Z        ami-6252620a    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201504032003\n2014-10-15T22:26:52.000Z        ami-721aa21a    aws-elasticbeanstalk-amzn-2014.09.0.x86_64-tomcat7java6-pv-201410152224\n2015-01-27T23:14:12.000Z        ami-a06d29c8    aws-elasticbeanstalk-amzn-2014.09.1.x86_64-tomcat7java6-pv-201501272310\n2015-01-14T01:12:09.000Z        ami-a8f280c0    aws-elasticbeanstalk-amzn-2014.09.1.i386-tomcat7java6-pv-201501140107\n2014-09-29T18:32:37.000Z        ami-bec177d6    aws-elasticbeanstalk-amzn-2014.09.0.x86_64-tomcat7java6-pv-201409291829",
      "Not bad, but it appears that there are both x86_64 and i386 versions. Lets filter to only x86_64 versions and then use GNU’s sort to sort on the first column of results.",
      "$ aws ec2 describe-images --owner amazon --query 'Images[?Name!=`null`]|[?starts_with(Name, `aws-elasticbeanstalk`) == `true`]|[?contains(Name, `x86_64-tomcat7java6-pv`) == `true`].[CreationDate,ImageId,Name]' --output text | sort -k1\n2014-09-29T18:32:37.000Z        ami-bec177d6    aws-elasticbeanstalk-amzn-2014.09.0.x86_64-tomcat7java6-pv-201409291829\n2014-10-15T22:26:52.000Z        ami-721aa21a    aws-elasticbeanstalk-amzn-2014.09.0.x86_64-tomcat7java6-pv-201410152224\n2015-01-27T23:14:12.000Z        ami-a06d29c8    aws-elasticbeanstalk-amzn-2014.09.1.x86_64-tomcat7java6-pv-201501272310\n2015-04-03T20:07:00.000Z        ami-6252620a    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201504032003\n2015-04-20T17:45:22.000Z        ami-58959230    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201504201742\n2015-05-18T19:18:26.000Z        ami-143b257c    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201505181913\n2015-06-15T21:35:00.000Z        ami-013aca6a    aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121",
      "Alright we are getting close to our goal of the latest and greatest AMI! Since sort on dates defaults to ascending order, we know that we can just take the last value and then print out the second column, using GNU’s awk (gawk).",
      "$ aws ec2 describe-images --owner amazon --query 'Images[?Name!=`null`]|[?starts_with(Name, `aws-elasticbeanstalk`) == `true`]|[?contains(Name, `x86_64-tomcat7java6-pv`) == `true`].[CreationDate,ImageId,Name]' --output text | sort -k1 | tail -n1 | gawk '{print $2}'\nami-013aca6a",
      "Fabulous. We maximized our use of JMESPath queries to reduce our resulting set to something very manageable. Lets use one more GNU tool in the Findutils suite called xargs to take that AMI id and make one more ec2 describe-images calls to see all the properties of that resulting AMI.",
      "$ aws ec2 describe-images --owner amazon --query 'Images[?Name!=`null`]|[?starts_with(Name, `aws-elasticbeanstalk`) == `true`]|[?contains(Name, `x86_64-tomcat7java6-pv`) == `true`].[CreationDate,ImageId,Name]' --output text | sort -k1 | tail -n1 | gawk '{print $2}' | xargs aws ec2 describe-images --image-ids \"[email protected]\"\n{\n    \"Images\": [\n        {\n            \"Hypervisor\": \"xen\",\n            \"Public\": true,\n            \"RootDeviceType\": \"ebs\",\n            \"KernelId\": \"aki-919dcaf8\",\n            \"State\": \"available\",\n            \"ImageLocation\": \"amazon/aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121\",\n            \"ImageOwnerAlias\": \"amazon\",\n            \"Name\": \"aws-elasticbeanstalk-amzn-2015.03.0.x86_64-tomcat7java6-pv-201506152121\",\n            \"VirtualizationType\": \"paravirtual\",\n            \"Architecture\": \"x86_64\",\n            \"CreationDate\": \"2015-06-15T21:35:00.000Z\",\n            \"ImageId\": \"ami-013aca6a\",\n            \"BlockDeviceMappings\": [\n                {\n                    \"DeviceName\": \"/dev/sda1\",\n                    \"Ebs\": {\n                        \"Encrypted\": false,\n                        \"DeleteOnTermination\": true,\n                        \"VolumeType\": \"standard\",\n                        \"VolumeSize\": 8,\n                        \"SnapshotId\": \"snap-521f631c\"\n                    }\n                }\n            ],\n            \"RootDeviceName\": \"/dev/sda1\",\n            \"ImageType\": \"machine\",\n            \"OwnerId\": \"102837901569\"\n        }\n    ]\n}",
      "There you have it! The latest and greatest Elastic Beanstalk Tomcat7 with Java6 x86_64 paravirtual AMI without searching on Google. You could make a script which supplies this ID to a CloudFormation template and never have to record AMI ids in your template again. Later on I’ll explore doing AMI lookups via CloudFormation using Lambda but I think this is enough for today! Keep in mind that these scripts are very dependent on naming conventions but AWS and other public AMI owners seem to be fairly consistent in their naming conventions.",
      "Conclusion",
      "We have explored in depth how JMESPath expressions can really help you narrow down what you are looking for from the AWS CLI. You should now have a better context to work with the more advanced features of JMESPath queries using the AWS CLI. I hope you find this exploration of expressions useful. I’m looking forward to writing more about how search and cloud work together. Please feel free to get in touch!"
    ],
    "summary_t": "AWS CLI documentation only covers using JMESPath result queries briefly. Let’s explore how much more you can do."
  },
  {
    "id": "1c53b2915b186a1ee24016ba381bd1fb",
    "url_s": "https://opensourceconnections.com/blog/2007/04/28/citcon-2007-notes/",
    "title": "CITcon 2007 Notes",
    "content": [
      "Im at CITcon (pronounced kitcon) 2007 in Dallas/Ft Worth. There are around 60 people here from all across the United States, including some of the heavy hitters of the Testing/Agile world including Martin Fowler, Chief Scientist of ThoughtWorks, Jason Huggins, inventor of Selenium, and Bret Pettichord, inventor of Watir, plus many more. Its been a great opportunity to meet many other folks that Ive heard of like Andrew Glover, president of Stelligent, who has been one of the key users and writers about DBUnit.",
      "CITcon uses the \"OpenSpaces\" format for running a conference. OpenSpaces encourages adhoc converstation and audience involvment by leaving the schedule and topics up to the particpants. No boring lectures, just lots of group discussion.",
      "Last night we were given the basic schedule of sessions and rooms:",
      "Then we came up with various session ideas. We came up with more sessions then we had time for, so over drinks people discussed the various proposals, and voted with tick marks what sessions interested them. Eventually a schedule solidified through negotiation between folks. My proposal is in \"Green Lantern\" at 3:15 PM, second row, fifth column in the chart!",
      "This morning we were all standing in the hall seeing what the schedule had changed to:"
    ],
    "summary_t": ""
  },
  {
    "id": "8ad8d04927574a82704593f5910a51e0",
    "url_s": "https://opensourceconnections.com/blog/2015/08/25/quepid-gets-organized/",
    "title": "Quepid Gets Organized",
    "content": [
      "Quepid has added Organizations to make it easier to collaboratively solve search relevancy problems with your team!",
      "Let’s say you are working on a particularly troublesome set of queries and you need to collaborate with other search relevancy or subject matter experts at your company. Ideally you’d want them to be able to review the same queries and results that you are reviewing. The problem is you don’t currently have a tool that allows you to easily share your queries and results with multiple members of your group. This is where Organizations in Quepid can help.",
      "##Add an Organization\nThe first step for using Organizations is to define your Organization (or even multiple Organizations!) within Quepid. To get started, click Manage Organizations in the dropdown menu.",
      "Once on the Manage Organizations screen, click the green New + button to define your organization’s name, then click Create to create your Organization.",
      "Give your Organization a name and click Create!",
      "Once you define your Organization, you can share access to specific Users within your organization, letting them share Cases.  Remember, Cases are what group your troublesome Queries, as well as the Scorers you are using.  To add a user, search for them via their email address.",
      "Type the email of the User you want to add",
      "##Share a Case with an Organization:\nOnce a User has been added to an Organization, that user can share Cases within that Organization.  Once a case is shared, any member of that Organization has access to view and rate it. Any member of the Organization can review and score results and queries within a case, just like the owner of that Case. To share a Case with an Organization, click on the share icon for the case and pick the Organization you would like to share it with.",
      "Select the Organization to begin sharing your Case",
      "##Share a Custom Scorer with an Organization\nWhat if your Organization wants to use a custom scorer? For example, you want to score your results using a different scale than the 1 to 10 that Quepid by default uses. By creating a custom scorer, you can  use a different scale, as well as adjust the specific scoring algorithm as well.  Examples of custom scales include a 1-4 scale, using a Fibonnaci sequence. Use your imagination in coming up with custom scoring models!  (Watch our blog for more on Custom Scorers coming soon!)",
      "Once you have created a custom scorer, you can then share it with other people in your Organization. When you do this, everybody will have access to the new Custom Scorer you have created. From the Scorer section of the Advanced Options page, click share on the Scorer you would like to share and pick the Organization you would like to share it with.",
      "We’d love to hear your feedback on how organizing and sharing cases across your organization works! Drop me a line at [email protected] to tell us how this works for you."
    ],
    "summary_t": "Quepid has added Organizations to make it easier to collaboratively solve search relevancy problems with your team!"
  },
  {
    "id": "e86a05d49fcd38c39c6bcd8b8260e12e",
    "url_s": "https://opensourceconnections.com/blog/2015/09/01/quepid-scores-with-custom-scorers/",
    "title": "Quepid Scores with Custom Scorers!",
    "content": [
      "Until recently, the only option for scoring results in Quepid was a simplistic 1 to 10 scale with a standard Quepid scoring algorithm. Now users can create their own scorer to accommodate any scale and create their own scoring algorithm that best fits their situation.",
      "For example, let’s say you want to use a scale for scoring that is based on the Fibonacci Sequence (1, 2, 3, 5, 8, 13). You can now do that!",
      "##Here’s how it works:",
      "###What is a Scorer?\nIn Quepid, a Scorer defines the scale and algorithm that the search relevancy user will use to rank the quality of the results returned by a query. You can view Scorers you own and that are shared with you through Organizations by selecting  Advanced Options from the User menu.",
      "A Scorer gives you javascript to set the algorithm you want to use for scoring, and sets the scale that you will use for scoring.  In the example below, you can see that the scale selected is 1-4, with 1 as the worst in red and 4 as the best in green.  You can also see that the Javascript also allows the query summary score to be calculated to be a percentage based on the scores selected, ranging instead from 1-100.",
      "This is an example of a Short Scale Scorer, where the scale is 1-4 and the query summary is displayed based on a 1-100 scale.",
      "###Creating a new Custom Scorer\nTo create a new Custom Scorer, select Advanced Options from the User menu. Click the Add button to create a new scorer. In the modal box you are given, give your scorer a name and define a scale. The scale will be the one used to rate your results within your queries. You can define a scale using the preset options presented or choose Custom Scale to define your own values.",
      "If you want to customize your scoring algorithm then you can actually modify the Javascript code run by the scorer! Save your scorer and start using it to rate your results.",
      "Define the characteristics of your new custom scorer and then start using it to rate your results!",
      "###Using your Custom Scorer\nNow that you have a custom scorer, you can use it in a couple of different ways:\n####Set a User Default Scorer\nIf this scorer will be the one and only scorer that you want to apply to all of your subsequently added cases and queries, then you can set it to be the User Default Scorer. At the top of the Advanced Options page, select the scorer you want to set as your default from the dropdown list. Once selected, this Scorer will be assigned to all queries and cases added after you have selected your default. All scorers- both owned by you and shared by others with you- are available to you to pick from.",
      "Select the Scorer you want to use as your User Default Scorer and it will be applied to all subsequently created cases and queries.",
      "####Pick a Scorer for an Individual Case or Query\nIf you want to set a specific scorer for an individual case or query you can do that too as well.\nTo set a scorer for a specific Case, click on the select scorer icon on the Case screen. In the modal that pops up, select the scorer you would like to use for that Case. All owned and shared scorers should be available to you to pick from.",
      "Select your scorer at the case level to apply the scorer to all queries within your case.",
      "To pick a scorer for a specific Query, click on the summary rating for the query. In the modal that pops up, select the scorer you would like to use for that query. All owned and shared scorers should be available to you to pick from.",
      "Select your scorer at the query level to apply the scorer to just that one case.",
      "###Sharing Scorers\nCustom scorers can also be shared with other users in an Organization. From the Scorer section of the Advanced Options page, click Share on the Scorer you would like to share and pick the Organization you want to share it with. Once a scorer is shared, other members of your organization can use them the same way you can.",
      "Need help crafting your own custom algorithm?  Want to let us know how you use custom scorers?  Drop me a line at [email protected]"
    ],
    "summary_t": "Starting in the summer of 2015, users can create their own scorer to accommodate any scale of ranking results and make their own custom scoring algorithm tha..."
  },
  {
    "id": "04ab8cc307028b1c1c1513289559c50e",
    "url_s": "https://opensourceconnections.com/blog/2015/09/16/announcing-quepid/",
    "title": "Quepid: Relationship Counseling for You and Your Search Engine",
    "content": [
      "Quepid: the missing ingredient in your relationship with your Solr and Elasticsearch. Just like true love, better search need not be so complicated</a>",
      "When I first got involved in search work, I noticed a fairly shocking shortcoming: improving the quality of search results is an abysmal experience. Despite the fact that search drives the user experience for many apps, improving search is miserable. The actual hands on work to tune Solr & Elasticsearch results is more like mysticism than a robust repeatable engineering practice. Ask a search developer about how to improve the quality of your search results and they’ll start talking about Solr’s edismax, boosts, or Elasticsearch’s Query DSL. Tweak this, boost that, skew this.",
      "Render the right incantations to the search engine and hope all your application’s many search use cases will end up solved. \"Tweak and Hope\" is the only solution you have. Unfortunately \"tweak and hope\" often turns into whack-a-mole as you constantly fix one issue only to find you’ve broken others. Many simply arrive at the conclusion that \"search is too hard\" abandoning this core component of the user experience. Sadly users notice. Sales decline. Customers abandon you for the competition.",
      "I’ve been pushing OpenSource Connections to make it our mission to do better than this. Through my blogging, speaking and writing a book on the topic, the art of improving search results has become my passion. I feel very strongly we can make this a process any developer, merchandiser, or content expert can participate in. It’s not mysticism: it’s engineering!",
      "As a major piece of that mission, we’re proud to announce our Quepid product has graduated beta. What is Quepid? Quepid makes improving your search results accessible and transparent. It allows developers, merchandisers, and user/content experts to participate in the conversation about how search should behave. It turns their feedback into firm metrics that measure search quality, without the voodoo.  Teams iterate in real time on search correctness, instantly seeing the impact of their changes, instead of blindly tuning, guessing, and breaking.",
      "",
      "I believe Quepid is the missing ingredient of Solr and Elasticsearch. I’ve enjoyed seeing how Solr and Elasticsearch have broken into exciting areas beyond search: analytics, recommendations, and even as primary data stores. Yet surprisingly few focus on Solr and Elasticsearch’s core functionality: the actual search ranking. It’s easy to see why: it’s not an easy problem.",
      "But at OSC we don’t shy from hard problems. We hope you’ll join us in our mission of making search better for everyone. We hope you’ll help us show all the search queries in the world some love :)",
      "Image ‘Tokyo Ginza’ by Manish Prabhune"
    ],
    "summary_t": "When I first got involved in search work, I noticed a fairly shocking shortcoming: improving the quality of search results is an abysmal experience. Despite ..."
  },
  {
    "id": "0e658daff3013bdd232e1fefcc2aaedc",
    "url_s": "https://opensourceconnections.com/blog/2015/09/18/the-simple-power-of-elasticsearch-analyzers/",
    "title": "The Simple Power of Elasticsearch Analyzers",
    "content": [
      "No need for analysis paralysis! :)</a> Get Relevant Search today and use offer code turnbullmu for 38% off all formats!",
      "In Chapter 4 of Relevant Search, we talk a LOT about Elasticsearch analyzers.  Without analyzers, your search engine would be a rather unintelligent string comparison system instead of a smart, powerful search engine. Analyzers are the text-processing pipeline that feed the search engine’s core data structures, controlling whether two tokens (basically words) match during a search.",
      "In this article I want to motivate you to build your OWN analyzers. If you can master analyzers, you can take direct control of the seeming intelligence inside Elasticsearch. There’s no code here, rather I want you to see exactly why you would reach for a custom analyzer. It’s something we do in almost every search project, yet we find many developers feel intimidated using them. Let’s explore more philosophically why analyzers give you tremendous, exacting control over the search engine’s ranking behavior.",
      "What are Analyzers? Why Do We Need Them?",
      "At the very core of the search engine is a highly tuned but pretty dumb data structure for matching strings. It’s so exacting and dumb, that it wouldn’t be able to tell CAT and cat are the same. To the data structure inside the search engine, these two strings are unique arrays of UTF-8 characters, not variants of the same word. CAT (\\x43\\x41\\x54) and cat(\\x63\\x61\\x74) don’t match, so end of story: a search for CAT fails to find documents with cat.",
      "Taken from Relevant Search: How documents are transformed into tokens indexed into a search engine",
      "Not so fast say your analyzers :) Analyzers turn strings into normalized tokens before searching/updating that dumb data structure. One step in analysis, for example, could be forcing text to be lowercased. This converts all instances of CAT into cat, causing the two variants to match, thus overriding the unintelligence inherent in the actual data structure.",
      "Other forms of analysis are more language specific. For example stemming is used to convert words to a root form. For English, the search engine’s dumb data structure can’t tell that running and run are the same. Yet with appropriate English stemming, running might be converted to run, allowing them to match. Other languages from German, Chinese, to Icelandic of course have their own unique rules for performing these sorts of operations (and even determining what constitutes a word!).",
      "Analyzers Are Ultimate Power!",
      "Ok big whoop, so just pick the analyzer for your language and move on, right! Not so fast. Analyzers actually are a vital control point in your search solution. By creating your own custom analyzers YOU decide when two terms are equivalent and should match in that dumb data structure. You can convert any snippet of text into SOMETHING that’ll match. Or go the other way – ensure two pieces of text are analyzed so that they definitely won’t match. Both could be valid choices, depending on the application.",
      "As an example, let’s say you’re building search for SciFi fans, and you’d like to show them web pages that discuss characters, ideas, etc from their favorite franchise (Star Trek, Star Wars, etc). Users search for Star Trek and Star Wars, expecting documents relevant to their topic to come back. It’d kind of work. But super fans don’t need to really mention the franchise at all in their discussions. Whole documents being searched read like",
      "Captain Picard is way cooler than Han Solo. Han Solo shot first, sure, but Picard stood down The Borg, Admiral Tomalak, and a whole slew of others.",
      "You search for \"Star Trek AND Star Wars\" and this won’t be found anywhere!",
      "Ah but analysis gives you an opportunity to make terms equivelant. One way you could use an analyzer is to curate a set of synonyms, indicating the relationship between prominent characters and their franchise. Synonyms are another step you can add to the analysis process. For example, here’s our sample synonym file:",
      "Captain Picard => Star Trek\nHan Solo => Star Wars\nNed Stark => Game of Thrones",
      "Ok so next you’d create a custom analyzer with just a synonym step. Now if we take our sentence and run it through analysis:",
      "Captain Picard is way cooler than Han Solo.",
      "this would be transformed into these tokens",
      "[Star Trek] [is] [way] [cooler] [than] [Star Wars]",
      "and of course, if we’re doing stemming, lowercasing, and several other useful steps for English, we could arrive at",
      "[star trek] [is] [way] [cool] [than] [star war]",
      "NOW your search for star trek ought to match something! You can cast a much wider net now. Moreover, as more names, ideas, etc in the document correspond to stuff from the Star Trek franchise, the relevance score for those documents will increase as they’re counted as additional matches on star trek! By making terms equivalent, you’re controlling not just how they match, but how they’re ranked and scored!",
      "That first stab was only the tip of the iceberg too, the number of options for controlling how tokens are emitted are endless. You can inject an entire library of filters and other steps into the process. You can reorder them. Do anything you need to solve your problem. Search isn’t hard – its just programming :).",
      "For example, you might decide to preserve the original text by changing how the synonyms are applied (here | means two tokens share space):",
      "[star trek|captain picard] [is] [way] [cool] [than] [star war|han solo]",
      "or filter out all the terms that DONT match your a list (known as keepwords):",
      "[star trek|captain picard] …  [star war|han solo]",
      "The options are endless! The real takeaway is with analyzers, you needn’t just pick an analyzer for a language. Instead, you can string together different pieces like lego blocks for manipulating text. Analyzers can be constructed of all sorts of different components, including",
      "character filters that control the processing of the string before its converted to tokens\n  a single tokenizer that control how the string is converted into tokens\n  token filters that allow you to manipulate each token (synonyms and lowercasing are token filters).",
      "The point is there’s simple power here that can improve your search results. You can manipulate analyzers and get tremendous gains in better, more relevant search. You can directly understand how matching works, control it, manipulate it precisely to your needs. A simple tweak here can solve a search problem efficiently, leaving you in absolute control.",
      "Better Text! More than Text!",
      "If you want to get a deeper, hands-on appreciation for this analyzer stuff, I really recommend Chapter 4 of Relevant Search. It’s probably one of the better chapters in the book. I didn’t even personally write it, but I have been recommending it to anyone getting started in search and search relevance! And spoiler alert: one of the coolest things we show is you can tokenize FAR MORE than text. Heck look here, I tokenized an image into \"tokens\" that are really RGB values :) Search engines are awesome!",
      "Happy searching, and don’t be shy getting in touch with questions, problems, consulting needs, and the like. Also check out Quepid which we use extensively to try these ideas out!"
    ],
    "summary_t": "In Chapter 4 of Relevant Search, we talk a LOT about Elasticsearch analyzers.  Without analyzers, your search engine would be a rather unintelligent string c..."
  },
  {
    "id": "6dbf2637559a997bba74b6c78084c758",
    "url_s": "https://opensourceconnections.com/blog/2015/09/22/elyzer-step-by-step-elasticsearch-analyzer-debugging/",
    "title": "Elyzer: Step-by-Step Elasticsearch Analyzer Debugging",
    "content": [
      "I love stringing together custom analyzers to solve my search problems. Analyzers control how search and document text are transformed, step-by-step into individual terms for matching. This in turn gives you tremendous low-level control of your relevance.",
      "Yet one thing has always bugged me with Elasticsearch. You can’t inspect the step-by-step behavior of an analyzer very easily. You have the _analyze API, which helps a great deal see the final output of the lengthy analysis process. But you can’t pry into each step to see what’s happening.",
      "For example from our book we have an analyzer that turns text into two-word terms for a very specific kind of matching:",
      "GET http://localhost:9200/tmdb/_analyze?analyzer=english_bigrams\ncaptain picard was cool\n\n{\n  \"tokens\": [\n    {\n      \"position\": 1,\n      \"type\": \"shingle\",\n      \"end_offset\": 14,\n      \"start_offset\": 0,\n      \"token\": \"captain picard\"\n    },\n    {\n      \"position\": 2,\n      \"type\": \"shingle\",\n      \"end_offset\": 18,\n      \"start_offset\": 8,\n      \"token\": \"picard wa\"\n    },\n    {\n      \"position\": 3,\n      \"type\": \"shingle\",\n      \"end_offset\": 23,\n      \"start_offset\": 15,\n      \"token\": \"wa cool\"\n    }\n  ]\n}",
      "This gives you SOME information. But there’s plenty of questions left. Why was was turned into wa? What happened in between? How was the text tokenized? What happened at each step?",
      "Introducing elyzer",
      "Instead of just working with the end result, you’d like to see every step. This is exactly what elyzer does. Instead of the JSON output above, it helps you with friendlier step-by-step debugging output like so:",
      "doug$ elyzer --es http://localhost:9200 --index tmdb --analyzer english_bigrams --text \"captain picard was cool\"\nTOKENIZER: standard\n{1:captain} {2:picard}  {3:was} {4:cool}    \nTOKEN_FILTER: standard\n{1:captain} {2:picard}  {3:was} {4:cool}    \nTOKEN_FILTER: lowercase\n{1:captain} {2:picard}  {3:was} {4:cool}    \nTOKEN_FILTER: porter_stem\n{1:captain} {2:picard}  {3:wa}  {4:cool}    \nTOKEN_FILTER: bigram_filter\n{1:captain picard}  {2:picard wa}   {3:wa cool}",
      "Each {} just shows the token position, a colon, then the token string.",
      "Here I can see, for example, was turned into wa because of the porter_stem filter in my analyzer that is before my bigram_filter.",
      "Being able to debug analyzers makes them much easier for me to work with! I can really control exactly how my text is expressed to the search engine to gain really tight control over how the search engine matches and ranks.",
      "I hope you check out Elyzer and give us feedback! Instructions for installation are up at the github repo. Also, check out our other search projects Quepid and Splainer as well. Don’t hesitate to get in touch to chat about how we can help you with your search problems!"
    ],
    "summary_t": ""
  },
  {
    "id": "423fe892d7e7ffaf6f545e7a12ff9dc3",
    "url_s": "https://opensourceconnections.com/blog/2015/09/24/using-dse-to-run-solr-rdd/",
    "title": "Using DSE to run Solr Spark Jobs",
    "content": [
      "Ever since Tim Potter from LucidWorks published his Spark Solr project, I’ve been wanting to play with it.",
      "This week I was at Cassandra Summit 2015 , and so took the opportunity to try out moving data into and out of Solr using Spark.   Since I’m at Cassandra Summit, I figured I would take the recently released DSE 4.8.0 for a spin as well.   These are my notes from working through the demo that Tim put out.  I recommend for background you read his blog post https://lucidworks.com/blog/solr-spark-sql-datasource/, which is what I followed to get some context.",
      "I first downloaded DSE and used the installer to put it into ~/dse folder.  I started a single DSE Analytics node via ~/dse/bin/dse cassandra -k to enable Spark.",
      "I then cloned the Github project at https://github.com/LucidWorks/spark-solr.",
      "The first gotcha that I ran into was actually setting up the Twitter connection.  It took me a while to figure out that I needed to create an \"app\" on Twitter’s platform in order to have OAuth credentials.   The direct link is https://apps.twitter.com/.  Create an app, and then you can get your user and access tokens.",
      "Next, you need to add a twitter4j.properties file into ./src/main/resources directory of the spark-solr project.  Using the system properties didn’t work for me.",
      "debug=true\noauth.consumerKey=YOUR_CONSUMER_KEY\noauth.consumerSecret=YOUR_CONSUMER_SECRET\noauth.accessToken=YOUR_ACCESS_TOKEN\noauth.accessTokenSecret=YOUR_ACCESS_TOKEN_SECRET",
      "Then I compiled via mvn clean package -DskipTests.  I did try running the tests, and they mostly ran successfully for me!",
      "I already had a SolrCloud cluster running, with ZooKeeper on port 2181, and had a collection1 using the standard default schema ready to go.   The moment of truth had arrived, would the SolrRDD code work with the version of Spark that comes with DSE?  Would all the versions of jar files work out?  I submitted the newly packaged Spark job to DSE:",
      "~/dse/bin/dse spark-submit --master local[2] --verbose --class com.lucidworks.spark.SparkApp ./target/spark-solr-1.0-SNAPSHOT-shaded.jar twitter-to-solr -zkHost localhost:2181 -collection collection1",
      "And it ran great!",
      "Since I am running just a single DSE Spark node running, it ran just a single process, so it’s pretty inefficient.   It took me a bit to figure out that once the job was running, I could go to http://localhost:4040 to monitor the progress of the twitter-to-solr job being run.  The UI for Spark has really improved since last fall when I last did Spark work.",
      "I then triggered a commit on the Solr collection:  http://localhost:8983/solr/collection1/update?commit=true, and then checked the documents: http://localhost:8983/solr/collection1/select?q=:&rows=0.  Yep, tweets are streaming in!",
      "I let the Spark job run for a while, and it seemed like there was no end in sight for downloading data. I wish via the Spark UI there was a way of figuring how many jobs would be run to bring all the data in.  I ended up letting it run for 20 minutes, and got to about 25,000 tweets.",
      "So now I wanted to play with the data.  Here the specific Spark Shell commands diverged from the README since I am using the DSE flavor of Spark.",
      "To start up the Spark Shell I ran:",
      "~/dse/bin/dse spark --jars ./target/spark-solr-1.0-SNAPSHOT-shaded.jar",
      "If you still have the twitter-to-solr job running, then go to http://localhost:4041, and you can see the Spark UI for this spark shell process, otherwise it will be on port 4040.",
      "I won’t go through each command, you can read about each one in the README, however here is the DSE specific version of the commands:",
      "import com.lucidworks.spark.SolrRDD;\nvar solrRDD = new SolrRDD(\"localhost:2181\",\"collection1\");\n\nvar tweetsRDD = solrRDD.query(sc,\"*:*\");\nvar count = tweetsRDD.count();\n\ncsc.setKeyspace(\"tweets\");\nvar tweetsDF = solrRDD.asTempTable(csc, \"*:*\", \"tweets\");\n\ncsc.sql(\"SELECT COUNT(type_s) FROM tweets WHERE type_s='echo'\").show();\n\ntweetsDF.printSchema();",
      "Notice that we have both tweetsRDD and tweetsDF as objects.  A DataFrame is quite different from a RDD, as discussed in the introductory blog post https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html.",
      "Once I had the data pulled into my Spark Shell, I started thinking, hey, how can I get it stored into Cassandra?   I tried for a while till I realized that a DataFrame isn’t what I needed, I needed an RDD.   The first step was to create a keyspace/table combo in Cassandra:",
      "CREATE KEYSPACE tweets\n  WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\nCREATE TABLE tweets.tweets (\n  id text PRIMARY KEY,\n  \"_indexed_at_tdt\" timestamp ,\n  \"_version_\" text,\n  \"accessLevel_i\" int,\n  author_s text,\n  \"createdAt_tdt\" timestamp,\n  \"currentUserRetweetId_l\" int,\n  favorited_b boolean,\n  id_l int,\n  \"inReplyToStatusId_l\" int,\n  \"inReplyToUserId_l\" int,\n  \"possiblySensitive_b\" boolean,\n  provider_s text,\n  \"retweetCount_l\" int,\n  retweet_b boolean,\n  \"retweetedByMe_b\" boolean,\n  source_t text,\n  text_s text,\n  text_t text,\n  truncated_b boolean,\n  type_s text\n);",
      "Okay, time for the moment of truth!",
      "tweetsDF.write.format(\"org.apache.spark.sql.cassandra\").options(Map( \"table\" -> \"tweets\", \"keyspace\" -> \"tweets\")).save;",
      "And then I checked via CQL, and there was my data!",
      "select * from tweets.tweets;",
      "I did try to figure out how to create the Cassandra table from the tweetsDF schema, but that will have to wait till SPARKC-231 is committed.  Until then, you need to manually create the table structure.",
      "Imported into Cassandra from Twitter, via a stop in Solr land.   The typical use case for moving data around is data in  Cassandra -> Solr, however now I can copy data FROM Solr into Cassandra!",
      "Or even better, compare datasets between Solr and other system using Spark.  That will be another post."
    ],
    "summary_t": "Notes from pulling and pushing data in Solr using Spark and DataStax Enterprise"
  },
  {
    "id": "992eaa71c535ccf115d652ce80533f3f",
    "url_s": "https://opensourceconnections.com/blog/2015/09/25/announcing-quepid-for-elasticsearch/",
    "title": "Announcing Quepid For Elasticsearch",
    "content": [
      "Carefully prune the bonsai tree with Quepid for Elasticsearch :)\n  </p>",
      "Good news! We’re proud to announce our test-driven search toolbench Quepid now supports Elasticsearch.",
      "Quepid solves fundamental problems that come with improving search results. Search tweaks are easy, yet getting all your searches returning the right results can be maddeningly difficult. You can never be sure you’re moving forwards or mired in quicksand of constantly breaking search.",
      "Quepid helps by bringing test-driven principles to tuning search results – what we call Test-Driven Relevancy. It helps define what good search results by incorporating your own business expertise from colleagues that know users the best: your merchandisers, domain and content curators. With this intelligence encoded in a tool, it gives the developer the ultimate test-driven tool for improving search transparently in a way the whole team can understand.",
      "For example, you’re in charge of the online retro VHS store Movie’s R Us. You need to be sure that when users type \"Rambo\" they get back the right results, including Rambo I-III and most importantly the classic Rambo movie \"First Blood\". But then requirements change, as they do for all kinds of software. Users come in and type \"Sylvester Stallone,\" expecting to see his films. Suddenly you need to solve a different problem. Can you boost and tweak search and ensure \"First Blood\" still comes back first for \"Rambo\"? Can you measure the quality of the search results as you tune? What happens when you add more search forms? Other search queries? Different use cases?",
      "",
      "While your juggling all these different use cases, you’re effectively engaged in programming work. Yet its a very peculiar kind of work. You’re programming a conversation with your users. The one your app has every day, far more frequently than your in-person conversations! It’s a conversation with your specific user audience, your content, and your own specific business aims. Moreover, it’s a kind of programming where possibly only about 20 lines of \"code\" (queries, analyzers, mappings and the like) have an impact on every use case. Going backwards somewhere is almost guaranteed.",
      "You need tools to make the whole process transparent and measurable to the whole enterprise. Quepid shows the whole team exactly why Elasticsearch thinks certain results are the right area. It measure the impact of many queries simultaneously, showing what’s improving and what’s getting worse. In short, it makes the whole process measurable and transparent. It’s crystal clear exactly where you’re succeeding in meeting customer expectations and where you’re failing – and why!",
      "We hope you’ll give Quepid a spin. Get in touch if we can help you in anyway with Quepid or your tough search problems. We love problems like this one or this one and probably yours as well :) Happy Searching!"
    ],
    "summary_t": "Good news! We’re proud to announce our test-driven search toolbench Quepid now supports Elasticsearch. Quepid helps by bringing test-driven principles to tun..."
  },
  {
    "id": "93e033a57580d87d4446c1ba5b8382b5",
    "url_s": "https://opensourceconnections.com/blog/2006/07/27/oscon-1st-day-exhibit-thoughts/",
    "title": "OSCON 1st day exhibit thoughts",
    "content": [
      "The biggest thing that surprised me was the preponderance of Perl developers at the convention. For a \"dying language,\" Perl seems to be pretty alive and well.",
      "I also expected to see a lot of people who wanted to learn more about Rails and Ruby (is this redundant or what?). Maybe they were all eyeball deep in training.",
      "We did build a 5 minute Rails app to show the winners of our drawing for the book \"Practice of an Agile Developer\" by Andy Hunt. Little did we know that wed have the pleasure of being two booths down from Andy, so its been fun dragging our winners down to Andys booth to have him autograph the book. I wonder if Ill see signed books popping up on eBay. We want 10% of the profits! :)",
      "We did hire two promotional models and a licensed massage therapist to help out at our booth. First off, all three of them are phenomenal! Kudos to Kelsey and Susan (our direct marketing consultants) and Pamlin (our LMT)!",
      "I had expected the flow of people through them to be much different than it wound up being. I thought that we could send Kelsey and Susan out on the floor and stage them at the exits to sessions and have them find people and drag them over to our booth. What they quickly ascertained was that they had no connection to people (after all, if youre looking to exit the session as quickly as possible to either go smoke or connect to the Net, youre not going to want to talk to anyone). From a conventioneers point of view, they were just another set of people to walk by en route to somewhere else. So, they came back, hung around the booth, and did a great job of engaging people and bringing them in to talk to us. Also, they were both incredibly quick studies and can now give our elevator speech better than we can!",
      "We also expected Pamlin to be a hit and for a line to go around the corner for her services. What we have discovered is that people are pretty reticent to have their feet rubbed. If we come back next year, were going to have her bring a backrub chair instead of the footrub recliner. However, Pamlin is also very bright and engaging, and while I doubt that she did massage services for more than 6 people (outside of the OSC team), I doubt there was more than a total of 30 minutes out of the 600 that the exhibit was open that she wasnt talking to someone.",
      "If youre ever going to exhibit in Portland (not for OSCON) and want to hire people who can help you out with your booth, then drop me a line and Ill put you in contact with them. One caveat: you cant have them for OSCON 2007. Theyre ours!",
      "We also conducted a vendor presentation yesterday, which well put online shortly. While I initially felt pretty unahppy about the results, after talking to Eric, I realized that it served its purpose. It was free advertising. Never turn down a chance to give a 20 minute spiel about what you do, even if the audience is pretty small. You never know whos going to hear the few key phrases that they need to hear to perk their ears and ask a few more questions.",
      "Finally, the two conversational topics that I discussed the most are Agile Development and business process analysis. Theres definitely a hunger for good information and training about Agile Development. Additionally, in my discussions, I think that I can safely say that, in general, IT managers have trouble convincing their business counterparts of the business case for changes in software systems. Knowing that a change is necessary is a first step, but being able to frame the discussion in the terms that the business managers understand is critical. A part of me is a little disappointed that the range of discussions wasnt a little wider, as I feel like our real strength is in our distributed development capabilities, namely our ability to take on projects and ramp up and integrate extremely capable teams in a rapid timeframe, but you go where the market demands, and it seems that business process analysis and Agile Development are the main pain points that Im seeing, at least of the attendees Ive spoken with."
    ],
    "summary_t": ""
  },
  {
    "id": "fb487eee234dff5aa75415eba25620cb",
    "url_s": "https://opensourceconnections.com/blog/2007/04/28/citcon-2007-retrospective/",
    "title": "CITcon 2007: Retrospective",
    "content": [
      "Its been an incredible day of meeting amazing people who are doing amazing things. Being with 40 of my peers in the CI field for a day and a half has been wonderfully invigorating.",
      "We had a wonderful setup, with plenty of large rooms and some great snacks:",
      "",
      "The rooms had many rolling office chairs in each, and no tables, which led to ad hoc groupings and a flexible mindset towards the conference:",
      "",
      "We wrapped up with a \"retrospective\" where we shared what we got out of the time, and our thoughts about the conference. A little different from a 5000 person conference like OSCON!",
      "",
      "",
      "Being able to have discussions with people like Martin Fowler over a beer or in a small group environment made this possibly my best conference ever!"
    ],
    "summary_t": ""
  },
  {
    "id": "0f9df9604d2ef3f31cd1f4dc8833052c",
    "url_s": "https://opensourceconnections.com/blog/2015/10/13/CWIT-support-blog/",
    "title": "OSC is Proud to Support Charlottesville Women in Tech (CWiT)!",
    "content": [
      "When the Charlottesville Women in Technology (CWiT) team first contacted Opensource Connections (OSC) about whether or not we wanted to sponsor CWiT for the year, to be honest I knew very little about the details of gender inequality in Science and Technology and even less about ways to help address that inequality. And I felt kind of bad that I didn’t know this information, being a woman in technology myself. I have certainly supported gender equality in other areas in the past, and when we made the decision to sponsor CWiT I was happy to get involved and learn more.",
      "And so I went to my first meeting of CWiT just a few months ago. And in my short time attending CWiT meetings, I have met many amazing women and learned a great deal about gender equality in the Science and Tech fields.",
      "A few things that I have learned about gender inequality are:",
      "Gender inequality in Science and Technology fields is really pronounced.  REALLY pronounced.  The percentage of Computer Science college graduates that were women in 2012 was around 18%. Read more about this here.\n  \n  \n    Gender inequality in Science and Technology has only gotten worse since the mid 1980’s. In 1984/85 the percentage of Computer Science degrees conferred to women was around 37%.  That means in the past 20 years, gender inequality in Computer Science has not improved, but has actually gotten worse dropping by 19% points. Read more about this here.\n  \n  \n    Girls’ interest often turns away from Science and Technology in middle school.  A 2009 American Society for Quality poll of kids ages 8-17 found that 24 percent of boys but only 5 percent of girls were interested in an engineering career. Read more about this here.  Various sources indicate that this decline is due to a number of factors, including lack of role models as well as social pressure to focus on other things in middle school.",
      "But there are actions we can take to address this gender inequality for women now and for our girls growing to be women in the future.  Here are just a few that stand out for me:",
      "Encourage girls to explore science and technological fields.  There are a number of programs both locally and nationally to work on addressing this:\n\n    \n      \n        Tech-Girls:  Tech-Girls is about nurturing girls interest in STEM. They work with girls and provide hands on activities that will engage their natural curiosity about technology. They work with parents, teachers and other partners to provide training resources and relationships to support the girls in this effort. To learn more check them out at  http://www.tech-girls.org.\n      \n      \n        Girls Excited about Math and Science (GEMS): GEMS is a UVA organization comprised of university students majoring in science, mathematics or engineering fields. They conduct weekly hands-on experiments and focus on a new aspect of science and technology each week, as well as have an annual science fair. Their goal is to foster an interest in STEM fields for young women and show them how much fun science can be!  For more information check them out at  https://atuva.student.virginia.edu/organization/girlsexcitedaboutmathandscience.\n      \n    \n  \n  \n    Encourage women to explore science and technological fields!  One great group that does just this is Girl Develop It.  Girl Develop it is a non-profit that exists to provide affordable and judgement free opportunities for adult women interested in learning web and software development. To learn more check it out at https://www.girldevelopit.com.\n  \n  \n    Encourage diversity in the workplace by giving women a chance in technological and leadership roles.  Leaders of both genders need to give women applicants a real chance at the jobs that are out there.  This means that hiring managers need to take a close look at women candidates as well when making hiring decisions.\n  \n  \n    Leaders of both genders need to make the modern technological workplace friendly to both women and men.  This means a wide variety of things to a wide variety of people and includes things like allowing schedule flexibility and access to good health care, but can include other things as well.  Leaders need to listen to the people they lead and adjust the modern workplace to work for those people.",
      "OSC is not the model of gender equality. We are a small firm and our percentage of women employees is about average for a computer software firm, but we are working on it.  We are committed to making changes in our own organization and working to be part of the solution in the larger science and technology community.",
      "OSC is proud to sponsor CWiT this year and to support their work to bring these issues to the forefront to give us all a chance to address them. We hope that by doing this, we can help level address the current gender inequality, and hopefully balance that out for future generations.  If you’d like to learn more about Charlottesville Women in Tech, please check them out at their website http://www.charlottesvillewomenintech.com."
    ],
    "summary_t": ""
  },
  {
    "id": "fc4dc58ab965aba544918dbec63bcfee",
    "url_s": "https://opensourceconnections.com/blog/2015/10/15/bad-behaviors-in-tuning-search-results/",
    "title": "Bad Patterns in Tuning Search Results",
    "content": [
      "We’re talking to customers about what challenges they are experiencing in returning highly relevant search results this week while we are at LuceneRevolution.   I was really happy when during the opening keynote by Grant Ingersoll, CTO of LucidWorks, he explicitly talked about examples of negative behaviors that we engage in when working on search relevancy. These are behaviors that even I have personally engaged in. Many are drivers that led us to create our search relevancy tuning dashboard Quepid.",
      "Negative Behaviors when Tuning",
      "Get Your Head out of the Weeds",
      "The first behavior that he brought up was locally optimizing your search relevancy.   What I call the \"whack a mole\" problem.  It’s very common to fixate on a specific search use case, and improve it the best that you can, only to discover that you’ve negatively impacted the rest of your search queries.   The \"Rena and Doug\" comic is an example of this negative behavior.",
      "",
      "With Quepid, you will catch yourself when you start \"locally\" optimizing, as your individual query score will go up, but your overall Q Score across all queries will go down.",
      "Pet Peeve queries",
      "Ahh! The query that the CEO comes up with in the middle of the night, and stumps your team.  It’s the very odd query that rarely if ever entered by your users, and yet leads to the perception that \"search sucks\".",
      "Without data, it’s very hard to convey to your (insert person with clout) that the bad results aren’t actually a problem.",
      "With Quepid, you can show your CEO metrics around search relevancy.  We establish a Q Score for relevancy, and show your CEO that fixing the pet peeve query negatively impacts your overall score.",
      "I like to solve Pet Peeve queries not by changing my search algorithem, but instead by using Solr’s Query Elevation Component to match the exact pet peeve query and return a hand crafted list of perfect results ;-).",
      "Note: while Quepid can show you the impact of trying to solve pet peeve queries, it isn’t a query log analytics tool.  It doesn’t track frequency of queries for example.  There are a number of great products out there for doing that.",
      "Was it you, or was it Oprah?",
      "The Oprah Effect refers to the almost magical power of a recommendation from Oprah Winfrey to drive huge amounts of sales.  Having your book be a Book of the Month choice is worth an additional million dollars in sales!",
      "In the world of relevancy tuning, what this means is that sometimes it can be hard to figure out if the reason things improved (or got worse!) is due to the changes you’ve made to the search algorithm, or because of something else that happened like a new user interface.   This is especially likely if you do your relevancy tuning in a different environment then production, and have a big release in conjunction with other changes, like a new UI.   So don’t pat yourself on the back unless you know it was your changes.",
      "With Quepid, you’re working, ideally, in your production environment.  You get immediate feedback on the impact of the changes, so that you’re confident that it’s your work thats making the different, not the new metadata that was added to the index.",
      "The Theme of 2015 LuceneRevolution",
      "Every LuceneRevolution has a different topic that seems to float to the top.  And this year it’s….   Relevancy!   Stay tuned for more posts from LuceneRevolution."
    ],
    "summary_t": "Courtesy of Grant Ingersoll’s keynote speech, some bad behaviors we all, including me, engage in when tuning search results!"
  },
  {
    "id": "395a8f789c4710eef54e87cf487d32dd",
    "url_s": "https://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/",
    "title": "BM25 The Next Generation of Lucene Relevance",
    "content": [
      "There’s something new cooking in how Lucene scores text. Instead of the traditional \"TF*IDF,\" Lucene just switched to something called BM25 in trunk. That means a new scoring formula for Solr (Solr 6) and Elasticsearch down the line.",
      "Sounds cool, but what does it all mean? In this article I want to give you an overview of how the switch might be a boon to your Solr and Elasticsearch applications. What was the original TF*IDF? How did it work? What does the new BM25 do better? How do you tune it? Is BM25 right for everything?",
      "BM25 and TF*IDF sit at the core of the ranking function. They comprise what Lucene calls the \"field weight\". Field weight measures how much matched text is about a search term.",
      "Classic Lucene Similarity: What is TF*IDF?",
      "TF*IDF is a rough way of approximating how users value the relevance of a text match. The intuition underlying TF*IDF is pretty straight-forward and relies on the two principal factors embedded in the name of the scoring formula that tend to correspond to how human minds tend to evaluate search relevance:",
      "Term Frequency aka tf: how often does \"dog\" occur in the article? 3 times? 10 times?\n  Inverse Document Frequency aka idf: The document frequency measures how many docs a term appears in. Inverse document frequency (1/df) then measures how special the term is. Is the term \"dog\" very rare (occurs in just one doc)? Or relatively common (occurs in nearly all the docs)?",
      "In other words, TF*IDF measures the relative concentration of a term in a given piece of text. If \"dog\" is common in this article, but relatively rare elsewhere, then the TF*IDF score will be high. This article ought to be thought of as very relevant to the search term \"dog.\" If \"dog\" occurs once here, but very prominently in many other docs, its score will be relatively low.",
      "One additional measure is the text’s length. \"Dog\" occurring twice in a 500 pg book says almost nothing about how much that book is about \"dog\". \"Dog\" occurring twice in a short tweet, however, means that tweet is very much about \"dog\"! Thus an additional bias is introduced called \"fieldNorms.\" This contribution gives a significant bias towards matching shorter documents over longer documents. The terms are more \"concentrated\" in the shorter doc, therefore that shorter doc is much more likely to be about the searched-for term and thus should be scored higher.",
      "Classic Lucene Similarity: Fudging TF*IDF",
      "Through constant experimentation, the field of Information Retrieval (the academic side of search) has realized that raw TF IDF values don’t quite correspond to user intuitions of relevance. If an article mentions \"dog\" six times is it twice as relevant as an article mentioning \"dog\" 3 times? Most users say no. Sure the article that mentions \"dog\" 6 times may be more relevant, but not twice as relevant. Similar considerations come into play for a term’s IDF. A term occurring in 500 docs is not twice as special as a term occurring in 1000.",
      "Instead, TF*IDF is modified so that TF, IDF, and field length aren’t taken directly. Instead of TF directly, sqrt(TF) is taken in the scoring formula. Documents with twice the number of terms as another document aren’t twice as relevant. Instead you get a TF score computed as follows",
      "Raw TF\n      TF Score\n    \n  \n  \n    \n      1\n      1.0\n    \n    \n      2\n      1.141\n    \n    \n      4\n      2.0\n    \n    \n      8\n      2.828\n    \n    \n      16\n      4.0",
      "Ok here with TF, a document with 16 terms is roughly twice as relevant as a document with 4.",
      "Similarly users don’t consider terms that only occur in 10 documents ten times as special as those that occur in 100 documents. Instead, the IDF score is computed as",
      "log ( numDocs / docFreq + 1) + 1",
      "Here numDocs is the number of documents in the corpus. For, say numDocs=1000, this corresponds to::",
      "Raw DF\n      IDF Score\n    \n  \n  \n    \n      1\n      7.214\n    \n    \n      2\n      6.809\n    \n    \n      4\n      6.298\n    \n    \n      64\n      3.733\n    \n    \n      128\n      3.048\n    \n    \n      256\n      2.359",
      "This grows more slowly. Here a term that occurs in only 4 documents is roughly twice as special as a term that occurs in 64 documents. Again, this makes sense intuitively to most users.",
      "What about the impact of a document’s length? How does that computed? This is computed based on another simple formula that also seems to work with user expectations:).",
      "1 / sqrt(length)",
      "Raw Length\n      Field Norm Score\n    \n  \n  \n    \n      1\n      1.0\n    \n    \n      2\n      0.707\n    \n    \n      4\n      0.5\n    \n    \n      64\n      0.125\n    \n    \n      128\n      0.088\n    \n    \n      256\n      0.0625",
      "So a length 128 document is roughly ten times less relevant as a match in a length 1 document. This makes a sort of sense based on our intuitions: if you match the only term in a length one document, well that document is absolutely all about that term! In a length 128 document, it’s one of a whole slew of terms and not necessarily what that document is all about.",
      "Classic Lucene Similarity Taken together…",
      "The overall formula is",
      "IDF score * TF score * fieldNorms",
      "or",
      "log(numDocs / (docFreq + 1)) * sqrt(tf) * (1/sqrt(length))",
      "with the caveats",
      "numDocs is actually maxDocs, which often counts deleted docs\n  fieldNorms are computed and stored as an 8 bit floating point value. Which is terrible precision and creates all kinds of fun problems!",
      "Enter BM25: The Next Generation of TF*IDF",
      "BM25 improves upon TF*IDF. BM25 stands for \"Best Match 25\". Released in 1994, it’s the 25th iteration of tweaking the relevance computation. BM25 has its roots in probabilistic information retrieval. Probabilistic information retrieval is a fascinating field unto itself. Basically, it casts relevance as a probability problem. A relevance score, according to probabilistic information retrieval, ought to reflect the probability a user will consider the result relevant. This is a topic that deserves its very own blog post, so I won’t cover it here!",
      "Instead of getting lost in very fascinating theory, I’d rather discuss intuitively how to use BM25. You’ll see the shape the ranking takes, while scary looking mathematically, actually makes a lot of intuitive sense.",
      "BM25’s Take on IDF",
      "Ok first, let’s get IDF out of the way. On a graph, BM25’s IDF looks very similar to classic Lucene IDF. The only reason for the difference here is its derivation from probabilistic information retrieval. Lucene makes one change to BM25’s regular IDF. BM25’s IDF has the potential for giving negative scores for terms with very high document frequency. So IDF in Lucene’s BM25 does this one amazing trick to solve this problem. They add 1 to the value, before taking the log, which makes it impossible to compute a negative value. The end result is an IDF that looks extremely similar to Lucene’s current IDF curve, as shown in the following graph.",
      "",
      "So for IDF, not a lot of surprises. We don’t need to alter our thinking about IDF in BM25. What’s more fascinating if you see (again outside the scope of this post) how IDF is derived from probability theory.",
      "BM25’s Take on TF",
      "Now let’s look at term frequency. It’s a bit of a lie to look at term frequency in isolation, but we’re going to try for exposition purposes to build up to the larger BM25 formula.",
      "Term frequency in BM25 dampens the impact of term frequency even further than traditional TF*IDF. The impact of term frequency is always increasing, but asymptotically approaches a value.",
      "Without any consideration for document length, term frequency follows the formula",
      "((k + 1) * tf) / (k + tf)",
      "as graphed below:",
      "",
      "As you can see, this curve approaches (k + 1) asymptotically (here k=1.2). It has a really interesting effect. More tf always means more relevance. However you quickly hit diminishing returns. You never get past k, but you always approach it! Classic Lucene tf, on the other hand, constantly increases and never reaches a saturation point.",
      "What is this k value? For BM25, k is often set to 1.2. Most leave k alone. Changing k though can be a useful tuning approach to modify the impact the TF. Modifying k clearly causes the asymptote to move. However what’s more important is that a higher k causes TF to take longer to reach saturation. By stretching out the point of saturation, you stretch out the relevance difference between higher and lower term frequency docs!",
      "How Does BM25 Use Document Length?",
      "Now the last section was a bit of a useful lie. The TF score above is further influenced by whether the document is above or below the average length of a document in the corpus.",
      "What does this look like? Well let’s built on the TF formula from before, introducing two variables: a constant b and a length value L. Taking the formula above and adding (1.0 - b + b * L) as a multiple on k in the denominator.",
      "((k + 1) * tf) / (k * (1.0 - b + b * L) + tf)",
      "Here L is how long a document is relative to the average document length. L is 2 if the document being scored is twice the corpus’s average document length. L is 0.1 if the document being scored is one tenth the average document length. L therefore is actually presented as |d|/avgDl – this document length divided by the average document length.",
      "",
      "As you can see in the graph, the end result for different values of L is that shorter docs hit the asymptote much faster. They saturate nearly right away to the best possible TF score. This makes sense, short documents have fewer terms. The more matches in these short docs, the more certain you can feel confident in the relevance. So the number goes up quicker. A lengthy book, on the other hand, takes many more matches to get to a point where we can feel confident. So reaching \"max relevance\" takes longer.",
      "The constant b will allow us to finely tune how much influence our L value has on scoring. Notice in the formula above, a b of 0 completely removes the influence of L, returning to the formula of the previous section. A higher b adds more document length influence on the scoring. In other words, in classic TF*IDF you always disabled norms on a field to remove the influence of field length. Here you can simply set b to 0 on the similarity to remove the impact of field length.",
      "All Together",
      "When BM25 is taken all together:",
      "IDF * ((k + 1) * tf) / (k * (1.0 - b + b * (|d|/avgDl)) + tf)",
      "You can see why we started from square one here!",
      "Will BM25 be appropriate for everything?",
      "I’m really impressed with BM25. I used it on the O’Reilly Library project for searching book chunks. There’s a lot of wisdom here! Term frequency saturation makes a lot of sense. So does modulating the influence of the field length.",
      "But they continue to make sense for article-length pieces of text. Not everything we search are blog posts or wikipedia pages. The similarity being used needs to change based on the kind of thing you are comparing. Title fields, for example have their own wonky proclivities. Indeed hosted search services Algolia and SwiftType make their business in part on being really really good at short snippet search.",
      "So as my buddy Alex from Solr Start asked me – is BM25 right for everyone? For everything? It’s a huge improvement on the core document search problem. But around the edges for numbers, images, and other entities being searched the win is not clear.",
      "This is a fascinating time to be a Lucene, Solr, or Elasticsearch developer. With BM25 becoming the default, we’re going to see directly what happens when theory  meets practice. Relevance is never a constant, it’s a user experience you’re crafting. Yet the cauldron that BM25 came out of is information retrieval competitions like Trec and the like. The real world can be so much different. Documents aren’t just documents but restaurants, products, news articles, tweets, doctors’ offices, and many other things. Maybe the right answer for your \"similarity\" is to always bring up tweets tweeted by friends, nearby with similar interests. Less about text similarity, and more about what’s important for users to find. In other words, search is just as much about crafting a user experience as anything else. That’s why we’re excited about Quepid to help understand the user expectations from search!",
      "Anyway, I’m thoroughly excited about the possibilities of BM25. It opens doors in Lucene’s baseline relevance capabilities and is going to really open a lot of doors for Solr and Elasticsearch capabilities! If you’d like to chat about whether BM25 or another relevance solution is right for your application, please contact us! Love to chat!"
    ],
    "summary_t": "There’s something new cooking in how Lucene scores documents. Instead of the traditional ‘TF*IDF’ Lucene just switched to something called BM25 in trunk. Tha..."
  },
  {
    "id": "31f9460fa11d8f367f69c9ac0dea89ef",
    "url_s": "https://opensourceconnections.com/blog/2015/10/21/recap-cassandra-summit-2015/",
    "title": "Recap of Cassandra Summit 2015",
    "content": [
      "This year OpenSource Connections rolled into Cassandra Summit with 4 attendees. Eric Pugh, Matt Overstreet, and John Woodell attended some of the 137 sessions offered. Matt walked away from day 1 as a Certified Cassandra Developer. On day 3 I delivered a talk outlining some of our work with Cassandra and Spark at the US Patent and Trademark Office. This year was packed with attendees, over 6k people were on site with 5k streaming.",
      "Keynote",
      "Patrick McFadin & Rachel Pedreschi kicked off the keynote with a live demo. Cassandra nodes were subjected to tornado forces (a blender), chaos monkeys with wire cutters, and Patrick with a fire axe. It was an awesome way to illustrate Cassandra’s ability to deal with node loss and keeping the application up.",
      "Next up Jon Hadaad and Luke Tillman demoed their KillrVideo application. This video sharing site is built with DataStax Enterprise 4.8 and runs on Microsoft Azure. It leverages multiple features of DSE including C*, Solr search, and Spark for analytics. All of the code that powers the application is available up on GitHub. Check it out!",
      "Following their quick walkthrough of the site’s features and associated DSE capabilities Microsoft took the stage to demo Cassandra deployment on Azure. With just a few clicks a 90 node C* cluster was up and running in China. Azure provides an excellent wizard for setting up multi-datacenter deployments with automated site-to-site VPN connections keeping data safe while being replicated. Speaking of datacenters, Microsoft has over 100 across 20 regions. C* tooling like OpsCenter is available to monitor the health of the cluster. Overall it seems like Microsoft Azure is a pretty solid platform for deploying C* in the cloud.",
      "Now the keynote took a turn toward the more technical side. Jonathan Ellis started with a hat-tip towards the C* drivers. Next he moved on to various types of databases and how they respond to failures. Examples included MongoDB and its behavior with network partitions (multiple elected masters stepping on each other). Then performance numbers showing operations per second on write, read, and balanced workloads. Cassandra clearly comes out ahead in distributed environments.",
      "Jonathan then discussed the roadmap of C*. There was supposed to be a large jump from v2.1 into v3.0. Instead this was changed to two releases, 2.2 (released in July) and 3.0 which is coming out this month. Features that rely on the new storage engine have been pushed out to v3.0, but everything else has been realized in v2.2. This includes JSON support, compatability with Microsoft Windows, DTCS, UDF and more.",
      "The JSON support is very simple and can make interactions with C* easier. It even works with User Defined Types! For example an insert statement may now look like this:",
      "INSERT INTO phone_numbers JSON\n'{\"id\": 1,\n  \"phone\": {\"mobile\": \"123-456-7890\"}\n  }';",
      "Attributes nested within a UDT are entered as child objects. Collection types are fully supported as well. This lowers the barrier to entry when getting started talking with C*.",
      "User-Defined Functions were explained a bit and given a demo. Functions may be written in languages supporting Java and the Java Scripting API. Interestingly enough by dropping the JRuby JAR on your classpath it is possible to write UDFs in Ruby as well.",
      "One of the killer new features in C* 3.0 is Materialized Views. They work a bit different than the secondary indexes in C* v2.2 and below. Instead of maintaining a local index for the data in a given table the data is distributed across the cluster. C* handles partitioning the data in accordance with the data model specified for the view. Materialized views are not free! Stress tests show that Materialized Views will cost 10% of operations per second on the cluster.",
      "Finally the keynote wrapped up with an outline of the new \"Tick-Tock\" release cycle for C*. With new features being developed in one cycle and a focus on bugfixes and improvements in the other.",
      "Sessions",
      "At this point the sessions began! There were so many great talks (some of which overlapped), I’m only going to outline a few here interspersed with tweets during the sessions.",
      ".@RussSpitzer is crushing it with his Spark Cassandra Connector talk. Back to the Future everywhere! pic.twitter.com/steyLIOmN9— Christopher Bradford (@bradfordcp) September 23, 2015",
      "Russ provided an excellent talk focusing on the Spark Cassandra Connector. There were great examples of methods exposed by the connector to leverage the underlying Cassandra datastore when processing through Spark.",
      "I'm watching @PaulRechsteiner go over how PagerDuty spans the WAN with synchronous replication across datacenters. #CassandraSummit— Christopher Bradford (@bradfordcp) September 24, 2015",
      "They seem to be ignoring some of the big features of Cassandra while capitalizing on the replication factor and quorum behavior. Well done.— Christopher Bradford (@bradfordcp) September 24, 2015",
      "PagerDuty intentionally performs synchronous writes across data centers in their application. The data needs to be extremely durable and may accomadate a little bit of delay to achieve this. They have also implemented their own transaction system within their application to confirm successful writes. Their talk now has me thinking about data in a few more dimensions when implementing distributed systems like C*.",
      "Macy's moved to C* from a RDBMS without any downtime. They covered how to slowly move traffic over and measure during the process. Awesome!— Christopher Bradford (@bradfordcp) September 24, 2015",
      "Macy’s moved their ecommerce platform to C* from DB2. Their team showed many aspects of the move from changing systems on an operational level and data modeling challenges.",
      "Woot, we're in the Sony PlayStation C* talk. This should be good. #CassandraSummit— Christopher Bradford (@bradfordcp) September 24, 2015",
      "Sony Computer Entertainment America discusses the PSN. This included current features and some that have just been released. How these features work and their effects on end users. They presented an interesting approach to balancing load on production clusters. In one instance there was a query access pattern which was hitting the cluster more heavily than others. Instead of just increasing the number of nodes on this cluster they split it off into its own that could be independently scaled.",
      "Great turnout at my #CassandraSummit talk. A big thanks to everyone who attended!— Christopher Bradford (@bradfordcp) September 24, 2015",
      "Demo code from my talk is available at https://t.co/qOWGgu306A. #CassandraSummit— Christopher Bradford (@bradfordcp) September 24, 2015",
      "My talk Cassandra & Spark at the USPTO came next. It covered spinning up team members across the enterprise on C* and it’s ins and outs along with a use case for Spark as an ETL pipeline. The demo code is available out on GitHub, along with slides on SlideShare.",
      "Standing room only at the DataStax Bulk Loading talk. Everybody wants to get their data ingested into Cassandra! #CassandraSummit— Christopher Bradford (@bradfordcp) September 24, 2015",
      "DataStax’s Brian Hess had an excellent talk on loading data into C*. He really broke down all the various ways to load data starting with cqlsh and ending with custom code to write out SSTables that could then be piped into sstableloader. He also developed a tool, cassandra-loader, which was measured and benchmarked as well. The room was packed and everyone took away some valuable data loading tips.",
      ".@chbatey is throwing down some knowledge around simulating C* failures quickly for testing. #CassandraSummit— Christopher Bradford (@bradfordcp) September 24, 2015",
      "To wrap things up Chris Batey delivered and excellent talk on testing interactions with Cassandra. He covered various methods to test failure scenarios in your application. How can you force a ReadTimeout? His tool scassandra stubs out a Cassandra server. With this tool in place the tests connect with the stubbed server which will provide any type of response desired. This can include valid data, timeout exceptions, or not at all (letting the driver detect the issue). scassandra looks to be an effective tool for writing resilient client applications. It’s worth noting that the Java Driver is now using scassandra to test itself!",
      "It is amazing to see how much the community has grown since my first Summit back in 2013. This year the community chose me as a MVP for Apache Cassandra. I am honored by this award and will continue to engage with the community at large. 2015 has been a great year for Cassandra, I’m looking forward to v3.0 and 2016.",
      "If you’re interested in evaluating Apache Cassandra for your business or are looking for some help with an existing application / cluster, don’t hesitate to get in touch to chat about how we can help you with your distributed data problems!",
      "",
      ""
    ],
    "summary_t": "A quick trip review of Cassandra Summit 2015."
  },
  {
    "id": "850a432f0f37c8a5eca806237dc77333",
    "url_s": "https://opensourceconnections.com/blog/2015/10/26/transition-support-from-legacy-oscs-got-your-back/",
    "title": "Transitioning Endeca, FAST, or Another Legacy Search Solution? Quepid's Got Your Back",
    "content": [
      "Your face after getting the bill for Endeca!",
      "Don’t you hate how much you spend on proprietary search technology? It feels like you are paying a ton for search solution that isn’t always providing you with the results you want. But you may be afraid to move away from that legacy search solution, for fear that it will have a negative impact on your sales bottom line.",
      "Open Source search solutions, like Solr and Elasticsearch (ES), offer major benefits over proprietary searches:",
      "Open Source search is often must more customizable than proprietary solutions. You can customize your search using query facilities. Plugins also offer robust enhancements for made-to-order search.\n  \n  \n    Open Source search is also constantly evolving to meet the needs of the community. Even though contributors are addressing their own specific needs, those solutions are also available to you to address your problems. Other people have maybe already be working to solve the same problem you are currently addressing.",
      "OSC’s Got Your Back",
      "OSC can help you transition from legacy search to a Solr or Elasticsearch solution while helping you avoid the negative impacts and even improve your search in the process.",
      "Quepid, OSC’s test relevancy platform, can help you make this transition away from legacy search, without making you feel like you are throwing caution to the wind.",
      "",
      "With Quepid you can:",
      "Write an adapter between any legacy search engine to view live search results to ensure that Solr/Elasticsearch give comparable or better results.\n  \n  \n    Import snapshots from other search engines to work against a starting point to ensure Solr/ES give comparable or better results.\n  \n  \n    Tune and constantly evaluate search to make sure you are moving forward.",
      "Not only that, but Quepid can help you build a foundation for incremental progress moving forward.  Once in place, Quepid lets you keep moving the ball forward on search.",
      "Our unique Q-Score TM keeps the ball moving on search, driving increased profits and user satisfaction.\n  \n  \n    If you need more support, OSC offers quarterly tuneups that let us revisit your search solution to tackle a new set of problems once a quarter.",
      "Are you ready to take the leap?",
      "Learn more about Quepid at Quepid.com or drop us an email to talk about our Quarterly Tune-Up (Write to us at [email protected] for more information!)",
      "Aaahh!!! Image by Evil Erin"
    ],
    "summary_t": "Quepid & OSC can help you transition from legacy search to a Solr or Elasticsearch solution while helping you avoid the negative impacts and even improve..."
  },
  {
    "id": "f44afddcc4a8f4230c3e669534ebaa5b",
    "url_s": "https://opensourceconnections.com/blog/2007/04/28/citcon-2007-rspec-is-the-new-testunit/",
    "title": "CITcon 2007: RSpec is the new Test::Unit",
    "content": [
      "My first session was entitled RSpec is great of what? and talked about a tool that helps you do \"Behavior Driven Development\". The key difference between RSpec as a validation tool from other xUnit tools like JUnit or Test::Unit is that instead of using \"Assert\" style validations we use \"Should\" style validations. Here is a short example of pulling up a web page and verifying the words \"Administer Bookshelf\" show up.",
      "$ie = $ie || Watir::IE.new\n$ie.goto http://localhost:3000/login/login\n$ie.text.should include(Administer Bookshelf)",
      "Compare the last line to something in the older \"Assert\" style:",
      "assert $ie.text.include?(`Adminster Bookshelf)",
      "The should syntax reads much easier! And by removing the word \"test\" from your vocabulary you remove the Tester/Developer divide. \"Should\" is something you would define before writing the code. \"Test\" is something you do after you write the code!"
    ],
    "summary_t": ""
  },
  {
    "id": "fac505cdfd7702aed76514060063cbe9",
    "url_s": "https://opensourceconnections.com/blog/2015/11/02/field-list-exclusions-in-solr/",
    "title": "Exclusions in Field Lists in Solr",
    "content": [
      "Back in 2012 Luca Cavanna opened an issue on the Solr Jira\nrequesting a field exclusion syntax. Right now it’s the 5th highest-voted ticket, and it’s easy to understand why.\nDocuments consisting of dozens, even hundreds of fields are common and without an exclusion syntax your choices are\nlimited to:",
      "All fields (\"*\")\n  Some fields that match a specific naming convention (\"section_*\")\n  Each field (\"section_1, section_2, section_3\"…)",
      "With atomic updates you need to store each field, so unwieldy field lists are quickly becoming the norm.",
      "About a year later Andrea Gazzarini added an excellent patch that went above and\nbeyond the original request and added a more flexible expression syntax for both exclusions and inclusions. What’s more\nhe provided excellent documentation about the patch\nas well. In short, with this patch you can do things like:",
      "Exclude a single field: fl=-forbidden_field\n  Exclude similarly named fields: fl=-forbidden_*\n  Use a question mark for a single character wildcard: fl=-forbidden_5?",
      "I should also mention some of the things you could already do with field lists like supply functions, transformers,\nand aliases (and aliases to functions and transformers). So when you use wt=csv to generate a report on your index\nyou can let Solr do a lot of the busywork for you.",
      "For now this exists only as a patch, so you’ll need to download the source, apply the patch, and build Solr yourself.\nIf you’d like to see this included as part of Solr I encourage you to vote on it!"
    ],
    "summary_t": "When you have a lot of fields, sometimes you just want to exclude a few."
  },
  {
    "id": "3bbad1d9b451626aa6f3a03eea9086a3",
    "url_s": "https://opensourceconnections.com/blog/2015/11/09/quepid-announcement/",
    "title": "Quepid Announced",
    "content": [
      "CHARLOTTESVILLE, Va., Nov. 9, 2015 – OpenSource Connections, a search and discovery consulting firm specializing in user engagement and relevancy, has launched Quepid, its search relevance testing and monitoring dashboard for Solr and Elasticsearch. Quepid leverages OSC’s years of expertise increasing revenue and user satisfaction with smarter search solutions for clients like O’Reilly Media and the US Patent and Trademark Office. Quepid is available at Quepid.com for a 30-day risk-free trial.",
      "Built on the idea of Test-Driven Relevancy, Quepid revolutionizes the process of improving search. Quepid gives marketing, domain and business experts the ability to influence the search relevance of search results. Quepid allows the whole team to:",
      "Migrate safely from expensive, legacy search solutions\n  Provide feedback on the quality of searches between old and new solutions\n  Visualize and measure the impact of every relevancy change\n  Examine side-by-side the impact of solutions before-and-after\n  Understand the search engine’s ranking behavior\n  Tune search relevancy armed with Quepid’s transparency and metrics",
      "\"These capabilities enable search developers to experiment with the impact of changes across the search platform and instantly learn the success or failure of these tests,\" said Doug Turnbull, author of Relevant Search and Search Relevance Practice Lead at OpenSource Connections.",
      "\"When tuning search-based applications, developers may make many incremental improvements over time, but ensuring that all changes made continue to work together to return the best possible query results can be a huge challenge,\" said Mike Matchett, Senior Analyst and Consultant with the Taneja Group. \"Quepid brings everything together so developers aren’t taking two steps forward and three steps back with each adjustment. Because both content and development specialists work together in a transparent and measurable environment like Quepid, they get to better results faster. Users will find their search queries performing consistently better.\"",
      "\"Quepid provides a way for the whole team to instantly explore search relevancy and its impact across dozens of search queries,\" said Eric Pugh, President of OpenSource Connections. \"Testers, marketing specialists and content experts can provide immediate feedback to developers, impacting their technical decisions. By actively participating in the search quality process, the feedback provided by these groups produces quicker iterations and richer results.\"",
      "\"Quepid is one of the necessary ingredients in our development process that helps Muck Rack stand out as a must-have tool for anyone looking to connect with the right journalists,\" said Rob Shapiro, Director of Product Strategy at Muck Rack, a PR, communications and journalism software built for and trusted by journalists, global PR agencies and Fortune 500 companies. \"When marketing and PR professionals search for keywords or phrases, Muck Rack helps them discover journalists writing articles, sharing links and tweeting about the topic so they build better relationships for their communications strategies. As Muck Rack’s engineers refine search algorithms and add functionality, we’re using Quepid to guarantee the results our visitors expect remain relevant and accurate. Search is a science, and Quepid continues to help us as we continually improve on our products.\"",
      "OpenSource Connections has also signed a reseller agreement with Flax, The Open Source Specialists, based in the UK. The partnership will see Flax offer OpenSource Connections’ Quepid solution, and associated professional services, to its current and future clients throughout Europe and Asia.",
      "According to Charlie Hull, Flax Co-founder and Managing Director, \"OpenSource Connections is uniquely positioned with its years of search experience and Quepid to add tremendous value to our portfolio. OSC understands what our clients need for success in search, and Quepid is an invaluable tool for those clients, leading to additional business and success for Flax. This partnership will be extremely valuable to our customers and an ideal fit for our business model.\"",
      "\"Quepid helped us develop best practice approaches across our global search relevancy program,\" said Karen Renshaw, Digital Product Manager of Search Relevancy for billion dollar component distributor RS Components, a Flax client. \"Using Quepid, we are now able to easily and iteratively test and understand the impact of search changes across our entire range of search queries, ensuring we make data-driven decisions. This is crucial for organizations where search is critical to the customer experience.\"",
      "Pricing and Availability\nQuepid is available now at  and enterprises can start with a risk-free 30-day trial. For pricing information, please review pricing information at .  For additional information, call (703) 232-1492.",
      "About Quepid\nQuepid is pronounced kyoo-pid and is a reference to \"giving search queries the love they deserve.\" Built by search experts, Quepid puts the enterprise team in control of Solr and Elasticsearch search results. Built on the idea of Test-Driven Relevancy, Quepid allows search developers to collaborate with marketing and content experts to identify, store and execute important queries, provide rankings that measure search query quality, tune search relevancy, and immediately visualize the impact of query tuning. All these capabilities empower experimentation and real-time changes that make truly impactful differences in search results.",
      "About OpenSource Connections\nOpenSource Connections (OSC) solves clients’ toughest search problems. Since 2007, OSC has offered unparalleled consulting solutions for clients like Cisco, the US Patent & Trademark Office (USPTO), O’Reilly Media, and Advance Auto Parts. OSC breaks the search consulting mold by putting users back at the center of search. OSC engages users through its unique search relevance practice and test-driven relevancy platform, Quepid. OSC’s capabilities rest on a combined 30 years of experience in open source search engines Solr and Elasticsearch and the distributed, NoSQL database Cassandra. OSC’s thought leadership is recognized through its gold partnership with DataStax and having authored numerous publications on search.",
      "OpenSource Connections and Quepid are registered trademarks of OpenSource Connections. All other product and company names herein may be trademarks of their registered owners."
    ],
    "summary_t": "OpenSource Connections, a search and discovery consulting firm specializing in user engagement and relevancy, has launched Quepid, its search relevance testi..."
  },
  {
    "id": "07c3b77a4474596c70a31ff7e85a8464",
    "url_s": "https://opensourceconnections.com/blog/2015/11/11/graph-phun-in-solr/",
    "title": "Graph Phun in Solr",
    "content": [
      "Recently Solr has acquired graph search capability similar to what you’d find in TitanDB or Neo4J. Essentially it acts as\na collector that traverses link_from to link_to, while optionally applying a filter query each step of the way. You can\nalso decide to return all connected hops or just the leaves.",
      "SOLR-7543: Create GraphQuery that allows graph traversal as a query operator",
      "Now, general purpose graph databases have a radically different storage model than Lucene’s Inverted Index, so\nperforming complex or deep graph queries is not an option. The trade-off is that the traversal depth is restricted to 4.\nAt that depth you’re still able to model something like Role-Based Access Control\nor query augmentation with hypernyms an hyponyms.",
      "Hyponyms and Hypernyms",
      "Role-based Access Control",
      "Since this is so new you’ll need to pull down the latest version of Solr and build it.\nIf you prefer git, you can also pull down the source from the GitHub mirror.",
      "To get a feel for how this works I decided to use WordNet1 to index some words along with their hypernyms with\npysolr and nltk. The indexing code itself isn’t that interesting so I won’t go into its details here, but here’s the\nGist.",
      "The other thing I wanted to experiment with is Solr’s new schemaless capability. So once I built Solr I ran it with\nbin/solr -e schemaless to get a blank collection.",
      "After the indexing code finished I opened up the Admin Console and checked out the schema. Sure enough, there were the\nfields, all indexed as string types. I also noticed that each was copied to the general _text_ field.",
      "Okay, now let’s do some graph stuph! (last joke like that, I swear).",
      "What are the hypernyms of \"dog\"?",
      "Well, that’s easy because we indexed them directly as fields:",
      "http://localhost:8983/solr/gettingstarted/select?wt=xml&indent=true&q=id:dog.n.01&fl=hypernyms",
      "{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":0},\n  \"response\":{\"numFound\":1,\"start\":0,\"docs\":[\n      {\n        \"hypernyms\":[\"canine.n.02\",\n          \"domestic_animal.n.01\"]}]\n  }}",
      "By the way, the reason it’s \"dog.n.01\" and not just \"dog\" is because WordNet is very specific about terms, and this happens to be the second word sense of \"dog\" the noun (unlike a perjoritive term for how someone looks, or the verb for following someone closely).",
      "What are the hypernym’s of the hypernyms of \"dog\"?",
      "So what we want to do here is start at the id field, look at the matching hypernyms field, then for each\nvalue there try to find a doc with that id:",
      "http://localhost:8983/solr/gettingstarted/select?wt=json&indent=true&q={!graph%20from=%22id%22%20to=%22hypernyms%22%20returnRoot=%22false%22%20returnOnlyLeaf=%22false%22%20maxDepth=3}id:dog.n.01&fl=id&echoParams=none",
      "(From now on I’ll omit the other parameters and just focus on the graph query.)",
      "{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":4},\n  \"response\":{\"numFound\":4,\"start\":0,\"docs\":[\n      {\n        \"id\":\"animal.n.01\"},\n      {\n        \"id\":\"domestic_animal.n.01\"},\n      {\n        \"id\":\"carnivore.n.01\"},\n      {\n        \"id\":\"canine.n.02\"}]\n  }}",
      "Notice we’re returning intermediate nodes of \"caninie.n.02\" and \"domestic_animal.n.01\". In the last query these values\nwere in the hypernyms field, now they’re in the id.",
      "What are all the hyponyms of \"canine\"?",
      "{!graph%20from=%22id%22%20to=%22hyponyms%22%20returnRoot=%22false%22%20returnOnlyLeaf=%22false%22%20maxDepth=2}id:canine.n.02",
      "{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":0,\n    \"params\":{\n      \"q\":\"id:canine.n.02\",\n      \"indent\":\"true\",\n      \"fl\":\"hyponyms\",\n      \"wt\":\"json\"}},\n  \"response\":{\"numFound\":1,\"start\":0,\"docs\":[\n      {\n        \"hyponyms\":[\"bitch.n.04\",\n          \"dog.n.01\",\n          \"fox.n.01\",\n          \"hyena.n.01\",\n          \"jackal.n.01\",\n          \"wild_dog.n.01\",\n          \"wolf.n.01\"]}]\n  }}",
      "How far up the chain of hypernyms of \"canine\" can I go?",
      "{!graph%20from=%22id%22%20to=%22hypernyms%22%20returnRoot=%22false%22%20returnOnlyLeaf=%22false%22%20maxDepth=-1}id:canine.n.02",
      "{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":6},\n  \"response\":{\"numFound\":12,\"start\":0,\"docs\":[\n      {\n        \"id\":\"chordate.n.01\"},\n      {\n        \"id\":\"vertebrate.n.01\"},\n      {\n        \"id\":\"mammal.n.01\"},\n      {\n        \"id\":\"placental.n.01\"},\n      {\n        \"id\":\"entity.n.01\"},\n      {\n        \"id\":\"physical_entity.n.01\"},\n      {\n        \"id\":\"object.n.01\"},\n      {\n        \"id\":\"whole.n.02\"},\n      {\n        \"id\":\"living_thing.n.01\"},\n      {\n        \"id\":\"organism.n.01\"}]\n  }}",
      "Notice we stopped at 10, instead of returning all 12 numFound.",
      "What are all the \"domestic_animal\"s in the index?",
      "In this query I added rows=1000 just to see what would happen. I’ll omit the bulk of the response…",
      "{!graph%20from=%22id%22%20to=%22hyponyms%22%20returnRoot=%22false%22%20returnOnlyLeaf=%22false%22%20maxDepth=-1}id:domestic_animal.n.01",
      "{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":14},\n  \"response\":{\"numFound\":213,\"start\":0,\"docs\":[\n      {\n        \"id\":\"feeder.n.01\"},\n      {\n        \"id\":\"stocker.n.01\"},\n      {\n        \"id\":\"head.n.02\"},\n      {\n        \"id\":\"puppy.n.01\"},\n      {\n        \"id\":\"dog.n.01\"},\n      {\n        \"id\":\"pooch.n.01\"},\n      {\n        \"id\":\"cur.n.01\"},\n      {\n        \"id\":\"feist.n.01\"},\n      {\n        \"id\":\"pariah_dog.n.01\"}, \n    ...\n      {\n        \"id\":\"burmese_cat.n.01\"},\n      {\n        \"id\":\"egyptian_cat.n.01\"},\n      {\n        \"id\":\"maltese.n.03\"},\n      {\n        \"id\":\"abyssinian.n.01\"},\n      {\n        \"id\":\"manx.n.02\"}]",
      "And that’s just the beginning",
      "The hierarchical nature of these queries means we can change one of the intermediate nodes and have radically different\nresults. Take, for instance, the RBAC use-case I mentioned at the beginning. By simply adding an operation to a\npermission, immediately all of the roles that have that permission and all of the subjects that have those roles gain\naccess to that operation. Likewise, catalogs can be reorganized by shifting one subcategoy to another without\nreindexing all of the products within them.",
      "Solr 6.0 is going to be a fun release!",
      "[1]: Princeton University \"About WordNet.\" WordNet. Princeton University. 2010. http://wordnet.princeton.edu"
    ],
    "summary_t": "Graphs. In Solr!"
  },
  {
    "id": "111f9f572367920e3a767968c99d2ba8",
    "url_s": "https://opensourceconnections.com/blog/2015/11/12/aws-reinvent-2015-las-vegas/",
    "title": "Recap of AWS re:Invent 2015 in Las Vegas",
    "content": [
      "Recap of AWS re:Invent 2015 in Las Vegas",
      "Amazon Web Services’ (AWS) re:Invent conference is one of the biggest cloud conferences in the world. With over 18,000 people in attendance, there are sure to be a few good ideas lurking about. For this article I’m going to go over a few of my favorite observations and share what significance they hold for future computing. Going all in was a very big theme this year for the conference. AWS trotted out many different companies to deliver the message, \"You can trust AWS\", but we already knew that. So what is happening behind the scenes? Ideas such as microservices and containerization are maturing. This maturation led to more breakout sessions talking best practices and patterns versus last year where mainly overviews were being given. Also AWS has been around long enough to have stores of companies that migrated to the cloud, out, and then back again. As always, some of the best minds in AWS were there to share their practices. Finally Amazon isn’t losing steam and continues expanding its numerous solutions for compute needs. It is clear that eventually no realm of technology is safe from their sights.",
      "AWS is eating software",
      "AWS’s secret sauce is the ability to deliver a well documented, API driven with UI service that delivers easy access to the technology stacks you want to run but don’t want to master. AWS announced some enhanced services as well as new ones. They have recently started into not only server side services like databases or on-demand servers but also business side services such as workstation proxies, email and now data analytic interfaces. Finally, AWS is better enabling audit and compliance as a service. Below is non-exhaustive list of some of the more interesting features they introduced.",
      "Infrastructure Services",
      "Infrastructure as a service is at the heart of all of Amazon Web Services offerings. As usual, AWS isn’t resting and is listening to customers to add the features desired.",
      "Amazon Elasticsearch Service: Amazon is taking all the fun difficult parts out of Elasticsearch such as authentication, cluster discovery and ELK pipeline management. Along the way they are exposing many of their metric pipelines such as Amazon CloudWatch Logs, AWS CloudTrail and Amazon CloudWatch to easily pipe the data from the various silos into Elasticsearch.\n  Amazon Lambda Update: AWS Lambda is the purest form of cloud computing; on demand functional code execution without any infrastructure management. They introduced official support for Python, a CRON like invoke service for Lambda functions as well as the ability to run Lambda functions within a VPC. These are all really big deals as they clear out some of the remaining hurdles for Lambda adoption across all cloud designs.\n  EC2 Container Service Update: The biggest update here is that AWS finally has their own container register service. The registry is catch up on AWS’s part. Google had this feature for over a year now and I think not being able to easily tie in your private containers was hindering some adoption of the technology.\n  EC2 Instance Update: The message here was on two parts. First: HUGE instances the X1, 100+ cores with 2TB of memory. Now you can finally run your entire datacenter on one instance! Second: Nano instances, T2.nano with 512 MB of memory and 1 compute core. This instance is perfect for running that low traffic site that still needs a little backend processing.\n  Amazon RDS Update - MariaDB: This is a big deal for AWS as they continue to roll out \"not owned by Oracle\" RDB choices. Maria is to MySQL as Jenkins is to Hudson. The OSS community forked the Oracle aquired product and continue their march towards freedom. AWS is now letting you use it without hassle.\n  Amazon Kinesis Firehose: The Firehose provides a simple API endpoint to submit metrics to a variety of AWS services. It takes your payload and puts it into S3 or Redshift immediately. Amazon is very good about taking a common use case for several of their services where it may take a little code lifting to enable and then spin it into another service. This is basically a combination of Amazon API Gateway, AWS Lambda, Amazon S3, and Amazon Redshift. I can imagine that integration into Amazon Elasticsearch Service isn’t far behind.\n  EC2 Dedicated Hosts: If on demand dedicated virtual instances weren’t enough to satisfy your auditor AWS has now introduced bare metal dedicated hosts. This partially fits into the compliance and audit section but is still infrastructure.",
      "Business Services",
      "Sometimes the hardest part of using the cloud is the journey there. Amazon is working to make this easier.",
      "AWS Import/Export Snowball: The Snowball is a physical device that AWS sends to you via FedEx to copy approximately 50TB of data per device and mail to back to AWS for them to load into S3. The saying, \"Never underestimate the bandwidth of the delivery truck\" still rings true.\n  Amazon QuickSight: This is a big shot over the bow of businesses such as Adobe SiteCatalyst, Oracle Business Analytics, Salesforce and other statistic warehousing firms. The secret sauce to QuickSight is that they immediately integrate into nearly all the data producing and consuming services that AWS offers.",
      "Compliance and Audit Services",
      "Amazon is trying their best to give business zero excuses to be on the cloud. With their latest suite of compliance and audit services, they are further positioning themselves as the best in the business to partner with, get work done, and do it in a secured fashion.",
      "Amazon Inspector: This service allows you to perform security scans on your infrastructure and run an agent on your instances to scan network, filesystem and process activity to ensure that best practices are being met or bring notice of those needed mediation.\n  AWS Config Rules: The config rules are Lambda functions that check the state of your infrastructure against rules you define as Lambda functions. The config rules can be launched via a configuration change (tracked by AWS Config) or a periodic trigger (Lambda CRON anyone?).",
      "Breakout Session Highlights",
      "One of the primary reasons to attend re:Invent is the breakout sessions and being able to talk to the creators and movers of the services. Even as the sessions slides and videos are freely available the best way to make time to learn and explore these sessions is to be there in person. I’ll make separate posts for my favorite sessions but here is the bullet list of those which I found interesting or insightful:",
      "BDT209 Amazon Elasticsearch Service: Being that it was announced a week prior, Amazon Elasticsearch Service was a little absent from the conference but they did have a quick overview session to present the latest service. The key factors to consider for Amazon Elasticsearch Service is that they have combined many of the features of the Elasticsearch, Logstash and Kibana (ELK) stack to make it easy to integrate into Amazon. Session video: [AWS re:Invent 2015\n          (BDT209) New! Amazon Elasticsearch Service for Real-time Analytics](https://www.youtube.com/watch?v=s7dJESec_dY) and slides: (BDT209) Launch: Amazon Elasticsearch For Real-Time Data Analytics.\n        \n      \n    \n  \n  \n    \n      \n        \n          SPOT304 Faster, Cheaper, Safer Products with AWS: Adrian Cockcroft Shares Experiences Helping Customers Move to the Cloud: The great cloud thinker Adrian Cockcroft of Battery Ventures dives into best practices and patterns that he evangelizes and sees emerging from his consulting experiences. Session video: [AWS re:Invent 2015\n          (SPOT304) How Adrian Cockcroft Helped Move Customers to AWS](https://www.youtube.com/watch?v=LMYYJuh9t70) and slides (SPOT304) Faster, Cheaper, Safer Products with AWS.\n        \n      \n    \n  \n  \n    \n      \n        \n          CMP407 Lambda as Cron: Scheduling Invocations in AWS Lambda: Granted this session lost a little luster as AWS launched Lambda Scheduled Events but it was still nice to see the creativity of folks when it came to invoking Lambda functions on a periodic basis. The TLDR; version is they used a CloudWatch alarm notification on a metric that would be either zero, one or unavailable. The notification was sent to a notification topic which triggered the Lambda function every minute (the evaluation time of the CloudWatch alarm). The function would then submit a one or zero to the metric ensuring that another alarm notification would trigger another minute later. Session video: [AWS re:Invent 2015\n          (CMP407) Lambda as Cron: Scheduling Invocations in AWS Lambda](https://www.youtube.com/watch?v=FhJxTIq81AU) and slides: (CMP407) Lambda as Cron: Scheduling Invocations in AWS Lambda.\n        \n      \n    \n  \n  \n    \n      \n        \n          SPOT210 Zynga’s Journey (Back) to the AWS Cloud: Zynga’s CIO shared his experience of migrating both away and back to the cloud. If you were unaware Zynga famously moved out of AWS in the 2011-2012 timeline however they are migrating back fully. The key problems they ran into was maintaining up-to-date hardware and skill sets after the initial datacenter build was completed. Most people interested in building the datacenter weren’t interested in the slow maintenance grind associated with actually running it. It turned out as well that AWS made it easier to track the cost of projects than it was in a datacenter simply because each team could have a sub-account of a master billing account. Zynga learned in a very roundabout way to value of cloud. Session video: [AWS re:Invent 2015\n          (SPOT210) Zynga’s Journey (Back) to the AWS Cloud](https://www.youtube.com/watch?v=BZfhhGvY3c0) and slides: (SPOT210) Zynga’s Journey (Back) to the AWS Cloud.\n        \n      \n    \n  \n  \n    \n      \n        \n          DVO305 Turbocharge Your Continuous Deployment Pipeline with Containers: Amazon had two of their own on hand to demonstrate a tightly integrated, repeatable deployment pipeline using AWS CodePipline, Jenkins and Amazon ECS. They demonstrated some great best practices such as naming the containers with a build ID for easy rollback and how to use multiple CodePipeline transition points to build, test and deploy to production a fully configured container. Session video: [AWS re:Invent 2015\n          (DVO305) Turbocharge Your Continuous Deployment Pipeline with Containers](https://www.youtube.com/watch?v=o4w8opVCI-Q) and slides: (DVO305) Turbocharge YContinuous Deployment Pipeline with Containers.\n        \n      \n    \n  \n  \n    \n      \n        \n          ARC309 From Monolithic to Microservices: Evolving Architecture Patterns in the Cloud: This was one of my favorite breakouts this year because it began with Adrian Trenaman, SVP, engineering of Gilt Groupe, giving a real world example of how the company Gilt slowly migrated from monolithic services to mainly using microservices. The session finished up having Derek Chiles, a manager in solutions architecture for AWS, highlight some great microservice patterns that AWS and Gilt developed together. Session video: [AWS re:Invent 2015\n          (ARC309) Microservices: Evolving Architecture Patterns in the Cloud](https://www.youtube.com/watch?v=C4c0pkY4NgQ) and slides: (ARC309) Getting to Microservices: Cloud Architecture Patterns.\n        \n      \n    \n  \n  \n    \n      \n        \n          SEC314-R AWS Config/Config Rules: Use AWS Config Rules to Improve Governance over Configuration Changes to Your Resources: AWS Config and Config rules is like AWS’s answer to Netflix’s Edda and Conformity Monkey tools. I love these tools (both AWS and Netflix’s offerings) because you can enforce convention and track change and relationships of your infrastructure even after it is gone. These records can really save your hide if for instance you accidentally deleted your continuous integration server which had an old kernel boot image that you have no idea which id it was. If you have no idea what I just said, you might want to enable AWS Config so that on that dark day when you need to know, it’ll tell you! Config Rules are the latest example of how AWS takes two services, Lambda and AWS Config in this case, and makes a great combination to better enable the consumer to just focus on what they need to get their job done. Session video: [AWS re:Invent 2015\n          (SEC314-R) New! AWS Config Rules: Improve Governance Over Configuration Changes](https://www.youtube.com/watch?v=sGUQFEZWkho) and slides: (SEC314) AWS for the Enterprise: Implementing Policy, Governance & Security.",
      "Conclusion",
      "AWS re:Invent 2015 was an excellent showcase and exploration of what is happening in the cloud.",
      "Continuous Deployment is the maturation of DevOps. I am looking forward to exploring how CD methods and as well new microservice patterns will help drive forward iterative search platform enhancements. Please get in touch to talk about how OSC can help with your search application environment."
    ],
    "summary_t": "AWS’s biggest event of the year is always good for refined ideas. Let’s review a few of Joe’s favorite practices, sessions and lessons learned from this year..."
  },
  {
    "id": "6d6555b2a555617e1bd3438e09b8b293",
    "url_s": "https://opensourceconnections.com/blog/2015/11/23/quepid-migrates-from-flask-to-rails/",
    "title": "Quepid Migrates from Flask to Rails",
    "content": [
      "Something has been cooking",
      "We recently worked on moving Quepid from a Python backend to a Ruby backend, something Joel Spolsky would frown upon:",
      "\"They did it by making the single worst strategic mistake that any software company can make:\n\n  They decided to rewrite the code from scratch.\"\n\n  – Joel Spolsky, referring to the Netscape rewrite that supposedly led to their doom.",
      "source: http://www.joelonsoftware.com/articles/fog0000000069.html",
      "So why did we go on and do something so \"dangerous\", and arguably \"stupid\"? The answer to that requires a little bit of history.",
      "Why we built Quepid",
      "OSC’s mission is to help clients build the best search experience tailored for their customers and users. And just like any builder, you are as good as the tools you have at your disposal.",
      "To be honest, the tools available for a search engineer are not so great. One of the nagging problems commonly faced is debugging why the search engine returned a specific set of results which did not match the expectations. So Splainer was born. Splainer helped developers understand what was driving the results and why.",
      "Search, though, is not just a developer problem. Search is a conversation between multiple parties. So although Splainer’s addition to the tool belt helped make the job easier for developers, something else was needed to help the conversation between the developers and the domain experts. The domain experts are the ones who know if the results set returned by the search engine. They are the ones who have the knowledge and expertise to judge the results of the search, so how can we bring that knowledge to the process and workflow that the developers follow? That’s where Quepid comes in.",
      "Tech Stack",
      "Splainer was built with AngularJS, and the reason behind it is very simple. Splainer needed to be visual, not a command line tool, in order to make it easy to decipher cryptic debugging messages coming from Solr (and later Elasticsearch). The browser is the obvious choice for that. Splainer also needed to be able to communicate to clients’ search engine regardless of whatever security is employed that might block requests to the servers. So with an in browser app, as long as the user is running it from within the firewall (or VPN), the app can then access the search engine, without being blocked (which wouldn’t be the case if the app is running on servers outside of the firewall). So AngularJS made sense for that, specially since Splainer did not save any data, in fact it did not have a database at all. It was purely a front-end, single page application that ran in the browser. And that worked very well.",
      "Quepid needed to leverage the power of Splainer, and add the missing pieces that were needed to integrate the domain experts into the flow. So it needed to have an AngularJS component, to be compatible with Splainer, as well as something to keep track of the judgments made by the users. So a backend with a connection to a database are now required.",
      "Since the majority of the application runs in one screen, the case screen, and that is heavily controlled by Angular, Quepid needed a backend that was light and could work with Angular in a nice way. It did not need a fully fledged MVC framework, but a light weight API. And since Quepid at the time was mostly used internally at OSC, and primarily by its creator, Doug, he picked the language he was most familiar with, which is Python, and Flask which provided a good starting point for a light weight backend.",
      "So to recap, these were the primary languages, frameworks, and dependencies for Quepid:",
      "Javascript\n    \n      npm\n      bower\n      Angular JS\n    \n  \n  Python\n    \n      Flask\n    \n  \n  MySql",
      "And that worked, for a while…",
      "Evolution of Quepid",
      "Not long after its creation, people started showing interest in Quepid. Which made us happy. But also meant that whatever used to work in a hacky way for OSC might not cut it as an enterprise solution where people pay money to use the service.",
      "",
      "Quepid needed more features, like [Organizations] (/blog/2015/08/25/quepid-gets-organized/) to share cases with other members. And as the feature set grew, so did the complexity of the application. Until we reached a point where the current stack was no longer conducive to developers being productive and fast.",
      "During development, we hit a number of road blocks that had us say something like \"This would be easier if we were using Rails\", or \"Rails takes care of this for us\".",
      "After a number of these, we decided it was time. It was time to say thank you to Flask, you served us well, but our journey together ends here.",
      "There were a bunch of cases where the tools in Flask weren’t as mature as those in Rails (eg. managing migrations). But mostly, the thing that probably convinced us the most that we should make the switch is the integration between Angular and the backend. With Flask, we almost always had to duplicate the code between the Python side and the Javascript side. Services in Angular had to mimic behavior in the Python APIs and services. Whereas on the Rails side, there are Ruby gems that exist to bridge between the two parts of the application.",
      "And as one very wise man once said:",
      "\"No code is more optimal than no code. No code is faster than no code. No code is more maintainable than no code.\"\n\n  – Aaron Patterson",
      "So we jumped on the opportunity to be able to delete code!",
      "The key to a migration like this is to pick a scope, and stick to it. We did not do a full rewrite (I can hear Joel give a sigh of relief here). We had a functioning application that people use and have become accustomed to. Our goal is to make the codebase more maintainable and easier to add new features to. So the scope we picked was to replace the backend, and only the backend. That is very important. It defined what the task is, and limited the possibility to venture on a long and never ending rewrite.",
      "There were many instances when the thought \"Ouuhh, let me refactor that\" would come to mind, but one must resist.",
      "",
      "The truth about code is that it might never be good enough. There will always be ways to improve it. But the budget and time constraints make that a tricky proposition. And the reality is that if we keep it up to the developer to control his desires, the dark side will eventually prevail. Which is why setting the scope ahead of time was key to making this process a successful one.",
      "What helped the transition from the Flask backend to the Rails backend is that we had previously done a reorganization of the code to make the APIs RESTful (as much as possible). We also had good test coverage that touched most of the aspects of the API. So, to a certain extent, replacing the APIs would be a smooth transition since not much from the Angular side, which is still the biggest piece of the app, had to change.",
      "The stack now became the following:",
      "Javascript\n    \n      npm\n      bower\n      AngularJS\n    \n  \n  Ruby\n    \n      Ruby on Rails\n    \n  \n  MySql",
      "Although there are still redundant pieces (eg. asset management in Rails and Bower), and parts that can be improved and cleaned up, this switch provides a solid basis for improving the codebase in the future.",
      "What’s in it for me?",
      "\"Great, good job guys. But what’s in it for me?\" you may ask. Well, that’s a great question.",
      "Rewriting an application is not easy. It requires a lot of effort and time, and if you ask Joel, it’s the single worst strategic mistake a dev team can make. So why did we do it? We did it so that we can better serve our customers.",
      "There are lots of things we want to do with Quepid. Lots of cool new features we want to add. Things that we hope you will love, things that will make you want to use Quepid more and more. We are excited about these features because we are all users of Quepid at OSC. We believe that Quepid is the best tool for relevancy tuning out there, and we depend on it as much as our clients do.",
      "So in order for Quepid to be the best it can be we need to make sure that developers can maintain the application, and fix any bugs that occur at a fast pace. And when it’s time to add something new, it should be a joy to work on the Quepid code base, not scary.",
      "So although you may not notice anything different in the app at the moment, we think that you will enjoy the progress that is coming in the near future. We hope you are as excited about what’s to come as we are.",
      "Happy Tuning!"
    ],
    "summary_t": "We recently worked on moving Quepid from a Python backend to a Ruby backend, something Joel Spolsky would frown upon.   Here’s why."
  },
  {
    "id": "38ddf8993fed8ff4ef28d2ff202a9bf3",
    "url_s": "https://opensourceconnections.com/blog/2015/12/15/solr-vs-elasticsearch-relevance-part-one/",
    "title": "Solr vs Elasticsearch for Relevancy -- Controlling Matching",
    "content": [
      "This article summarizes parts of Relevant Search. Use discount code turnbullmu to get 38% off!",
      "</a>",
      "Solr vs Elasticsearch! There’s been a TON of posts. But has anyone ever compared them when it comes to traditional – you know – controlling regular ole search results!?",
      "In my work, I spend a lot of time helping people improve the relevance of their Solr/Elasticsearch search results (I’m writing a book on the subject!). In this first in a series of articles, I want to help you see the forest for the trees when it comes to deciding between these two technologies for traditional search problems. Problems where you’re likely to need to tune the relevance of the results for your needs. Frankly, you probably need to do this, even if you don’t think you do. Everyone’s headed for that search bar, is your search going to \"get\" them or just return a random set of 10 results?",
      "Anyway, evaluating Solr vs Elasticsearch with regards to relevance comes down to comparing them across three criteria:",
      "Ability to control matching – what results can be said to match/not match the search?\n  Ability to control ranking – within the set of matched results, what results are most relevant?\n  Ability to create plugins – how deeply can you manipulate the matching/ranking of results beyond APIs?",
      "In this blog article, I’m going to begin with item 1: comparing how these search engines let you manipulate matching.",
      "More Alike Then Different",
      "Before we begin to dive in, it’s important to note that Solr and Elasticsearch are in many ways more alike than different. Both search engines give you a tremendous amount of ability to manipulate search relevance at a very fundamental level. Being open source search engines based on Lucene, they sit apart from black-box commercial offerings that limit how much relevance tuning you can do. They both support an extremely wide array of use cases from traditional document based search, to geo-aware, graph search, to well you name it. Both provide rich query DSLs and text analysis DSLs that let you tightly control the matching and ranking process. This is why you can almost entirely apply our book Relevant Search to either search engine just fine!",
      "That being said, while the core features are the same, the ergonomics and the APIs can make your search solution easy or hard! Let’s dive into matching, to see how one search engine makes controlling matching rather straight forward, while another one requires deeper thinking and engineering work.",
      "Solr vs Elasticsearch Controlling Matching",
      "Controlling matching comes down to fine tuning how text transforms into individual tokens. The search engine treats tokens as the fundamental, atomic entity that must compare exactly for a \"match\" to be declared. You need a normalization process to get tokens from a search to match tokens from a document. For example, a search for all uppercase \"CAT\" will not match an all lowercase \"cat\" without the proper text manipulation steps to lowercase both sides. A less silly example is deciding whether you should treat \"kitty\" as a synonym for \"cat.\" Should a \"kitty\" search term be normalized to \"cat\"? When you think of all the many forms of english words (run, running, ran?) you can see how important this normalization process is for deciding exactly when to match and when not to match.",
      "This normalization task is controlled by you via a feature known as analysis (as we’ve discussed). Analysis controls the process for manipulating text from documents/searches are transformed into tokens. The art of search relevance often boils down to controlling this process to carefully discriminate between matches/non matches.",
      "Both Solr and Elasticsearch support the underlying library of Lucene analyzers. The ergonomics of creating an analyzer differ only superficially. Solr (until recently) encourages you to work in an XML configuration file (schema.xml) to control this process. Elasticsearch, on the other hand, give you a RESTful JSON API for doing the same work.",
      "Both search engines give you the same ingredients to work with to build analyzers. You manipulate the original character stream through several character filters. You break up the character stream into tokens using a tokenizer. Finally you apply a configurable sequence of token filters to trim, slice, delete, generate, and otherwise manipulate the tokens.",
      "To tightly control this process, in addition to analyzing the content placed in the search engine, both Solr and Elasticsearch let you specify a seperate query analyzer. Recall the query analyzers transforms the search string to tokens to control how they’ll match against tokens generated from text placed in the index. This is great, for example, if you wanted to expand your search out to include a list of extra synonyms to search for in addition to the original search query.",
      "Solr, however, comes with two serious deficiencies with query analysis. The biggest source of heartache is the so-called sea biscuit problem. To make a long story short, Solr’s default query parser breaks up text by whitespace before passing it to a query time analyzer. So if you have a synonym rule that maps the phrase \"sea biscuit\" to a single word seabiscuit, it doesn’t work!",
      "The reason is that each whitespace delimited query term is analyzed entirely separately, oblivious of the remaining text in the query string. The query-time analyzer takes as input the single word [sea]. The synonym filter does not see the token [biscuit] following. The problem isn’t just limited to synonyms, it impacts any process that needs to manipulate more than one token at a time. This can be rather limiting and surprising when trying to control the matching process. Queue sad trombone.",
      "This problem is constantly the source of surprise,  relevance bugs, and user group discussion. Even moderately skilled Solr users are surprised by this behavior. The situation is not entirely hopeless. There exist numerous Solr plugins for this issue. Solr also gives you a lot of power to write just about any plugin to deeply control this yourself. Not everyone though wants to figure out someone else’s plugin or deal with writing Java.",
      "Solr’s other problem is that it only lets you control the query side analyzer in a field’s configuration. Elasticsearch lets you control the analyzer to use at just any level, including passing the analyzer to use when running a query itself! Elasticsearch’s options are quite broad.",
      "At search time, the sequence is slightly different:\n* The analyzer defined in the query itself, else\n* The analyzer defined in the field mapping, else\n* The default analyzer for the type, which defaults to\n  * The analyzer named default in the index settings, which defaults to\n  * The analyzer named default at node level, which  defaults to\n  * The standard analyzer",
      "This comes in very handy. Say for example, you’d like to query one field sometimes by using synonyms and another way without synonyms. Elasticsearch lets you query the same field different ways simply by passing an analyzer argument with the query. Sometimes you’d pass in the synonym analyzer, other times you might simply use the default. Solr, on the other hand, would have you duplicate content to another field to pick up a different query analyzer.",
      "In conclusion, Elasticsearch is the clear winner when it comes to matching. Elasticsearch gives you the least amount of surprise and most depth of configurability when it comes to manipulating matching. You can do quite a lot without resorting to writing plugins.",
      "Winner:",
      "",
      "Next Time! Ranking!",
      "Next time, we’ll discuss how Solr and Elasticsearch’s query APIs match up. Will Elasticsearch be dominant here as well? To complete the trilogy we’ll also compare how plugable each search engine is. Which search engine will win? :)",
      "And of course if you need help with a tough search problem, don’t hesitate to contact us!"
    ],
    "summary_t": "Solr vs Elasticsearch! There’s been a TON of posts. But has anyone ever compared them when it comes to traditional – you know – controlling regular ole searc..."
  },
  {
    "id": "a9e21ab1eee9b0a9240dac4237f5a301",
    "url_s": "https://opensourceconnections.com/blog/2007/05/03/openwfepy-and-dynamic-language-toolkit/",
    "title": "OpenWFEpy and Dynamic Languages Toolkit",
    "content": [
      "As I noted in an earlier post, I just started a project called OpenWFEpy. Its a Python port of the OpenWFE workflow engine, and if youd like to be on the mailing list just let me know.",
      "In between bouts of manually converting the files, Ive been searching for some automation I can use to help me. One possibility I explored was creating a Ruby parser using Antlr. For whatever reason that didnt prove as fruitful as Id hoped, but Andrew left me an interesting comment regarding an Eclipse plugin project called Dynamic Languages Toolkit (DLTK).",
      "As the name implies, DLTK is designed to be a common framework from which to build dynamic language IDEs. If youve tried to use Eclipse for anything other than Java programming, youve probably witnessed widely varying feature sets between language plugins. Its API bridges the gap between the Eclipse framework and a language parser, so once you have a parser built and plugged into the API, all the cool features of a great IDE are immediately available to you.",
      "Included in the toolkit is a Ruby IDE that they built on their API. They didnt include the grammar, but as long as the parser isnt coupled too tightly I could either use it stand-alone or add some sort of \"Convert to Python\" refactoring. This path seems a little more promising than building from scratch the parser Ill need, but as with the earlier spike, if I spend too much time getting to know the tools Ill have less time to do the actual work."
    ],
    "summary_t": ""
  },
  {
    "id": "88515ec09a65c1f50df2cf8f60813226",
    "url_s": "https://opensourceconnections.com/blog/2015/12/22/exploring-custom-typecodecs-in-the-cassandra-java-driver/",
    "title": "Exploring Custom TypeCodecs in the Cassandra Java Driver",
    "content": [
      "TL;DR check out the source on GitHub. For more information dig in to the explanation below.",
      "While working with the C* Java Driver I keep running into error messages when writing to timestamp fields. This issue usually pops up when I bind a value in my insert statement to a Joda DateTime object. The driver accept this variable and throws an exception while serializing it for execution. Here’s some sample code to illustrate the behavior:",
      "session.execute(\"INSERT INTO my_table (partition_key, some_timestamp) VALUES (?, ?)\", \"bar\", DateTime.now());\n\n// Exception in thread \"main\" com.datastax.driver.core.exceptions.InvalidTypeException: Value 1 of type class org.joda.time.DateTime does not correspond to any CQL3 type",
      "This is understandable, the Java driver doesn’t know how to take my object and serialize it. I could convert all of my DateTime objects into Java Date objects with dateTime.toDate(), but this feels a bit clunky:",
      "session.execute(\"INSERT INTO my_table (partition_key, some_timestamp) VALUES (?, ?)\", \"bar\", DateTime.now().toDate());\n// YAY THIS WORKS, but ALL dates need to be converted.",
      "Today the Java driver has a new version in release candidate 2.2.0. Among other things this release features a new system for serializing / deserializing custom objects into native CQL types. Finally I can insert my DateTime objects! Let’s take a look at how the new TypeCodec system works and implement a simple DateTimeCodec.",
      "First change the version of the Java driver to at least 2.2.0-rc3.",
      "<dependency>\n  <groupId>com.datastax.cassandra</groupId>\n  <artifactId>cassandra-driver-core</artifactId>\n  <version>2.2.0-rc3</version>\n</dependency>",
      "Now let’s try our sample query again.",
      "session.execute(\"INSERT INTO my_table (partition_key, some_timestamp) VALUES (?, ?)\", \"bar\", DateTime.now());\n\n// Exception in thread \"main\" com.datastax.driver.core.exceptions.InvalidTypeException: Value 1 of type class org.joda.time.DateTime does not correspond to any CQL3 type\n// Caused by: com.datastax.driver.core.exceptions.CodecNotFoundException: Codec not found for requested operation: [ANY <-> org.joda.time.DateTime]",
      "There’s some new output in our exception indicating a Codec could not be found. Let’s implement one! A TypeCodec has four methods that must be implemented parse(), format(), serialize(), and deserialize(). In the TypeCodec.java file there is a TimestampCodec defined which shows how the driver expects a timestamp returned. We can base our class on a few of the methods here, but much more simplified.",
      "First up is our constructor it calls the super class indicating the CQL DataType and Object class our codec handles. Note in our examples the class is DateTimeCodec.",
      "public DateTimeCodec() {\n  super(DataType.timestamp(), DateTime.class);\n}",
      "There are other TypeCodec classes that may be extended, like TypeCodec.StringParsingCodec<T>. In our case we are extending TypeCodec<T> with T specified as DateTime from the Joda Time library. Now that we have signaled the CQL and Java data types our code handles it is time to get to work.",
      "The first method to implement parses a String value and returns a DateTime object. Naturally this would be the parse() method. Looking at the TimestampCodec implementation and JavaDocs for TypeCodec we can see the expected input values. Our method should handle valid data from CQL along with null and \"NULL\". Instead of doing a bunch of parsing like the TimestampCodec we will just delegate to the DateTime.parse() method instead.",
      "@Override\npublic DateTime parse(String value) {\n  if (value == null || value.equals(\"NULL\"))\n    return null;\n\n  try {\n    return DateTime.parse(value);\n  } catch (IllegalArgumentException iae) {\n    throw new InvalidTypeException(\"Could not parse format: \" + value, iae);\n  }\n}",
      "With parse() implemented it is time to explore the format() method. format() takes an instance of DateTime and converts it to a CQL literal value. null values should return the String \"NULL\". In our case the timestamp is internally stored as a long since epoch. On a DateTime object we may retrieve this value with a call to getMillis(). This in turn gets converted to a String and returned.",
      "@Override\npublic String format(DateTime value) {\n  if (value == null)\n    return \"NULL\";\n\n  return Long.toString(value.getMillis());\n}",
      "So far our methods have been pretty simple. Let’s get into the really interesting parts of TypeCodec, the serialize() and deserialize() methods! First up we have serialize() method. This takes the Java value and serializes it into the appropriate value for the given CQL type. A protocolVersion is also supplied to help inform us of how to pack this value. In our case there isn’t any complex logic here. If the value is null return that, otherwise convert the value to a long or BIGINT in CQL. Fortunately the encoding of BIGINT values is handled elsewhere in the driver and we can reuse that code. Check out the implementation below.",
      "@Override\n public ByteBuffer serialize(DateTime value, ProtocolVersion protocolVersion) {\n   return value == null ? null : BigintCodec.instance.serializeNoBoxing(value.getMillis(), protocolVersion);\n }",
      "Deserializing the data is just as simple with the following code:",
      "@Override\npublic DateTime deserialize(ByteBuffer bytes, ProtocolVersion protocolVersion) {\n  return bytes == null || bytes.remaining() == 0 ? null: new DateTime(BigintCodec.instance.deserializeNoBoxing(bytes, protocolVersion));\n}",
      "Now with all the methods in our custom codec implemented it’s time to take it for a spin. In our application we get an instance of the Cluster and register our custom codec.",
      "// Setup the cluster connection\nCluster cluster = Cluster.builder().addContactPoint(\"127.0.0.1\").build();\n\n// Configure our DateTimeCodec\nCodecRegistry codecRegistry = cluster.getConfiguration().getCodecRegistry();\ncodecRegistry.register(new DateTimeCodec());",
      "Now whenever we pass a DateTime object in for a TIMESTAMP field the driver knows how to convert it. Our code from earlier now inserts without exceptions!",
      "session.execute(\"INSERT INTO my_table (partition_key, some_timestamp) VALUES (?, ?)\", \"bar\", DateTime.now());\n// SUCCESS",
      "This is awesome for insert statements, but what about reads? What if we want to read a timestamp from Cassandra and have a DateTime returned instead of a java.util.Date? That’s easy! Since we have already registered a codec for parsing values and converting them to / from the native CQL type the driver knows what to do. We just need to let it know that we’re interested in that particular type. This can be accomplished with the get() method on a Row. Our first argument is the name of the column to retrieve (or index), then we specify the class we would like returned.",
      "ResultSet results = session.execute(\"SELECT * FROM my_table WHERE partition_key = 'foo'\");\nfor (Row row : results) {\n  DateTime value = row.get(\"some_timestamp\", DateTime.class);\n\n  System.out.println(value);\n  System.out.println(value.toLocalDate());\n}\n\n// 2015-12-21T12:11:40.723-05:00\n// 2015-12-21\n// ANOTHER SUCCESS!",
      "We have successfully implemented a TypeCodec for converting CQL timestamp objects into Joda Time DateTimes. This could be extended out to cover other types as well. Some examples from the driver’s feature description include serializing and deserializing JSON from a String field or parsing a UDT value into a custom POJO. TypeCodecs appear to be very powerful with quite a bit of flexibility. I’m looking forward to seeing how other in the community leverage this feature to expand on Cassandra’s already powerful capabilities.",
      "If you are looking for help with a tough Cassandra or Spark problem, let us know!"
    ],
    "summary_t": "Explore the new TypeCodec system in the C* Java Driver"
  },
  {
    "id": "ae7756eb09f4eff74f67bbafd0d75c30",
    "url_s": "https://opensourceconnections.com/blog/2015/12/22/what-does-an-osc-relevancy-tuneup-look-like/",
    "title": "What Does an OSC Search Relevancy Tuneup Look Like?",
    "content": [
      "So you’ve stood up Solr or Elasticsearch. Your search app is humming along. However, how do you know your search understands what your users are asking for? Users are notorious for ignoring everything on your page other than the search bar. They go straight to the search bar and tell you what they want. With just as much speed, they give up and leave when they can’t find what they want.",
      "Understanding what users want is the problem of search relevancy. This is where we come in. We’ve written the book on the topic. Many clients with search well in hand simply need a leg up in the relevancy front. For these clients we offer quarterly relevance tuneups on their search.",
      "What do these tuneups look like? What will you get from participating in one?",
      "Well every tuneup varies a little, depending on your needs and requirements. But I want to lay out the general lay of the land.",
      "Go / No Go Assessment",
      "On your very first tuneup, we meet to make sure this model can be succesful for you. Most importantly, we determine whether a Q Score – a numerical measurement of whether users are satisfied with search – can be applied to your search. We evaluate whether our tuning tool, Quepid, built to measure your Q Score, is appropriate to your problems. We determine whether there’s a safe relevancy test environment that can be used for experimentation. Perhaps most importantly, we help coach the business and content owners – engaging them to be involved in the process from day one.",
      "Once we’ve pinned down how and whether this can be measured, we move to the next discussion: targeted problems to solve with immediate return on investment. You’ll help us select which problems to tackle during the tuneup. We choose carefully, selecting what can be done during the two week tuneup. However, you’d be surprised what can be done in 80 hours of tuning work!",
      "Tuneup",
      "Using the Q Score established in this assessment, our consultants get to work! With the Q Score the tuneup team can begin pushing the dial towards better search results. With Quepid they measurably push the ball forward and never backwards. You see precisely which queries are improving and where trouble spots arise. Armed with the power of Quepid, our team iterates on new ideas and solutions in Solr or Elasticsearch. Over two weeks of time we make a measurable and significant improvement to your search quality.",
      "The tuneup phase remains highly collaborative. The technical consultant works most efficiiently paired with business or domain expertise. As problems crop up non-technical representatives of the user or business provide immediate guidance to the direction of search tuning. This might mean working closely with your merchandisers, marketing department, user experience team, content curators, or other domain experts.",
      "Many clients don’t yet engage their business in how search should work. For these clients we help by bringing in our own content strategists to focus the team, coach them on evaluating content, and help them understand how users want to interact with content.",
      "At the conclusion of the tuneup you’re left with a set of specific, targeted Solr and Elasticsearch modifications to run with. You implement the strategies and roll them into your application.",
      "##Interregnum\nAfter the tuneup, our team writes a brief set of recommendations for activities to complete before the next tuneup. These activities entail anything that would be an invaluable asset during the next tuneup. For example, they might include recommendations for:",
      "Tracking user metrics in your search application. Determine where users are satisfied and dissatisfied in production\nSpecific larger projects to build search related assets. Should specific synonyms be curated? A taxonomy/ontology be built? Should an explicit strategy around recommendations are user experience be explored in greater depth?\nSpecific content curation steps. Would certain improvements to content be useful for improving your relevance? Are there ways new forms of content can be more cleanly entered into the search engine? How can existing content be cleaned up?\nSteps to engage the broader business in search. Is search seen as a pure heads down technical problem the business doesn’t interact with? If so, we would recommend e\nSteps to improve the technical teams skill with search. Is specific training warranted?",
      "The aim is to \"give you homework.\" Some of these projects however could be part of additional consulting help we engage with. Regardless, we wish you well and schedule your tuneup for the next quarter!",
      "Schedule your tuneup today!",
      "So if you’re curious about this (or any other) consulting offering, please be sure to get in touch and we’ll happily discuss options with you!"
    ],
    "summary_t": ""
  },
  {
    "id": "ef94d63db06f918b61ebeaee9d9a9a0b",
    "url_s": "https://opensourceconnections.com/blog/2016/01/11/java_beans_and_solrj_and_realtime_get_oh_my/",
    "title": "JavaBeans and SolrJ and Realtime Get Oh My!",
    "content": [
      "A project that I’m working on was previously using AWS DynamoDB for state tracking of processing workloads.  We got, very late in the project, the mandate to move away from DynamoDB.  Don’t ask ;-)",
      "Since we already had Solr in our toolkit, I moved our job tracking objects from being backed by DynamoDB to being backed by Solr.   The JobTracker pojo was already using annotations for persisting into DynamoDB, so adding on the Solr @Field annotation for each property was really easy.",
      "Here is our job tracking object JobTracker, ready to be persisted into either DynamoDB or Solr:",
      "@DynamoDBTable(tableName = \"JobTracker\")\npublic class JobTracker {\n\n  @DynamoDBHashKey(attributeName = \"Id\")\n\t@Field(\"id\")\n\tprivate String id;\n\n  @DynamoDBAttribute(attributeName=\"JobConfig\")  \n\t@Field(\"jobConfig\")\n\tprivate String jobConfig;\n\n  @DynamoDBAttribute(attributeName = \"Status\")\n\t@Field(\"status\")\n\tprivate String status;",
      "We save the object into Solr via an instance of SolrClient:",
      "jobTrackerSolrClient.addBean(jobTracker);",
      "We load the object back from Solr via:",
      "SolrQuery q = new SolrQuery();\nq.set(\"q\",\"id:\" + objectId);\nq.set(\"fl\",\"*\");\nQueryResponse queryResponse = jobTrackerSolrClient.query(q);\nList<JobTracker> foundDocuments = response.getBeans(JobTracker.class);\nreturn foundDocuments.get(0);",
      "However, the first wrinkle that came up was we had situations where multiple job processors would do a select against the JobTracker collection for PENDING jobs, and return the same document, and start processing it in parallel.  So it was time to enforce only one job tracker could update the status to RUNNING from pending.  This is easily done via Solr’s optimic versioning.",
      "The first challenge was simple, I needed to add a Solr specific annotation to my JobTracker pojo for the _version_ field:",
      "public class JobTracker {\n\n  @Field(\"_version_\")\n  private Long version;\n\n  public Long getVersion() {\n    return version;\n  }\n\n  public void setVersion(Long version) {\n    this.version = version;\n  }",
      "If the _version_ field doesn’t match on the update, then Solr returns a HTTP 409 code.  To deal with this scenario, I had to wrap the addBean call:",
      "public void save(JobTracker jobTracker) throws IOException,\n\t\t\tSolrServerException, VersionConflictException {\n\t\ttry {\n\t\t\tjobTrackerSolrClient.addBean(jobTracker);\n\t\t} catch (RemoteSolrException rse) {\n\t\t\tif (rse.code()==409){\n\t\t\t\tthrow new VersionConflictException(jobTracker.getId());\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow rse;\n\t\t\t}\n\t\t}",
      "The VersionConflictException object just signals to my application that the update failed.  It’s a very simple class that just stores the objectId of what you tried to persist to Solr.  I did debate storing the object as part of it, but couldn’t come up with a good reason.",
      "public class VersionConflictException extends Exception {\n\n\tprivate static final long serialVersionUID = 1618255109313434652L;\n\tprivate String objectId;\n\n\tpublic VersionConflictException(String objectId) {\n\t\tsuper();\n\t\tthis.objectId = objectId;\n\t}\n\n\tpublic String getObjectId() {\n\t\treturn objectId;\n\t}\n\n}",
      "This all looked really great till I realized, in writing my unit test, that since my JobTracker pojo is very long lived, that once I saved it once, the second time it saved it would trigger the VersionConflictException because the save method in Solr, a call to /update request handler, doesn’t return the new _version_.",
      "JobTracker jobTracker = new JobTracker();\nJobTracker.setId(\"testId\" + System.currentTimeMillis());\nJobTracker.setStatus(JobStatus.PENDING);\n\nassertNull(jobTracker.getVersion());\n\nmapper.save(jobTracker);\n\njobTracker.setStatus(JobStatus.RUNNING);\nmapper.save(jobTracker)[ // BOOM, VersionConflictException",
      "Argh.   Nothing is ever easy.",
      "Okay, so this is what I get for using Solr as a database.   So on to the use of the real time get request handler.   This would let me quickly get back the _version_ field.   The call is actually quite simple:",
      "SolrQuery q = new SolrQuery();\nq.setRequestHandler(\"/get\");\nq.set(\"id\", id);\nq.set(\"fl\", \"*\");\nQueryResponse response = jobTrackerSolrClient.query(q);",
      "However, that is when I ran into one of the assumptions behind SolrJ, which is that if you are using JavaBeans, then you are working with the standard data structure returned by doing a /select?q=*:* type of query.",
      "I needed the /get request handler to do the real time get query, and the results format is slightly different.  So doing response.getBeans() was blowing up.  Again ARGH.",
      "So I did a bit of splunking in QueryResponse.java,  and discovered a hitherto unknown Java class, DocumentObjectBinder.   This class basically takes care of mapping from Solr objects lik SoldDocument and SolrDocumentList to pojos.   I got very lucky and copying the code out of SolrJ to my project it just worked:",
      "SolrDocumentList sdl = new SolrDocumentList();\nSolrDocument doc = (SolrDocument)response.getResponse().get(\"doc\");\nif (doc != null){\n\tsdl.add(doc);\n}\n\nif (!sdl.isEmpty()){\n\tDocumentObjectBinder dob = new DocumentObjectBinder();\t\t\t\n\tList<JobTrackingRecord> foundDocuments = dob.getBeans(JobTrackingRecord.class,sdl);\n\treturn foundDocuments.get(0);\n}\nelse {\n\treturn null;\n}",
      "Now, when I save a long lived pojo, I just do a realtime get to update the version, so I can keep doing saves:",
      "jobTracker.setVersion(loadJobTracker(jobTracker.getId()).getVersion());",
      "So far it seems to be working very well.",
      "Update!  After writing this blog, I dug around in the real time get code base, re-read the wiki, and discovered that if my parameter was ids, versus id, then the results are returned in the standard format, and I can use the response.getBeans() method.  Oh well.  Still glad to learn about DocumentObjectBinder"
    ],
    "summary_t": "SolrJ is a really great client library, until you try to use some of the newer features of Solr, and have to jump through some hoops."
  },
  {
    "id": "bedb6f23e64586430948e2a42da801fc",
    "url_s": "https://opensourceconnections.com/blog/2016/01/18/jruby-kafka-v2-0-v1-5/",
    "title": "JRuby Kafka v2.0 and v1.5 released",
    "content": [
      "The stars aligned December 31, 2015 and jruby-kafka got two releases v2.0 and v1.5 to celebrate and end the year on a high note. Both versions are now available via rubygems.org. A great part of jruby-kafka is that it uses Kafka’s native JARs to deliver the most feature complete Kafka implementation in the Ruby language. The gem files include all JARs necessary so just gem install jruby-kafka, require 'jruby-kafka', and away you go. The most recent updates were primarily community contributions. Thank you to Nicolas Fraison for helping get v1.5 ready which maintains compatibility with the previous v1.X, fixes a callback bug and includes the latest 0.8 point release for Kafka (0.8.2.2). See #37 update to fakfa 0.8.2.2 for more details. Thanks to Elias Levy v2.0 introduces a more parallel friendly consumer.",
      "jruby-kafka is used by two official Logstash plugins: Kafka input and Kafka output.",
      "As always pull requests and suggested enhancements are always welcome. I’m working on a Travis CI build to improve the initial PR quality. The jruby-kafka build will improve the quality of the project over time."
    ],
    "summary_t": "jruby-kafka now supports Kafka 0.8.2.2 releasing v2.0 and v1.5"
  },
  {
    "id": "823f068c330ed58fc3a28a9d65f6f3eb",
    "url_s": "https://opensourceconnections.com/blog/2016/01/22/solr-vs-elasticsearch-relevance-part-two/",
    "title": "Solr vs Elasticsearch for Relevancy: Battle of the Query DSLs",
    "content": [
      "This article summarizes parts of Relevant Search. Use discount code turnbullmu to get 38% off!",
      "</a>",
      "Last time on The Young and The RESTful, aka Elasticsearch vs Solr, we took a look at the two search giant’s ability to solve search relevance problems. We discussed how relevance comes down to controlling:",
      "Matching – what should be in/out of the set of results?\n  Ranking – how will the result set be ordered?",
      "Last time we discussed matching, where Elasticsearch was the clear winner. This time, we’ll take a look at controlling ranking. More specifically, we’ll see what happens when the two search engine’s Query DSLs duke it out! Next time, we’ll discuss how deeply you can plug each search engine to customize search relevance.",
      "In this article, I avoid a blow-by-blow feature comparison. At this point in the genre of Elasticsearch vs Solr comparisons, the blow-by-blows quickly get out of date. These two search giants quickly catch up to one another at the feature level.",
      "Rather it’s more useful to see the forest for the trees here. Think of each search engine’s Query DSL as a search ranking programming language. This is what needs comparing. Each programming language has specific syntactic and semantic quirks you’ll have to deal with in your work. Sure, both search engines interpret your queries into the same Lucene \"machine code,\" but can differ as much as Ruby, Python, or Haskell from each other.",
      "What you’ll find is that Solr is a terse, Perl-like experience. On the other hand, Elasticsearch can feel a bit more like Java’s verbosity mixed with Python’s emphasis on being explicit. To see what I mean, we’ll start with simple cases of individual query construction and move on to more complex formulations.",
      "Individual Query Construction: Is Terse Just The Worst?",
      "Let’s begin to put together basic queries to introduce each search engine’s query DSL. We’ll start by looking at Solr, then comparing roughly equivelant constructions in Elasticsearch’s query DSL.",
      "Above we compared Solr’s Query DSL to Perl. Solr’s Query DSL is like Perl for all the same reasons you love and hate Perl. For Solr diehards, nothing else feels as powerful. Compactness feels good when you’re writing the code. And if you’re steeped in its mysteries, glancing at a query conveys a lot of information without much scanning. On the other hand, just like Perl, other developers not steeped in the language easily feel lost. If you don’t do it everyday, Solr queries can quickly become \"write only,\" – hard to read and maintain.",
      "Solr’s terse syntax originates from its origins as an entirely URL-based query language. It came up in a time in the Web when APIs entirely driven through the URL’s query parameters were in vogue. So Solr tries to fit everything you’d like to say in the URL bar. For simple searches, this makes a load of sense. A URL like http://solr.quepid.com/solr/statedecoded/select?q=text:(dog catcher law) seems fairly intuitive. Adding parameters to that search, such as &q.op=AND also makes sense (here we set the default operator to AND, making the query dog AND catcher AND law). Yet Solr expands on this with its localparams syntax, which scope parameters to a query. As an example, another way to rewrite the last query might be {!lucene q.op=AND}text:(dog catcher law). If you know localparams, this snippet is readable and easy to scan. If you don’t, it seems arcane.",
      "Another aspect of Solr’s query DSL comes into focus in that last bit of localparams code. Solr’s interprets queries using a query parser. Above, we specified the query parser for our query string as lucene using the bang symbol {!lucene…}. Solr comes with a broad range of query parsers, each with their own parameters and syntax. Let’s look at an example using the \"edismax\" query parser. This example searches the provided keywords over two fields (catch_line and text), sums the relevance scores, and only returns results that match 50% of the query terms {!edismax mm=50% tie=1 qf='catch_line text'}dog catcher law. Now that’s quite terse! Each query parser, it should be added, tends to be contributed to Solr by different authors. So while this creates a broad library of query parser, each query parser can have different syntax and parameters.",
      "Contrasting Solr, Elasticsearch ops for more verbosity and readability when building queries. Elasticsearch came up at a time when RESTful, JSON APIs were in vogue. Elasticsearch jives more with our current API sensibilities. You build Elasticsearch queries as JSON objects, as follows:",
      "{\"query\": {\n    \"multi_match\": {\n        \"query\": \"dog catcher law\",\n        \"fields\": [\"text\", \"catch_line\"],\n        \"minimum_should_match\": \"50%\",\n        \"type\": \"most_fields\"\n    }\n}}",
      "This verbosity pays off. It’s much easier for the uninitiated to look at the JSON and guess what’s happening. It’s clear that there’s a query, of type \"multi_match\" being passed a query stirng \"dog catcher law.\" You can see clearly the fields being searched. Without much knowledge, you could make guesses about what minimum_should_match ormost_fields might mean.",
      "It’s also helpful that Elasticsearch always scopes the parameters to the current query. There’s no \"local\" vs \"global\" parameters. There’s just the current JSON query object and its arguments. To appreciate this point, you have to appreciate an annoying Solr localparams quirk. Solr localparams inherit the global query parameters. For example, let’s say you use the following query parameters q=dog catcher law&defType=edismax&q.op=AND&bq={!edismax mm=50% tie=1 qf='catch_line text'}cat (search for dog catcher law, boost (bq) by a ‘cat’ query). Well your scoped local params query unintuitively receives the outside parameter q.op=AND. More frustratingly, with this query you’ll get a deeply befuddling \"Infinite Recursion\" error from Solr. Why? because hey guess what, your local params query in bq also inherits the bq from the outside – aka itself! So in reality this query is bq={!edismax mm=50% tie=1 q.op=AND bq='{!edismax mm=50% tie=1 q.op=AND bq='...' qf='catch_line text'} qf='catch_line text'}. Solr keeps filling in that ‘bq’ from the outside bq, and therefore reports the not so intuitive:",
      "org.apache.solr.search.SyntaxError: Infinite Recursion detected parsing query ‘dog catcher law’",
      "To avoid accepting the external arguments, you need to be explicit in your local params query. Here we set no bq and change q.op to OR.",
      "bq={!edismax mm=50% tie=1 bq='' q.op=OR qf='catch_line text'}",
      "To me, at this atomic query-by-query construction level Elasticsearch is the clear winner. Elasticsearch helps you create queries with few surprises. (Look at the text above, much of it is spent explaining Solr quirks). However if terseness is high on your list, you might prefer Solr once you’ve digested the quirks you’ll encounter.",
      "Composing Queries: Where Terse Wins The Purse",
      "One area, however, where Solr shines is composing queries together. When you’ve advanced in your search knowledge you’ll eventually end up working on more than just one query. You’ll string multiple queries, boosts, and function queries (aka math) to come up with large, complex ranking solutions. Here Solr gives you more powerful higher-level programming constructs. Elasticsearch, on the other hand, focusses on the common use cases. By focussing on JSON, Elasticsearch’s hierarchical queries mean you can repeat yourself quite a bit, say if you want to execute multiple queries at the same time.",
      "What do we mean? Well Solr’s desire for terseness has created features like parameter substitution and dereferencing. These features let you reuse parts of queries in a fairly readable fashion. Moreover, Solr’s function query syntax gives you an extremely powerful function (the query()) function that lets you combine relevance scoring and math more seamlessly than the Elasticsearch equivalents.",
      "As an example, how would you multiply the relevance scores of two queries together? Let’s say a phrase query on your title field, and a normal term-by-term text score on your body field. Well in Solr, here’s how you could do it:",
      "usersQuery=dog catcher law&\nphraseQuery={!field f=catch_line v=$usersQuery}&\ntextQuery={!edismax qf=text v=$usersQuery}&\nq=_val_:\"product(query($phraseQuery),query($textQuery))\"",
      "Now the q parameter here drives the relevance scoring. We place the user’s text query into our usersQuery parameter (something we create). We construct two additional variables phraseQuery and textQuery to execute the query we want to run on our corpus using usersQuery. Finally with q we combine the queries together by multiplying them in a function query (the wonky _val_ syntax).",
      "Now this is something you can’t do in Elasticsearch. Elasticsearch’s function queries sandbox you to a single query’s relevance score. You don’t have access to Elasticsearch’s query DSL for more than a single query to work with the score mathematically. You can only combine text scores through subscribed formulations (Boolean queries, etc).",
      "This is a rather big drawback for power users. The above multiplication example, is a surefire way to have to relevance scores amplify each other very strongly. Elasticsearch’s function queries, in their defense, try to cover the most important use cases where these power user strategies might matter. Yet in the course of developing our book, we noted a couple of times where we wished we had that extra relevance score to play with to demonstrate interesting relevance strategies.",
      "Solr also lets you reuse query parameters. By giving parameters names, you can restate any of the parameters very easily without creating a giant, repetitive JSON query object. For example, if for our Solr query we wanted to also filter out anything that didn’t match our textQuery, well that’s rather simple. We simply apply a filter query that refers to our main query:",
      "usersQuery=dog catcher law&\nphraseQuery={!field f=catch_line v=$usersQuery}&\ntextQuery={!edismax qf=text v=$usersQuery}&\nfq=${textQuery}&\nq=_val_:\"product(query($phraseQuery),query($textQuery))\"",
      "Complex Elasticsearch queries, on the other hand become copy-pasta. You end up with a complex JSON object like below (note this doesn’t exactly replicate the Solr query above, as multiplying query scores isn’t possible). The names you can give Solr sub queries help understand the larger intent of the programmer, even if the individual lego blocks are more terse and harder to read.",
      "{\n    \"query\": {\n        \"filtered\": {\n             \"query\": {\n                \"bool\": {\n                    \"should\": [\n                        {\"match_phrase\": {\n                            \"catch_line\": \"dog catcher law\"\n                        }},\n                        {\"match\": {\n                            \"text\": \"dog catcher law\"\n                        }}\n                    ]\n\n                }\n                \n            },\n            \"filter\": {\n               \"query\": {\n                    \"match_phrase\": {\n                        \"catch_line\": \"dog catcher law\"\n                    }\n                }     \n            }\n      \n        }\n    }\n}",
      "To me, Solr wins here. Its easier to compose queries together in many arbitrary mathematical scoring combinations. Naming parts of queries helps readability. You’re not limited to only using relevance scores in prescribed ways in function queries. Solr enables you to avoid repeating yourself, making for less verbose and easier to read relevance solutions at the \"big picture\" level.",
      "Close to Lucene or Speak the Language of a Query Parser?",
      "We noted earlier how Solr relies on query parsers to interpret user queries and their parameters. Query parsers translate the query parameters into underlying Lucene queries. What’s interesting is that Elasticsearch works very differently. Instead, Elasticsearch’s Query DSL exposes more Lucene primitives to the user. For example in the last query, you saw a lengthy Boolean query",
      "\"bool\": {\n         \"should\": [\n            ...             \n         ]\n    }",
      "Boolean and \"SHOULD\" clauses are very much a Lucene concern. They correspond directly to how Lucene puts queries together. Solr, on the other hand, often hides these details from you in its query parsers. For example, you might not realize that writing q=text:law&bq=catch_line:dog is effectively the same as doing two boolean SHOULD clauses. Solr says \"use a boost query, the query parse will take care of you.\" Elasticsearch says \"learn how SHOULD queries score things, and use that primitive.\"",
      "This to me comes down to preference. In Solr, you tend to put more \"application\" search logic into a query parser. You tend to, as you’ll see next time, write your own query parsers with domain-specific search semantics meaningful to your use case. Elasticsearch on the other hand encourages you to build big JSON queries that look more like Lucene-ese. You have pieces closer to the search-engine metal. But using such lower level features can be challenging to convey semantics meaningful to your search task. Rather these semantic components tend to go in your search application.",
      "So Who Won?",
      "Result: Tie (it Depends)",
      "Hopefully you’ve noticed a lot of \"it depends\" in this discussion. To me there’s no clear winner. If you want a readable search syntax that corresponds to Lucene’s metal, you’ll like Elasticsearch. It’s more explicit. It’s easy to build a mental model of what the search engine’s doing. If you like terse, abstracted semantics. If you build complex queries, the verbosity and low-level detail of Elasticsearch may put you off. Rather you might prefer steeping yourself in Solr-ese.",
      "What I tend to see is that Solr succeeds for more advanced relevance use cases. Elasticsearch tends to create fewer surprises, but it can be harder to \"push the boundaries\" simply through the query DSL. Looking at the trends in the community bare this out as well. At the last Lucene Revolution, Solr seemed to attract far, far more talks on relevance and Information Retrieval. Elasticsearch, on the other hand, still focusses heavily on the analytics side to search. You can deliver strong solutions in either, but perhaps the communities are reaching a fork in the road, with Solr remaining focussed on advanced pure search problems and Elasticsearch going after analytics?",
      "So what did I get wrong? I’m sure I missed something, and I’d love to hear your feedback on this article!",
      "Finally, if you need help choosing between Elasticsearch or Solr for your use case, don’t be shy about getting in touch!"
    ],
    "summary_t": "Last time we discussed matching, where Elasticsearch was the clear winner. This time, we’ll take a look at controlling ranking. More specifically, we’ll see ..."
  },
  {
    "id": "04aab4951c025878d38fae95303a9532",
    "url_s": "https://opensourceconnections.com/blog/2007/05/03/rspec-simplifies-communications/",
    "title": "RSpec simplifies communications?",
    "content": [
      "At CITcon, I learned a lot about RSpec, a Ruby library for writing out specifications that is used in BDD: Behavior Driven Development.",
      "This has got me to thinking about one of the key challenges in software development: communciation. We spend massive amounts of time retelling requirements from one person to another, and translating them from one format to another. A Business Analyst writes up the requirements in a Word document. A developer translates those requirements into unit tests and code, and lastly a Tester verifies the functionality using yet another suite of testing tools! Its no wonder so many software development projects suffer delays.",
      "One of the key points that Martin Fowler made at CITcon was that he expected the roles of Business Analyst and Tester to merge, and the roles of Developer and Tester to merge. If you think about it, it makes sense. The Business Analyst knows best what the results should be, so shouldnt that person be the one testing the code works correctly? And, the Developer is the one who knows (or should know!) what the weakest points in the code are, and so they should be best suited to test the code.",
      "So what does merging roles have to do with RSpec? Easy, RSpec is currently the best lightweight way of writing specifications the way a Business Analyst might, that are runnable tests that a Tester might run. Now, I know that there are lots of ways of poking holes in this statement, but as a thought experiment, Rob, a colleague of mine, and I are going to attempt this, with me filling the role of the merged Business Analyst/Tester, and him as the developer. We need to write a bit of code for a shipping module for an ecommerce site that has somewhat unusual pricing rules. I have written some RSpec specifications that currently fail, and hell write the code until they pass. All without us communicating except via the specifications that Ive written.",
      "I ran rake spec:doc and produced this summary of the specifications I had written:",
      "Shipping for non-combineable products\n- should have the base price for just one\n- should multiple linerarly the base price for more then one\n\nShipping for combineable product\n- should have the base price, and extra fields filled in, but not used for just one\n- should add shipping_factor_second to shipping base for two items\n- should add shipping_factor_third to shipping_factor_second to shipping_base for three items\n- should keep adding on the third_shipping_factor for any additional items beyond 3\n\nshipping for multiple types of products\n- should handle combinable and noncombinable products in same cart",
      "Quite nice output from just pulling data out of some RSpec code, looks somewhat like what I would write as some requirements. Ill be posting a followup on how Rob does with just those specifications to use in grokking my meaning!"
    ],
    "summary_t": ""
  },
  {
    "id": "7cc4bef910ef374df7228ccf22074739",
    "url_s": "https://opensourceconnections.com/blog/2016/02/05/luwak/",
    "title": "How to use Luwak to run preset queries against incoming documents",
    "content": [
      "Overview",
      "Quite a while ago Flax released Luwak as a document monitoring and alerting library. It was designed to solve the problem of running a lot of predetermined queries against an incoming document and see which ones match. In that sense it’s a lot like Elasticsearch’s percolator. As a library, some integration work needs to be done in order to reap its benefits, so the bulk of this article will describe different ways you can do that. But first let me give you a brief overview of its internals:",
      "Major Classes",
      "MonitorQuery",
      "Each query Luwak keeps track of is stored in a structure called a MonitorQuery:",
      "private final String id;\n    private final String query;\n    private final Map<String, String> metadata;",
      "Each MonitorQuery is then stored in a small Lucene index. You can add whatever you like as metadata, for example you might add a name and email address if you’re sending out alerts based on the query. The query itself undergoes some Lucene-style analysis by the Presearcher so that it can be efficiently indexed and matched by the incoming documents.",
      "Presearchers",
      "If you have a query containing just the terms \"red\" and \"blue\", you know that a document that doesn’t have those terms will not match. So you can turn documents into more efficient search strings by just retaining terms in your query index. The job of the Presearcher is to implement that strategy. Luwak has a number of these strategies that strike some balance between accuracy, speed, and memory efficiency. For example, the MultipassTermFilteredPresearcher can be made more accurate by examining more query building strategies, but this accuracy comes at the expense of more memory and longer runtime. The documentation on the Presearchers will give you some idea of how they will perform, but you’ll likely want to experiment to find the right one for your application.",
      "CandidateMatchers",
      "Once the Presearcher has selected candidate queries that could possibly match the incoming document, a CandidateMatcher then makes the final determination on which queries do match. The process is a lot more similar to searches you’re used to seeing in that this is where you collect, score, and highlight hits. The different implementations allow you to skip things you don’t need. For example, if you just want to use the MonitorQuery as a way to add a category field to the document, you don’t care about the score of the queries that matched and you don’t need to highlight the sections that matched. In that case a SimpleMatcher may be what you need. On the other hand if your queries are complex and a match on a specific clause is important, you might want to use the HighlightingMatcher so that you can show which parts of the incoming document triggered the match.",
      "Monitor",
      "The Monitor is what orchestrates the indexing of queries and matching them against documents. You can have it keep its index completely in-memory, serialized to some specific place on disk, or delegate the actual index writing to another class if you’re using Luwak in a Solr plugin. In any case, the Monitor is what you’ll use for basic Create/Read/Update/Delete (CRUD) operations on the queries. And most importantly, the Monitor is what takes documents and matches them against queries. Each time you call match() you can use a different CandidateMatcher to receive different details about the match. For example the first run might be just a SimpleMatcher, and depending on the metadata of the MonitorQuery you might run match again to highlight the documents.",
      "Integration Strategies",
      "Make a Stand-alone Executable",
      "If you have a fairly static set of queries you need to run against, it might make sense to just build an executable jar and run it. This has the advantage that it scales independently of Solr and does not impact its resources. It would also fit in well with an index pipeline composed mainly of shell scripts. The downside is that you’ll have to design a separate process to maintain the queries themselves, and you’d have another program to build, maintain, monitor, and distribute.",
      "Add it to Your Ingester Utility",
      "If you already have a Java process that uses SolrJ to send documents to Solr, adding Luwak to it would be a simple way to implement query-based classification or alerting independent of Solr. You’ll still have the issue of query maintenance to address, but again, your ingester scales independently of Solr and the overall impact to your existing architecture will be minimal.",
      "Build a Luwak Web Service",
      "Like the stand-alone approach, a web service would be a flexible way to use Luwak to match queries to documents posted to it. It also has the advantage that CRUD operations on the queries can be implemented in a RESTful way. This also opens up Luwak capabilities to other services on your network (as in a microservice or service bus). Scaling would be easy, but the main drawbacks to this approach would be latency and the added infrastructure complexity.",
      "Create a Solr RequestHandler",
      "Solr itself isn’t too shabby at building and maintaining indexes, and it has a handy web interface for adding documents and querying. So if your documents are already going to Solr it might make sense to implement Luwak as a RequestHandler. You’ll still have a choice to make though: Should Solr or Luwak write the query index. For smaller query collections that can easily fit in memory, Luwak might be the better choice (just like Solr keeps synonyms and stopwords in memory.) For huge collections of queries though, Solr’s sharding capability might be necessary. A distributed RequestHandler is a little more complex than an everything-is-local RequestHandler, so your choice will depend on your intended query volume. Creating a Solr RequestHandler will get you a lot closer to a complete solution, but you’ll still need to add calls to that handler in order to finish things.",
      "Implement an UpdateRequestProcessor",
      "Lastly, if you know you want to run Luwak matchers against every incoming document to Solr you could add Luwak as an UpdateRequestProcessor (URP). URPs are configured as a chain in your solrconfig.xml and can do things like add/change/delete fields on incoming documents or even reject the document altogether. As with the RequestHandler approach you’ll still need to decide whether Solr or Luwak maintains the query index. In an URP chain, Luwak can add fields to your documents for query-based classification or generate new notification documents. Since it’ll be an intimate part of Solr you’re going to have to monitor its impact on indexing speed and latency, and shard your document collection if these are degraded to an unacceptable degree.",
      "Parting thoughts",
      "So far I’ve only worked Luwak into a web service, so I can’t say for certain whether there are pitfalls in tighter integrations with Solr. If you go the URP route I think you’ll still want to make a RequestHandler to do CRUD on the queries. For a web service talking to non-Java clients, take a look at Luwak’s InputDocument.Builder class. You can use it to build arbitrary InputDocuments without having to hardcode a POJO resource beforehand.",
      "The Luwak code itself is a pleasure to work with in that it’s flexible, modular, documented, and tested. There’s even a benchmark project included that gives some idea of what a minimal application would look like. If you’ve used Luwak or are planning to, send me an email or tweet – I’d love to hear about it!"
    ],
    "summary_t": "In Soviet Solr, Document searches Query!"
  },
  {
    "id": "dda4cd3f3f82a4945a9b14fe866e76f4",
    "url_s": "https://opensourceconnections.com/blog/2016/02/29/learning-whats-relevant-without-the-search/",
    "title": "Learning What's Relevant Without The Search",
    "content": [
      "What’s \"relevant\" can transcend any particular search query or user. Shopkeepers for eons have highlighted today’s relevant items in the store window. A book store owner knows cheesy romance novels spike in beach season. They might realize a biography of Alexander Hamilton has suddenly become relevant due to a popular musical based on the book. In my experience, the secret weapon to search relevance is sometimes as simple as being savvy about what’s hot for your audience and biasing search in that direction. And it doesn’t stop at search: recommendations, or any place you display your content can leverage this information.",
      "The newspaper is another classic example. Traditionally, an editor chooses the most important headlines for today’s printed edition. The most relevant headlines for the newspaper’s audience go at the top of the page in large letters.",
      "",
      "Newspaper editors know how their audience evaluates their stories. They know the audience’s expectations of the brand (Wall St. Journal, and NY Post have very different headlines, for very different audiences). They know what stories out of their pool of content will appear most relevant to today’s reader.",
      "Being a good shopkeeper or newspaper editor is a carefully honed skill. But even those of us savvy with trends can be surprised by what suddenly becomes hot to today’s audience. Trailing metrics like sales might fail to keep up with the latest trends. Other metrics like raw page views, or sales, can be driven by a number of unfortunate biases aside from trends.",
      "So increasingly, I’m curious about measuring general relevance, especially as it changes quickly and unexpectedly with trends: How do you keep up with trends? How can you validate what you sense is hot and relevant to your audience? For example, if I were picking the hot toys for this Christmas season, how would I know what to choose? I’d be a bit like this fellow:",
      "",
      "Data Science To Save Lame Parents",
      "One answer is the math behind Rating and Ranking. I’ve been reading Who’s #1 by Amy Langville and Carl D. Meyer (thanks Rene Kriegler!). In it, the authors explain how to rank competitors as the result of several scored contests. The most obvious examples are sports teams. How do you arrive at a fair ranking of the sports teams in a league after each contest using stats like the score differential, or other offensive/defensive stats?",
      "For example, if you know the Orioles beat the Yankees 7-5 and the Yankees beat the Red Sox 3-2 but the Red Sox beat the Orioles 5-4, how would you rank the teams?",
      "Who’s #1 explores dozens of techniques for ranking competitors after numerous contests. Some sports like US College Football, involve hundreds of teams that each play a total of 11 games a piece – presenting one kind of challenge. Others, like Baseball, involve a small number of teams engaged in a large number of contests, presenting quite another kind of ranking challenge.",
      "Ok, sports are cool – but how does it save us in our job as an out-of-touch shopkeeper? Well we can use the science of ranking things to learn what items are hot. We can engineer fair contests between two products at a time. We can use the result of these contests, along with the science of ranking, to adjust and reorder the naive, initial attempt at a hotness ranking.",
      "For example, if we decide clicks are the important metric, we can let two items duke it out as our daily deal. Shown side-by-side and receiving equal footing, we can evaluate which of our supposed items receive the most clicks. We can adjust the ranking of our items in the same way that the techniques in \"Who’s #1\" adjust the ranking of sports teams. Instead of contests between the Orioles and Yankees, we have a click-fight between Tickle-Me-Elmo and Hello Barbie and other products.",
      "We can learn which product is more relevant to today’s shoppers. Then we can bias search results, recommendations, or other systems towards more topical or exciting items. We can also use this data to dynamically control our landing pages with implicit search – highlighting items that are seasonal and relevant without needing to know anything about users.",
      "Ok so how do we accomplish this?",
      "Elo’s System of Predictive Ranking",
      "There are numerous methods that attempt to arrive at an accurate rank. Elo’s System has a lot of appeal to me. It’s simple to understand and doesn’t involve keeping track of large matrices. It learns a rank that can be used to predict contests with other competitors. It can be calibrated to learn faster (biasing towards recent contests) or learn more slowly (biasing towards historical contests).",
      "What is Elo’s System? Elo’s System originates from the chess world, and is used to rank chess players. It has two basic steps given a contest between any two competitors:",
      "Use the difference in rank between the competitors to predict the outcome, in terms of the proportion of points each competitor should receive\n  Learn a new rank based on how much each competitor outperformed or underperformed the expectation",
      "As an intuitive example, let’s consider a game between the Yankees, which we’ll give an initial Elo rank of 123.5 and Red Sox given 42.1. As you can see, with Elo’s system, ranks are arbitrary floating point values, the higher the rank the better. So here the Yankees clearly should beat the Red Sox. However, let’s suppose the game ends with the Red Sox defeating the Yankees 12-2.",
      "Let’s walk through the math! As stated in step 1 above, Elo’s System examines the rank of each player to determine how such a game should end. To do this, it uses as a building block the logistic function, which can be expressed in Python as:",
      "def logistic10(x):\n    return 1.0 / (1.0 + 10.0**x) # 1 / (1 + 10^x)",
      "The logistic function has a nice property of being bounded between 0 and 1. As x becomes negative, it tends towards one. As x becomes positive, it tends towards 0. Elo’s System uses this property to determine the proportion of points that the Yankees and Red Sox should score based on their ranks.",
      "How is this done? Elo’s System does this by passing the difference in the ranks between the two teams. Fortunately, logistic10(rankA-rankB) + logistic10(rankB-rankA) always add up to 1. Given that property, you can get a sense for how the proportion of the 14 total points that game should be divied up.",
      "Let’s do that! So the expected proportion of points that should be awarded to the Red Sox is logistic10(rankYankees - rankRedsox), or:",
      "logistic10(123.5-42.1)",
      "which turns out to be ridiculously small:",
      "3.981071705534921e-82",
      "Our ranks eventually self correct to very small numbers as Elo’s System plays out over many contests. However, you’d prefer to have more ergonomic rankings. So Elo’s System simplifies things by dividing the rank difference by a parameter. This divisor, known as the logistic parameter, is often set to 400. Restating the code from above:",
      "logistic10((123.5-42.1) / 400.0)",
      "This equates to a prediction that the Red Sox receive 0.385 of the points. Or, in a 14 point game, this would equate to ~5.3 points. With this prediction made, we’ve accomplished step 1 above – predicting the outcome using the relative ranks of the two teams.",
      "Finally, we update the ranks based on the outcome of the contest (step 2 from above). It seems that the Red Sox might be better than we thought! Our expected score for the Red Sox 0.385 of the available points, but they got 0.857 of the points! Elo’s System updates the ranks in proportion to this difference:",
      "rankRedsox = rankRedsox + (0.857 - 0.385)",
      "Looking at that rank – it looks like there’s a miniscule increase. Which doesn’t seem very fair! The Red Sox rank only gets nudged a little, but they tromped the Yankees! So Elo’s System adds a K parameter. I think of K as a learning rate for those familiar with machine learning parlance. It controls how much of the point difference to bleed into the updated rank. K is often set to 40 for a regular-season baseball game:",
      "rankRedsox = rankRedsox + 40 * (0.857 - 0.385) # Now 62.396",
      "Viola! You’ll hopefully notice two beautifully symmetric properties to Elo’s System:",
      "The rank difference predicts the score – it predicts that a contest between a rank 50 vs 47.5 might be a tight game. However a rank 50 vs a rank -100 should mean the first team demolishes the second\n  The performance of competitors predicts the rank. As teams over or underperform, their rank updates accordingly – thus improving the rank’s predictive power.",
      "The rank carries very useful information about exactly how well a competitor should do. It’s not a bland uninformative ranking of 1,2,3,… Instead, due to this useful property you’ll get a clumpy distribution of competitors. Some top competitors might clump together near the top of the pack, some meh, some abysmal. All in proportion to how well they are at destroying opponents.",
      "Enough with the Sports References – which of my products are cool?",
      "But you didn’t come here to chat about baseball. You came because you wanted to evaluate which of your products, news articles, or whatever are most relevant, hot, and topical.",
      "There’s nothing particular about sports in Elo’s System I’ve introduced. It can be used to have any entity compete – products, articles, whatever.",
      "To show this, I’ll wrap up the Elos System code above into some handy reusable Python code, like one with the interface in this class (you can see the fleshed out code here in github):",
      "class EloModel:\n    def __init__(self, teams, K=400.0, logisticParam=400.0):\n        pass\n\n    def predict(self, teamA, teamB, totalPoints=1.0):\n        \"\"\" Given the rank difference between the teams, predict\n            the outcome of a contest where totalPoints will be\n            awarded\"\"\"\n        pass\n\n    def report(self, teamA, scoreA, teamB, scoreB):\n        \"\"\" Report a contest between team A with scoreA and teamB with scoreB\n        \"\"\"\n        pass",
      "Here an Elos System is initialized with a set of competitors (a Python dictionary mapping name -> initial rank). As contests move forward, we record their outcome using the report method – reporting which teams played by their name and the scores that resulted. This method performs steps 1 and 2 from above, updating the ranks based on how far the actual contest was from a predicted one.",
      "Like I said, Elo’s system applies to a great deal more than sports. We can use it to evaluate products we’ve promoted side-by-side. For example, let’s start with my lame dad assumptions about what products are popular:",
      "products = {\n    \"tickle-me-elmo\": 5.0,\n    \"fruit-loops\": 4.0,\n    \"legos\": 3.0,\n    \"army-men\": 2.0,\n    \"lame-shirt\": 1.0\n}",
      "On my e-commerce site’s main page, let’s say I’ve managed to engineer a fair contest between each item. Perhaps there’s a simple box on the landing page with these items to choose from. Perhaps two are shown to users as part of the site’s \"daily deals\" – or in a marketing email. (engineering fair contests, and/or handicapping them, could be a blog post of it’s own!).",
      "Let’s presume you’re recording clicks on each item. And let’s say, you have some interesting outcomes you start to record about how users react when they’re side by side:",
      "# Record the contest, lame-shirt does unexpectedly well\n# Counts are numbers of clicks\nmodel = EloModel(teams=products)\nmodel.contest(\"tickle-me-elmo\", 6000.0, \"lame-shirt\", 10.0)\nmodel.contest(\"tickle-me-elmo\", 6000.0, \"legos\", 8000.0)\nmodel.contest(\"army-men\", 1000.0, \"lame-shirt\", 10000.0)",
      "Interesting, my initial rankings suspected \"tickle-me-elmo\" would result in more clicks. However, the \"lame-shirt\" is apparently so lame its become cool again! Huh, just like trucker hats come back into fashion – so do silly t-shirts. Must be one of those spirit wolf shirts nobody told me was cool again…",
      "After several other contests, with lame-shirt triumphing unexpectedly I arrive at a final ranking.",
      "model.contest(\"fruit-loops\", 1000.0, \"lame-shirt\", 10000.0)\nmodel.contest(\"army-men\", 4000.0, \"tickle-me-elmo\", 5000.0)\nmodel.contest(\"legos\", 1000.0, \"lame-shirt\", 1000.0)\n\nprint(model.teams)\n# Output\n{\n\t'tickle-me-elmo': -56.955071725981966,\n\t'army-men': -137.78126294101122,\n\t'fruit-loops': -121.72026109587489,\n\t'legos': 170.00556640578898,\n\t'lame-shirt': 161.45102935707902\n}",
      "What’s hot right now is lame shirts and NOT tickle me elmo!",
      "This is a rather crucial piece of intelligence I’ve gathered about my products. I’ve tried stocking the \"shelves\" in a couple different arrangements, and I’ve arrived at some interesting conclusions about what’s hot and surprisingly what’s not! Lame spirit wolf shirts and legos have shown to drive great interest from customers. Nobody seems to love elmo anymore :(. Army men and fruit loops cereal are even worse off!",
      "With enough of these trials, over enough products we can make timely adjustments to what’s hot and relevant to our current customers. We can adjust our understanding of trends in the marketplace very quickly over a large number of products. We can see what products/items are attracting interest or purchases. Perhaps you can even get a sense for how effective promotions are, and stock up inventory beforehand across a hundred of possible \"hot\" items. The possibilities are endless.",
      "If clicks measure search success to you, then these ranks carry the raw proto-stuff of a relevant search result. And if clicks don’t measure relevance to you, then Elo’s System can be easily instrumented to use whatever method you’d like to score competitors for \"relevance\" (purchases, click-with-dwell, etc). Whatever you use, this system gives your content a direct, raw feature about basic relevance. A feature that predicts, independent of other factors, how likely a user will find this item interesting.",
      "In some ways this is \"learning to rank\" lite. In my experience with search relevance biasing search using a good, general hotness measures can give you more ROI, than a heavily instrumented machine learning solution trying to optimize each search. Finally, Elo’s System is particularly attractive given the small amount of data you store – putting less stress on your infrastructure than giant matrices per item. In future blog posts, I hope to discuss using an Elo rank with a search engine like Solr or Elasticsearch.",
      "The part of the article where we say \"yeah but it depends\"",
      "I think this is an interesting method to explore. But before you go rewire your website to run contests between products, I think there’s some considerations to keep in mind",
      "What should the value of K be? Recall from above, K is the learning rate. A higher value of K means your contests have a significant impact on rank. A lower value means a granular impact.\n  Should K change per contest? In sports prediction, K often increases with the stakes. Playoff games have higher impact on the updated rank. Perhaps Black Friday for an e-commerce site should adjust K?\n  You: clicks! Me: clicks? really? Clicks are more controversial indicators of relevance than you might expect. Indeed this paper from Microsoft Research shows only a 45% correlation between clicks and relevance. In my experience, other domain specific factors that happen after the click can be more predictive\n  Sure Elo’s System is attractive, but would other methods, such as contextual multi-armed bandits, be better? What about the other ranking methods in Who’s #1?\n  Does your domain have a sense of \"hotness\" or is relevance more about other factors? The patent and trademark office doesn’t have hot patents that all the examiners are excited to read! Though maybe they do, but not in the same way that there’s a hot romance novel hitting the shelves.\n  How many contests can you run at once? How do you make them fair or otherwise handicap them?\n  How many items is Elo’s System realistic for? Hundreds? Thousands? Millions? And with many competitors, how much do the ranks lose their predictive ability?",
      "Regardless, this may be an interesting area to explore for you. Thoughts on this article? Tough search relevance problems you need solved? Be sure to contact us. We can sprinkle on search engine brains, NLP, and a little data science to turn your users searches into sales :)"
    ],
    "summary_t": "What’s ‘relevant’ can transcend any particular search query or user. Sometimes search relevance is sometimes as simple as being savvy about what’s hot for yo..."
  },
  {
    "id": "2b94aa80a101782cb212a40b5c162a66",
    "url_s": "https://opensourceconnections.com/blog/2016/03/01/migrate-safely-from-gsa-to-solr/",
    "title": "How to Migrate Safely from Google Search Appliance to Solr",
    "content": [
      "It’s terrifying to contemplate switching search engines. Unfortunately, Google Search Appliance is going away fast. Google hasn’t left much time to get off their platform: they close the doors in 2017. If search is core to your value add, then you need to proceed carefully. Upsetting the applecart with search and relevance can lead to frustrated users, lost sales, and lack of engagement.",
      "How do you prove a migration from GSA to Solr was successful? How do you know you’re on track with quality and relevance? How do you migrate with little rink?",
      "Our product Quepid was built exactly for this kind of thing. Quepid lets you snapshot GSA (or any search engines) current results. You can then compare your new Solr implementations search results to GSA.",
      "The first step is to create a Quepid case by walking through the handy-dandy case setup Wizard. This happens automatically when you create a new account. As you go through the wizard, take note of your case id in the URL bar, below we’re setting up case 724:",
      "",
      "The next step is to generate a simple CSV file that details information about your current search results. This file respresents a handy frozen snapshot in time of GSA results in a way Quepid can import.",
      "Below we’ve given our snapshot a name \"Movies GSA,\" a timestamp, pasted in our case id (724). You’ll also note the 3 columns that detail for the rhesults for each search query. Don’t fret, it looks hard – but really we simply list your unique document identifier for each search string alongside the position that document occurs in GSA. So document 37710 is the 1st (and only) result for the search for \"Johnny Depp.\"",
      "",
      "Finally, we select \"relevance cases\" from the menu and select \"view all cases\"",
      "",
      "Simply select \"Import Snapshots from CSV\" and follow the instructions to upload your CSV file:",
      "",
      "Navigate back to your case using the \"Relevance Cases\" dropdown, hit refresh, and viola – your queries are there.",
      "Select \"Compare Snapshots,\" and do pick our new snapshot:",
      "",
      "Now you can see the difference between your (completely untuned) Solr on the left and your GSA on the right, as shown in this screenshot:",
      "",
      "At first blush, these Solr results for Johnny Depp aren’t so great. That’s ok, you’re just getting started with Solr.",
      "Now it’s up to you to get relevance back up to par across your most important queries. With GSA as a baseline, Quepid gives you a ton of power to move the dial on relevancy closer to your ideal results. Here’s where you, of course, want to buy Relevant Search. But lets just play with one quick tweak to show how Quepid enables your improvements.",
      "The first place you might go to play with your Solr relevance, is in our Solr sandbox, simply click \"Tune Relevance\" and begin playing with a template that applies a set of relevancy parameters to all your queries, controlling how Solr will be queried, boosted, and otherwise fenegled to get better results. Here we’re just passing q=<query string> to Solr:",
      "",
      "But let’s take this one step further, with our GSA snapshot open let’s expand the fields we’re searching. I won’t get into the Solr trivia here, but we’ll add a Solr parameter that controls which fields are searched:",
      "",
      "We’re getting there! Here’s where the journey begins tuning relevance with Quepid. Solr may start out not quite as capable as GSA, but Quepid gives you the platform to get you there. You can begin to explore additional features – rating search results, defining how you want to measure relevance with custom scorers. Most importantly, you have a safe sandbox to play with ideas and see how your ideas impact all your queries! Be sure to contact us if you’re contemplating a switch!"
    ],
    "summary_t": "It’s terrifying to contemplate switching search engines. If search is core to your value add, then you need to proceed carefully. How do you prove a migratio..."
  },
  {
    "id": "f7792a6d91de2e78e87f29aef11abd2b",
    "url_s": "https://opensourceconnections.com/blog/2016/03/02/searching-the-unexpected/",
    "title": "Searching the Unexpected",
    "content": [
      "Maybe I’m the only one, but it seems like the internet is getting smaller every day. There was a time when the internet felt like a giant room of unopened doors. Not unlike the door warehouse in Monster’s Inc. Each door led to a spiraling web of tangential information.",
      "Enter search",
      "As the internet has continued to balloon in size, the search engine has become the primary way of interacting with the internet, and data in general. One might even say that search is eating the world.",
      "You search your phone, your friends, your photos. Everything.",
      "Search is great. It is unparalleled at jumping directly to content that you know exists. But it doesn’t allow for the same free-style exploration as the early web web did in the days of directories and – shudder – site rings.",
      "Today, there seems to be little incentive for the big search engines to route people to the obscure, the niche, the interesting.",
      "Search is often a black box. Either what you’re looking for is there, or it isn’t. When a search engine doesn’t work, users are  left scratching their heads.",
      "Search is often self reinforcing. The more a search engine learns about your interests, and habits the less new things it will show you.",
      "In other words it is hard to search for something that you don’t know exists.",
      "Even our media gets filtered through the same channels. Predominantly, RSS feed readers have been replaced with news aggregators.",
      "Diversifying search",
      "Imagine the process of buying a house, for example. When looking for a house I’m most likely to search for a specific address, or possibly loan rate comparisons.",
      "But I’m also potentially interested in:",
      "The current real estate market & climate\n  How to avoid common home buying mistakes\n  A glossary of the top real estate terms\n  The average home sale price in my area\n  How quickly homes are disappearing from the market",
      "Now if your job is to index everything on the internet, then it’s hard to be this specific. But adding information like this could be in the best interest of a property sales application. Even if some of the content is pulled in from other sources, aggregating domain specific knowledge could benefit your users.",
      "Even the big search engines add information and recommendations to their results in limited ways.",
      "Showing the time and weather when you search for a city\n  Displaying a carousel of local movie posters when searching for showtimes\n  Displaying related organizations or people to the items you are searching for",
      "The more specific your search engine is, the more opportunity there is to break away from plain search results and introduce an experience that is finely tuned to your audience.",
      "The reality is that most of the time search needs to be optimized simply for recall and precision. Sometimes when building very specialized search applications however it can be nice to introduce a bit of playfulness into the results. Tell me something I didn’t know about the topic I’m looking for. Not in a way that obscures the primary objective, but that adds value to the experience.",
      "Currently search results simply list a bunch of available destinations but given enough intelligence search pages have the potential to function more like maps. Maps that give users context, and a progression of resources between points A and B.",
      "Hopefully you’ll keep this in mind if you come across a search project that might benefit from a bit of serendipity.",
      "Mix things up. Search doesn’t have to be boring."
    ],
    "summary_t": "Search is great. It is unparalleled at jumping directly to content that you know exists. But it doesn’t allow for the same free-style exploration as the earl..."
  },
  {
    "id": "b6fd33023eabac17ff96bdf432990cbf",
    "url_s": "https://opensourceconnections.com/blog/2007/05/03/scrum-war-stories-is-coming/",
    "title": "Scrum War Stories is coming!",
    "content": [
      "Come share your experiences with implementing Scrum with your peers at West Main Seafood next Wednesday, May 9th from 5:30 to 8. Over beer and dinner well share hard won lessons from the trenches of Scrum, what the cool tools are to support Scrum teams, and maybe even get philosophical and talk about where we think Scrum will be in a couple of years!",
      "Jeff Sutherland, co-creator of Scrum came to Charlottesville to present to the Neon Guild about Scrum. When he asked who was using it, maybe 40% of the room raised their hands! So clearly there are already a pool of Scrum practitioners, but we dont have a forum for sharing our experiences. Scrum War Stories will be a very casual event, open to anyone, from those doing Scrum, to those thinking about it!",
      "When: May 9th from 5:30 to 8 PM",
      "Where: West Main Seafood (old Awful Arthurs) map",
      "Who: Anyone who is using Scrum, thinking about Scrum, or quit using Scrum!",
      "Help: [email protected] 434-466-1467 or [email protected]"
    ],
    "summary_t": ""
  },
  {
    "id": "64e3ce038dab2558558cbf9eac7d59e2",
    "url_s": "https://opensourceconnections.com/blog/2016/03/14/lost-in-the-amazon/",
    "title": "Getting Lost in The Amazon: How North Face Site Search secured my sale",
    "content": [
      "Bad site search can halt an e-commerce sale in its tracks. To show that, I wanted to share what happened with a recent purchase of mine. In this tale of online purchasing, Goliath (amazon) frustrated me while another brand’s online store (North Face) differentiated itself through site search to close my sale. Through sharing the story, I hope you see in human terms how real shoppers get frustrated and leave e-commerce experiences that don’t \"get\" them.",
      "Anyway, so I’m shopping for a new laptop backpack. As a traveling consultant, this is a rather crucial purchase for me: I take my laptop everywhere and travel quite a bit to visit clients. So I need to choose carefully.",
      "My first stop, of course, is Amazon. I enter a search for \"laptop backpack.\" I’m immediately overwhelmed with the thousands of laptop backpacks that Amazon returns. It dawns on me that there are many many many kinds of laptop backpacks. And amazon sells ALL of them. There’s the laptop bag for my parents. There’s uncomfortable looking hipster backpacks. There’s the bag for the gamer hauling their rig to the video game party. There’s laptop bags for kids. Ugh so many, as you can see below:",
      "",
      "Amazon isn’t quite returning what I personally need in a laptop backpack. Lost in this jungle of stuff. Luckily, determined, I do become aware of features I care about in a laptop backpack:",
      "Being able to hold a Nalgene water bottle\n  Hip belt to take the stress off my shoulders\n  Comfortable for long walks or bike rides\n  Enough space for my laptop, accessories, clothes for overnights\n  Checkpoint friendly at airports",
      "Can't filter using features I care about!",
      "As you can see from my list of requirements, I have active, traveling person laptop bag needs. I want to walk, bike in comfort with my laptop with me. Can I tell this to Amazon somehow? Ugh, looking at Amazon’s search interface (see image to the left), there’s nothing that helps me filter options that would give me active-wear bags.",
      "I’m stuck. I’m fruitlessly scanning thousands of backpacks on Amazon’s search results. There’s no way I’m going to make progress pouring over the thousands of results. The search UX is abysmal for my needs – I can’t even hazard to guess, looking at the results, whether items meet my criteria. In despair, I try a ridiculous query \"water bottle pocket 16 inch hip belt laptop backpack\" and amazon boots me out of the laptop backpack category altogether! It delivers horribly irrelevant search results from numerous departments. Amazon is confused. It’s offering too much. We’re not getting each other. In frustration, I am about give up.",
      "Luckily, I catch a small break. I notice there’s one thing in common to the backpacks I gravitate towards. I like the North Face brand bags. They seem to have features I like. They appear to have a place for a water bottle. They have hip belts. They seem comfortable. Most importantly I know something about The North Face brand. They’ve attached pretty strongly in my mind to being active and outdoorsy. I realize \"these are my people. They get me!\" Unfortunately, even still it’s hard for me to evaluate these bags in Amazon, even after filtering down to this brand.",
      "That leads me to try the The North Face online store. I think perhaps they might have a more satisfactory sales experience. And by sales experience – I mean onsite search experience. My hope is the search will understand me better, and let me compare and filter across backpack features important to me.",
      "I go to thenorthface.com and type \"laptop\" into the search bar, off the bat there’s fewer results all with laptop backpacks that appeal to me. But most importantly I can be very specific about features I care about as an active person. Look at the screenshot below, and you’ll see a filter that hits EXACTLY on one of my criteria:",
      "",
      "Awesome!",
      "North Face paid attention to features I care about when shopping for an active person’s laptop bag. Additionally – I noticed other features I care about. Reflective bag? Great idea when I bike home late at night! ACA – American Chiropractic Association certified. Great! I don’t want my pack giving me back aches on long walks! Northface.com’s search talks to me – suggests features I hadn’t even considered in a backpack. We understand each other.",
      "It’s a fascinating example of what we talk about all the time in our search relevance practice. It turns out, you don’t need to compete with Amazon in your product search experience. Instead customize a savvy sales experience that reflects what’s special about your brand and shows special care to your audience.",
      "Another way to think through it is the in-person sales experience. Amazon’s site search is sometimes the snotty teenager saleperson at K-mart. They don’t know very much about laptop bags, nor do they get your active-person needs. They mostly just point you at the right aisle, and stare blankly as you blather about hip belts and reflective gear. The North Face search, on the other hand, is like chatting with an employee at a North Face outlet. They’re \"my people.\" They’re probably active, outdoorsy types themselves. Similarly, North Face’s search feels like it gets my priorities. I feel like I’m in the right place. It ranks and filters on features an active person cares about, helping me make decisions using my active-person criteria.",
      "Good site search is your most important salesperson. Without something there that understands and guides users, your users certainly will go elsewhere for their purchase. If you work to understand and help them feel at home, you’ll close sales. If you create a vanilla search solution that shows lack of care, shoppers will feel unwanted and go elsewhere – just like they would if the sales staff was unresponsive.",
      "OSC helps e-commerce sites customize their search UX and relevance to close sales. Heck we wrote the book on search relevance! Contact us if you’d like our help."
    ],
    "summary_t": ""
  },
  {
    "id": "011e81b8e8ea13a2e83243aaab31a7f9",
    "url_s": "https://opensourceconnections.com/blog/2016/03/29/semantic-search-with-latent-semantic-analysis/",
    "title": "Semantic Search with Latent Semantic Analysis",
    "content": [
      "A few years ago John Berryman and I experimented with integrating Latent Semantic Analysis (LSA) with Solr to build a semantically aware search engine. Recently I’ve polished that work off, integrated it with Elasticsearch, and sunk my teeth in a few levels deeper. I wanted to get a sense for whether this technique could be made really useful for building semantically aware search applications – and how exactly to do that. It turns out getting LSA to work requires a lot of data cleaning, feature modeling, and careful tuning.",
      "What is LSA? How might it improve search relevance?",
      "So we all know how a search engine works. Users enter search terms and the search engine uses an index to match documents that contain those terms. Search engines work well, but require very strict term matching. Search engines don’t know, without help, that \"cat\" and \"kitty\" correspond to similar ideas.",
      "Looking at the search documents another way, you can discern relationships between terms and documents that get at broader ideas. One way to view these documents is as a big term-to-document matrix that maps each document to each term. Each entry mentions how often each term occurs in each document. Ala something like this:",
      "TermDocMatrix:",
      "doc1\n      doc2\n      doc3\n      doc4\n    \n  \n  \n    \n      cat\n      0\n      1\n      0\n      2\n    \n    \n      kitty\n      0\n      2\n      0\n      0\n    \n    \n      the\n      6\n      5\n      4\n      5\n    \n    \n      dog\n      2\n      0\n      1\n      0\n    \n    \n      pooch\n      3\n      0\n      3\n      0",
      "If you look at this matrix, patterns jump out at the savvy reader. Some documents clearly are about cats (docs 2 and probably 4). Other documents are about dogs (1 and 3). One might even look at this matrix and realize that the word \"kitty\" is pretty closely related to \"cat.\" Armed with this insight, we might have the capacity to take user searches for \"kitty\" and return document 4, as it seems to have a very strong affinity to the idea of \"catness\" without.",
      "Latent Semantic Analysis is one technique that attempts to recognize these patterns. Latent Semantic Analysis runs a matrix operation called Singular Value Decomposition (SVD) on the term-document matrix. A Singular Value Decomposition can be interpreted many ways. Fundamentally, it factors the matrix into something of a simpler form. Mathematically it’s a way of saying that our matrix can be decomposed (some might say compressed) around the corpus’s main ideas:",
      "TermDocMatrix = catness + dogness",
      "Most importantly, when you look into \"catness\" and \"dogness\" you’ll see the components give you insight into that topic’s overall strength, each term’s affinity for that topic, and each document’s affinity for that topic, as in:",
      "catness = <eachTermsCatness> * corpusCatnessStrength * <eachDocsCatness>",
      "Here",
      "eachTermsCatness: a vector that corresponds to every term’s affinity for the \"cat\" idea\n  eachDocsCatness: corresponds to every document’s affinity for the \"cat\" idea\n  corpusCatnessStrength: how strongly catness occurs in the corpus",
      "It’s beautiful! If we look at \"eachTermsCatness\" we might see something listing every term in our corpus and its affinity to the idea of \"cat\"",
      "<cat:2.0, kitty: 1.0, the 0.0, banana, pooch -2.01, dog -2.0~>",
      "And each document’s \"catness\" has a similar look and feel, only with elements for each document:",
      "<doc4: 2.0, doc 2:1.9, doc3:-1.9, doc1:-2.0>",
      "And when you multiply these together, you get a rough affinity for each term-document pairing and the \"catness\" idea. Perhaps something like",
      "doc1\n      doc2\n      doc3\n      doc4\n    \n  \n  \n    \n      cat\n      0\n      0.6\n      0\n      0.5\n    \n    \n      kitty\n      0\n      0.4\n      0\n      0.3\n    \n    \n      the\n      0\n      0\n      0\n      0\n    \n    \n      dog\n      -0.4\n      0\n      -0.3\n      0\n    \n    \n      pooch\n      -0.6\n      0\n      -0.7\n      0",
      "If we kept going, and added back in the other components (like dogness), we get back to our original matrix:",
      "doc1\n      doc2\n      doc3\n      doc4\n    \n  \n  \n    \n      cat\n      0\n      1\n      0\n      2\n    \n    \n      kitty\n      0\n      2\n      0\n      0\n    \n    \n      the\n      6\n      5\n      4\n      5\n    \n    \n      dog\n      2\n      0\n      1\n      0\n    \n    \n      pooch\n      3\n      0\n      3\n      0",
      "In other words, Latent Semantic Analysis should let us break up a big term-document matrix into constituent, semantically meaningful parts. We can say, this giant matrix can really be factored into these 50 topics. And instead of us asserting the topics they’re flushed out of the matrix.",
      "We should be able to use this to do a lot of smart things to improve search relevance:",
      "Detect synonyms – noticing kitty is closely related to \"cat\" and perhaps synonymous\n  Cluster terms – which ideas go well together: \"cat\" \"kitty\" etc\n  Cluster documents – which documents seem to carry similar topics – noticing docs 2 and 4 are \"cat\" documents\n  Automatically tag documents – noticing docs 2 and 4 are \"cat\" docs, you might choose to tag them with that topic to organize information\n  Remove unimportant topics from the index. I said above you can reconstruct the matrix from the topics. What if you set the strength of some of the weaker topics to \"0.\" Ignoring unimportant topics also helps the performance of many of the computational steps\n  Compress the term-document matrix. Term-document matrices are notoriously sparse, so by representing a 1000x1000 matrix by a 50 document to topic and topic to term vectors saves tremendous amount of space.",
      "It should be the best thing since sliced bread, right?",
      "The LSA Bread is Not Pre sliced",
      "The last section mentioned the \"theory\" (or marketing) of why LSA is so awesome. In reality, LSA takes a great deal of work, trial and error to get results that seem correct to expert human eyes. (In fact, I suppose now is as good a time as any to say the results above are entirely contrived for illustrative purposes :-p ).",
      "I want to detail the hands-on practical realities of LSA. It’s not as magical awesome sauce as you might be led to believe. In fact, without careful attention by someone that knows what they’re doing, you’re probably just as likely to screw up search relevance as improve it. Let’s see why.",
      "Stopwords",
      "Indeed, my example above is contrived and flawed a bit on purpose. You’ll notice I left out a huge component of the text. The use of the word \"the.\" The SVD matrix operation has no idea that \"the\" is a pretty meaningless word in English. In reality, the matrix above might get decomposed into something like:",
      "TermDocMatrix = theness_catness + dogness",
      "Here, theness_catness is a blend of the word \"the\" and the cat words. This happens because if you notice in the matrix above, the stopwords occur slightly more frequently in cat documents than dog documents.",
      "Another way to look at this is to understand that the SVD operation underlying LSA aims to decompose the matrix for later reconstruction. It acts as a compression operation. Accounting for \"the\" is just as important as accounting for our intuitive topics: cats and dogs.",
      "Zipf’s Law vs LSA",
      "Ok but maybe our matrix is just something weird about \"cat\" docs. We can generally ignore this problem for larger corpuses, right? No. Given the statistical distribution of words in documents, the problem becomes starker. Let me introduce you to my little friend, Zipf’s law. Zipf’s law states that the frequency of terms in a corpus of natural language follows the rather stark Zipfian distribution, as shown in the two graphs below:",
      "",
      "Zooming in on the most common words:",
      "",
      "How terms occur in text follows the The Zipfian distribution, as shown above. Some words occur very frequently, and the drop off is very fast. If \"the\" occurs say one in every 10 words with \"cat\" and \"dog\" 1 in 1000, it’s not clear LSA can actually model meaningful topics through the noise of stopwords. Let’s walk through why.",
      "Compare a 100 word document to a 50 word document:",
      "100 word document:  (cat: 1,\tthe: 10)\n50 word document: (cat: 1, the: 5)",
      "When you include other traditional stopwords, the picture gets noisier",
      "100 word \"cat\" document:  (cat: 1, the: 10, a: 5, of: 4)\n50 word \"cat\" document: (cat: 1, the: 5, a:3, of: 2)",
      "The feature with the greatest variability in our corpus is the relative occurrence of stopwords due to document length. Further, long documents have more unique words, which means even more potential for stopwords.",
      "Because of this, matrix factorization notices the biggest difference in documents is the number of stopwords. As this is due to length, it causes the most prominent topic to be stopwords that occur in long documents, as shown in this decomposition:",
      "TermDocMatrix = long_stopword_docs + catness + dogness",
      "Another way of saying this is SVD assumes in a given document terms will be normally distributed across documents. The word \"the\" should just be noise to SVD if our documents present a nice bell curve of \"the\" occurrences. But that’s not how \"the\" is distributed in English text, and that causes LSA to focus on these large \"the\" differences document to document. The distribution of \"the\" is extremely sensitive to length where the distribution of cat is is extremely resistant to modifications in document length.",
      "More Stopwords Than Meet The Eye",
      "Ok if you’re screaming \"no duh\" remove the stopwords already. Everyone knows stopwords are bad!",
      "The problems with Zipfian don’t stop at traditional stopwords. Other terms, what I’ll call pseudo-stopwords, are words clearly unrelated to what we’d consider meaningful topics. Yet these psuedo-stopwords live in a big middle gray area in the Zipfian distribution of useful English text without being overly specific.",
      "For example, after removing the obvious English stopwords (the, a, of, etc), the gender of the text comes through as the next most prominent documents (notice it in the Zipfian graph above). Documents written about men, using very commonly used words like \"he,\" \"him,\" \"his\" surface. Or for Question-and-Answer text like scifi stackexchange, different ways of asking questions form their own topics (\"who\" vs \"what\").",
      "Words like \"he\" and \"what\" still occur very far to the left in the Zipfian hierarchy. So just like stopwords, they vary significantly with length, giving you a topic decomposition like:",
      "TermDocMatrix = he_docs + she_docs + catness + dogness",
      "Yet it’s not even that clean, you’re just as likely to get your cat and dog topics munged into the he/she documents. For example, if more people write about their female cats and male dogs:",
      "TermDocMatrix = he_dog_docs + she_cat_docs",
      "The situation seems hopeless!",
      "No Duh: It’s called TF*IDF …. Right",
      "You’re probably still screaming at your screen at me. Everyone knows you should use TF*IDF, not term frequency directly to score common terms lower than rare, specific ones.",
      "What is TF*IDF? The classic way search engines and NLP account for language’s Zipfian distribution is to divide by a term’s \"document frequency.\" Terms like \"the\" occur in every document. So in our cat/dog example above a single \"the\" in a document is really a ¼ (one occurrence over four documents). Cat is more rarely used, so a single cat occurrence in a document gets scored as ½.",
      "Our goal is to make the stopwords and pseudo-stopwords appear as noise, with no impact in the topic modeling, while our important words \"cat\" and \"dog\" really stick out.",
      "Using direct TF / DF, you’d get this matrix:",
      "doc1\n      doc2\n      doc3\n      doc4\n    \n  \n  \n    \n      cat\n      0\n      1/2\n      0\n      2/2\n    \n    \n      kitty\n      0\n      2/1\n      0\n      0\n    \n    \n      the\n      6/4\n      5/4\n      4/4\n      5/4\n    \n    \n      dog\n      2/2\n      0\n      1/2\n      0\n    \n    \n      pooch\n      3/2\n      0\n      3/2\n      0",
      "Restated:",
      "doc1\n      doc2\n      doc3\n      doc4\n    \n  \n  \n    \n      cat\n      0\n      0.5\n      0\n      1.0\n    \n    \n      kitty\n      0\n      2.0\n      0\n      0\n    \n    \n      the\n      1.5\n      1.25\n      1.0\n      1.25\n    \n    \n      dog\n      1.0\n      0\n      0.5\n      0\n    \n    \n      pooch\n      1.5\n      0\n      1.5\n      0",
      "This helps quite a bit. The problem is less pronounced than using term frequency directly. But dividing by document Frequency doesn’t account for document length, you’ll still have long documents with higher TF*IDF scores for stopwords. In other words, DF doesn’t go far enough to ameliorate a situation like our \"cat\" vs \"stopwords\" example from above:",
      "100 word \"cat\" document:  (cat: 0.5, the: 2.5, a:1.25, of: 1.0)\n50 word \"cat\" document: (cat:0.5, the: 1.25, a:0.75, of: 0.5)\n(assuming cat has DF of 2, the of 4)",
      "The longer documents still have quite a few more stopwords (the, a, of…) despite their TF*IDF score. So LSA still groups documents with a bias towards longer documents.",
      "Accounting for Document Length",
      "For tuning against document length, you have a couple of options. The most obvious thing to do is to bias towards shorter documents. For example, divide a term’s TF*IDF score by the total number of terms in that document. This can help scenarios where very long documents create artificial stopword-heavy topics.",
      "But consider what happens if you go too far in this direction. If \"persian\" is extremely rare in your animal corpus, your scores might look something like",
      "25 word \"persian\" document: (persian: 5.0, he: 0.01, it: 0.15)\n50 word \"persian\" document: (persian: 10.0, he: 0.02, it: 0.30)",
      "This can create topic biases in the other direction. Suddenly the SVD operation must account for a handful of documents with very strong \"persian\" features. Instead of your topics being psuedo-stopwords, they’re instead a highlight of terms too obscure to be considered natural topics.",
      "The situation is much worse if you consider the occasional mention of off-topic words. For example, what if someone wrote about their cat Mr Puffinpants in a short document.",
      "25 word \"Puffinpants\" document: (puffinpants: 100.0, he: 0.01, it: 0.15)",
      "Suddenly you get a very powerful \"puffinpants\" topic, all just because puffinpants is rare and mentioned prominently in a short text document.",
      "LSA requires work!",
      "The takeaway from all this is to get LSA to work as a topic modeling approach, you need to do a lot of tuning. If you model your document’s features with just TF or TF*IDF, expect to get topics latent to English itself. If you bias too heavily towards rare terms or short documents, expect to get topics that cover more obscure topics.",
      "There’s a million other options we could take this, and really I could fill many more blog posts with my trials and tribulations. For example:",
      "Try more mature TF*IDF based calculations, such as Lucene’s TF*IDF or BM25. You’ll likely still need to tune calculations for your case, as LSA is so sensitive to tuning.\n  \n  \n    Perhaps we shouldn’t count term frequency at all. Simply mark a 1 or 1.0 / doc freq if the term is mentioned, or a 0 if it isn’t. This would sidestep Zipf for term frequency. However long documents still contain more terms, even if they’re scored lower. Further frequent terms, though scored lower, will still occur across more documents. We try to account for by this dividing by document frequency, but for psuedo-stopwords (\"he\" or \"she\") this sometimes isn’t enough.\n  \n  \n    Use skipgrams from the document text as LSA’s \"document\". Skipgrams group a word with the words occurring N skips after the present word. For example \"Mary had a little lamb\" becomes \"Mary had\", \"Mary a\", \"Mary little\", \"Mary Lamb.\" This gives you word groupings in a narrow window after an initial word. However if you’re trying to detect synonyms, like \"cat\" ~ \"kitty\", its unlikely synonyms will be used in the same sentence. Its more likely synonyms are spread out throughout a document to avoid repetitiveness in text.\n  \n  \n    Experiment with selecting a random N words from a document. This seems like a good idea until you realize that this random selection will be biased towards Zipf. Even if you select 100 unique terms, you’re very likely to get stopwords and psuedo-stopwords.\n  \n  \n    Limit LSA to fields that behave better than prose. In my experimentation titles, like the question titles at Stackexchange, tend to have fewer stopwords and are more \"on topic\" in their verbiage. This tends to have a positive impact on LSA. Trey Grainger used LSA on the user search strings in his knowledge graph experimentation, which seems to confirm my suspicions that LSA works better on titles and other \"entity rich\" fields.",
      "Suffice it to say, The deeper I get into this, the more I want to experiment!",
      "I’m curious about your ideas around using LSA! Let me know what tuning options you’ve implemented succesfully, I’m eager to learn from what you’ve tried!",
      "LSA and the Lurking Psuedo topics",
      "You’ll notice that the trick with LSA is landing on topics that we consider \"natural.\" LSA can oscilate between overly broad, vague topics like gendered pronouns and overly specific topics like documents about Mr. Puffinpants.",
      "This, arguably, is the main feature, not the bug of LSA.",
      "Think of it another way, imagine you’re trying to figure out what’s important in alien language. They say",
      "\"zurb zurb zeple xe\"\n\"zow zeple xe zow\"\n\"zub zeple zub xe\"\n\"zow zee zoo zow\"",
      "The most prominent difference here is some utterances have \"zow\" and some have \"zurb.\" If LSA told you this, you’d feel one step closer to translating an Alien language!",
      "Unfortunately, we don’t care about general latent topics. We already know English prose is written using differently gendered pronouns. We instead want insights about our corpus about our domain, be it pets or science fiction trivia. LSA needs a lot of tuning to get there. You can get it there.",
      "My final verdict on LSA is to set it aside for use on general prose-like text. I’m going to turn my focus towards Latent Dirichlet Allocation (LDA) produces better results with less tuning. Look for future blog posts as I learn more about LDA!",
      "And as always, if you have any comments or if you’re interested in seeking our semantic search services, be sure to contact us!"
    ],
    "summary_t": ""
  },
  {
    "id": "a252e6249dd23c0f76561fb01218cad0",
    "url_s": "https://opensourceconnections.com/blog/2016/03/30/hey-kibana-is-anyone-listening/",
    "title": "Hey Kibana, is anyone listening to my Podcast?",
    "content": [
      "\"Dashboard all the things!\" was my motto leaving Elastic{ON} this year.  I played a game that week trying to think of all the previously complex software projects that might be pretty trivial in the face of the current Elastic Stack.",
      "Could we hook up all the treadmills in all the gyms in Washington DC to a Beats emitter and dashboard that with Kibana?  Sure.  What about the number of times my faucet drips at night? Absolutely, as a weekend project.  What about my coworker Chris Bradford’s old app that counted the number of times the dogs in his old office would bark? Yep, and he could have spent a lot more time working and less time writing apps about not being able to work.",
      "So it’s no surprise that when I wondered if anyone was actually listening to Search Disco my first thought was \"How do I get this into a dashboard?\"  This is how.  So grab your access logs and come along with me!",
      "First thing, I needed a way to track downloads.  If I could get an IP, timestamp and filename for each download that would be a pretty good start.  We are self hosting our little podcast so that was pretty easy.  Both Amazon’s S3 storage and Google’s cloud storage make this pretty easy.  We are hosting Search Disco on the Google cloud, so I dug up some instructions to enable access logs for our podcasts bucket and in no time I was collecting data.",
      "Enable logs in Google Storage: https://cloud.google.com/storage/docs/access-logs\n  Enable logs in Amazon S3 http://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html",
      "In order to use that data, it needs to get into Elasticsearch.  But how do we load it?  Does Elastic have a magic CSV endpoint like Solr?  Is there a helper or import script?",
      "No, but I’m really happy with what it does have.  The answer (and thanks to everyone who groaned at the last paragraph for waiting) is Logstash.  It takes a couple hours to get up to speed on Logstash, but you won’t regret it.  Let’s have a look at the config I wrote to process the access logs.  I’ve added some comments to help explain what’s in the file.",
      "# First: where is the data coming from?\ninput {\n  file {\n    path => \"/Users/mattoverstreet/Documents/search-disco/logs/podcast-logs/search_disco_usage*\"\n    type => \"podcast-log\"\n    start_position => \"beginning\"\n  }\n}\n\n# Next: how can we understand the data; and, does it need\n# to be cleaned up or changed?\nfilter {\n\t# our file is in csv format, so we’ll need to list the columns\n  csv {\n    columns => [\"time_micros\", \"c_ip\", \"c_ip_type\", \"c_ip_region\", \"cs_method\", \"cs_uri\", \"sc_status\", \"cs_bytes\", \"sc_bytes\", \"time_taken_micros\", \"cs_host\", \"cs_referer\", \"cs_user_agent\", \"s_request_id\", \"cs_operation\", \"cs_bucket\", \"cs_object\"]\n    separator => \",\"\n  }\n  # Kibana needs a @timestamp, so we’ll try to convert the\n  # time_micros field.  The date filter will handle the\n  # conversion and then set the value of @timestamp.\n\tdate {\n    match => [ \"time_micros\", \"UNIX_MS\" ]\n  }\n}\n\n# Last: where is the log data going to be stored?\noutput {\n  elasticsearch {\n    action => \"index\"\n    hosts => \"localhost\"\n    index => \"logstash-sd-logs\"\n    workers => 1\n  }\n}",
      "If you are following along with your own access logs, don’t run that one just yet.  You will see some very odd data.  Looking in the access log CSV I noticed a problem with the download timestamp, it’s in microseconds!  Checking the Logstash docs for the date filter, it can handle milliseconds or seconds for a UNIX timestamp; but not microseconds.  So, what now?",
      "There are a couple ways to solve this problem.  I’ve taken advantage of the fact that Logstash is Ruby on the inside to process the timestamp.  Let’s process our original timestamp, convert it to seconds and tell Elasticsearch we want to use it for the @timestamp attribute.  To do that we’ll add another filter.",
      "# add this to the ‘filter’ section after the csv block\n  ruby {\n    # Chop some decimal places off the time_micros field\n    code => \"event['time_micros'] = (event['time_micros'].to_i / 1000)\"\n  }\n  # now our date filter will work!\n  date {\n    match => [ \"time_micros\", \"UNIX_MS\" ]\n  }",
      "After I wrote this I realized this could actually be done with a mutate filter using a pretty simple gsub as well.",
      "Now we are ready to suck in some data. Let’s save our Logstash config in the conf directory of the logstash folder as gcloud.json.  From the logstash dir you can run something like:",
      "bin/logstash -f conf/gcloud.json",
      "Now, spin up Kibana, accept some defaults and your data is ready to dashboard!",
      "But wait, wouldn’t it be nice to show where people where downloading from?  I have an IP for each download, I could cross reference it with a GeoIP database to find the location and save that into Elasticsearch to visualize.  How much work do I need to do to make that happen?  Not much, as it turns out.  Let’s update our Logstash job to handle that at import time.",
      "# Add this to the end of the ‘filter’ block in our config.\n  geoip {\n    source => \"c_ip\"\n    target => \"geoip\"\n    add_field => [ \"[geoip][coordinates]\", \"%{[geoip][longitude]}\" ]\n    add_field => [ \"[geoip][coordinates]\", \"%{[geoip][latitude]}\"  ]\n  }\n  mutate {\n    convert => [ \"[geoip][coordinates]\", \"float\"]\n  }",
      "Next time, lets spin up Kibana and configure some visualizations!",
      ""
    ],
    "summary_t": "\"Dashboard all the things!\" was my motto leaving Elastic{on} this year.  It’s no surprise that when I wondered if anyone was actually listening to the Search..."
  },
  {
    "id": "b1b1602b5a9b1b1b6df3314fb4d509a7",
    "url_s": "https://opensourceconnections.com/blog/2016/03/30/search-precision-and-recall-by-example/",
    "title": "Search Precision and Recall By Example",
    "content": [
      "The following is an excerpt from our upcoming book Relevant Search from a chapter written by OSC alum John Berryman. Use discount code turnbullmu to get 38% off!",
      "Precision and recall are two fundamental measures of search relevance. Given a particular query and the set of documents returned by the search engine (the result set), these measures are defined as follows:",
      "Precision is the percentage of documents in the result set that are relevant.\n  Recall is the percentage of relevant documents that are returned in the result set.",
      "Admittedly, these definitions are a little hard to follow at first. They may even sound like the same thing. In the discussion that follows, we provide a thorough example that will help you understand these definitions and their differences. You’ll also begin to see why it’s so  important to keep these concepts in mind when designing any application of search.",
      "Additionally, we demonstrate how precision and recall are often at odds with one another. Generally, the more you improve recall, the worse your precision becomes, and the more you improve precision, the worse your recall becomes. This implies a limit on the best you can achieve in search relevance. Fortunately, you can get around this limit. We explore the details in the discussion that follows.",
      "Precision and recall by example",
      "Let’s lead with another example. And this time just to be different, let’s use, oh, I don’t know, fruit. After you recover from the wormy apple incident of the previous section, you go back to the fruit stand and consider the situation in more detail.",
      "Figure 4.2 Illustration of documents and results in the search for apples",
      "When you originally went to the fruit stand, you were looking for apples—but more specifically, your search criteria was \"red, medium-sized fruit.\" This criteria led you to the search results indicated in figure 4.2. Let’s consider how this result set can be described in terms of precision and recall. Looking at the search results, you have three apples and three red, medium-sized fruits that aren’t apples (a tomato, a bell pepper, and a pomegranate). Restating the previous definition, precision is the percentage of the results that are correct. In this case, three of the six results are apples, so the precision of this result set is (3 ÷ 6) × 100, or 50%. Furthermore, upon closer inspection of all the produce, you find that there are five apple choices among the thirteen fruits types available at the stand. Recall is the percentage of the correct items that are returned in the search results. In this case, there are five apples at the fruit stand, and three were returned in the results. The recall for your apple search is (3 ÷ 5) × 100, or 60%.",
      "In the ideal case, precision and recall would both always be at 100%. But this is almost never possible. What’s more, precision and recall are most often at odds with one another. If you improve recall, precision will suffer and your search response will include spurious results. On the other hand, if you improve precision, recall will suffer and your search response will omit perfectly good matches.",
      "Figure 4.3 Example search result set when loosening the color-match requirements",
      "To better understand the warring nature of precision and recall, let’s take a look at this phenomenon in the context of our fruit example. If you want to improve recall, you must loosen the search requirements a bit. What if you do this by including fruit that’s yellow? (Some apples are yellow, right?) As shown in figure 4.3, you do pick up another apple, thus improving recall to 80%. But because most apples aren’t yellow, you’ve picked up two more erroneous results, decreasing precision to 44%.",
      "Figure 4.4 Example search result set when tightening the size requirements",
      "Let’s go the other way with our experiment. If you tighten the search criteria—for example, by tightening the definition of medium sized, you have results that look like those in figure 4.4. Here precision increases to 67% because you’ve removed two slightly un-medium fruits. But in the process, you’ve also removed a slightly oversized apple, taking recall down to 40%.",
      "Although precision and recall are typically at odds with one another, there’s one possible way to overcome the constraints of this trade-off: more features. For instance, if you include another field in your search, for flavor, then the tomato would be easy to rule out because it’s not sweet at all. But, unfortunately, it’s not always easy to identify new features to pull into search. And in this particular case, if you decided to go around flavor sampling the fruit in order to identify apples, you’d probably soon have an upset produce manager to contend with!",
      "",
      "Would you like to learn more?",
      "If you’re interested in more, be sure to grab the book! And get in touch with OSC if you have tough search problems you need help with!"
    ],
    "summary_t": "Precision and recall are two fundamental measures of search relevance. But it doesn’t take a PhD to understand them. Any shopper of fine fruits can understan..."
  },
  {
    "id": "69a533d28cd2567a93ecb60a68d483bb",
    "url_s": "https://opensourceconnections.com/blog/2016/04/07/generating-deployable-quepid-artifacts-with-containers/",
    "title": "Generating Deployable Quepid Artifacts with Containers",
    "content": [
      "This is part 1 of a 3 part series on our move to containers as a platform for Quepid.",
      "As Quepid has matured OSC has gone through an evolution of operational tools to run the platform. Quepid’s migration to Rails prompted an exploration of deployment methods for our new stack. Under the Python Flask application we would log in to the appropriate environment’s application server and run a script. It would perform a git pull, run migrations, and  restart the application process. There are deployment tools for Python such as Fabric, but this workflow worked well for us given the number of hosts and team size. With our additional developers and the switch to Ruby we have Capistrano which like Fabric handles code deployment and execution of remote tasks. What about taking a different tack. Let’s dive into containers, a modular way to encapsulate deployable artifacts.",
      "Containers are interesting, they contain all components required to run the application. This includes the language runtime, its dependencies, our application code, and any configuration files. By encapsulating the entire application stack we get powerful guarantees. If I build a container on my machine and it runs, I am certain that it will run on anyone elses machine. We don’t have to quibble over Ruby or gem dependencies. Everything needed to run is there in the container. Instead of sending that container to a team member, let’s instead ship it out to the staging environment. There is no need to investigate whether a dependency is present, we push the container and run it. Deployment is a breeze! If production experiences an issue we can pull down that specific version, run the application, or poke around inside the container to investigate.",
      "Containers run in isolation. We can run multiple instances of the same container (or different versions!) on the same host. Each container receives it’s own isolated disk and network space. Linking containers together with shared resources is also possible. We could have a containerized application writing out logs to a directory, then a separate containerized process which pulls the logs from a shared directory and ships them off to a log service. In this approach the containers are each responsible for one task. The log collection container could even be paired with other services. Think of containers as following the single responsibility principle.",
      "Ok, so containers are awesome where do I get started? We looked around at a few different technologies in the space LXC, Rkt (pronounced Rocket), and Docker. At the time LXC was the most mature, but had a steep learning curve. One of our goals was not to require a ton of training or tooling around our eventual solution. Rkt was the new kid on the block. Some major companies invested in it’s technology and container specification. It also has a fun feature around cryptographically signed containers. Unfortunately at the time it was very young with interfaces and formats still in flux. It’s worth noting that it has since hit version 1.0 since our exploration and deserves another look. That leaves us with Docker, a container engine with a massive following. It’s containers are built off a manifest, called Dockerfile, that is included with your application.",
      "Docker seems like a solid choice, how does one bootstrap on it? Our Quepid developers run OS X or Linux machines. Docker is linux native, those developers didn’t have any special hoops beyond installing the package from their respective package managers. The OS X devs had a few hoops to jump through. Today there is a wonderful Docker Toolbox suite of tools to setup a complete environment. When we were investigating this last year it didn’t exist. On our machines we installed boot2docker a tool for launching a virtualbox based virtual machine with the docker daemon running and the docker CLI tools. This VM is pretty slim since all other dependencies are wrapped up in the container. Another option worth exploring is the CoreOS Vagrant image. It runs a VM just like boot2docker, but also provides some additional features and supports VMWare as well as Virtual Box. In both of these cases docker CLI commands are forwarded to the VM.",
      "Now that docker is running, building a new container is a simple call to docker build, the Dockerfile is processed line by line with the final container being composed of layers representing the disk state after each command is executed. This is pretty powerful as layers may be reused if a previous layer has not been changed. For instance if I install Ruby in the first layer, copy our application in, then precompile our frontend assets the likelihood that the Ruby layer will be regenerated is small. We can forcibly bust the cache, but really we want fast build times.",
      "All Dockerfile’s start with a base defined with the FROM command. In some cases the base is an empty layer, in others it may already contain build tools, language runtimes, and other packages. An example being Phusion’s Passenger-docker which includes everything required to run a Ruby, Python, or Node app with their application server passenger. It’s a great starting point for beginners and solves a lot of the stack early on in the Dockerfile. We started here and ended up taking a different approach. Instead of including all of those extra pieces need it made more sense to start with an slimmer base. In our case we went with CentOS. This provides a bunch of basic commands and the yum package manager.",
      "Next we install additional packages our Rails application needs. This includes Ruby, NodeJS, and MariaDB libraries. These packages are installed with the RUN command which executes yum install commands. The next big step is to COPY the application into the container. This copies in all of the code and assets to the specified path within the container’s filesystem. Finally we RUN our \"install\" rake tasks. These include pulling in bower dependencies, installing gems with bundler, and pre-compiling assets.",
      "With all of these steps listed out in the Dockerfile we run docker build -t quepid .. This pushes the build context out to our VM where it runs through each layer. At this point we iterated quite a bit. Testing different layering strategies, do we install a bunch of packages together or each one individually? It all depends on your environment and team. Eventually a container is produced. Let’s think of the container as an environment where commands may be executed. The command could be to start the application server or possibly execute a rake task. We could even tell it to run /bin/bash and explore the container in our terminal. In our Dockerfile we hint at how the container should be run as an application with the CMD entry. Now when we call docker run it knows what to execute within the container. There are many more commands supported within a Dockerfile, be sure to explore some other other features available.",
      "There is beauty in the simple Docker commands and Dockerfile. We iterated on this to the point where our Continuous Integration system builds our containers and ships them to a registry for the containers. The CI system may also start the container and verify it spins-up and serves requests. At that point it may run integration tests against the container. Thus further ensuring that the artifact to be deployed meets requirements.",
      "Now our Quepid Rails application is contained. All dependencies required to run are packaged together in a way that makes it easy to run and ship around. We have guarantees around component versions and isolation. Join us in the next installment, Shipping Containers, where we explore running containers outside of a developer’s machine and in a real environment. We discuss scheduling and tooling around containers and their execution. If you’re interested in exploring containers in your infrastructure Get in touch! We’ll be happy to explore your use case with our team of operations experts."
    ],
    "summary_t": "An exploration of how we contained Quepid with the power of Docker."
  },
  {
    "id": "ecfaacdb4bf0860b3ea55f127887d338",
    "url_s": "https://opensourceconnections.com/blog/2007/05/03/turning-waterfall-contracts-into-scrum-contracts/",
    "title": "Turning Waterfall Contracts into Scrum Contracts",
    "content": [
      "This Monday we were fortunate to hear Dr. Jeff Sutherland give a presentation on Scrum at our monthly Neon Guild meeting. He was going to be doing some training at Inova Solutions, but the Guild got a great overview of Scrum as an added bonus.",
      "One of the difficulties Dr. Sutherland touched on during his presentation was introducing Scrum to managers who only know of the \"waterfall\" method of running projects. I think this mainly impacts contracting organizations at two key points: Requirements Gathering and Delivery.",
      "Chunking",
      "The first challenge is to break large projects into smaller ones. Customers envision the final product with varying clarity. However, when they put their trust in an outside organization to deliver it they want to know exactly what they are going to be getting for their money. So, at the outset of a project they seek to protect their investment through rigorous requirements gathering and design phases. This gets exacerbated every time those involved go through a failed project, because smart managers seek to fix their processes and not the minutiae particular to any one project. Therefore the process that bares the brunt of their scrutiny is Requirements Gathering (and by association, Design.)\nI believe the key here is to show how breaking their project into smaller chunks limits their overall exposure and gives them the flexibility to adjust the project requirements as business needs change. After all, the goal of any project is to address business needs and not a particular distillation of them represented by a requirements document. The map is not the territory.",
      "Deliverables",
      "However, the chunking of the project must also include a modification of delivery expectations. In Waterfall projects, success is measured by three questions: Was it on time, Was it within budget, and Do all features work? What burns longer projects is the Sisyphean nature of technology, in that the features you build become obsolete as you create them. The longer it takes you to deliver, the more irrelevant the project becomes. Therefore, each subproject or Sprint addresses what the customer needs right now, rather than several months down the road. In a lot of ways this breaks the PMI definition of \"project\" in that the end state is less defined. But since you are incorporating change into the contract, that end state is going to be much more useful to the client.",
      "The Contract",
      "Which brings us to the contract itself. You will need to translate your Scrum project into a contract that will pass legal scrutiny. This means that you have to adjust traditional contracts that state \"We will do X by Y for $Z.\" Those contracts place little or no responsibility on the client beyond an initial requirements specification, acceptance testing, and payment. In Scrum, customer participation in the Sprint Review is essential, and belongs in the contract. One strategy could be to only contract for the initial Sprint. But if the client is already thinking Big Project, you could trade features that are contractual obligations for Sprint Reviews that are contract termination clauses. In either case, the responsibility of project \"success\" is going to be shared much more than in a traditional project. As compensation, \"success\" will no longer be check marks on a project plan but a measurable increase in business value."
    ],
    "summary_t": ""
  },
  {
    "id": "6aaa84e9a8919ba0f395daad00c51046",
    "url_s": "https://opensourceconnections.com/blog/2016/04/11/shipping-containers-effectively-deploying-containers-in-the-cloud/",
    "title": "Shipping Containers - effectively deploying containers in the cloud.",
    "content": [
      "This is part 2 of a 3 part series on our move to containers as a platform for Quepid. Be sure to check out Part 1 on generating deployable containers. Part 3 will be linked here once it has been published.",
      "In our last installment we discussed wrapping an application and all of its dependencies in a single easily deployed container. Running the container locally is trivial, but how can this be deployed into a hosting environment? Our deployment exploration has now shifted to getting the artifacts shipped!",
      "CoreOS, a linux distribution, was first up for testing. This distribution is designed from the ground up to run containers. We briefly mentioned the coreos-vagrant project for running containers locally in a virtual machine. It has very clear documentation around release versions and channels. It is trivial to test out new versions of the OS and receive updates across production, beta and alpha releases. CoreOS ships with some additional technologies for scheduling and coordinating clusters of servers. Instead of managing servers individually CoreOS pushes the concept of running a cluster of resources. From here a scheduler may assign containers evenly across the available resources. This leads to better resource utilization and a reduction in the number of required hosts.",
      "CoreOS provides Fleet, a scheduling system built atop SystemD’s unit files. To deploy a container we generate a SystemD unit file and push it to the cluster. This file describes resource requirements and conditions. A sample requirement being that another container must also be present on this host, or a condition mentioning any service conflicts. Fleet then chooses a system on the cluster to execute the unit. All interactions with the cluster are through the fleetctl command line tool.",
      "Another entry in the CoreOS toolbox is Etcd, a distributed key-value store. Etcd allows for reading and writing of values as well as watching keys or their directories. Within CoreOS, node restarts are coordinated by holding an election within etcd among all nodes that need to restart. Applications may store their configuration in etcd. This allows changes to be made to the running configuration with change events pushed to the application.",
      "We spun up a CoreOS cluster and took it for a spin. The first container deployed housed Elasticsearch. We crafted a unit file for fleet that describes the commands to start and stop our container. This file is then submitted to the cluster with fleetctl load. The fleet service handles finding a machine and running the job. Within our unit file we may specify a Conflicts section which indicates that it should not be run on a node already running this unit (in our case we want to avoid port collision). As part of this configuration it was necessary to design side-car processes. These would run alongside other units in the cluster and register their name and address within etcd. For instance if another service in our cluster depends on Elasticsearch it could query etcd for the address.",
      "Around this time a lookup meetup group DevOpsCV hosted a meeting with Brian Akins of CoreOS giving an overview of Docker and CoreOS. While there we were introduced to Kubernetes, also known as k8s, another open source scheduling system. Kubernetes started as an open source project at Google. It was designed as an automated system for running containers at massive scale. This year’s GCP NEXT conference dove into some of the history of Kubernetes and the experiences Google pulled from it’s predecessor Borg. To get started with Kubernetes we took a look at another Vagrant powered project from CoreOS, coreos-kubernetes. This allowed us to experiment with a local K8S cluster.",
      "Kubernetes provides a few basic building blocks to orchestrate applications. A Pod represents a collection of containers that should be run together. Why would we run multiple containers together? Within Quepid we run an Elasticsearch service listening for searches against a couple of sample datasets. This service is composed of three distinct components each wrapped in a container. The first container runs the Elasticsearch process. Since this service is exposed to the internet let’s place some access control in front of it. Our second container runs Nginx as a reverse proxy to the first with a set of security rules. Finally our third container runs a process which pulls a JSON backup of the sample dataset and streams it into the Elasticsearch service. This combination of containers together makes up a distinct unit of functionality within our stack and gets scheduled as such in a pod.",
      "Now that we have a pod let’s talk about scheduling it on the cluster. We can take the pod definition, a YAML file, and deploy it with the kubectl command line tool. This will fire up the defined containers together on a node in the cluster and make them available at an internal cluster IP. It is now possible to talk with the running Elasticsearch service! Unfortunately servers don’t stay up forever, hardware fails. When this happens the pod is gone and our customer’s are left without an Elasticsearch service to query. Fortunately Kubernetes has another building block available, the Replication Controller, to handle this. Replication controllers automate the process of scheduling pods on the cluster. In another YAML file we define the number of instances of the pod along with the pod definition. This gets pushed to the cluster via kubectl just like the pod YAML. Now Kubernetes handles scheduling our pods. When a node goes down the cluster automatically spins-up replacement pods. It’s also worth noting that scaling the number of application instances up is trivial. We update the replication controller YAML file and push the change out. It immediately schedules a new pod.",
      "At this point we have a highly available Elasticsearch resource, but if we want to talk to it we need the pod’s IP address AND knowledge of how to setup a route to the cluster! Kubernetes is ready for us again with the Service component. Services route traffic both within the cluster and from outside services to the appropriate nodes & pods. When we define the replication controller certain metadata may be included within the pod template. In our example we label any pods created with the key \"application\" and value \"sample-elasticsearch\". Next the service is generated with a selector that looks for the same label key-value pair. Any traffic directed at the service will automatically be forwarded along to an appropriate pod. It also generates a DNS entry within the cluster pointing at the service. Externally all nodes on the cluster start listening on a common port. Any traffic to that port is then forwarded to the internal service. This handles routing in a highly available environment where the Pod IP address isn’t necessarily known given we are no longer doing the scheduling.",
      "It is common to have a few environments where an application runs. For example I may use a \"staging\" environment as a place to test code before pushing to the \"production\" environment. In our initial build out on Kubernetes all pods had an \"environment\" label which the services used to direct traffic appropriately. Each of the replication controllers and services had the environment name as part of their names. prod-es-svc or prod-es-rc were common. We ended up running into name length restrictions so we started abbreviating. This worked, but felt a bit clunky. After speaking with a Google engineer we were directed to Namespaces. Namespaces allow us to logically group resources together in an isolated manner. We use namespaces to group our environments.",
      "With these powerful components we started assembling our stack. Application components were assembled and services deployed. Everything was going well until pods stopped launching. A quick look at kubectl get po revealed that all new pods were being scheduled, but there state was pending. What made this interesting was when we deleted a pod one of the pending pods would start up. Digging a little deeper revealed resource limits! By default every container is assigned a number of CPU and RAM resources. Each server has a finite amount of resources available based on CPU core count and memory capacity. By not setting any limits Kubernetes used the defaults. We took another pass through our definitions assigning appropriate values. For instance the data loader on our pod doesn’t need the same amount of CPU as the actual Elasticsearch process. After adjusting these values more pods came online, but not all of them. Even with the adjusted values we were still short on resources. Our next move was to resize the cluster. After bringing a new node online Kubernetes started scheduling pods on the new hardware. Today resource utilization is balanced across all nodes in the cluster.",
      "Resizing the cluster is still an operational task. You have to spin-up the new node and join it to the cluster. There is also a Kubernetes master that must be running somewhere. This in turn communicates with a etcd cluster which in turn should probably be running on multiple nodes for HA purposes. Instead of setting up and configuring a cluster we decided to use the hosted Google Container Engine (GKE) product of the Google Cloud Platform (GCP). This product provides a fully managed Kubernetes Master node and etcd cluster. Our nodes are placed within a Google Compute Instance Group. All instances in this group are templated with the same configuration. Increasing the number of nodes in our cluster is a simple change to the number of instances in the group. They have also integrated with some of the products within GCP to utilize built in components like load balancers.",
      "We investigated some other closed-source solutions for running containers. Amazon provides two solutions for running container based applications. Elastic Beanstalk was the first to support running Docker containers. EB abstracts away infrastructure away making deploying on the cloud simple. It supports some awesome features like environment cloning and automated scaling. Next is the more recent EC2 Container Service. This option has Amazon managing the EC2 instances and other AWS resources to operate your application. These are both very capable options, but we wanted to avoid lock-in with a particular vendor. While we are using hosted GKE, a product of a particular vendor we can easily move to another provider if needed. All of the component definitions will work regardless of where K8S is running.",
      "With Kubernetes being our platform of choice we created some tooling around it. First we updated our CI pipeline to automatically build containers and test their functionality prior to pushing to a repository. Next we focused on our Rails application. We developed a suite of rake tasks for interacting with the cluster. Our first task handles version number updates in the replication controllers. Once they are updated it performs a rolling restart of the application. If for some reason the new application doesn’t come up engineers are notified to fix the problem before it impacts customers. We need log output to help with troubleshooting. GCP provides a web interface to drill-down in to container logs, but a quick rake task to tail them in our terminal was a welcome feature. Finally we have tasks for scaling up and down the number of pods on our cluster. Should we need additional capacity we’re just a quick command away.",
      "Our Quepid stack is running and available to customers. All services are highly available with automated systems to scale and deploy. Operations tasks are trivial with either a web-interface or simple CLI tools to handle most tasks. With Kubernetes powering our infrastructure we have a rock solid platform for hosting our application. Join us in the final installment where we dive in to Grand Central, our tool for on-demand QA & feature environments with no need to deploy! We explore its setup, use, and internals. If you’re interested in exploring containers in your infrastructure  say hi! We’ll be happy to explore your use case with our team of operations experts."
    ],
    "summary_t": "Now that our applications are wrapped up in containers we focus on the process of shipping them to production. Here we explore our journey across various too..."
  },
  {
    "id": "25fe8fa9c448d41cc919afe41e46c478",
    "url_s": "https://opensourceconnections.com/blog/2016/04/21/run-elasticsearch-in-your-shell/",
    "title": "Run Installed Elasticsearch from Your Shell",
    "content": [
      "Got Elasticsearch installed on an Ubuntu/Debian box? Want to run it directly from the command line? It’s handy to recreate how Elasticsearch actually runs on your box. You might want to debug something. Or perhaps you’d just like to see the log statements scroll by this console window. This might also be handy if you’re running the zip or tarred versions directly and want to know what all the knobs and dials do.",
      "I’ve pieced this together by",
      "Observing how the service script /etc/init.d/elasticsearch runs the Elasticsearch bootstrap script. It sets up a variety of environment variables (detailed below) that configure Elasticsearch.\n  Observing how the bootstrip script /usr/share/elasticsearch/bin/elasticsearch runs Elasticsearch’s Java code. It sets up JVM settings and executes Elasticsearch’s Java main class.",
      "Home sweet ES_HOME",
      "First things first. The most important thing when setting up Elasticsearch is to set your ES_HOME. ES_HOME is an environment variable pointing where Elasticsearch is installed. If no other settings are specified, Elasticsearch operates solely out of this directory. Open up a shell and set this all important variable:",
      "export ES_HOME=/usr/share/elasticsearch",
      "Other destinations",
      "But ES_HOME doesn’t have the final say. Elasticsearch will use other directories for logs, data, and configuration. Indeed, in my debian package install of Elasticsearch, I have files flung all over /var and /etc. Let’s set those environment variables as well.",
      "export LOG_DIR=/var/log/elasticsearch\nexport DATA_DIR=/var/lib/elasticsearch\nexport CONF_DIR=/etc/elasticsearch",
      "Running Elasticsearch",
      "Finally, startup Elasticsearch as the elasticsearch user, passing in arguments that Elasticsearch’s bootstrap script expects. This form of running this script is plucked out of the service script. But I’ve made a few alterations. I remove the -d argument so Elasticsearch isn’t daemonized. I’ve also added bash -x to observe the bash commands as they’re running.",
      "sudo -u elasticsearch bash -x $ES_HOME/bin/elasticsearch --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR",
      "You’ll have something like the following:",
      "",
      "Now verify Elasticsearch is operational with curl",
      "[email protected]$~ $ curl -XGET http://localhost:9200\n{\n  \"name\" : \"Alexander Lexington\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"2.2.2\",\n    \"build_hash\" : \"fcc01dd81f4de6b2852888450ce5a56436fd5852\",\n    \"build_timestamp\" : \"2016-03-29T08:49:35Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"5.4.1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}",
      "Running Elasticsearch with Java directly",
      "If you want to be extra clever, observe the Java command output by the bin/elastcsearch bootstrap script. Fetch it. Tinker with it! You can run Elasticsearch directly with Java, just by adding sudo -u elasticsearch to the command:",
      "sudo -u elasticsearch /usr/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp '/usr/share/elasticsearch/lib/elasticsearch-2.2.2.jar:/usr/share/elasticsearch/lib/*' org.elasticsearch.bootstrap.Elasticsearch start --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.conf=/etc/elasticsearch```",
      "From here you can get into real trouble! You could add commandline parameters for remote debugging. Blow Elasticsearch up with alternate garbage collection settings. Whatever mayhem happens to be required at the moment.",
      "I should add that inspecting ./bin/elasticsearch, there’s a better way to pass, say, remote debugging arguments. In reality, what you should do is use ES_JAVA_OPTS to pass aditional JVM params like so:",
      "export ES_JAVA_OPTS=\"-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n\"",
      "And be sure to pass this to the shell your sudo command triggers:",
      "sudo -u elasticsearch ES_JAVA_OPTS=$ES_JAVA_OPTS bash -x $ES_HOME/bin/elasticsearch --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR",
      "Logging – want to make your eyes bleed?",
      "Now its not very interesting to run Elasticsearch from the commandline if there’s nothing to look at. So how can we turn on some console logging?",
      "One option is to edite /etc/elasticsearch/logging.yml to change logging levels, pushing stuff out to the console, etc.",
      "But its far simpler just to use ES_JAVA_OPTS from the previous section to override the logging level temporarily. The console appender is already enabled for the root logger, so simply set your desired logging level temporarily:",
      "export ES_JAVA_OPTS=\"-Des.logger.level=INFO\"",
      "and reexecute.",
      "sudo -u elasticsearch ES_JAVA_OPTS=$ES_JAVA_OPTS bash -x $ES_HOME/bin/elasticsearch --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR",
      "Roll that beautiful Elasticsearch bean footage:",
      "",
      "That’s it! And as always if you need Elasticsearch consulting help, don’t hesitate to get in touch!."
    ],
    "summary_t": ""
  },
  {
    "id": "21adc2beb7b32668a5703186227eff8f",
    "url_s": "https://opensourceconnections.com/blog/2016/05/03/splainer-elasticsearch-relevance-sandbox/",
    "title": "Splainer: The Elasticsearch Relevance Sandbox That Tells You Why",
    "content": [
      "Have you looked at a scoring explain out of Elasticsearch? Perhaps you’ve had a tricky relevance problem. You’ve needed to debug Elasticsearch’s scoring.",
      "Unfortunately the relevance scoring for Elasticsearch is a beast. Here’s a simple match query, and its corresponding explain",
      "{\n  \"query\": {\n    \"match\": {\n      \"title\": {\n        \"query\": \"star trek\"\n      }\n    }\n  },\n  \"explain\": true\n}",
      "corresponding relevance explain:",
      "{\n\t\"value\": 4.369631,\n\t\"description\": \"sum of:\",\n\t\"details\": [{\n\t\t\"value\": 1.9833124,\n\t\t\"description\": \"weight(title:star in 385) [PerFieldSimilarity], result of:\",\n\t\t\"details\": [{\n\t\t\t\"value\": 1.9833124,\n\t\t\t\"description\": \"score(doc=385,freq=1.0), product of:\",\n\t\t\t\"details\": [{\n\t\t\t\t\"value\": 0.6737103,\n\t\t\t\t\"description\": \"queryWeight, product of:\",\n\t\t\t\t\"details\": [{\n\t\t\t\t\t\"value\": 5.8877306,\n\t\t\t\t\t\"description\": \"idf(docFreq=22, maxDocs=3051)\",\n\t\t\t\t\t\"details\": []\n\t\t\t\t}, {\n\t\t\t\t\t\"value\": 0.11442614,\n\t\t\t\t\t\"description\": \"queryNorm\",\n\t\t\t\t\t\"details\": []\n\t\t\t\t}]\n\t\t\t}, {\n\t\t\t\t\"value\": 2.9438653,\n\t\t\t\t\"description\": \"fieldWeight in 385, product of:\",\n\t\t\t\t\"details\": [{\n\t\t\t\t\t\"value\": 1,\n\t\t\t\t\t\"description\": \"tf(freq=1.0), with freq of:\",\n\t\t\t\t\t\"details\": [{\n\t\t\t\t\t\t\"value\": 1,\n\t\t\t\t\t\t\"description\": \"termFreq=1.0\",\n\t\t\t\t\t\t\"details\": []\n\t\t\t\t\t}]\n\t\t\t\t}, {\n\t\t\t\t\t\"value\": 5.8877306,\n\t\t\t\t\t\"description\": \"idf(docFreq=22, maxDocs=3051)\",\n\t\t\t\t\t\"details\": []\n\t\t\t\t}, {\n\t\t\t\t\t\"value\": 0.5,\n\t\t\t\t\t\"description\": \"fieldNorm(doc=385)\",\n\t\t\t\t\t\"details\": []\n\t\t\t\t}]\n\t\t\t}]\n\t\t}]\n\t}, {\n\t\t\"value\": 2.3863184,\n\t\t\"description\": \"weight(title:trek in 385) [PerFieldSimilarity], result of:\",\n\t\t\"details\": [{\n\t\t\t\"value\": 2.3863184,\n\t\t\t\"description\": \"score(doc=385,freq=1.0), product of:\",\n\t\t\t\"details\": [{\n\t\t\t\t\"value\": 0.73899555,\n\t\t\t\t\"description\": \"queryWeight, product of:\",\n\t\t\t\t\"details\": [{\n\t\t\t\t\t\"value\": 6.4582753,\n\t\t\t\t\t\"description\": \"idf(docFreq=12, maxDocs=3051)\",\n\t\t\t\t\t\"details\": []\n\t\t\t\t}, {\n\t\t\t\t\t\"value\": 0.11442614,\n\t\t\t\t\t\"description\": \"queryNorm\",\n\t\t\t\t\t\"details\": []\n\t\t\t\t}]\n\t\t\t}, {\n\t\t\t\t\"value\": 3.2291377,\n\t\t\t\t\"description\": \"fieldWeight in 385, product of:\",\n\t\t\t\t\"details\": [{\n\t\t\t\t\t\"value\": 1,\n\t\t\t\t\t\"description\": \"tf(freq=1.0), with freq of:\",\n\t\t\t\t\t\"details\": [{\n\t\t\t\t\t\t\"value\": 1,\n\t\t\t\t\t\t\"description\": \"termFreq=1.0\",\n\t\t\t\t\t\t\"details\": []\n\t\t\t\t\t}]\n\t\t\t\t}, {\n\t\t\t\t\t\"value\": 6.4582753,\n\t\t\t\t\t\"description\": \"idf(docFreq=12, maxDocs=3051)\",\n\t\t\t\t\t\"details\": []\n\t\t\t\t}, {\n\t\t\t\t\t\"value\": 0.5,\n\t\t\t\t\t\"description\": \"fieldNorm(doc=385)\",\n\t\t\t\t\t\"details\": []\n\t\t\t\t}]\n\t\t\t}]\n\t\t}]\n\t}]\n}",
      "Wow! Are your eyes bleeding yet? Imagine debugging non-trivial queries, like function_score queries, nested boolean queries, or multi_match?",
      "Now I’m being a little hard on Elasticsearch. Under the hood is an extremely sophisticated search engine, with every little function pluggable. Because every little bit can change, you need this kind of deep visibility into the search engine’s behavior.",
      "But you certainly don’t need deep visibility for day-to-day work! Splainer helps you see the forest for the trees. It starts with a top-down view of relevance explain. Showing you the strength of each match, and letting you drill down into the explain with a simpler, human-readable breakdown of what’s happening.",
      "As an example, below we’ve set up Splainer with the same query as above, only this time you get a very top-down view of exactly how relevance is working:",
      "",
      "Above, notice the match explanation on the left and the documents on the right. The matches give you a very high level sense of what factors are determining each document’s relevance. You see the magnitude of each relevance factor. Need to drill a little deeper into the ranking math? Click \"detailed,\" to get a human-readable description of the explain:",
      "",
      "And when you need to really dig into what’s happening, you can see use the Full Explain tab to get back Elasticsearch’s JSON explain.",
      "You’ll notice Splainer gives you a JSON editor to tweak your Elasticsearch query to analyze the impact. In this way, it acts as a sandbox. A convenient place to dork around with different ideas. You shouldn’t feel terrified of Elasticsearch’s query DSL! Embrace it! Play with it! See what happens!",
      "(Puts on Billy Mays Mask) and that’s not all! One of the best features of Splainer is its ability to share URLs. For example, you can see the example above by going  here. This is actually the killer feature of Splainer. With URLs you can share your work with colleagues, saying \"hey buddy, what do you think of these results?\"",
      "Using Splainer",
      "Using Splainer is as easy as following along in this GIF. Simply go to http://splainer.io and enter your Elasticsearch search URL and Query DSL query. Then hit \"splain this.\" Click \"Tweak\" to bring out the Query DSL editor.",
      "",
      "Enabling CORs",
      "Currently you need CORS enabled in elasticsearch.yml to chat with Splainer. Here’s the snippet we use in elasticsearch.yml",
      "http.cors.allow-origin: \"/https?:\\\\/\\\\/(.*?\\\\.)?(quepid\\\\.com|splainer\\\\.io)/\"\nhttp.cors.enabled: true",
      "In the future, we may release this as an Elasticsearch site plugin or Chrome extension to avoid the CORS annoyance. Stay tuned!",
      "Relevance: you can do this!",
      "Splainer has been a game changer for our relevance practice. As I write in Relevant Search, instead of Relevance being mystical, it should be a transparent and reliable engineering practice. It should be accessible to everyone. How else can the entire team make investments in relevance? Splainer helps achieve this by boiling down the Elasticsearch explain to something more easily understood by engineers, not the purview of search engine mystics.",
      "All of these features are available in our product Quepid – our test-driven relevance toolbench.  Quepid stores a set of keyword searches and analyzes your search solution’s correctness against validation criteria. Did fixing the \"sandals\" query break the \"dress shoes\" query? Quepid tells you, preventing you from shipping search algorithm changes that damage your bottom line.",
      "But back to Splainer. Splainer for Elasticsearch is in beta. So while I’ve been using it for my own work, I’m sure you’ll find a few bugs. Splainer is open source. So your bugs and contributions are welcome. If you’d like to chat Splainer, Quepid, or our Solr/Elasticsearch relevance consulting, please get in touch!"
    ],
    "summary_t": "Have you looked at a scoring explain out of Elasticsearch? Perhaps you’ve had a tricky relevance problem. You’ve needed to debug why scoring works the way it..."
  },
  {
    "id": "f1848fa8d0c7fa09cfa2c49b6ee3af44",
    "url_s": "https://opensourceconnections.com/blog/2007/05/08/capturing-rake-return-codes/",
    "title": "Capturing Rake return codes",
    "content": [
      "Based on Martin Fowlers whole pitch on declarative versus imperative builds, I converted a basic Ruby script into a Rake based script. However, the one gotcha I hit was that running rake dist, the return code wasnt returned. Yes, rake would say it failed:",
      "(in E:/irocket/continuum/working-directory/6)\nrake aborted!\nI should fail the build\nE:/irocket/continuum/working-directory/6/rakefile:310",
      "We are using Continuum, a very simple to set up CI tool, and it requires whatever script it calls to return 0 on success, and > 0 on failure. So even though Rake would say the build failed, it always return 0. It turns out that some people have run into this problem:",
      "Rake bug on Windows\n  Ruby Gem Creates Broken Batch File for Rake",
      "I didnt want to hack up my rake.bat and rake.cmd files, so instead I am calling ruby directly:",
      "ruby -x e:\\irocket\\ruby\\bin\\rake.bat dist",
      "Hopefully the next version of Rake will have this resolved! The Continuum project went through this same pain for Ant and Maven scripts, and have posted their fixed batch files."
    ],
    "summary_t": ""
  },
  {
    "id": "221a33ffebd35c35bdefd8613138c9dc",
    "url_s": "https://opensourceconnections.com/blog/2016/05/18/its_a_ballon_a_blimp_no_a_dirigible/",
    "title": "It’s a Balloon! A Blimp! No, a Dirigible! Apache Zeppelin: Query Solr via Spark",
    "content": [
      "This blog post contains the code from the talk I gave at the NYC Solr/Lucene Meetup entitled It’s a Balloon! A Blimp! No, it’s a Dirigible. Apache Zeppelin: Query Solr via Spark.",
      "Synopsis",
      "Apache Solr powers search and navigation for many of the world’s largest websites. Solr is widely admired for its rock-solid full-text search and its ability to scale up to massive workflows. But Solr has moved beyond its roots as just a full-text search engine. Today, people use Solr for aggregating data, powering dashboards, geo-location, even building knowledge graphs! In fact, Solr is so powerful, it’s the standard engine for big data search on major data analytics platforms including Hadoop and Cassandra. Critical data is being accessed through Solr’s rich query interface and, now, big data engineers are including Solr as one more data store in the analytics processing chain. But, as we expand the data pipeline to include diverse data stores, we need consistent ways of working across different data access patterns and representations.",
      "Enter Apache Spark. Apache Spark has seen a meteoric rise as the tool for big data processing. Spark makes distributed computing as simple as running a SQL query. Well, almost!\nSpark’s core abstraction, the Resilient Distributed Dataset (RDD), is capable of representing pretty much any data store, including Solr. So, let’s see how we can integrate Apache Solr into our data processing pipeline using Apache Spark using the Solr Spark library.",
      "We’ll talk about the implications and opportunities of treating Solr as just another RDD. To top it off, we’ll demonstrate how to use your existing SQL skills with SparkSQL – declarative programming over distributed datasets in real time!",
      "Finally, we’ll tie it all together with a new Apache project that marries the best of Jupyter Notebook (the favorite tool of data scientists) and the best of distributed computing (Apache Spark and SparkSQL). Apache Zeppelin is the interactive computational environment for data analytics. Just like Jupyter/iPython Notebook, Zeppelin supports collaboration, data exploration and discovery, and rich graphs and visualizations. But its deep integration with Spark means Apache Zeppelin is the \"interactive analytics notebook\" for Big Data.",
      "Code",
      "",
      "Find some bugs?  What would you like to see?  Let me know!"
    ],
    "summary_t": "Solr is no longer has to be a lonely siloed search engine, isolated from the rest of your data repositories.  Pair Solr up with technologies like Zeppelin an..."
  },
  {
    "id": "ebd50d7da89b92f8997b370ba20d4b4d",
    "url_s": "https://opensourceconnections.com/blog/2016/05/24/flume-with-solr-6-morphlines/",
    "title": "Building Apache Flume to use Solr 6 as a sink",
    "content": [
      "Using the latest Solr with Apache Flume can be difficult because the MorphlineSolrSink in the 1.6 release is stuck supporting Solr 4. However, Cloudera contributed solr-morphlines-core and solr-morphlines-cell to the Solr project and with a few tricks Flume can be updated to the latest and greatest Solr. I’ll demonstrate some of the problems I ran into running 1.6 out of the box and how I made it through those problems. Let’s explore how to upgrade Flume to use a newer Solr library and how to build it reliably. For this article, I am focused on using apache-flume-1.6.0.bin.tar.gz, apache-flume-1.6.0-src.tar.gz, solr-4.10.3.tgz. and solr-6.0.0.tgz. The first part will focus on getting Flume to work with Solr 4.10.3 and then Solr 5.2.1 and 6.0.0.",
      "Flume doesn’t come with every plugin built out of the box but if you build the full project from source you get a more fully ready package. Also Flume expects some jars to be installed or included with external projects. I wanted to be able to just run Flume without having to juggle dependencies. With some pom file updates I was able to get a fully built Flume artifact that will work with Solr 6 out of the box. The major problem that I kept running into was dependency issues.",
      "Flume 1.6.0 and Solr 4.10.3",
      "When I tried running Flume out of the box with the binary distribution I ran into a number of problems. Following the Flume 1.6.0 User Guide setup steps, I setup a simple spooling directory source with a MorphlineSolrSink. Next, I followed the CDK Morphlines Documentation to setup the morphlineFile. Another thing I wanted was to use the cdk-morphlines-core-stdlib grok feature. The documentation states, \"Morphlines ships with several standard grok dictionaries.\" but because the cdk only builds runtime dependencies the dictionaries are left out. You need to either make your own dictionary file or copy those to a well known path and use them with the dictionaryFiles path. Another option is to use the dictionary string where you just make a massive string with all the patterns you need in the config, ie",
      "dictionaryString : \"\"\"\nSPACE \\s*\nDATA .*?\nGREEDYDATA .*\n\"\"\"",
      "After following all the instructions, my pipeline was ready and I ran Flume using",
      "bin/flume-ng agent -Dflume.root.logger=INFO,console --conf conf -f conf/flume.conf -n a1",
      "and encountered my first error.",
      "2016-05-23 10:30:03,848 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SINK, name: k1 started\n2016-05-23 10:30:03,849 (lifecycleSupervisor-1-1) [ERROR - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:253)] Unable to start SinkRunner: { policy:[email protected] counterGroup:{ name:null counters:{} } } - Exception follows.\njava.lang.NoClassDefFoundError: org/kitesdk/morphline/api/MorphlineCompilationException\n        at java.lang.Class.forName0(Native Method)\n        at java.lang.Class.forName(Class.java:264)\n        at org.apache.flume.sink.solr.morphline.MorphlineSink.start(MorphlineSink.java:93)\n        at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)\n        at org.apache.flume.SinkRunner.start(SinkRunner.java:79)\n        at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: org.kitesdk.morphline.api.MorphlineCompilationException\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 13 more\n2016-05-23 10:30:03,851 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.sink.solr.morphline.MorphlineSink.stop(MorphlineSink.java:106)] Morphline Sink k1 stopping...",
      "This is a classic missing jar exception. Anytime you are getting null objects and java.lang.NoClassDefFoundError, a dependency is missing. FLUME-2392 indicates that Kite SDK won’t be included so you can either modify the pom file or you need the jars on your classpath somewhere. It’s probably a philosophical conversation for the best approach to containing these jars, ie install packages independently, use mvn to retrieve jars, include dependencies in the pom, etc but my approach is to try to just use Maven and it’s plugins to get the payload I want to deploy. With that in mind, let’s follow the recommendation to remove the <optional>true</optional> from the kite-morphlines-all dependency in the flume-ng-morphline-solr-sink project within the apache-flume-1.6.0-src code. After removing the optional tag, I rebuild the project at the root using",
      "mvn clean package -Dmaven.test.skip=true",
      "My final artifact builds into flume-ng-dist/target/apache-flume-1.6.0-bin.tar.gz.",
      "So now that we have a new build lets try it out before we distribute the tar.gz.",
      "cd ./flume-ng-dist/target/apache-flume-1.6.0-bin/apache-flume-1.6.0-bin\nbin/flume-ng agent -Dflume.root.logger=INFO,console --conf conf -f conf/flume.conf -n a1",
      "Blast, another error.",
      "2016-05-24 11:15:50,052 (lifecycleSupervisor-1-2) [ERROR - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:253)] Unable to start SinkRunner: { policy:[email protected] counterGroup:{ name:null counters:{} } } - Exception follows.\njava.lang.NoClassDefFoundError: org/apache/zookeeper/KeeperException\n        at org.kitesdk.morphline.solr.SanitizeUnknownSolrFieldsBuilder$SanitizeUnknownSolrFields.<init>(SanitizeUnknownSolrFieldsBuilder.java:68)\n        at org.kitesdk.morphline.solr.SanitizeUnknownSolrFieldsBuilder.build(SanitizeUnknownSolrFieldsBuilder.java:52)\n        at org.kitesdk.morphline.base.AbstractCommand.buildCommand(AbstractCommand.java:302)\n        at org.kitesdk.morphline.base.AbstractCommand.buildCommandChain(AbstractCommand.java:249)\n        at org.kitesdk.morphline.stdlib.Pipe.<init>(Pipe.java:46)\n        at org.kitesdk.morphline.stdlib.PipeBuilder.build(PipeBuilder.java:40)\n        at org.kitesdk.morphline.base.Compiler.compile(Compiler.java:126)\n        at org.kitesdk.morphline.base.Compiler.compile(Compiler.java:55)\n        at org.apache.flume.sink.solr.morphline.MorphlineHandlerImpl.configure(MorphlineHandlerImpl.java:101)\n        at org.apache.flume.sink.solr.morphline.MorphlineSink.start(MorphlineSink.java:97)\n        at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)\n        at org.apache.flume.SinkRunner.start(SinkRunner.java:79)\n        at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: org.apache.zookeeper.KeeperException\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 20 more",
      "So the core problem here again is a dependency is missing; ZooKeeper in this case. FLUME-2840 offers a patch which removes the test scope from all ZooKeeper. Even though the parent project has ZooKeeper in a compile time scope, the default Maven profiles that are set, hbase-98, maven-3, nonThrift, not-windows (if running linux), ossrh and hbase-98 scopes ZooKeeper as a test dependency so it is overriding the parent. So download the FLUME-2840.patch and fix it up.",
      "curl -O https://issues.apache.org/jira/secure/attachment/12783022/FLUME-2840.patch\npatch -p1 <FLUME-2840.patch",
      "Okay so that fixes the ZooKeeper dependencies. Firing up flume after another build gives the following warning.",
      "2016-05-24 11:50:45,030 (lifecycleSupervisor-1-1) [INFO - org.apache.solr.common.cloud.ConnectionManager.waitForConnected(ConnectionManager.java:207)] Waiting for client to connect to ZooKeeper\n2016-05-24 11:50:45,032 (lifecycleSupervisor-1-1-SendThread(localhost:2181)) [INFO - org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:966)] Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n2016-05-24 11:50:45,088 (lifecycleSupervisor-1-1-SendThread(localhost:2181)) [WARN - org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1089)] Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\njava.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)\n        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)",
      "This warning is great! It means we finally have our Flume process trying to talk to Solr. Go ahead and fire up the example Solr 4.10.3 in cloud mode with",
      "bin/solr -c -e cloud -noprompt",
      "Keep in mind that the example Solr cloud starts a ZooKeeper instance on the port 1000 greater than the Solr port, so 8983 for Solr and 9983 for ZooKeeper. Fire up Flume and drop a message in a file in the spooling directory, like",
      "echo joe > /tmp/messages/joe-messages",
      "We see the following.",
      "[INFO - org.kitesdk.morphline.stdlib.LogInfoBuilder$LogInfo.log(LogInfoBuilder.java:64)] output record: [{id=[joe]}]",
      "So far so good. Let’s see if it gets to ZooKeeper.",
      "[INFO - org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:849)] Socket connection established to localhost/127.0.0.1:9983, initiating session",
      "Check again! Okay this message is going through right!?",
      "2016-05-24 12:42:50,112 (SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.sink.solr.morphline.MorphlineSink.process(MorphlineSink.java:163)] Morphline Sink k1: Unable to process event from channel c1. Exception follows.\norg.apache.solr.client.solrj.impl.CloudSolrServer$RouteException: org/apache/http/pool/ConnPoolControl\n        at org.apache.solr.client.solrj.impl.CloudSolrServer.directUpdate(CloudSolrServer.java:360)\n        at org.apache.solr.client.solrj.impl.CloudSolrServer.request(CloudSolrServer.java:533)\n        at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:124)\n        at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:68)\n        at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:54)\n        at org.kitesdk.morphline.solr.SolrServerDocumentLoader.sendLoads(SolrServerDocumentLoader.java:140)\n        at org.kitesdk.morphline.solr.SolrServerDocumentLoader.sendBatch(SolrServerDocumentLoader.java:131)\n        at org.kitesdk.morphline.solr.SolrServerDocumentLoader.commitTransaction(SolrServerDocumentLoader.java:94)\n        at org.kitesdk.morphline.solr.LoadSolrBuilder$LoadSolr.doNotify(LoadSolrBuilder.java:104)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Connector.notify(Connector.java:57)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Connector.notify(Connector.java:57)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Connector.notify(Connector.java:57)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Connector.notify(Connector.java:57)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Notifications.notify(Notifications.java:96)\n        at org.kitesdk.morphline.base.Notifications.notifyCommitTransaction(Notifications.java:61)\n        at org.apache.flume.sink.solr.morphline.MorphlineHandlerImpl.commitTransaction(MorphlineHandlerImpl.java:148)\n        at org.apache.flume.sink.solr.morphline.MorphlineSink.process(MorphlineSink.java:156)\n        at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)\n        at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NoClassDefFoundError: org/apache/http/pool/ConnPoolControl\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        at org.apache.http.impl.client.SystemDefaultHttpClient.createClientConnectionManager(SystemDefaultHttpClient.java:118)\n        at org.apache.http.impl.client.AbstractHttpClient.getConnectionManager(AbstractHttpClient.java:466)\n        at org.apache.http.impl.client.AbstractHttpClient.createHttpContext(AbstractHttpClient.java:286)\n        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:851)\n        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805)\n        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:784)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:448)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n        at org.apache.solr.client.solrj.impl.LBHttpSolrServer.doRequest(LBHttpSolrServer.java:340)\n        at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:301)\n        at org.apache.solr.client.solrj.impl.CloudSolrServer$1.call(CloudSolrServer.java:341)\n        at org.apache.solr.client.solrj.impl.CloudSolrServer$1.call(CloudSolrServer.java:338)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        ... 1 more\nCaused by: java.lang.ClassNotFoundException: org.apache.http.pool.ConnPoolControl\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 29 more",
      "Back to dependency hell. What’s going on here? The root error is Caused by: java.lang.ClassNotFoundException: org.apache.http.pool.ConnPoolControl which indicates a problem with the httpclient dependency in the org.apache.httpcomponents group. We can try to track down the discrepancy by using Maven’s maven-dependency-plugin dependency:tree command.",
      "mvn dependency:tree -Dverbose -Dincludes=org.apache.httpcomponents:httpclient\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Flume NG Morphline Solr Sink 1.6.0\n[INFO] ------------------------------------------------------------------------\n[INFO] \n[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ flume-ng-morphline-solr-sink ---\n[INFO] org.apache.flume.flume-ng-sinks:flume-ng-morphline-solr-sink:jar:1.6.0\n[INFO] +- org.apache.flume:flume-ng-core:jar:1.6.0:compile\n[INFO] |  \\- org.apache.thrift:libthrift:jar:0.9.0:compile\n[INFO] |     \\- org.apache.httpcomponents:httpclient:jar:4.2.1:compile (version managed from 4.1.3)\n[INFO] \\- org.kitesdk:kite-morphlines-all:pom:1.0.0:compile\n[INFO]    \\- org.kitesdk:kite-morphlines-solr-core:jar:1.0.0:compile\n[INFO]       \\- org.apache.solr:solr-core:jar:4.10.3:compile\n[INFO]          +- org.apache.hadoop:hadoop-auth:jar:2.4.0:compile (version managed from 2.2.0)\n[INFO]          |  \\- (org.apache.httpcomponents:httpclient:jar:4.2.1:compile - version managed from 4.2.5; omitted for duplicate)\n[INFO]          \\- (org.apache.httpcomponents:httpclient:jar:4.2.1:compile - version managed from 4.3.1; omitted for duplicate)",
      "It appears that solr-core, and hadoop-auth directly, is dependent on a later httpclient version than what is provided during compile time due to Flume’s parent pom. So let’s bump httpclient from 4.2.1 to 4.3.1 in Flume’s parent pom.xml and build again. Run it all and, boom, another exception.",
      "2016-05-24 13:02:14,049 (SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:160)] Unable to deliver event. Exception follows.\norg.apache.flume.EventDeliveryException: Failed to send events\n        at org.apache.flume.sink.solr.morphline.MorphlineSink.process(MorphlineSink.java:186)\n        at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)\n        at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.solr.client.solrj.impl.CloudSolrServer$RouteException: org/apache/http/concurrent/Cancellable\n        at org.apache.solr.client.solrj.impl.CloudSolrServer.directUpdate(CloudSolrServer.java:360)\n        at org.apache.solr.client.solrj.impl.CloudSolrServer.request(CloudSolrServer.java:533)\n        at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:124)\n        at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:68)\n        at org.apache.solr.client.solrj.SolrServer.add(SolrServer.java:54)\n        at org.kitesdk.morphline.solr.SolrServerDocumentLoader.sendLoads(SolrServerDocumentLoader.java:140)\n        at org.kitesdk.morphline.solr.SolrServerDocumentLoader.sendBatch(SolrServerDocumentLoader.java:131)\n        at org.kitesdk.morphline.solr.SolrServerDocumentLoader.commitTransaction(SolrServerDocumentLoader.java:94)\n        at org.kitesdk.morphline.solr.LoadSolrBuilder$LoadSolr.doNotify(LoadSolrBuilder.java:104)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Connector.notify(Connector.java:57)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Connector.notify(Connector.java:57)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Connector.notify(Connector.java:57)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Connector.notify(Connector.java:57)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.AbstractCommand.doNotify(AbstractCommand.java:150)\n        at org.kitesdk.morphline.base.AbstractCommand.notify(AbstractCommand.java:132)\n        at org.kitesdk.morphline.base.Notifications.notify(Notifications.java:96)\n        at org.kitesdk.morphline.base.Notifications.notifyCommitTransaction(Notifications.java:61)\n        at org.apache.flume.sink.solr.morphline.MorphlineHandlerImpl.commitTransaction(MorphlineHandlerImpl.java:148)\n        at org.apache.flume.sink.solr.morphline.MorphlineSink.process(MorphlineSink.java:156)\n        ... 3 more\nCaused by: java.lang.NoClassDefFoundError: org/apache/http/concurrent/Cancellable\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.createMethod(HttpSolrServer.java:380)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n        at org.apache.solr.client.solrj.impl.LBHttpSolrServer.doRequest(LBHttpSolrServer.java:340)\n        at org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:301)\n        at org.apache.solr.client.solrj.impl.CloudSolrServer$1.call(CloudSolrServer.java:341)\n        at org.apache.solr.client.solrj.impl.CloudSolrServer$1.call(CloudSolrServer.java:338)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        ... 1 more\nCaused by: java.lang.ClassNotFoundException: org.apache.http.concurrent.Cancellable\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 11 more",
      "Searching around yields that org.apache.http.concurrent.Cancellable is part of httpcore so lets add that to our parent dependency tree.",
      "<dependency>\n    <groupId>org.apache.httpcomponents</groupId>\n    <artifactId>httpcore</artifactId>\n    <version>4.3.1</version>\n</dependency>",
      "Build and run and it appears we have no errors. Let’s commit to Solr and see if our document is there.",
      "curl 'localhost:8983/solr/gettingstarted/update?commit=true'\ncurl 'localhost:8983/solr/gettingstarted/select?q=*:*&wt=json&indent=true&rows=0'\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":5,\n    \"params\":{\n      \"q\":\"*:*\",\n      \"indent\":\"true\",\n      \"rows\":\"0\",\n      \"wt\":\"json\"}},\n  \"response\":{\"numFound\":1,\"start\":0,\"maxScore\":1.0,\"docs\":[]\n  }}",
      "Hurrah! We have numFound equal to one. Our document is successfully loaded and we have used Flume 1.6 (with updates) to load our data into Solr 4.10.3.",
      "So getting Flume to run Solr 4.X is not exactly easy at first glance but it works.",
      "What happens if we try to use this setup against Solr 5.2.1? Before running 5, make sure you shutdown any other Solr 4.10.3 instances running",
      "bin/solr stop -all",
      "Then go ahead in the solr-5.2.1 directory and create the cloud example as we did before.",
      "bin/solr -c -e cloud -noprompt",
      "When we try to index, another ERROR occurs.",
      "2016-05-24 13:54:22,748 (lifecycleSupervisor-1-1) [ERROR - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:253)] Unable to start SinkRunner: { policy:[email protected] counterGroup:{ name:null counters:{} } } - Exception follows.\norg.apache.solr.common.SolrException: Plugin Initializing failure for [schema.xml] fieldType. Schema file is /tmp/1464112462569-0/conf/managed-schema\n        at org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:595)\n        at org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:166)\n        at org.apache.solr.schema.ManagedIndexSchema.<init>(ManagedIndexSchema.java:72)\n        at org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:171)\n        at org.apache.solr.schema.ManagedIndexSchemaFactory.create(ManagedIndexSchemaFactory.java:45)\n        at org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java:69)\n        at org.kitesdk.morphline.solr.SolrLocator.getIndexSchema(SolrLocator.java:181)\n        at org.kitesdk.morphline.solr.SanitizeUnknownSolrFieldsBuilder$SanitizeUnknownSolrFields.<init>(SanitizeUnknownSolrFieldsBuilder.java:70)\n        at org.kitesdk.morphline.solr.SanitizeUnknownSolrFieldsBuilder.build(SanitizeUnknownSolrFieldsBuilder.java:52)\n        at org.kitesdk.morphline.base.AbstractCommand.buildCommand(AbstractCommand.java:302)\n        at org.kitesdk.morphline.base.AbstractCommand.buildCommandChain(AbstractCommand.java:249)\n        at org.kitesdk.morphline.stdlib.Pipe.<init>(Pipe.java:46)\n        at org.kitesdk.morphline.stdlib.PipeBuilder.build(PipeBuilder.java:40)\n        at org.kitesdk.morphline.base.Compiler.compile(Compiler.java:126)\n        at org.kitesdk.morphline.base.Compiler.compile(Compiler.java:55)\n        at org.apache.flume.sink.solr.morphline.MorphlineHandlerImpl.configure(MorphlineHandlerImpl.java:101)\n        at org.apache.flume.sink.solr.morphline.MorphlineSink.start(MorphlineSink.java:97)\n        at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)\n        at org.apache.flume.SinkRunner.start(SinkRunner.java:79)\n        at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.solr.common.SolrException: Plugin Initializing failure for [schema.xml] fieldType\n        at org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:193)\n        at org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:486)\n        ... 26 more\nCaused by: org.apache.solr.common.SolrException: Must specify units=\"degrees\" on field types with class SpatialRecursivePrefixTreeFieldType\n        at org.apache.solr.schema.AbstractSpatialFieldType.init(AbstractSpatialFieldType.java:113)\n        at org.apache.solr.schema.AbstractSpatialPrefixTreeFieldType.init(AbstractSpatialPrefixTreeFieldType.java:43)\n        at org.apache.solr.schema.SpatialRecursivePrefixTreeFieldType.init(SpatialRecursivePrefixTreeFieldType.java:37)\n        at org.apache.solr.schema.FieldType.setArgs(FieldType.java:166)\n        at org.apache.solr.schema.FieldTypePluginLoader.init(FieldTypePluginLoader.java:141)\n        at org.apache.solr.schema.FieldTypePluginLoader.init(FieldTypePluginLoader.java:43)\n        at org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:190)\n        ... 27 more",
      "This error is due to the sample configuration using a managed-schema in combination with the org.kitesdk.morphline.solr.SolrLocator. It just doesn’t know how to handle a managed-schema. To work around this, start a Solr Cloud example but choose the basic_config instead. Trying again, we get a very similar ERROR.",
      "2016-05-24 14:01:37,506 (lifecycleSupervisor-1-7) [ERROR - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:253)] Unable to start SinkRunner: { policy:[email protected] counterGroup:{ name:null counters:{} } } - Exception follows.\norg.apache.solr.common.SolrException: Plugin Initializing failure for [schema.xml] fieldType. Schema file is /tmp/1464112897417-0/conf/schema.xml\n        at org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:595)\n        at org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:166)\n        at org.apache.solr.schema.IndexSchemaFactory.create(IndexSchemaFactory.java:55)\n        at org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java:69)\n        at org.kitesdk.morphline.solr.SolrLocator.getIndexSchema(SolrLocator.java:181)\n        at org.kitesdk.morphline.solr.SanitizeUnknownSolrFieldsBuilder$SanitizeUnknownSolrFields.<init>(SanitizeUnknownSolrFieldsBuilder.java:70)\n        at org.kitesdk.morphline.solr.SanitizeUnknownSolrFieldsBuilder.build(SanitizeUnknownSolrFieldsBuilder.java:52)\n        at org.kitesdk.morphline.base.AbstractCommand.buildCommand(AbstractCommand.java:302)\n        at org.kitesdk.morphline.base.AbstractCommand.buildCommandChain(AbstractCommand.java:249)\n        at org.kitesdk.morphline.stdlib.Pipe.<init>(Pipe.java:46)\n        at org.kitesdk.morphline.stdlib.PipeBuilder.build(PipeBuilder.java:40)\n        at org.kitesdk.morphline.base.Compiler.compile(Compiler.java:126)\n        at org.kitesdk.morphline.base.Compiler.compile(Compiler.java:55)\n        at org.apache.flume.sink.solr.morphline.MorphlineHandlerImpl.configure(MorphlineHandlerImpl.java:101)\n        at org.apache.flume.sink.solr.morphline.MorphlineSink.start(MorphlineSink.java:97)\n        at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)\n        at org.apache.flume.SinkRunner.start(SinkRunner.java:79)\n        at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.solr.common.SolrException: Plugin Initializing failure for [schema.xml] fieldType\n        at org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:193)\n        at org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:486)\n        ... 24 more\nCaused by: org.apache.solr.common.SolrException: Must specify units=\"degrees\" on field types with class SpatialRecursivePrefixTreeFieldType\n        at org.apache.solr.schema.AbstractSpatialFieldType.init(AbstractSpatialFieldType.java:113)\n        at org.apache.solr.schema.AbstractSpatialPrefixTreeFieldType.init(AbstractSpatialPrefixTreeFieldType.java:43)\n        at org.apache.solr.schema.SpatialRecursivePrefixTreeFieldType.init(SpatialRecursivePrefixTreeFieldType.java:37)\n        at org.apache.solr.schema.FieldType.setArgs(FieldType.java:166)\n        at org.apache.solr.schema.FieldTypePluginLoader.init(FieldTypePluginLoader.java:141)\n        at org.apache.solr.schema.FieldTypePluginLoader.init(FieldTypePluginLoader.java:43)\n        at org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:190)\n        ... 25 more",
      "It appears that we are hitting a wall with the org.kitesdk.morphline.solr.SolrLocator object and need to get Kite SDK updated.",
      "Updating Solr on Flume",
      "Flume is well designed for modularity and we can take advantage of so good design decisions by Cloudera to update Flume to use a later Solr version. As mentioned at the beginning, Cloudera contributed solr-morphlines-core and solr-morphlines-cell to Apache Solr directly and they kept the function calls the exact same. So we can exclude the cdk-morphlines-solr-core and cdk-morphlines-solr-cell dependencies and instead use the official Solr libraries instead. KITE-999 briefly discusses this switch but doesn’t really spell it out so it is sort of that situation where to know what to do, you already have to know what to do.",
      "So to accomplish this, lets update the flume-ng-morphline-solr-sink pom.xml to exclude the two cdk resources and instead include the solr dependencies. First we update the kite-morphlines-all to exclude these portions.",
      "<exclusion>\n  <groupId>org.kitesdk</groupId>\n  <artifactId>cdk-morphlines-solr-core</artifactId>\n</exclusion>\n<exclusion>\n  <groupId>org.kitesdk</groupId>\n  <artifactId>cdk-morphlines-solr-cell</artifactId>\n</exclusion>",
      "Then we want to add the new Solr resources.",
      "<dependency>\n  <groupId>org.apache.solr</groupId>\n  <artifactId>solr-morphlines-core</artifactId>\n  <version>5.2.1</version>\n</dependency>\n\n<dependency>\n  <groupId>org.apache.solr</groupId>\n  <artifactId>solr-cell</artifactId>\n  <version>5.2.1</version>\n</dependency>",
      "Finally in the parent pom.xml there is a solr-cell entry that needs to be updated to 5.2.1 as well. Because we are excluding the cdk-morphlines-solr-core the SolrLocator call ends up using org.apache.solr.morphlines.solr SolrLocator instead of org.kitesdk.morphline.solr SolrLocator. Let’s see what happens now when we use Flume on Solr 5.2.1.",
      "2016-05-24 14:26:10,816 (lifecycleSupervisor-1-1) [INFO - org.apache.solr.core.SolrResourceLoader.<init>(SolrResourceLoader.java:138)] new SolrResourceLoader for directory: '/tmp/1464114370809-0/'\n2016-05-24 14:26:11,091 (lifecycleSupervisor-1-1) [INFO - org.apache.solr.core.SolrConfig.refreshRequestParams(SolrConfig.java:927)] current version of requestparams : -1\n2016-05-24 14:26:11,188 (lifecycleSupervisor-1-1) [INFO - org.apache.solr.update.SolrIndexConfig.<init>(SolrIndexConfig.java:158)] IndexWriter infoStream solr logging is enabled\n2016-05-24 14:26:11,192 (lifecycleSupervisor-1-1) [INFO - org.apache.solr.core.SolrConfig.<init>(SolrConfig.java:239)] Using Lucene MatchVersion: 5.2.1\n2016-05-24 14:26:11,263 (lifecycleSupervisor-1-1) [INFO - org.apache.solr.core.SolrConfig.<init>(SolrConfig.java:329)] Loaded SolrConfig: solrconfig.xml\n2016-05-24 14:26:11,266 (lifecycleSupervisor-1-1) [INFO - org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:450)] Reading Solr Schema from /tmp/1464114370809-0/conf/schema.xml\n2016-05-24 14:26:11,296 (lifecycleSupervisor-1-1) [INFO - org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:478)] [null] Schema name=example\n2016-05-24 14:26:11,489 (lifecycleSupervisor-1-1) [INFO - org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:577)] unique key field: id",
      "Great and after testing a document ingestion, everything works as expected and we have successfully integrated Flume with Solr 5!",
      "If you bump those depedencies to 6.0.0, Flume with work with Solr 6! Fantastic.",
      "Conclusion",
      "Wow getting projects to work together can sometimes be harder than it appears at first. At OSC we love tackling these hard problems and getting your data searchable get in touch to see how we can help today!",
      "Code",
      "The preceding examples used the following config files.",
      "",
      "",
      "You can find everything ready to compile from these sources:",
      "Flume 1.6.0 using Solr 4.10.3\n  Flume 1.6.0 using Solr 5.2.1\n  Flume 1.6.0 using Solr 6.0.0",
      "Sidebar",
      "Just because we got everything working doesn’t mean there aren’t some nagging things left over.",
      "Managed Schema",
      "I can hear you saying, \"Joe what about a managed schema? You just glossed over the fact that it isn’t working!\" And you are totally right and the answer is, it’s complicated. Didn’t we encounter an error when using it? Yes indeed we did but on May 29, 2014, Gregory Chanan made the commit Give SolrLocator the ability to handle managed schemas. which is included in release-1.1.0  release-1.0.0 release-0.18.0 release-0.17.1 release-0.17.0 release-0.16.0 release-0.15.0 of Kite SDK, however it only supports Solr 4 and so some of the enhancements haven’t been ported to the Solr distribution. So in this case, we are kinda stuck. Perhaps a little love will be given to the Solr release to enable the fix.",
      "Old References in Flume Documentation",
      "After running Flume the first time and following the documentation, I got the following error.",
      "2016-05-24 10:55:19,330 (lifecycleSupervisor-1-0) [ERROR - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:253)] Unable to start SinkRunner: { policy:[email protected] counterGroup:{ name:null counters:{} } } - Exception follows.\norg.kitesdk.morphline.api.MorphlineCompilationException: No command builder registered for name: readLine near: {\n    # conf/morphline.conf: 13\n    \"readLine\" : {\n        # conf/morphline.conf: 14\n        \"charset\" : \"UTF-8\"\n    }\n}\n        at org.kitesdk.morphline.base.AbstractCommand.buildCommand(AbstractCommand.java:281)\n        at org.kitesdk.morphline.base.AbstractCommand.buildCommandChain(AbstractCommand.java:249)\n        at org.kitesdk.morphline.stdlib.Pipe.<init>(Pipe.java:46)\n        at org.kitesdk.morphline.stdlib.PipeBuilder.build(PipeBuilder.java:40)\n        at org.kitesdk.morphline.base.Compiler.compile(Compiler.java:126)\n        at org.kitesdk.morphline.base.Compiler.compile(Compiler.java:55)\n        at org.apache.flume.sink.solr.morphline.MorphlineHandlerImpl.configure(MorphlineHandlerImpl.java:101)\n        at org.apache.flume.sink.solr.morphline.MorphlineSink.start(MorphlineSink.java:97)\n        at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)\n        at org.apache.flume.SinkRunner.start(SinkRunner.java:79)\n        at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)",
      "A Google search later, it turns out that the link in the Flume documentation leads to the Cloudera Development Kit which was renamed in 2013 to Kite SDK and the error happened because my morphlines.conf contained importCommands : [\"com.cloudera.**\", \"org.apache.solr.**\"] which was what the old docs which Flume linked to said.  The project is actually using the new Kite SDK so updating the line to importCommands : [\"org.kitesdk.**\", \"org.apache.solr.**\"] fixed it up."
    ],
    "summary_t": "Learn how to update Apache Flume to use the correct MorphlineSolrSink for your Solr 5 or Solr 6 deployment."
  },
  {
    "id": "2d994c33ecbfb9c22b66174cdc5d16de",
    "url_s": "https://opensourceconnections.com/blog/2016/06/01/thoughts-on-algolia/",
    "title": "Thoughts on Algolia (vs Solr & Elasticsearch)",
    "content": [
      "After getting cranky on one algolia blog post, and having a Search Disco episode with Julien Lemoine CTO of Algolia, I’m left fascinated by the solution.",
      "Algolia, so hot right now!",
      "Algolia presupposes that we’re all going to want instant search (aka search-as-you-type). So they’ve built extremely good, hosted instant search. Everything you’d want to do with instant search is there. Just read this amazing post on query understanding",
      "Typo tolerance, \"Doug Trunbull\"\n  Prefix search \"Doug Turn\"\n  Prefix with typo tolerance \"Doug Trun\"\n  Decompounding (sometimes this can be typo tolerance) DougTrun\n  A query-time lemmatizer, and more…",
      "It’s like instead of search, you have autocomplete on steroids. Couple this with saner default ranking than Elasticsearch or Solr, and you’re left with a pretty compelling product.",
      "I say all that as a pretty rabid Lucenist. I wrote a book about Relevance in Lucene. I’m very biased towards Lucene, as I’ve seen many compelling solutions built with Solr and Elasticsearch as well.",
      "One thing though that I’ve learned about Lucene-based search is you can, with a good team, build just about any searchy thing with it. Yet it does take a team. Lucene-based search isn’t really meant to work well \"out of the box\" for your solution. Regardless of how easy Elasticsearch has made it, Lucene-based search is a framework, not a solution. It’s a set of really amazing search-focussed data structures that you can cobble together relatively easily to do Your Thing™. Even if Your Thing™ means altering details as low-level as how the index gets stored to disk!",
      "Another way to say that is you could build Algolia in Elasticsearch (or get close enough). You can’t build Elasticsearch in Algolia. You gain from Algolia focus on a specific problem. You sacrifice deep customizability and extendability of open source. Yet another way to say it is to compare to search solutions to web apps. In many ways Algolia is like building a site with a site builder like Wix. Lucene is more like building your own web app with developers behind it, and all the associated low-level considerations, annoyances, but also power.",
      "Case in point is Algolia’s performance comparison to Elasticsearch. In Algolia’s tests, Algolia claims up to 200x performance improvement. On average, there is more of a 10-20x performance improvement (still impressive). However, Algolia chose the lowest common denominator in instant search in Elasticsearch: fuzzy queries and prefix queries. As is written in Elasticsearch: The Definitive Guide another common approach that improves speed tremendously is to use ngrams. Basically avoid the query-time fuzzy work and build a giant data structure that can handle it.",
      "Now ngrams have their own problems. They grow your index. However, in the case of 2 million documents with lots of short text, it might not bloat the index that much. And I suspect it would have improvements of orders of magnitude in performance. If a bloated index became a problem, we could produce fewer ngrams of larger size. There’s also caching to consider: I bet both solutions cache results for each keystroke query. So I wonder how that colors the Algolia vs ES consideration.",
      "We might even reverse terms placed in the index to get suffix queries to catch earlier typos. Or do exotic things with fuzzy queries and ngrams simultaneously. We might even write a Lucene query that focusses on typos. Look at all the power here!",
      "The point though is I’ve made you start to think about how you’d solve the problem. Algolia already has built a solution! Why not just run with theirs? Well there’s a couple of reservations I would have going whole-hog into the Algolia camp:",
      "Turn-key can often turn into lock-in. There are examples of hosted search solutions (and databases being acquired and the new owner (In FoundationDB’s case Apple) not being interested in supporting the existing business.\n  You care about \"things not strings.\" Algolia’s solution strongly focusses on specific string matching. Lucene’s approach to relevance focusses more abstractly on terms as features of content, using TF*IDF as a feature similarity system (our book largely discusses relevance in these terms).\n  You’re doing anything close to non-traditional. You have a specific query language to implement. You need to explicitly map vernaculars between experts and lay-people. You want to do learning-to-rank. You want to do use controlled vocabularies, build semantic search. You have specific Geo concerns. All these are features you can build into Solr/ES and you’re locked into what Algolia gives you.\n  You want to deeply manipulate the search engine’s behavior. This is a huge part of Lucene’s sweet spot.",
      "But there’s a couple of reasons I would strongly consider Algolia",
      "You have a small team, but good search is important. Algolia works pretty well out of the box. It’s got a good level of configurability of ranking that can include both text and numeric values like popularity with some geo support.\n  You primarily need to support single-item lookups. Algolia’s approach is ideal for cases such as needing to lookup \"banana\" and match on bananas. Algolia may not make sense for users that type \"minion fruit\" and expect bananas.\n  You need to support typos. Lucene solutions to this are awkward. I hope that they’ll get better and faster, but fuzzy searching is not Lucene’s sweet spot.",
      "Here’s a couple pieces of Algolia marketing I would disagree with:",
      "Algolia likes to point out that hosting Elasticsearch is going to be hard. I think with options like Bonsai and Elastic Cloud, this is hardly the case. With a good ES host, you basically have a good \"API in the cloud\" that’s just as easy to work with as any other service.\n  Algolia wants you to believe Elasticsearch is not a good search engine. It’s only good at big data analytics and \"big data search\" (not sure what that means). In the spirit of finding \"things not strings\" I would disagree. It just takes work and understanding what’s special about your search solution.\n  Algolia hopes everything is going to become instant search. Yet in my experience, the preponderance of search experiences (even many driven by Algolia) are autocomplete first to select keywords, and then keyword search. This is still in Lucene’s sweet spot for search.\n  I believe the benchmarks Algolia provides, but it’s noted that they don’t try faster instant search strategies on Elasticsearch. We can’t recreate their benchmarks ourselves. Algolia seems trustworthy, but I wish I could test this independently. I also would like to see them rerun against newer Elasticsearch versions.",
      "But Algolia points out important weaknesses in Lucene’s relevance model",
      "Typos and fuzzy matching: To the extent that the world wants instant search with typo tolerance, Lucene-based search is hard to get working. I’ll also believe it’s slower than Algolia’s focussed solution (though I can’t recreate the benchmarks).\n  Elasticsearch/Solr defaults for relevance are hard to tune. As Algolia rightly points out, the dismax ranking function yields fairly confusing results. We write about that phenomenon here. You can and should get away from these defaults, but I wish search made more sense out of the box.\n  Elasticsearch and Solr primitives for relevance feel low-level. Algolia’s feel higher level and more focussed on creating a common understanding between the business and developers. This is one big reason we built Quepid. Even still, with all the options in Solr/ES you’ll still get blank stares from the business when you start speaking in terms of boolean queries, function queries, and what-not.",
      "My big takeaway is I’m actually pretty enthusiastic about Algolia for the right use cases. But you need to be certain it’ll satisfy your needs. I hope this has made you a more informed shopper. In our relevance consulting practice we’d love to help you figure out which solution is right for you. Be sure to get in touch to discuss which solution (Solr, Elasticsearch, Algolia) is right for your needs!"
    ],
    "summary_t": "Algolia presupposes that we’re all going to want instant search (aka search-as-you-type). How does it compare to Solr and Elasticsearch in the marketplace of..."
  },
  {
    "id": "c0f23cd493899ad2e581346689746ed2",
    "url_s": "https://opensourceconnections.com/blog/2016/06/06/interpreting-google-analytics-site-search/",
    "title": "Interpreting Google Analytics for Site Search",
    "content": [
      "This article provides some guidance on understanding user behavior using what’s provided in Google Analytics. Specifically, it examines the statistics provided under the \"search terms\" page, found under \"Behavior\" -> \"Site Search\" -> \"Search Terms\". For example, here’s a sample of OSC’s search terms in Google Analytics",
      "",
      "Interpreting search terms",
      "Perhaps most obviously, the \"search terms\" column show you what users are searching for. But its worth stopping to interpret what a search term might be telling you. For example, a search for \"fancy restaurant dress code\" on a restaurant search engine obviously points at a specific kind of use case for search. Instead of focussing on this specific search, think how it might actually represent a broader pattern: users come to search to look up the dress codes for specific fine-dining restaurants. Likely many search terms stand in for these broader use cases.",
      "Search experts like to use the word information need to describe in greater detail what a user is looking for. This is another way of analyzing search terms. Sometimes a user with one information need will use many different searches. You’ve probably experienced this. If you’re shopping for a toy for your child, your information need may be something along the line of",
      "\"I want a toy for my son, hopefully educational, below $10, and nothing noisy\"",
      "Now usually we don’t sit and write all that down. Much of it may be unconscous. Rather, through the act of searching and refining, we hunt for something that might match our requirements. For example, if we’re trying to satisfy the above information need, we likely will search multiple times, using queries such as:",
      "\"educational toy\"\n  \"educational toy for boys\"\n  \"non electronic educational toy\"\n  \"non electronic educational toy\" (adds price filter)",
      "Eventually we’ll find the right toy using all the facilities of the search inderface and purchase the toy.",
      "The goal in search analytics is to discover use cases and information needs. What sorts of things are people looking for? What patterns do they use to search for them? Can we give users better hints that put them on the right path? Are we dissapointing them or delighting them? Interpreting the analytics depends requires care: your specific knowledge of your users and content must be brought to bare.",
      "Kinds of Information Needs",
      "Its worth noting that search satisfies many kinds of information needs. Some information needs correspond to \"single item\" searches. They attempt to find the one right most relevant answer. Others are about gathering information and comparing across many relevant results. In other words: some users know exactly what they want; others want to use search to explore, compare, and research.",
      "This is worth pointing out because it directly influences how you interpret analytics. Users that want the \"one right answer\" expect the first result to match. The ideal case for these users is they click the first result and appear to leave the result satisfied. A happy exploratory searcher, on the other hand, may issue many searches. They may click on every result. They may scan the results and visit the next page.",
      "It’s important to consider whether your searchers are exploratory or single-item. Its likely your search application has a mixture of both: information needs with 1 right answer, and others that involve \"shopping around\" for different options. For a medical system’s site, trying to pick the right allergist corresponds to exploratory \"shopping around\" information needs. Support the user making a complicated the decision. Searching for a specific doctor’s name, for example, isn’t about shopping around. Its about navigating to the one exact answer.",
      "Analyzing search terms, in summary",
      "Always analyse search terms by considering:",
      "What broader use case might it represent?\n  What might the true information need be?\n  Is the information need \"known item\" or \"exploratory\"?",
      "Interpreting each stastistic",
      "With a sense for what the information need behind each search query, let’s begin to take apart the statistics that Google Analytics gives us.",
      "Total unique searches",
      "Common searches should work well. But before focussing excessively on the most popular search term, consider how diverse your search term are. To determine this, examine the percentage of searches (the number in gray).",
      "Some search applications have what some refer to as a long tail. This is a fancy way of saying the application has an unusually diverse set of search terms. The top terms represent a relatively low overall percentage. Other applications are referred to as short tail they are unusual in that only a handful of search terms drive most traffic. Perhaps the top 10 terms in total account for 25% of the searches.",
      "An example of a long tail search application might be Google or Amazon. They must satisfy an extreme number of searches and no one search accounts for a significant proportion of all results. A short term search application might be one for a restaurant site. The top query might be \"hours\" or \"menu.\" These may account for 75% of the search traffic.",
      "If you’re a short tail application, use the total unique searches as a way to prioritize those exact searches. For a long tail application, instead of focussing on the specific keyword searches, focus on patterns in information needs and use cases. The specific search terms should stand in as exemplars of a general use case to focus on.",
      "Results pageviews/search",
      "This column indicates how many pages (as in the user clicking \"next page\") the searcher viewed. Viewing a single page (a value 1.0) here is often ideal. Viewing many pages is often seen as a negative behavior.",
      "However this depends on the application and information need. Exploratory searchers might simply be performing an exhaustive analysis of the results. In a B2B e-commerce application, for instance, a purchasing officer may wish to comprehensively review every widget to ensure that the widget is a good price and meets all the requirements. This might point to usability improvements you can make to give users a single-glance comprehensive understanding of the results. It might also point at ways your search UI might enable decision making by highlighting important criteria users depend on for decision making.",
      "For exact item searches (search for a specific doctor name, product part number, etc) paging can be deadly. It indicates a terrible case: the specified item wasn’t #1, it wasn’t even in the top 10! Paging for these users should be minimized.",
      "As always, interpreting paging depends on trying to understand your users and their information needs!",
      "Time after search & average search depth",
      "Time after search and average search depth indicate how much the site was interacted with after clicking a search result. Usually higher time and search depth indicate that search isn’t a problem. However, it may indicate a problem with other parts of the site.",
      "Single-item searchers may be experiencing something frustrating with the site. For example a difficult checkout process. Perhaps it should normally take two clicks to checkout, but the search depth oddly averages to 5 clicks. That may indicate a problem.",
      "Users gathering information should be expected to spend considerable time after the search and on the next result. Searching for articles, the user should be expected to have a search depth of 1 following by several minutes reading. Additional search depth could mean clicking on a linked article that’s interesting.",
      "It’s important to note one reason high search depth/time after search might point at poor search. It could indicate users give up on search and try the site’s other navigation features. For example, they might search on \"mexican food near me\" in a restaurant search. Maybe they didn’t like what they clicked on, so they began using the site’s other navigational features such as a browse to restaurant by genre.",
      "Refinements and exits",
      "Refinements are changes in search terms to alter, narrow, or widen the search. For example, in the \"toy search\" example above, many refinements were issued to find the toy that matched all the user’s requirements. Refinements aren’t always bad: they should be expected. But lets break down cases of good/bad refinements.",
      "Expect a vague search to have many refinements. For example a user may start with \"mexican restaurant\" and refine to \"mexican restaurant tacos\" or \"mexican restaurant in Brooklyn.\" Just as above for the \"toy search\" the user refined search to further express the information need. Users refine to explore and understand a data set. Sometimes they know they’re using vague terms and as long as the search engine is keeping the conversation alive and acting in an understandable matter, then users are eager to keep exploring. One important thing to note is users need relevance feedback from the application to refine: they need good hit highlighting that demonstrates reasonably relevant results.",
      "That being said, a hugh number of refinements combined with a high exit rate likely indicate a problem. It indicates users are in an area where the conversation between themselves and the search application has broken down. Also obvious, specific, single item searches (non exploratory) with high level of refinements are also problematic and likely point at a relevance problem.",
      "One aside on refinements: highly performant search encourages more refinements. Fixing a performance problem may lead to more refinements. This is a good thing! The faster your search is, the more ability your users have to explore and find. As long as the application behaves sensibly with good relevance feedback, these refinements may be perfectly fine.",
      "Other stats to track",
      "These are stats not in google analytics that would be wise to track.",
      "0 result searches: Users have information needs they expect you to satisfy, yet you are unable to serve them. May also indicate misspellings or typos that you’re not handling.\n  Conversions: If you have explicit goals (such as a purchase) track whether search leads to these.\n  Click Depth: what position in the search results is clicked on. For example, if the users always click on the 5th result, the clickdepth would be 5. Particularly for single item searches, low click depth is preferred. Statistics such as mean-reciprical rank summarize the click depth across a set of searches.",
      "Let the experts help!",
      "OpenSource Connections wrote the book on search relevance! Let us help you improve your site search with analytics-driven tuneups where we guarantee to move your business metrics forward through search. Get in touch to learn more!",
      "References & further reading",
      "Google Analytics Site Search Documentation\n  Search Analytics for Your Site, by Louis Rosenfeld\n  Enterprise Search, 2nd Edition, by Martin White\n  Relevant Search, by Doug Turnbull and John Berryman"
    ],
    "summary_t": "How to make heads or tails of your Google analytics for site search. What do the stats mean? When do they point to problematic searches?"
  },
  {
    "id": "9589e451c99dfcecccc7cb8f562c0b15",
    "url_s": "https://opensourceconnections.com/blog/2016/06/06/recommender-systems-101-basket-analysis/",
    "title": "Recommender Systems 101: Basket Analysis",
    "content": [
      "You’ve probably seen \"buyers also bought\" recommendations. To drive more sales, you highlight items that are frequently purchased together. For example, the screenshot below shows \"frequently bought together\" for a monitor on Amazon.",
      "",
      "Basket Analysis",
      "One source for recommendations is basket analysis. Basket analysis literally means analyzing shoppers baskets. If you could line up all your user’s carts, and count how many items are purchased together, you can make informed recommendations to shoppers. For example, check out these baskets:",
      "",
      "You can see above that pie crust typically goes with green apples. You can use this information to:",
      "Implement \"Buyers also bought\" recommendations\n  Suggest to shoppers that they might be missing something when they check out\n  Perform personalized recommendations\n  Personalize search to cater to user’s past purchases",
      "All in service of better sales!",
      "And as an aside, while it’s useful to think about finding patterns in baskets, you can define your \"basket\" to be anything very narrow (an actual basket) to something very broad (every item a user has ever purchased). And don’t limit your thinking to just shopping! User browse sessions, interactions at conference booths, all these things can be \"baskets.\"",
      "Can we just use raw counts?",
      "Let’s poke around how one would use basket analysis to implement a basic \"buyers also bought\" recommendation system. What would be the first pass you’d make at implementing this?",
      "One naive implementation of basket analysis works based on just raw counts. Given the baskets below, what should be recommended to users that purchase \"pie crusts\"?",
      "Basket A\n      Basket B\n      Basket C\n      Basket  D\n      Basket E\n    \n  \n  \n    \n      Eggs\n      Eggs\n      Eggs\n      Eggs\n      Eggs\n    \n    \n       \n      Milk\n      Milk\n      Milk\n      Milk\n    \n    \n      Pie Crust\n      Pie Crust\n      Pie Crust\n      Cereal\n      Ground Beef\n    \n    \n      Green Apples\n      Green Apples\n      Pumpkin Filling",
      "With a count based solution, you:",
      "Filter down to baskets with just pie crusts (baskets A, B, and C)\n  Count the other items in those baskets\n  Recommend the most common items in those baskets as related to pie crusts",
      "If we follow those steps, we can start with basket A beginning our counting:",
      "Eggs (1)\n  Green Apples (1)",
      "Next to basket B",
      "Eggs (2)\n  Green Apples (2)\n  Milk (1)",
      "Finally adding in basket C, you get:",
      "Eggs (3)\n  Green Apples (2)\n  Milk (2)\n  Pumpkin filling (1)",
      "Examining the counts, you see we’d recommend that pie crust shoppers buy eggs.",
      "But wait, that doesn’t seem particularly smart. If you examine our other baskets, everyone buys eggs. The pie crust baskets have specific items that have a more specific affinity: other pie ingredients – green apples and pumpkin filling. These items don’t occur in other baskets. Because of this concentration, we ought to be really promoting these items.",
      "Using just raw counts, you’ll always bias towards global popularity and not what’s special about this subset of the data. You don’t capture essential pie-ness of this data set. This problem has a name: the Oprah book club problem. The name comes from book recommendations. Everyone buys what Oprah recommends. Using these raw book sales you’d recommend that owners of Science Fiction read Eat Pray Love. Just because Eat Pray Love ends up on every Science Fiction fan’s bookshelf doesn’t mean that it’s particularly related to science fiction fan’s tastes.",
      "Statistically, we need to test the significance of the relationship, not just the raw count. Let’s explore ways of getting at the significance of a relationship between two items:",
      "Naive similarity measures (Jaccard and z-tests)",
      "The next approach to performing recommendations is to use the humble Venn Diagram. Venn diagrams are always helpful, right? They always helped me get at least a C in middle school math :). Anyway, if we thought about baskets that had eggs and pie crust, we might see the following:",
      "",
      "In the last section, we just used raw counts in the scoring. Which corresponds to the size of the filled in black area below:",
      "",
      "This amount of overlap is larger than the amount of overlap comparing pie crust to green apples:",
      "",
      "As we stated in the last section, the size of the overlap shouldn’t be taken just at face value. This overlap size corresponds to raw counts. Instead this overlap should be considered relative to how many total baskets have eggs OR pie crust. In other words, how much of the overlap dominates the larger yellow area below?",
      "",
      "If the overlap (black area) is very close to the total number of baskets with either eggs or pie crust (yellow area) then we’re onto something statistically. Mathematically, this is known as the Jaccard Similarity. Given two sets A and B, the Jaccard similarity is the intersection of A and B (the black area, the raw counts from before) divided by the union of A with B (the yellow area, all baskets with eggs or pie crust). Restating mathematically, you could say: A  AND B / A OR B.",
      "Think about it this way. If items always occur in baskets together (say peanut butter and jelly), then their Jaccard similarity will be 1:",
      "Baskets with both peanut butter and jelly: 500\n  Baskets with either peanut butter or jelly: 500\n  Jaccard Similarity: 1.0 (exactly alike)",
      "Compare that to two foods that never go together: sugar free pancake syrup and Bob’s giant bag o’ sugar:",
      "Baskets with both sugar free syrup and giant bag o’ sugar: 2\n  Bags with either sugar free syrup or giant bag o’ sugar: 1000\n  Jaccard Similarity = 2 / 1000 = 0.002 (very low similarity)",
      "Mathematically, if we revisit the pie crust baskets and score eggs and apples, we see this born out:",
      "Eggs & Pie Crust:",
      "Baskets with both eggs and pie crust: 3\n  Baskets with either eggs or pie crust: 5\n  Jaccard Similarity: 3 / 5 = 0.60",
      "Compare to Pie Crust and Green Apples",
      "Baskets with both pie crust and green apples: 2\n  Baskets with either pie crust or green apples: 3\n  Jaccard Similarity: 2/3 or 0.6666…",
      "As we see with Jaccard, apples beat out eggs for pie crust purchasers (though just barely!)",
      "Problems with Jaccard",
      "It may be a bit surprising that eggs still are highly recommended for pie crust purchasers. Indeed, this is a problem with Jaccard similarity. When we look at the distribution of product purchases, we can see the problem:",
      "",
      "Some items are purchased by everyone (eggs). Others are moderately purchased (pie crust). But there’s an extremely long tail of extremely rare purchases of things like organic llama meat and prank black teeth bubble gum.",
      "When these rare items overlap, the Jaccard similarity scores the relationship very high. If someone happened to buy organic llama meat and black teeth bubble gum at the same time, we’d suddenly assume they were related. Their Jaccard score is 1/1 as their Venn diagrams overlap exactly.",
      "One obvious problem is we don’t have a way of seeing if enough occurrences of each item have occurred to make a statistically significant judgement. Prank bubble gum and organic llama meat have each only been purchased once. Perhaps if they had been purchased hundreds of times together, we’d feel more confident in the outcome of Jaccard similarity. A Jaccard score of 450 / 500 should actually be seen as more interesting than the spurious 1/1 (or even 5/5) occurrences.",
      "Other Significance Measures",
      "Jaccard is a fairly naive significance measure. Another avenue researches have gone down, as Ted Dunning points out in his paper, \"Accurate Methods for the Statistics of Surprise and Coincidence\" is traditional statistical measures.",
      "It most significance measures, you’re attempting to show the Null Hypothesis is very unlikely. The null hypothesis says that one event has no relationship to another event. For example, when comparing peanut butter to prank bubble gum: the null hypothesis would be as following:",
      "The existence of peanut butter in my cart does not impact the probability of prank bubble gum in that cart.",
      "So, let’s look at our rare item: prank bubble gum. Let’s say we expect, based on the global purchasing patterns, that out of 500 peanut butter baskets we expect 0.01 prank bubble gum purchases. According to a standard distribution, only after 2 happenstance purchases we begin to discard the null hypothesis. We might begin to think there’s a relationship between prank bubble gum and peanut butter:",
      "",
      "The bell curve above is the null hypothesis. It says all things being equal peanut butter baskets should have a total of 0.01 prank bubble gums. We decide to throw out the null hypothesis when we see that we’re beginning to push into space the null hypothesis considers low probability. In other words the probability that this bell curve is true (known as the p-value) gets below some threshold (often 0.05 or 0.01).",
      "Above you see that according to the normal distribution, having just 2 purchases of prank bubble gum in our peanut butter baskets begins to look like the null hypothesis doesn’t hold. Perhaps there’s a significant relationship here!",
      "However, this is pretty problematic. Our data isn’t distributed by the normal distribution above. Purchases (and other phenomena like natural language) tend to follow a power law distribution. In other words, the short tail/long tail graph from above:",
      "",
      "In a normal distribution, the tail doesn’t stretch nearly as far. In the graph of purchases above, the long tail stretches pretty far before dropping to extreme unlikeliness. One or two spurious occurrences of prank bubble gum aren’t seen nearly as significant.",
      "In total we’ve arrived at two important conclusions:",
      "Jaccard similarity removes any information about how often two events co-occur to be a significance test,\n  Normal distribution significance measures get extremely attracted to spurious rare events.  A better statistical model needs to be used to measure the significance of long-tail phenomena like product purchases",
      "In the next section, we’ll discuss a better statistical model for the significance of purchases in basket analysis.",
      "Log Likelihood Similarity",
      "Ted Dunning’s paper tackles this problem with a different statistical approach. Ted Dunning notes that the graph of word frequencies in a document, product purchases in a basket, and many other phenomena don’t track the normal distribution. Rather, the binomial distribution is more appropriate.",
      "What’s the binomial distribution? The binomial distribution tracks the frequency at which a coin flip lands heads m times in a row after N flips. It not exceedingly rare that a coin will land heads once. Twice in a row a bit rarer. The probability of 5 heads in a row is really rare, and so on.",
      "Ted realized that the distribution of product sales, words in text, and many other phenomena can better be modeled with coin flips. Or instead of coins with an imaginary 100,000 sided die. Each side of the die corresponds to a product in the store a shopper could choose. The die is heavily weighted though. The chances a shopper rolls \"eggs\" is significantly more likely than the chance a writer rolls a \"prank bubble gum.\" Yet there’s still 100,000 sides, so the occasional prank bubble gum is expected. Repeated \"prank bubble gum\" purchases though become more significant in the same way repeated heads become more significant in coin flips.",
      "Compared to the normal distribution, when you look at a pretty rare event (p=0.001) and how likely it will occur one time in 100 trials, it’s not astronomically rare. The probability is roughly 0.01.",
      "Computing Log Likelihood",
      "The name \"log likelihood\" comes is a more general statistical significance test. It comes from the idea, similar to the significance tests above, how likely the null hypothesis holds. In our case, the null hypothesis that two items occur in carts independently of one another. Another way to say that is that",
      "P(A) = P(A | B) = P(A | ~B).",
      "In English: the probability of A equals probability of A given B, equals the probability of A given not B. If A is peanut butter and B is jelly, what we’re saying is that the probability of peanut butter doesn’t change whether or not there’s jelly baskets. Similarly, we’d expect B to be unaffected by A (jelly to be unaffected by peanut butter):",
      "P(B) = P(B | A) = P(B | ~A).",
      "The alternative hypothesis is that there’s something going on between peanut butter and jelly. That the occurence of one tends to create significant deviations from the global popularity.",
      "Now we could get into doing actual log likelihood ratios. Laying out the formula for the binomial distribution and deriving the log likelihood test to accept or discard the null hypothesis. This is derived in great detail in Ted’s paper.",
      "What’s interesting though is the connection between the binomial theorem and information theory. (that connection deserves its own lengthy post). Neveretheless, with this connection Ted’s blog post describes a more straightforward way to compute log likelihood. I want to give you a walkthrough of his calculation with a more intuitive interpretation. This helped me quite a bit to map the underlying map to my own personal sense of what \"significant\" meant.",
      "First, Ted steps away from probabilities to compute just against the the total count. He divides up all baskets (not just the peanut butter/jelly ones) into four possible groupings, depending on whether or not our target items have been purchased (peanut butter or not peanut butter).",
      "Baskets with Peanut Butter\n      Baskets w/o Peanut Butter\n    \n  \n  \n    \n      Baskets with Jelly\n      95\n      5\n    \n    \n      Baskets w/o Jelly\n      5\n      99895",
      "Here, 95 people bought peanut butter AND jelly. Column 1 holds all the peanut butter purchases. Column 2 holds the non peanut butter purchases. Remember our null hypothesis is that these should be the same. Yet intuitively, we see here that 95 / 100 purchases of jelly in the presence of peanut butter (or probability of 0.95). Compare this to a 5 / 99900 purchases of jelly when no peanut butter (probability of ~0.0005). So clearly it seems the null hypothesis doesn’t hold.",
      "Here comes the information theory connection. Ted notes that another way of showing this is to examine the Shanon’s entropy between each row sum (jelly and no jelly baskets) and each column sum (peanut butter and no peanut butter baskets), and compare that to the entropy of the whole table. He says the significance of the relationship can be tested with approximately:",
      "Entropy( col1_sum, col2_sum) + Entropy (row1_sum, row2_sum) - Entropy( each cell of the table)",
      "Or just using peanut butter & jelly:",
      "Entropy( peanut butter, no peanut butter) + Entropy (jelly, no jelly) - Entropy(pb and jelly, pb and no jelly, jelly and no pb, neither pb nor jelly)",
      "What’s this Entropy function? Entropy here means the opposite of information. Intuitively, if we can subdivide a value so that 90% goes in bucket A, and 10% goes in bucket B there’s quite a bit of information here. There’s a definite skew towards A. There’s also low entropy. If 50% goes in bucket A and 50% goes in B, then entropy is rather high. Everything is mixed up, and you can’t tell A from B.",
      "This sounds like obvious subtraction, but it’s not. The quantity of available information (the sum of the numbers) also matters. So 10,000 split into 5000 and 5000 is much higher entropy than 10 split into 5 and 5.  (The best way to grok the exact math is to look at the Mahout source code",
      "( Now stay with me on this rabbit hole, don’t worry I’ll bring it around to something a bit more intuitive :) )",
      "In the peanut butter & jelly table above, the sum of column 1: 100 vs the sum of column 2: 99900 shows there’s a clear subdivision between each column. The entropy is very low. The information is very high. Similarly you can repeat this for each row’s sum to test the presence of peanut butter with or without jelly. These are factored into the Entropy of col/row sums above.",
      "We take those low values and look at them in the context of the whole table’s entropy. If the table’s cell are all the same, then this entropy will be very high. In the table above, the table’s overall entropy is very low, cells are split into dramatically different values. Conversely, imagine if you have 25 purchases of peanut butter with jelly, 25 without peanut butter, 25 without jelly, 25 without either. This is very high entropy. In this case the high entropy counts against the log likelihood score.",
      "So takeaways are:",
      "It’s good when the rows and columns sum to something pretty close. (PB is close to non PB; Jelly close to non Jelly)\n  It’s bad when the table values are all relatively close.\n  It’s good when there’s more total baskets (as potential entropy gets higher)",
      "Based on those rules, for two items to have a significant relationship (positively or negatively), the values should be spread heavily across a diagonal in the table.",
      "For example, a positive relationship distributes a lot of its values across the upper left to lower right diagonal:",
      "A and B occur together a lot. (upper left)\n  Many baskets lack A or B (lower right)\n  Not a lot of baskets have just A or just B (lower left, upper right)",
      "A strong negative relationship goes across the other diagonal",
      "A and B don’t occur together a lot\n  Many baskets with just A (bottom left) and just B (upper right)",
      "If we compare two tables with the same overall entropy, you can see both a positive and negative relationship:",
      "Positive relationship:",
      "B\n      not B\n      row sums\n    \n  \n  \n    \n      A\n      1000\n      5\n      1005\n    \n    \n      not A\n      5\n      1000\n      1005\n    \n    \n      col sums\n      1005\n      1005\n      table entropy: high",
      "Negative relationship:",
      "B\n      not B\n      row sums\n    \n  \n  \n    \n      A\n      5\n      1000\n      1005\n    \n    \n      not A\n      1000\n      5\n      1005\n    \n    \n      col sums\n      1005\n      1005\n      table entropy: high",
      "In both of these cases we’ve identified something that defeats the Null hypothesis. In the first case the presence of A perhaps predicts B. In the second case the presence of A negatively predicts B.",
      "And here’s some cases where the Null hypothesis (no relationship positive or negative) seems more likely:",
      "B\n      not B\n      row sums\n    \n  \n  \n    \n      A\n      100\n      400\n      500\n    \n    \n      not A\n      400\n      500\n      900\n    \n    \n      col sums\n      500\n      900\n      table entropy: moderate",
      "Note the row and column sums differ here (there’s less row sum/col sum entropy).",
      "If you’re curious about playing with different scenarios with log likelihood, I’ve created this Google spreadsheet you can use to experiment with different scenarios and compare to Jaccard similarity.",
      "Now you may be wondering, should we rerun this data on our baskets above? Actually log likelihood doesn’t work particularly well in these smaller data sets. You need enough to begin to see the long tail of purchases more concretely. Remember the entropy calculation here increases in proportion to the total possible information available?",
      "Another point is as Ted Dunning pointed out to me: these scores are significance tests. They’re intended to point out what’s statistically interesting. They’re not really great at ranking or scoring (a 500 vs a 700 is probably both equally interesting).",
      "What’s next?",
      "Ok so you’ve had a tour through some of the basic statistics you need to build a recommendation system. I’m increasingly interested in how search engines can be used to implement recommendation systems. This excites me because perhaps you can get all of the above without needing to turn on Spark or Map Reduce to do large distributed calculations. In future blog posts I hope to explore how far we can get with Solr and Elasticsearch in implementing really good recommendation systems. So stay tuned!",
      "I also hope to extend this to discuss personalized search and other relevance features! Look for more blog articles in the future! And don’t hesitate to contact us should you have any search relevance or recommendations needs!"
    ],
    "summary_t": "You can understand the statistics behind recommendation systems! Let’s start with basket analysis"
  },
  {
    "id": "f567b1fbbe2a939704a3807deda41a48",
    "url_s": "https://opensourceconnections.com/blog/2016/06/08/building-go-replay-gor-on-windows/",
    "title": "Building Gor (Go Replay) on Windows",
    "content": [
      "Gor, or Go Replay, is a great tool to replicate traffic from a production system to a test environment. The tool itself comes prebuilt for OSX and Linux however it lacks a Windows build as there aren’t any free CI tools providing a Windows environment. With that in mind, let’s go over the steps to build this great tool on Windows.",
      "Gor is built using the Go Programming Language. Go compiles code to a native binary for your operating system and uses GNU tools to compile so we have to assemble some parts to get this all working. There are nuances in building Windows binaries, especially around a i386/i686 (32-bit) versus amd64 (x86_64 64-bit), so for this build we will focus around using 32-bit versions of tools. Listed below are all the components you need to get Gor built along with the specific versions I tested with.",
      "go1.6.2.windows-386.msi - Go binary distribution from golang downloads\n  WinPcap_4_1_3.exe - WinPcap Driver + DLLs from Download WinPcap for Windows\n  WpdPack_4_1_2.zip - WinPcap Developer’s Pack from WinPcap: Developer Resources\n  x86-mingw32-build-1.0-sh.tar.gz2 - MinGW minimalist GNU development environment from MinGW\n  Git-2.8.4-32-bit.exe - Git for Windows from Downloading Git\n  gor-35696a4 - Gor tool commit 35696a4 from gor github",
      "First install mingw. You can follow the mingw getting started document for details but the basics are as follows. MinGW should be installed to C:\\MinGW. You will want to set an environment variable to point to C:\\MinGW\\bin. You can do this in your current session by running:",
      "set PATH=%PATH%;C:\\MinGW\\bin",
      "Next using mingw-get you will install gcc. If the command doesn’t work, ensure that the c:\\MinGW\\bin is on the path.",
      "mingw-get install gcc",
      "Go ahead and install WinPcap and enable the service. Once you have that installed you will want to extract the developer pack and copy everything in the Include directory to c:\\MinGW\\include and copy from Lib the files libpacket.a libwpcap.a Packet.lib wpcap.lib to C:\\MinGW\\lib (excluding the files from x64). This will ensure that you have the necessary headers and files to build Gor’s raw TCO listener.",
      "Finally, to get everything working together, install Go from the msi using defaults (check go install docs for more details). Ensure go works from the command line and then setup your workspace environment as follows. You may want to setup the GOPATH as a Windows user environment variable.",
      "mkdir %HOMEPATH%\\work\nset GOPATH=%HOMEPATH%\\work",
      "Run the Git installer and make sure git works from the command line.",
      "We are in the homestretch. The steps now are very similar to the gor compilation page.",
      "cd %GOPATH%\ngo get github.com/buger/gor\ncd src/github.com/buger/gor\ngo build -ldflags=\"-extldflags \\\"-static\\\"\"",
      "After that, you should have a working gor.exe in your project directory! You can now use Gor in your Windows environment. Enjoy testing with live data!",
      "gor.exe --input-raw :8080 --output-stdout",
      "ProTip",
      "If you plan to test gor.exe by calling it locally, ie localhost, 127.0.0.1 or even the external ip, ensure that you Install Microsoft Loopback Adapter or you won’t see the traffic unless you test from another remote machine. This is also covered in Wireshark’s Loopback capture setup."
    ],
    "summary_t": "Gor (Go Replay) lets to push production traffic to tests systems. Let’s see how to build it on Windows."
  },
  {
    "id": "245fd2e06270671baddeb19ebc10f611",
    "url_s": "https://opensourceconnections.com/blog/2007/05/08/corrupting-jruby-to-my-pythonic-ways/",
    "title": "Corrupting JRuby to My Pythonic Ways",
    "content": [
      "Last week I downloaded the source to JRuby and began poking around its source. My hope is that eventually I can convince it that converting Ruby source to Python is a better use of its time than Java class files. Ive had to decipher large Java projects before and I was dreading the prospect of untangling inter-class dependencies, but as it turns out, JRuby is a great example of how to modularize. I think its also an example of how unit tests can show developers new to a project how to use it.\nThe packages in JRuby are mostly dedicated to interpreting Abstract Syntax Trees (AST) into Java, but the lexer and parser packages are tantalizingly close to the root. After a quick survey of the classes contained therein, I took a look at what sort of unit tests were available specifically for the parser.",
      "One of the things I found during my package perusal was something called an AST node visitor. Better than that, I found a specific package for node rewriting. Better still, there was a unit test for it! Presumably this is for code reformatting of Ruby itself, but\nAfter running the unit test and checking out what it did, I created a new project and started hacking. The built in rewriter doesnt do anything interesting – it just regurgitates what its fed without comments – so my first job was to try a little tweak that rewrites a Ruby subclass from \"subclass < parentclass\" to Pythons \"subclass(parentclass):\". Since JRuby already told me how to test this thing, it was simplicity itself to verify that my change had its desired affect.",
      "Ive only begun to pick at the syntax differences between Ruby and Python, but going through each compilation unit has the advantage of providing a road map. Its also started me thinking about one layer above base syntax to simple code refactoring. Another interesting possibility is the use of AST graphs as a tool for code review. I guess I should reexamine the joys of Metaprogramming!"
    ],
    "summary_t": ""
  },
  {
    "id": "ebba33a263e3b1e67aeb1a13680b45c1",
    "url_s": "https://opensourceconnections.com/blog/2016/06/21/relevant-search-published/",
    "title": "Relevant Search, Published!",
    "content": [
      "Buy now with a discount! Use offer code mlturnbulllt to get 50% off for a limitted time!",
      "Elasticsearch and Solr devs take note: our book, Relevant Search is out. This book captures not just my experience, not just John Berryman’s experience but OpenSource Connection’s years of experience using Solr and Elasticesarch to build smarter search applications.",
      "To quote the book’s description",
      "Users expect search to be simple: They enter a few terms and expect perfectly-organized, relevant results instantly. But behind this simple user experience, complex machinery is at work.\n  Relevant Search demystifies relevance work. Using Elasticsearch, it teaches you how to return engaging search results to your users, helping you understand and leverage the internals of Lucene-based search engines. Relevant Search walks through real-world problems using a cohesive philosophy that combines text analysis, query building, and score shaping to express business ranking rules to the search engine. It outlines how to guide the engineering process by monitoring search user behavior and shifting the enterprise to a search-first culture focused on humans, not computers. You'll see how the search engine provides a deeply pluggable platform for integrating search ranking with machine learning, ontologies, personalization, domain-specific expertise, and other enriching sources",
      "We hope you find the book useful in your work. Manning’s author online forum is available for any feedback about the text.",
      "Also feel free to get in touch with us directly about the book or our firms consulting around search, relevance, and recommendations."
    ],
    "summary_t": "Elasticsearch and Solr devs take note: The book on Lucene-based relevance is out!"
  },
  {
    "id": "6d0c5ec80e820d6626db013c8c4d608e",
    "url_s": "https://opensourceconnections.com/blog/2016/06/23/solr-multi-word-synonym-solutions-2016/",
    "title": "What's up with multi-term synonyms in Solr?",
    "content": [
      "There were some questions floating around the Solr mailing lists about multi-term synonyms and a few notable answers are as follows. The short version is, it’s complicated and every use case has different considerations. Doh!",
      "An aside, I’ve been giving hon-lucene-synonyms some love since December. I got it working on Solr 5.3.1 and Solr 6.0.0 but neglected the documentation. The latest release of hon-lucene-synonyms included a number of namespace changes which weren’t completely reflected in the README.md so there has been some confusion as to how to get the plugin running. With that, the hon-lucene-synonyms README.md is now update to date explaining how to get the plugin working in Solr 6.0.0.",
      "Doug Turnbull said Re: Solutions for Multi-word Synonyms,",
      "Honestly half the time I run into this problem, I end up creating a\nQParserPlugin because I need to do something specific. With a QParserPlugin\nI can run whatever analysis, slicing and dicing of the query string to\nmanually construct whatever I need to\n\nhttp://www.supermind.org/blog/1134/custom-solr-queryparsers-for-fun-and-profit\n\nOne thing I often do is repeat the functionality of Elasticsearch's match\nquery. Elasticsearch's match query does the following:\n\n- Analyze the query string using the field's query-time analyzer\n- Create an OR query with the tokens that come out of the analysis\n\nYou can look at the field query parser as something of a starting point for\nthis.\n\nI usually do this in the context of a boost query, not as the main edismax\nquery.",
      "OSC has gone on to open source and document his solution here.",
      "Bernd Fehling added Re: Solutions for Multi-word Synonyms,",
      "you should really try to build your own solution for Multi-term Synonyms\nbecause every need is different and you can customize it for your special\nuse case, like adding a Thesaurus.\n\nhttp://www.ub.uni-bielefeld.de/~befehl/base/solr/InsideBase_eurovocThesaurus.html",
      "From myself Re: Solutions for Multi-word Synonyms (where APT refers to Lucidwork’s auto-phrasing tokenfilter),",
      "The auth-phrasing-token (APT ) filter is a two pronged solution that\nrequires index and query time processes versus hon-lucene-synonyms (HLS)\nwhich is strictly a query time implementation. The primary take away from\nthat is, APT requires reindexing your data when you update the autophrases\nand synonyms while HLS does not.\n\nAPT is more precise while HLS is more flexible.",
      "Note that hon-lucene-synonyms is also very useful for when you have a single term in documents but want multiple multi-term synonyms to find it. For example you could have FDA in your documents but can make matches like Food and Drug Administration,Food Drug Administration=>FDA which allows multi-term synonyms to be search for and inserted without reindexing the entire system.",
      "Update 2016-06-24: Scott Stults pointed out that Querqy, maintained by René Kriegler, is another alternative. Querqy describes itself well in its README.md,",
      "Querqy is a framework for query preprocessing in Java-based search engines. \nIt comes with a powerful, rule-based preprocessor named 'Common Rules \nPreprocessor', which provides query-time synonyms, query-dependent boosting \nand down-ranking, and query-dependent filters. While the Common Rules \nPreprocessor is not specific to any search engine, Querqy provides a plugin \nto run it within the Solr search engine.",
      "Because Querqy is a general toolset to manipulate queries it runs on top of Solr via a query handler. Most everything is implemented through a rules.txt file which is fed through rewrite chains.",
      "personal computer =>\n    SYNONYM: pc\n\npersonal computers =>\n    SYNONYM: pc",
      "Great stuff! The world of search is ever expanding. Whether you are using an existing plugin or trying to write a new one please reach out and contact us!"
    ],
    "summary_t": "Tackling query time synonym expanding including multi-term synonym processing in Solr 5 and Solr 6."
  },
  {
    "id": "27e01e9e36c8a7a0683fc61441232149",
    "url_s": "https://opensourceconnections.com/blog/2016/06/27/introducing-samia-ansari-datastart-intern/",
    "title": "I'm Samia, and I'm analyzing racial, gender, and ethnic representation in cancer clinical trials.",
    "content": [
      "My name is Samia, and I am being jointly hosted by OSC and Sartography, LLC as a DataStart intern for the next couple of months. You can read more about the brand new DataStart program here. There are 6 of us interns scattered about the Southern states this summer, and we’re all working on different data-intensive problems in various sectors.",
      "I’m a master’s student at the University of Georgia, where I study biomanufacturing and bioprocessing with a focus on pharmaceutical and medical device regulatory affairs. My undergraduate and early career background is in biochemistry and organic chemistry, but I have business experience in project coordination, scientific technical writing and research product consulting. All this to say that I am no data scientist, so I am fully prepared to leave my comfort zone this summer and pick up some new skills!",
      "My project is an analysis of racial, gender, and ethnic representation on cancer clinical trials conducted between 2002 and 2012. This topic is near and dear to my heart, not just for personal reasons, but also as someone who’s worked in clinical development. I believe strongly that all people have a right to equitable access to the benefits of cancer research, and one of the first steps is ensuring that new drugs are tested in the people they are meant to help.",
      "At OSC, I will be learning to use data management tools to get a basic handle on my data before I can begin my analysis proper. I’ll be wrangling three main datasets:",
      "Aggregate Analysis of ClinicalTrials.gov (AACT): This is a relational database made available by the Clinical Trials Transformation Initiative, a consortium of about 60 federal and private organizations, specifically for the purpose of facilitating analysis.",
      "Surveillance, Epidemiology, and End Results (SEER) data: The National Cancer Institute’s SEER program collects and distributes data on cancer incidence and survival in the United States from cancer registries covering about 28% of the United States population. This data is made available to researchers upon written request. The NCI helpfully bundles it with a proprietary statistical software package.",
      "U.S. Census Population Data (With Bridged Race Categories) for 2002-2012: This is census data that has been standardized a bit to fit the government’s incredibly clunky set of racial categories. The numbers for people 85 and over are kept in separate tables, so I had to contact a scientist at the CDC for those!",
      "I’m really excited to work on this project and journal some of my experiences as a complete newbie to the data science field!"
    ],
    "summary_t": "My name is Samia, and I am a DataStart intern for the next couple of months.  My project is an analysis of racial, gender, and ethnic representation on cance..."
  },
  {
    "id": "9a555a77e4947e2a0146718ec36d4ad3",
    "url_s": "https://opensourceconnections.com/blog/2016/06/29/top-mistakes-made-in-search/",
    "title": "Top 7 Mistakes Organizations Make With Search",
    "content": [
      "This article reflects on lessons learned from our book Relevant Search. Readers of this article can use discount code oscrelsearch to get 39% off your purchase! Buy Now",
      "After much sweat and tears our book Relevant Search is out! Relevant Search reflects the wisdom we’ve acquired over the years helping many clients improve search. I thought it would be an appropriate time to recap where many organizations get stuck with search",
      "Simple search for \"coffee\" on costco returns espresso machines and coffee-colored furniture. Would you revisit this site after this experience?",
      "1. Millions for SEO. Not a dime for in-app/on-site search",
      "Sure placing highly on Google is important. Yet when users end up on your site, you need to keep helping them. One of the most common interactions users try is through your site search. Most implement a canned site search that dissapoints users, not one customized to their needs. This teaches users that there’s nothing special about your site. Sure they found you on Google, but will they remember to come back to you? Or are you just yet another landing page? A bump on the road that doesn’t stand out from the Google noise?",
      "The best SEO is building such an amazing experience that users remember to type your\nURL into the address bar.",
      "2. Thinking of search as a technical problem",
      "Search developers rarely know what \"relevant\" means for your application. A search application built for a hospital purchasing department looks nothing like a search application built for finding travel accomidations. Only non-technical managers, domain experts, and those familiar with users can define search requirements. Sure interesting technical problems abound in search, information retrieval, and natural language processing. But these solutions are meaningless wastes of money without understanding the user and business goals behind search. The bottom line, you need more than developers and data scientists. You need colleagues that understand your users, the domain, and the business to improve search.",
      "3. Failing to study users",
      "Have you heard of the HPPO? The \"Highest Paid Person’s Opinion\"? Instead of studying real users, the CEO comes along and types a pet query. The results are terrible. The search team tucks tail and tries to solve the problem. It turns out though that the CEO was executing searches no user ever runs.",
      "Getting information on how your users search is hard. It takes constant work from analysts, domain experts, and technologists. Some applications require heavy understanding of analytics. Others applications are very domain specific and require traditional usability studies. Regardless of the work required, the payoff is enormous. Getting into your users heads, understanding what they intend with their searches, understanding how they search is the key to optimizing relevance for your needs.",
      "4. Failing to iterate quickly and measure",
      "Search, and especially relevance work, is highly iterative. You need to validate your relevance investments on a regular basis. Many organizations, however, find it painful to deploy regular tweaks to their relevance solution. They want to \"wait\" until perfection is achieved. Instead we advocate tweaking search against validation criteria using Test Driven Relevancy approaches. Upon deploying you need to validate these changes are helping: preferably A/B testing against key performance indicators that indicate how search is impacting your business and users. These indicators have been identified and validated by studying your users (and knowing what you hope to get out of search). Build yourself enough slack to fail fast and roll changes back if you need to!",
      "5. Trusting black-box, silver-bullet products",
      "The search marketplace is unfortunately replete with black-box solutions that promise to psychically scan your users brains and give them exactly what they want. One common pattern is to track how users click on results and change search ranking accordingly. This can actually make search worse when done poorly. If you hear a marketing person utter the words \"machine learning\" and \"big data\" without a technical person to explain the approach, run the other way!",
      "Beyond the marketing noise, there is a lot of buzz around the idea of Learning to Rank these days. Learning to Rank is the science behind tracking user behavior and optimizing relevance. But the first step in doing learning to rank is to be really good at understanding your user’s behavior. Studying your users is non-trivial and takes work (as we said above).",
      "Before getting deep into machine learning, work on simpler and more fundamental approaches to relevance. How does your search engine’s ranking function work? Is it prioritizing the factors that a general user sees as important? Ninety percent of a good search solution can be obtained just by learning how to better use your search engine.",
      "6. Discounting guidance & perceived relevance",
      "Good search is not just great relevance ranking. Users frequently need guidance to support the experience. This encompasses a number of user experience features: spell checking, typo tolerance, autocomplete. Good autocomplete guides users to search terms you feel confident will yield good results for them. Spellchecking and typo tolerance help to avoid scenarios where users get 0 results. Faceted browsing is another common form of in-search navigation. Often letting users guide themselves through facets and filters is far simpler than trying to solve really hard relevance problems.",
      "Perhaps the most overlooked feature is highlighting. Highlighting points out to users why results matched the query with highlighted research. Yet many forget to use highlighting. The result is many times the \"relevance\" problems are really perceived relevance problems. Users type search terms but can’t figure out why a result matched. Perhaps only the title is shown. So they assume that the result is not relevant. Showing highlighted keywords in context helps them understand how the search engine is working. It lets the refine their queries carefully so they can explore and find.",
      "7. Limitting your imagination to traditional search",
      "Various relevance-driven components in Stackoverflow's UI",
      "There’s more you can do to bring users to content they need. The future is relevance-driven applications. Recommendations, conversational user experience, voice search to name a few.",
      "A good example of this is stackexchange. You can see on the image on the right some examples. Stackexchange puts search and recommendations in quite a bit of the application.",
      "Enter a question, Stackoverflow tries to find duplicates\n  Close a question, Stackoverflow suggests candidate duplicates\n  Suggest meta posts to active users (shown on right)\n  Suggest jobs relevant to the current user  (shown on right)\n  Link questions related to this question (shown on right)",
      "Fundamentally it’s still all about the idea of relevance: bringing users to content they need. If you can master this, you’ll build personalized experiences that augment and go beyond search to answer user’s questions often before they even think to ask them!",
      "The future is information retrieval",
      "Everyone is on the machine learning bandwagon these days. And with good reason: the cloud is commoditizing compuiting similar to how fossil-fuels commoditized mechanical labor. Machine learning will revolutionize many industries.",
      "But machine learning for what exactly? I’d argue that what many are trying to do is to make applications smarter for you. Information retrieval, the science behind search and recommendations, is exactly about aligning users information needs to content. It’s getting you the right vacation home. It’s finding the speakers that will make your convertible sound amazing with the top down. It’s about suggesting the ideal \"scenic route\" through the country based on your ideal mix of mountain views and winery stops.",
      "Sure Information Retrieval has been around in search for decades. It’s been in recommendation systems for years. But Information retrieval is in every emerging form factor: conversational UX and voice search. It’s in your Apple watch and Google Inbox when it decides what notifications are relevant for you. It’s in the doctor’s chart as she makes treatment recommendations for you. There’s a \"relevance\" problem behind each of these use cases, and THAT is where the smart applications will differentiate themselves from the boring point-and-click applications of today.",
      "Everyone is lost thinking about AI and skynet. But Information Retrieval, not AI, is the next decade. Are you ready to harness it? If you want a leg up on getting to the smarter future, read Relevant Search. It will give you the fundamentals of how to prioritize the right user signals in relevance-driven applications.",
      "Get the experts!",
      "We’re the Information Retrieval experts. Our focus is building relevance-driven applications of tomorrow. If you need a better search solution, more relevant search results, or are interested in building recommendation-driven solutions, be sure to get in touch and we’d love to chat."
    ],
    "summary_t": "After much sweat and tears our book Relevant Search is out! Relevant Search reflects the wisdom we’ve acquired over the years helping many clients improve se..."
  },
  {
    "id": "cbc84c0fbebcae43634d83af10b14a3e",
    "url_s": "https://opensourceconnections.com/blog/2016/06/30/deploying-aact-oracle-database-using-docker/",
    "title": "Deploying AACT Oracle Dump File into the Cloud with Docker",
    "content": [
      "My summer project is supporting our Data Science intern Samia in her quest to understand if the racial, gender and ethnic representation in cancer clinical trials reflects the general distribution in the population at large who have these cancers.",
      "One of the key datasets that she is working with is the data from ClinicalTrials.gov.   The database is called the Aggregate Analysis of ClinicalTrials.gov or AACT database.  Initially we looked at some pipe delimited dumps of the data, but then I realized to query over that was going to be a pain of reverse engineering the schemas and primary and foreign keys.   Fortunantly there is a dump of the original database available.  So, it was time for me to take the crash course on learning how to restore Oracle databases!  This blog post covers the steps that I took.",
      "What Version of Oracle?  How do I install Oracle on OSX?",
      "I was almost defeated before I began, because I started googling around to find out how to install Oracle on OSX, and it turned out to be a daunting task.  There are freaking 10 different guides about this on Oracle’s site!   But then I remembered that I don’t actually care about the nitty gritty of Oracle.  I just want to have it running as a black box.  Enter Docker, specifically a Oracle 11G docker image kindly made by wnameless.",
      "I read the README and fired up Oracle via Docker:",
      "docker run -d -p 49160:22 -p 49161:1521 --name oracle -e ORACLE_ALLOW_REMOTE=true wnameless/oracle-xe-11g",
      "Notice the proxying of the ports?  The SSH port is proxied to 49160 and the SQL driver port is proxied from 1521 to 49161.  The magic variable ORACLE_ALLOW_REMOTE was passed in as well. You can log into the running process via:",
      "docker exec -it oracle bash",
      "The next challenge was that to interact with Oracle you need sqlplus, a tool I haven’t used in years.   I tried a quick brew install sqlplus but no joy.  Again, Docker to the rescue:",
      "docker run --interactive guywithnose/sqlplus sqlplus system/[email protected]:49161/xe",
      "Thank you guywithnose for packaging up sqlplus and making it easy to connect to my locally running image.",
      "Okay, time to restore an Oracle dump file",
      "This turned out to be quite cumbersome, and the documentation on AACT wasn’t perfect.   However, here are the set of steps that I did to make it work.",
      "First I had to get the dump file downloaded.  I ran a series of docker exec commands to install some prerequisities and then download the file.",
      "docker exec oracle apt-get install unzip\ndocker exec oracle wget http://library.dcri.duke.edu/dtmi/ctti/2016_Refresher/AACT201603_oracle.ZIP\ndocker exec oracle unzip AACT201603_oracle.ZIP",
      "Then create the file system structure that the new database will be restored into:",
      "docker exec oracle mkdir -p /data\ndocker exec oracle chmod -R 777 /data\ndocker exec oracle mv aact.dmp /data",
      "Now it’s time to set things up in the Oracle database side of things.  Fire up sqlplus again via docker run --interactive guywithnose/sqlplus sqlplus system/[email protected]:49161/xe.  And then run these SQL commands:",
      "create directory data as '/data';\ncreate tablespace CLINTRIALSGOV_OUT_D datafile '/data/data.dbf' size 800M extent management local autoallocate;\nALTER DATABASE DATAFILE '/data/data.dbf' AUTOEXTEND ON NEXT 1M MAXSIZE 12024M;\n\ncreate profile DCRI_SERVICE limit;",
      "Okay, now you are ready to run the import program.  I tried to run it via a docker exec but had various difficulties.  So just logged in and then ran it.",
      "docker exec -it oracle bash\nimpdp system/[email protected] directory=data dumpfile=aact.dmp schemas=clintrialsgov_out logfile=import_aact.log",
      "You can monitor the progress via some queries:",
      "SELECT FILE_NAME, BYTES FROM DBA_DATA_FILES WHERE TABLESPACE_NAME = 'CLINTRIALSGOV_OUT_D';",
      "Okay, but now it’s just running on my local laptop!  How is Samia going to access it?",
      "This also stumped me a bit.  I started out messing around with Google Compute Engine and Kubernetes, but honestly, since I am running a simple 1 node database, I don’t need all the magic of Kubernetes.  The easiest path to putting Oracle online was to use Docker Cloud.",
      "First off I went to the Cloud Settings and gave Docker Cloud my AWS keys.",
      "Second I went and create a new Service.  I searched Docker Hub for wnameless/oracle-xe-11g and found it.",
      "In the General Settings I gave it an alias of \"oracle\", and kept all the other defaults.  Under Ports I said to publish the Oracle ports of 1521 and 22.  I also added the ORACLE_ALLOW_REMOTE=true as a Environment Variable.   Clicked Create and Deploy and in a few minutes, like around 5 or 7, I had a online Oracle node.",
      "To load the data, I followed all the steps above, except that instead of doing docker exec I had to do docker-cloud exec.  To connect to the oracle container running in Docker Cloud you need it’s name, which you can figure out via:",
      "docker-cloud container ps",
      "To log on you don’t have a -it or -interactive parameter:",
      "docker-cloud exec oracle-xe-11g-13db8b82-1 bash",
      "Do all of that and you should have the AACT database online!  This isn’t a perfect solution, for example I’m not mounting a persistent Volume to store the data on.  So if the container goes down, so does the dataset.  But this is good enough for playing with the data.  In the next edition, I’ll show you how I connect up Apache Zeppelin and some Spark queries!"
    ],
    "summary_t": "ClinicalTrials.gov is a wealth of information.  But the only database format they support is an Oracle dmp file.  Follow along as I help our data science int..."
  },
  {
    "id": "f2c62fe9afec383c781508055eaeef9a",
    "url_s": "https://opensourceconnections.com/blog/2016/07/07/relevance-investments-ebook/",
    "title": "Guide to Making Search Relevance Investments, free ebook",
    "content": [
      "How well does search support your business? Are your investments in smarter, more relevant search, paying off? These are business-level questions, not technical ones!",
      "After writing Relevant Search we find ourselves helping clients evaluate their search and discovery investments. Many invest far too little, or struggle to find the areas to make search smarter, unsure of the ROI. Others invest tremendously in supposedly smarter solutions, but have a hard time justifying the expense or understanding the impact of change.",
      "That’s why we’re happy to announce OpenSource Connection’s official search relevance methodology!",
      "Download Guide to Relevance Investments",
      "This guide lays out a framework for incrementally implementing and evaluating relevance solutions against what matters to your business. It describes how to measure the success of investments against your business’s bottom line, be it purchases, page views, or good will.  It describes the ideal environment for test-driven-relevance style workflow, with some steps to get there. It discusses the importance of frequent, continuous deployment and measurement against your high-level business metrics.",
      "We believe any good process is a work-in-progress. This is our first pass, at a fairly high level. This is sure to evolve with the industry and what we learn. But we also want to hear from you! We’ve met great success with this methodology in our consulting practice, but is there anything you would add or change?",
      "It’s free for you to use and download right here. We’re not even going to ask for your email so a sales person can annoy you later. However, if you do enjoy it or need relevance help we hope you get in touch. Or sign up for our newsletter below."
    ],
    "summary_t": "We’re happy to announce today we’re releasing our OpenSource Connection’s official search relevance methodology. It lays out a framework for incrementally im..."
  },
  {
    "id": "dbe5260e04d692e97b668b4d09f2d739",
    "url_s": "https://opensourceconnections.com/blog/2016/07/26/learning-the-shape-of-my-data/",
    "title": "Learning the shape of my data.  Working with the AACT data.",
    "content": [
      "As I mentioned the other week, I’m analyzing participation rates of gender, ethnic, and racial minorities in cancer clinical trials concluded between 2002-2012. I’m working with 3 datasets from which I’ve been gleaning the information relevant for my analysis. I started out by looking at the Aggregate Analysis of ClinicalTrials.gov (AACT), made available by CTTI in pipe-delimited and Oracle .dmp formats.  Eric graciously re-assembled the .dmp data into a usable Oracle database for me. I then used a program called DBVisualizer to do some basic Structured Query Language (SQL) queries.",
      "I’ve never worked with relational databases before, so I started by opening the data dictionary Excel file that’s packaged with AACT. This file contains not just the dictionary of variables, but also a graphical representation of the database schema that looks a bit like a flowchart. A schema is a map of a dataset’s structure that explains the relationships between tables. Once I realized which tables and variables were actually relevant for my purposes (thankfully, not all of them!), I set about learning some rudimentary SQL to help me retrieve the specific information I’m after.  The tutorials at Vertabelo were pretty simple to follow and I highly recommend them.",
      "At first, I was a bit puzzled at how little usable information I was getting from my initial queries. I couldn’t figure out why condition_browse.mesh_term like '%cancer%' wasn’t grabbing the entire list of cancer clinical trials for me. Then I learned that AACT data is indexed using Medical Subject Headings (MeSH), the U.S. National Library of Medicine’s thesaurus for indexing of medical articles. MeSH codes each cancer as a type of neoplasm. So breast cancer is going to appear in a MeSH-indexed database as \"breast neoplasm.\" What a difference one string makes!  Since there are a dizzying number of cancer types, I chose to focus on 5 common ones: breast, prostate, lung, ovarian, and colorectal.  I also didn’t account for the date information to be coded as strings rather than…dates. So my queries have been a little clunky, but effective.",
      "I’m also working with the National Cancer Institute’s Surveillance, Epidemiology, and End Results (SEER) Program cancer registry data, which provides cancer case information and incidence rates for a sample of the U.S. population (roughly equivalent to 28% of the country). I had to fill out a request and get permission to access it, which felt moderately fancy. NCI helpfully bundles the data with a special statistical package called SEERStat designed to facilitate epidemiological research. The software is an interface by which users can tailor exactly what types of tables (called matrices) they’re after. It’s very customizable, and I created some user-defined variables to reflect the time period and cancer types of interest, then executed a session and got a really neat table organized perfectly for my needs. I like that the SEERStat variables I’ve created are shareable, and I’ll be including them with my research findings in the interests of transparency. Here’s a snippet of an executed SEER*Stat session:",
      "",
      "The third and final source of data for my project originates from the U.S. Census bridged-race population estimates. At first, I went and downloaded each .txt file for each year separately. Then I discovered an online tool called CDC WONDER that provides a breezy online interface for querying the most recent population estimates by factors like age, race, ethnicity, gender, etc. I was easily able to get all the population data I needed, within seconds.",
      "By applying the cancer incidence rates obtained via SEER*Stat to the Census numbers, I’ve been able to estimate the national incidence numbers for each cancer, for each gender/ethnic/racial group. Eric and I are working to clean up the last bit of AACT data and then I can start doing some more advanced calculations with my new friend R!"
    ],
    "summary_t": "I’m working with two data sets (AACT and SEER) and finally seeing the challenges in how the data is laid out versus how I thought it would be!"
  },
  {
    "id": "6fa29dd0f350a3f351b7a15b236ab6d2",
    "url_s": "https://opensourceconnections.com/blog/2016/08/01/search-for-lunch/",
    "title": "Search for Lunch: this search expert will come speak for free at your company's lunch and learn",
    "content": [
      "I want to share what I know at your company’s lunch and learn! For free!",
      "The problem with lunch and learn’s is that everyone’s busy. It’s challenging to find enough time to put together a good presentation to the group.",
      "Luckily, I’m here to help. You can now snag a free hour of my time to speak to your developers over your favorite web conferencing technology.",
      "Sharing what I know is a big part of my passion. I just finished writing a book on search relevance. I speak regularly at conferences and meetups. I have several prepared presentations. If you’re using Solr or Elasticsearch, let me help you by speaking virtually at your lunch and learn!",
      "Simply email me, and I’ll share presentations I can deliver with very little preparation. Topics like",
      "Learning to Rank – Using machine learning to optimize search relevance. See the mechanics behind the magic. See what is learning to rank\n  Search Management 101 – Getting the entire team moving in the right direction on search. Often search is thought of as a tech problem, when it is more often about organizational and process issues.\n  Search Relevance w/ Solr & Elasticsearch 101. My Ghost in a Search Machine talk. How a search engine works, and what weird neat things can you do with it to manipulate ranking & matching.\n  Test Driven Search Relevancy – How can we improve relevance over time while maintaining quality. Talks about our product Quepid and the general principles of Test Driven search Relevancy\n  Personalization & Recommendation Systems using Search – techniques for using Solr and Elasticsearch to personalize and recommend content\n  Hacking Lucene internals for custom search results – learn about custom Lucene queries which let you implement custom scoring\n  How to evaluate search relevance investments – An introduction to OSC’s search relevance methodology that you could apply to your problems",
      "I’ll be sure to leave plenty of time for discussions and questions so you can pick my brain on a few of your own problems!",
      "Schedule your Lunch and Learn with Doug!"
    ],
    "summary_t": "The problem with lunch and learns is that everyone’s busy! It’s challenging to find enough time to put together a good presentation to the group. Luckily Dou..."
  },
  {
    "id": "449ccd5d7f6220a15173d0f9c7396c3f",
    "url_s": "https://opensourceconnections.com/blog/2007/05/10/scrum-war-stories-recap-2/",
    "title": "Scrum War Stories Recap",
    "content": [
      "Last night we had wonderful participation for our first Scrum War Stories dinner, with 9 folks from 7 different organizations showing up at West Main. The discussions we had were wide ranging and for me the value was in the interaction with my peers!",
      "The main themes that came up over dinner from my POV were:",
      "Change is hard. People reach a comfort level in their own niche, and dont see the larger problems challenging the business that they need to change to overcome.\n  \n  \n    Oldstyle approaches to supervising project teams dont work. With 3 week Sprints, there isnt enough time for an outside group to influence the Scrum team. I argued that instead of an outside group supervising/inspecting the Scrum team, they need to merge. They need to be members of the team, or be the Product Owner.\n  \n  \n    Strong Product Owners are key!",
      "",
      "Scott Stults mentioned that the term \"Sprint\" really is a misnomer, because you arent sprinting to a finish line, but instead maintaining a constant pace, that allows time for other things like process improvement, people on vacation, technical development. He proposed the term \"Relay\", which I really liked. In a relay, different people handle the baton, and the results arent measured on any individuals performance, but the performance of the team as a whole.",
      "Not sure how we get the Scrum world to change from \"Sprint\" to \"Relay\", but I may start trying to use it in everyday speech.",
      "Many thanks to everyone who showed up, and a big thank you to Aaron Buchanan from Inova for cosponsoring the event with me, it wouldnt have happened without his enthusiasm."
    ],
    "summary_t": ""
  },
  {
    "id": "c570619c261b21b7597096488a096f49",
    "url_s": "https://opensourceconnections.com/blog/2016/08/08/guide-to-relevance-for-content-teams-diagnosis/",
    "title": "Content expert's guide to diagnosing site search relevance problems",
    "content": [
      "Most people find site search maddening to manage. How are your users searching? Is search helping or hurting the bottom line? It’s one thing to try a few pet peeve queries, and be annoyed at the results – but do these reflect how users actually search?",
      "In this series of articles, I want to give you, the content person, a very practical and straight-forward guide to managing site search. We’ll start by discussing diagnosis – how to find problems. We’ll use a simple, free analytics tool (google analytics). We’ll make a few naive assumptions about these analytics that act as a good starting point.",
      "These steps cover many of the steps in our relevancy consulting methodology. However, more closely targeted at content teams managing content in a CMS needing to satisfy a few thousand important search queries over thousands/tens of thousands of documents. We’ve assumed you’re able to setup google analytics for site search, but if you have any trouble follow Google’s straightforward instructions.",
      "Exporting Search Keywords from Google Analytics",
      "First let’s look at Google Analytics for site search. Here’s a small sample of OpenSource Connection’s site search analytics, showing top queries. We don’t get nearly the number of searches as our clients, but it’s a nice baby data set to play with.",
      "",
      "Second, we need to diagnose specific problems by playing with these search terms in a spreadsheet. Export google analytics search terms to excel or google sheets. Here we’ll use google sheets.",
      "Select \"show rows\" -> 5000 to get a meaningful sample of the most important queries\n  Select \"export to google sheets\" up towards the top",
      "This will give us a spreadsheet to work with with these stats. We’ll use the statistics here to find popular search queries with the most problems.",
      "",
      "We’ll cover a high-level strategy for interpreting what’s here. If you’d like an in-depth rundown of how to interpret these statistics, I refer you to our blog article on google analytics for site search.",
      "But a few things to point out. You’ll notice that \"total unique searches\" follows what us nerds call a logarithmic scale. That just means that the most popular query has many times more searches than the next most popular, and so on (Solr has 11 searches, Cassandra 4 etc). Eventually you get to a point where almost all the searches have occurred exactly once. This is what you’ll hear referred to as the \"long tail\" – the large number of search keywords that have occured rarely in the history of your application.",
      "Compute a PER score",
      "We’re going to create a column that combines several of the likely problematic statistics into a single statistic. The search keywords that score high for this statistic probably indicate a problem area for further investigation.",
      "We’ll create a new column that performs the calculation combining several problematic statistics",
      "log(total unique searches) * Results Page View * % search exits * % search refinements",
      "To review these stats briefly",
      "log(total unique searches) – This measures the importance of the search query. We could multiply \"total unique searches\" directly, but as the popular search occurs so much more than the next most popular, this would really blow away any other search. So we take the log to dampen down its impact a bit.\n  Results page views – How many results pages did a user view (how often did they hit \"next page\")? More pages means users could be struggling to find the item they want on the first page.\n  Search exits – How often did users exit from the search page? If users exit straight from search, it could mean they’re frustrated by not being able to find anything.\n  Search refinements – How often did users refine their search (instead of searching for solr, they added solr consulting to the search). This could indicate your search couldn’t satisfy their keywords and they had to try an alternate approach.",
      "I like to call the combination of these stats a PER Score as in (pages * exits * refinements). This formula basically scores each keyword by popularity log(total unique searches) and higher page/exit/refinement statistics. The highest score would be a popular search term where users are paging, refining, and exiting a lot. However, it’s also useful in finding search keywords that are moderately popular that have big problems (lots of paging, exits, refinements). This points  area that needs further investigation by your team as users seem to be frustrated.",
      "We’ll adjust the formula slightly to account for cases where these values are 0. Final formula for the spreadsheet above becomes",
      "=log(B9+1)*(C9 + 1)*(D9 + 1)*(E9 + 1)",
      "Interpreting a PER score",
      "PER scores are an appropriate metric for roughly gauging search-as-navigation. Where users expect the top result to match what they’re looking for. They are searching for a single item (or small set of items) that match their keywords.  For many content sites serving the general public, this is often the most common use case. It’s often the most important to get right.",
      "However, it’s important to note this isn’t every search use case. Some users use search to research and compare different items, not just find the single best match for their search. For these users, PER score doesn’t make as much sense. Users like legal researchers, patent examiners, or advanced shoppers might view every result, hit next page frequently, and refine heavily so that no stone is unturned in their research. These statistics just point to a dogged researcher, not an annoyed searcher. Other analytics (outside the scope of this article) likely make sense for them. If you’re curious about the tradeoffs between these use cases, learn more about the concepts of precision and recall.",
      "The takeaway is the PER score isn’t perfect. But a high PER score is what I’d call a \"bad smell.\" Something probably isn’t right with this search, and you should sniff out the reasons this query seems to be having so many pages, exits, or refinements. It’s possible you could isolate the reason to something not bad (users exit because you give them the answer on the search page) or related to a complex search use case.",
      "Back to OSC’s site search, I’ve calculated the PER score and have sorted on that column to hunt for problem areas",
      "",
      "This points to some interesting patterns in our search. Even though these aren’t that statistically significant, my sense of our site and our company points to a few areas that could be improved:",
      "Most obviously: It’s very important that searches for various technologies (solr, lucene, cassandra, algolia) work well. They have a high \"total unique searches\" which drives up their PER score. I should do everything I can to understand those users and deliver what they want.\n  The problematic search for \"intern\" surprised me. It’s marginally popular and seems to have issues: users always need to perform refinements. Knowing what I know about OSC, this is likely an OSC employee searching for blog posts written by interns.  I’m pretty certain that we don’t have any way of search answering the question \"this article is by an intern.\" Perhaps we should work to address this use case?\n  Someone searched for \"slate\" then refined what they were looking for (notice refinements of 100%). I know Eric Pugh wrote some articles about using Lowes data and a search engine to find some grout for his counter tops. Is this what they were after?",
      "With most clients with many thousands of search users, we can usually find many surprising problems with search. This method goes beyond just highlighting what’s popular, but also fairly popular search queries somewhere that have big problems. Immediate resolution of these items can make a big difference in a business’s bottom line.",
      "Takeaways",
      "For relevance, you’re always iterating on your understanding of analytics. There’s no magic number that says \"good search\" that Google analytics (or any tool) can give you. You need to apply your own domain knowledge to decide what behaviors like paging, refinements, and exits mean for your application.",
      "But the PER score is a good starting point for finding problem areas. You need to start somewhere to find issues. Don’t get too obsessed with perfect before moving to resolve the underlying technical/content issues for what you’re finding. We advocate for iterating fast and rebenchmarking search quickly. In future articles, we’ll discuss the importance of testing fixes and deploying relevance solutions quickly to production. We’ll also discuss the benefits of other statistics that tools like google analytics don’t track such as click depth (did the user select the 1st, 2nd, … or 10th search result?), click through rate, and conversions (did a user buy something?).",
      "Hire The Experts",
      "You can find solutions to your search that have great ROI. If you’d like us to evaluate how to measure your site search, feel free to get in touch and check out our relevancy services page."
    ],
    "summary_t": "In this series of articles, I want to give you, the content person, a very practical and straight-forward guide to managing site search. We’ll start by discu..."
  },
  {
    "id": "089f543726a5743516a1250d9e19046d",
    "url_s": "https://opensourceconnections.com/blog/2016/08/21/recommendations-systems-not-as-cool-as-friends/",
    "title": "Your Recommendation Systems Aren't As Cool As My Friends",
    "content": [
      "What is more important in a recommendation?",
      "That the recommendation is accurate based on my tastes OR\n  That the person making the recommendation is someone I like and respect",
      "The traditional regime of recommendation systems has been obsessed with (1). What’s the uplift of recommendation algorithm A vs recommendation algorithm B? Which is driving more click-thrus and conversions?",
      "There’s something fundamentally broken in the this way of thinking though. I don’t care what the computer says. I care about what my friends say. The meaningful music I’ve discovered over the last 10 years has been music liked by someone I respect. Friends. Other musicians I like. Music critics. Online celebrities.",
      "Because how do I decide when to really give music a chance? What music should I really listen deeply to? And which music do I listen to passively without much thought and consideration?",
      "When a friend tells me I really should try out an album, my first interaction with the album might be clumsy and painful. I might not like it. But I tell myself \"my friend Sean really knows music.\" He’s a thoughtful guy, and I want to understand what interests him. So I keep listening. I give the album a second chance, and a third. I really want to like it. I put work into it. And eventually I do enjoy it.",
      "When a computer recommends music (or movies, art…) I give it one fleeting chance. If I don’t like it I don’t put additional work into it. It’s either a catchy tune that I like on first listen, or I ignore the music. The music feels cheaper. Like top 50 radio. Despite the actual \"accuracy\" of the recommendation, I have a hard time disentangling my enjoyment of the music from my relationship with the thing doing the recommendation. Stupid computer, what does it know?",
      "Recommendation systems treat our tastes as singular and pure. As if my interaction with an item highlights my inherit nature. Yet in social circles, making recommendations is akin to persuasion and influence. If you give a recommendation, you hope to convince your friends to like something you enjoy. Conversely, if you recieve a recommendation from someone you like, you’re incentivized to like it. By liking the item, you can deepen your relationship with that person.",
      "In other words, there’s often a deeper, human story to why we like the things we do. \"Happiness is only real when shared\". Our tastes are fungible units operating in a social sea. It’s much more fun to enjoy something with a friend than by ourselves. How many of our deepest held music preferences arise out of a desire to fit into a clic at high school? Or because of romantic courtship?",
      "I think we need to fundamentally rethink the recommendation interaction. How can we do that? Well here’s a few spitball ideas (tell me yours!). Each of these ideas probably deserves its own blog post in turn :).",
      "Instead of finding anonymous users extremely similar to me, focus on similar friends even if they only share that share a handful of my tastes\n  Don’t make recommendations to users, make sharing easier. Prompt users with \"Doug might like \" -- would you like to let Doug know? And be sure you can link to anything.\n  Deanonymize recommendations and find users with other factors in common with the target user (Bob lives in Charlottesville and likes X)\n  Focus on what the musicians I like in turn like. When I learn Led Zepellin is an influence on the Smashing Pumpkins (a band I like), I’m inclined to check out Led Zepellin. In graph terminology, these influencers would be \"supernodes.\" We often discount them because of the oprah book club problem. But perhaps there’s really an oprah-book club solution!\n  Understand users don’t value computer-generated recommendations. But use this to your advantage: focus on quantity and serendipity over accuracy. Focus on passive interaction modes that intentionally lack gravitas: radio stations, \"channel flipping,\" auto mixing after a chosen song, etc where its ok if 90% of what a user sees is \"meh\" because sometimes 10% is fun.\n  Conversely: I wonder if many computer generated recommendations helps create conundrums of choice. Can this be avoided by putting more social weight into the recommendations? I always wanted Netflix to just show me what Roger Ebert likes, not predict what I might like\n  Understand expert-curated reviews by humans are more effective recommendations than collaborative filtering. AVClub, Roger Ebert, and The Wirecutter have done more to influence my buying decisions than the aggregation behavior of Amazon users similar to me. Hire a really good reviewer/editor and let them build a following, don’t spend a million bucks on a kaggle competition.",
      "Underlying some of these ideas is an assumption that we can know or figure out the user’s social network. How might we do that? Most obviously we do what Spotify does: explicitly invite the user to connect to their Facebook account.",
      "However, alternatively we might attempt to implicitly gather user relationships. We could do this by making sharing easier and embedding a token in the URL being shared between user A and user B. Many sharing events between A and B could be assumed to be a relationship. Further, we can measure how much weight user B gives to shares from user A, basically measuring the weight B gives to relationship A.",
      "User B receives a shared item X from user A\n  Relationship Weight = B’s apparent preference for item X / B’s predicted affinity for X based on Collab filtering",
      "If B really likes X (they watch the movie; listen to the music), but B really shouldn’t like X (X is completely off the radar of B’s machine predicted tastes) then perhaps B really respects and listens to A. Perhaps B appears to be really working on changing their tastes to match A?",
      "This could also be used to track \"fandom\" as a basis for recommendation. Here instead of A and B sharing between themselves, we simply note that when B is presented items that \"Roger Ebert\" likes, they seem to work heavily against their natural tastes. I have no reason to like TP-Link routers based on my Amazon buying history. But it was Wirecutter’s list of \"routers everyone should buy.\" You could measure the fact that I gave a lot of stock to what Wirecutter said if I in give the router a really good review even though I shouldn’t have liked it based on machine-based recommendation methods.",
      "There’s many veins of thoughts to keep going in this direction. These ideas of mind are blind gropes in a new direction – I suspect significant research is needed. I’m sure there’s a great deal already out there on measuring how human influence can change tastes. For example, I wonder – what are the features of a relationship where person A is likely to modify their tastes to person B? Perhaps Doug Turnbull knows, I’ll have to ask him.",
      "But the main takeaway is the future of recommendations is more about fitting in existing systems of influence, and less about pure collaborative filtering. In some ways, I think we might need less machine learning and more machine-guided social interactions.",
      "If you’re interested in exploring these ideas do please get in touch to let me know how much you completely disagree with everything I’ve just said :). And check out our free lunch and learn’s if you want to chat about these ideas with me one on one with your team."
    ],
    "summary_t": "Users listen to their friends, not your recommendation system. Is there a way to reimagine recommendation systems around social influence?"
  },
  {
    "id": "517143dc812e3d91d970219386fd334a",
    "url_s": "https://opensourceconnections.com/blog/2016/09/09/better-recsys-elasticsearch/",
    "title": "High-Quality Recommendation Systems with Elasticsearch",
    "content": [
      "Did you know technical books can have spoilers? Well ours does :-p and so spoiler alert: we close our book, Relevant Search, making an argument that search engine technology is a great platform for delivering recommendations. We argue recommendations and search are two sides of the same coin. Both rank content for a user based on \"relevance\" the only difference is whether a keyword query is provided.",
      "We argue that search engines provide the ideal framework for implementing easy-to-tune recommendations. In fact, I’ll be talking about this idea at Lucene Revolution in not too long (see also my coauthor’s talk about this subject at Elastic{On})",
      "In Relevant Search, we implement recommendations using Elasticsearch aggregations. We just introduce these ideas. We tease :). In this blog article, I want to get beyond teasing, and begin exploring how to deliver good recommendations with Elasticsearch.",
      "(Note: While it’s not required reading, definitely check out my previous article on basket analysis for more information.)",
      "Aggregrations: Find what people who like this movie also like",
      "Ok, let’s dig in! You’re running a movie website. You’ve got a set of users and you want to know what to recommend to them. Well, one idea you might have is to index each user as a document, like so (movies_liked here is setup as a keyword analyzed field, which I’m omitting):",
      "PUT recs/user/1\n{    \"movies_liked\": [\"Forrest Gump\", \"Terminator\", \"Rambo\", \"Rocky\", \"Good Will Hunting\"]}\n\nPUT recs/user/2\n{    \"movies_liked\": [\"Forrest Gump\", \"Terminator\", \"Rocky IV\", \"Rocky\", \"Rocky II\", \"Predator\"]}\n\nPUT recs/user/3\n{    \"movies_liked\": [\"Forrest Gump\", \"The Thin Red Line\", \"Good Will Hunting\", \"Rocky II\", \"Predator\", \"Batman\"]}\n\nPUT recs/user/4\n{    \"movies_liked\": [\"Forrest Gump\", \"Something about Mary\", \"Sixteen Candles\"]}",
      "We want to perform recommendations for users that like Terminator. In other words, we want to know what users who like Terminator also like. This is very basic \"basket analysis\" – it’s the fundamental building block of recommendations. The name comes from the idea of looking at user’s shopping baskets and finding statistically interesting relationships. Famously, grocery chains learned that people who bought diapers also frequently bought beer. Insights like this can help extremely valuable to users and businesses. Here’ we’ll focus in on Terminator to find what’s statistically interesting to Terminator viewers.",
      "With documents that store user history, how do we figure out what viewers of Terminator would like? Well the first idea that might come to you is to run a terms aggregation. A terms aggregation is a traditional facet. It gives a raw count of the terms for a field in the current search results. In a typical application terms would probably be movie ids, but we’ll use titles here. We’ll search for \"Terminator\" and aggregate on the \"movies_liked\" field to get a raw count of the movies that occur frequently alongside Terminator. Basically like so:",
      "POST recs/user/_search\n{\n    \"query\": {\n        \"match\": {\n            \"movies_liked\": \"Terminator\"\n        }\n    },\n    \"aggregations\": {\n        \"movies_like_terminator\": {\n            \"terms\": {\n                \"field\": \"movies_liked\",\n                \"min_doc_count\": 1\n            }\n        }\n    }\n}",
      "The end result, just  focussing on the aggregations is the following breakdown",
      "\"movies_like_terminator\": {\n        \"doc_count_error_upper_bound\": 0,\n        \"sum_other_doc_count\": 0,\n        \"buckets\": [\n        {\n            \"key\": \"Forrest Gump\",\n            \"doc_count\": 2\n        },\n        {\n            \"key\": \"Rocky\",\n            \"doc_count\": 2\n        },\n        {\n            \"key\": \"Terminator\",\n            \"doc_count\": 2\n        },\n        {\n            \"key\": \"Good Will Hunting\",\n            \"doc_count\": 1\n        }\n}",
      "If you scan the user records above, you’ll see users that like Terminator also coincides with 2 users that also like Forrest Gump, 2 users that like Rocky, and so on. As an aside, if we repeat this for all of your favorite movies – searching also for \"Terminator\" OR \"Predator\" OR \"Say Anything\" and all your favorite movies, then you’ll get collaborative filtering: issue a search to find the users that like the same movies you do. Then you’ll examine the raw counts to see movies you haven’t watched that co-occur frequently wtih movies you like.",
      "But is this raw count approach a good approach? We discussed both in Relevant Search and in the basket analysis blog article that raw co-occurrence counts are really poor ways of doing recommendations. Raw counts prioritize what’s globally popular over what’s interesting here. In this example, for instance, everyone likes Forrest Gump. So if you used this as a method for recommendations, every user will be recommended Forrest Gump. We refer to this problem as the \"Oprah Book Club\" problem – a bias towards cross cutting popularity that isn’t particularly useful.",
      "What’s more interesting here is the relative count of movies in the Terminator results. For example, look at Rocky. Every user that likes Rocky also likes Terminator – they have exactly 100% overlap! Another way of saying this is Rocky occurs 100% here in the foreground, and only 50% globally: in the background. This seems like a great recommendation for users that like Terminator!",
      "Measuring meaningful user-item connections with Significant Terms",
      "To transcend the problems with raw counts, Significant Terms Aggregration comes in handy. This aggregration measures the kind of statistically significant relationships we need to deliver meaningful recommendations. It’s job is to calculate not raw counts, but specifically the statistical significance of terms in the current results when compared to the background corups. In my basket analysis article we discussed the pros/cons of different ways of scoring significance, let’s explore what significant terms does.",
      "But let’s take it out for a spin before thinking about it too critically:",
      "POST recs/user/_search\n{\n    \"query\": {\n        \"match\": {\n            \"movies_liked\": \"Terminator\"\n        }\n    },\n    \"aggregations\": {\n        \"movies_like_terminator\": {\n            \"significant_terms\": {\n                \"field\": \"movies_liked\",\n                \"min_doc_count\": 1\n            }\n        }\n    }\n}",
      "The only thing that’s different above is we use a significant_terms calculation. We set the min_doc_count to 1 to work well in our really puny data set.",
      "And indeed, this solves the immediate problem, we see that our recommendations appear more appropriate, with Forrest Gump nowhere to be seen:",
      "\"buckets\": [\n            {\n               \"key\": \"Rocky\",\n               \"doc_count\": 2,\n               \"score\": 1,\n               \"bg_count\": 2\n            },\n            {\n               \"key\": \"Terminator\",\n               \"doc_count\": 2,\n               \"score\": 1,\n               \"bg_count\": 2\n            },\n            {\n               \"key\": \"Rambo\",\n               \"doc_count\": 1,\n               \"score\": 0.5,\n               \"bg_count\": 1\n            },\n            {\n               \"key\": \"Rocky IV\",\n               \"doc_count\": 1,\n               \"score\": 0.5,\n               \"bg_count\": 1\n            }\n         ]",
      "Thinking Critically about JLH Significance Scoring",
      "That’s it, right? No need to dive further? Not quiet: you’ll need to understand how this scoring works in the context of your data. To build really good recommendation systems you need to be able to think more deeply about the scoring used. How did significant terms arrive at this ranking? As we saw in the basket analysis article, different forms of significance scoring have their own pros and cons. Choosing the wrong method can have disastrous consequences for the quality of recommendations.",
      "I’m not going to take apart every form of significant terms scoring in this article (there’s many options), but let’s dive into the default method to teach ourselves how to think through these problems. The default is called JLH. It multiplies two values",
      "(foregroundPercentage / backgroundPercentage)  * (foregroundPercentage - backgroundPercentage)",
      "\"Foreground\" here means the percentage this item occurs in the current search results (the users retrieved that like our item). For example, above Rambo’s foreground percentage in Terminator movies was 100%. The background percentage is the global percentage in the whole collection. For Rambo above it’s 50%.",
      "JLH Scoring on Extremely Common Items",
      "How would you go about evaluating the appropriateness of this scoring to your recommendations use case? Let’s think of a couple of hypothetical scenarios. These let us take apart the scoring mechanics around some imaginary scenarios. Then you can think critically about how realistically they might play out in your real-world data. While we continue to use movies, I’m only playing with hypothetical scenarios here: don’t mistake this for an analysis of JLH scoring’s appropriatteness  against a real data set like Netflix or Movielens.",
      "The first scenario, which we’ve already discussed is the  movie everyone likes. In our data set, this is Forrest Gump. 99.999% of users like Forrest Gump.",
      "Indeed such an extremely popular movie doesn’t fare particularly well with JLH scoring. With the (foreground/Background) term, the best Forrest Gump could achieve would be 100 / 99.999, barely > 1. Similarly (foreground - background) or (100 - 99.999) would get just below one. As you’ll see below, this is a pretty low number for JLH.",
      "But is this a fair scenario? I would argue it’s rare in most domains that an item would be liked by close to 100% of the users. It’s more likely that a much smaller number has a measurable preference for a film. For example, sure perhaps everyone likes Forrest Gump, but most methods of detecting a \"like\", such as an explicit rating or an implicit watch/click, will not show 100% of the users interacting with Forrest Gump. Sure, I like Forrest Gump, but last time I watched it was probably 10 years ago. It’s a bit of a passive preference. I’ve seen it, I’m unlikely to be excited when I see it show up on Netflix.",
      "A more likely scenario is the most popular movie or show is something that, say, 20% of the users like. How does JLH scoring fare here? If this movie, let’s say Good Will Hunting, is liked 100% by this set of users, then the highest score would be (100 / 20) * (100 - 20) or 5 * 80 or 400. Much higher than the perhaps unrealistic Forrest Gump scenario.",
      "Items of Average Popularity",
      "Most movies show single digit percentage of users that like the movie. Let’s move on to an average movie: Rocky IV, liked by 4% of users.",
      "How well does Rocky IV fare with JLH? Well repeating the JLH movie, if 100% of the foreground users like Rocky, we get (100 / 4) * (100 - 4) or 25 * 96 = 2400. Wow that’s significantly more than the likelihood of being recommended Good Will Hunting!",
      "More \"normal\" deviations",
      "With a 400 best possible score for popular Good Will Hunting and a 2400 for average popularity Rocky IV, it seems like JLH is just terrible for recommendations, right? Not quite. Well let’s think about the likelihood of these \"best case\" scenarios. How reasonable is it for 100% of users that like one item to like another item? How likely is it that the foreground probability of liking an item deviates so dramatically from the background preferences?",
      "Well that depends heavily on the relative distribution of item preferences in your data. One could argue, for example, that if you like Rocky III there’s a very high probability you’ll also like Rocky IV. Perhaps close to 100% overlap.",
      "However, if your data tends to not change as much between the background and foreground data sets, perhaps this isn’t as big of a deal. For example, let’s consider cases where the expected value of the foreground is based on a normal distribution, centered around the background percentage. Let’s consider a hypthothetical constant change between background and foreground is a change of about 75% (Rocky IV could be 7+/-3%; Good Will Hunting could be 20 +/- 15%). In these perhaps more realistic scenarios, we would expect scores:",
      "Rocky IV best score: (7/4) * (7-4) = 5.25\n  Good Will Hunting best score: (35 / 20) * (35 - 20) = 26.25",
      "In other words, an equal proportionate shift rewards the popular movie much more than the one of middling popularity. The first term (the division) will be equivalent between the two movies. The other term (the subtraction) however has the potential for providing a much higher multiple for the popular movie given the shift.",
      "Why might this be? As the creator of JLH, Mark Harwood, points out this is precisely what JLH is designed to do. JLH reflects patterns seen in many recommendation data sets: popular items don’t change dramatically in popularity. Even when scoped to other seemingly related items they deviate far less from the background popularity. We mentioned that everyone purchases eggs in the basket analysis article. If almost everyone buys eggs, scoping egg purchases to other omellete ingredients probably shows only a small increase in total proportion of egg purchases. But we still want reasonable recommendations for these omelette chefs: so even mild increases in egg popularity should seen as significant.",
      "Given this, perhaps it’s more typical that Good Will Hunting only deviates +/- 25% from the background. It just so happens, this makes Good Will Hunting’s highest score to be the same as less popular Rocky IV:",
      "(25 / 20) * (25 - 20) = 6.25",
      "In other words, if we examine, say, Sense and Sensibility, Good Will Hunting increasing 25% in popularity scores as highly as our hypothetical Rocky IV increasing 75%.",
      "Final thoughts on JLH Significance Scoring for recommendations",
      "Given what we’ve just seen, JLH assumes less popular items are more likely to increase their foreground probability than background percentage.",
      "Typical foreground percentages tend to stay fairly close to background percentage, as in you won’t suddenly see a 100% overlap between two items\n  The foreground percentage for popular items deviates much less than the foreground percentage of less popular items.",
      "The most important takeaway is this criteria is dictated by the statistical patterns in YOUR data. You need to evaluate typical patterns in your data. When you focus on typical preferences what’s the relationship between foreground and background percentages? This is the hard, domain specific work you need to pursue to build a great recommendation system.",
      "But there’s one nefarious case we haven’t yet considered. What about extremely rare movies that happen to overlap? We saw in the basket analysis article that this was the achilles heel for the Jaccard similarity. So let’s consider one such scenario, involving a two movies that just so happened to be liked very rarely and by exactly one user:",
      "Two films Dumb Movie and Weird Art Film each occur twice. And it just so happens that Dumb Movie and Weird Art Film were both liked by one user that happens to like obscure and/or bad movies:",
      "PUT recs/user/2124\n{    \"movies_liked\": [\"Weird Art Film\", \"Forrest Gump\"]}\n\nPUT recs/user/2125\n{    \"movies_liked\": [\"Weird Art Film\", \"Dumb Movie\"]}\n\nPUT recs/user/2126\n{    \"movies_liked\": [\"Dumb Movie\", \"Rambo\"]}",
      "The background percentage of these movies is very low. Perhaps 0.001%. When recommending movies to people that like Weird Art Film, the foreground percentage of \"Dumb Movie\" becomes 50%. Suddenly very common! How does this score?",
      "(50 / 0.001) * (50 * 0.001) = 2499950.0",
      "Eeegads! That’s a high score.",
      "Suffice it to say, just as discussed in Ted Dunning’s paper on Surprise and Coincidence this result is unreasonably high. We only have a very small handful of users that like these movies. Probably not enough to make statistically robust conclusions about significance. You can see this if you consider how far the foreground and background percentages are from each other. If the data shows +/- 25%, we’d eexpect this rare movie’s foreground percentage to be some value under 0.001 to just above 0.001. Definitely not 50. This value probably defies our beliefs about the typical patterns we see in our data.",
      "So in addition to the recommendations above, there’s another important criteria to evaluate when applying any form of significance scoring:",
      "Find a reasonable lower bound of number of likes that begins to reflect typical statistical significance between foreground and background relationships. If its more typical for a fg percentage to be +/- 3% centered around the background, but one rare movie shows wild deviations, consider it an outlier and set a reasonable minimum.",
      "(It’s worth noting a high lower bound in the minimum document frequency is a good idea anyway for performance, lest you run all these expensive calculations on every spurious term that occurs).",
      "The big takeaway, is it’s your data that matters with any method of measuring significance. What’s a typical relationship? And what on its face appears to wildly defy expectations?",
      "Towards a Bayesian Approach",
      "I want to keep exploring ways of scoring co-occurences in search engines in future articles. But I want to leave you by exploring one area I find fascinating. In all this talk about foreground and background percentages, I can’t help but think about Bayes formula:",
      "P(A | B) = P(B | A) * P(A) / P(B)",
      "Another way of thinking about what we’re trying to do is we’re trying to understand the probability of liking one item given the probability of liking another item. That’s exactly what P(A | B) expresses. Scoped to all the users that like B (perhaps \"Terminator\") what is the probability of liking some A (perhaps \"Rambo\").",
      "P(Rambo | Terminator) = P(Terminator | Rambo) * P(Rambo) / P(Terminator)",
      "Well P(Rambo | Terminator) corresponds to the foreground percentage. While P(Rambo) corresponds to the background percentage. In other words:",
      "foregroundRambo = ( P(Terminator | Rambo) / P(Terminator) ) * backgroundRambo",
      "I’ve alluded to performing your own data analysis work. Don’t just try to find this relationship between foreground and background probabilities. But try to figure out a typical relationship between foreground and background percentages. You can then use the typical general relationship to evaluate scoring methods, and find a good lower bounds where you feel confident in the statistical patterns in the scoring. If we think about Bayes formula a bit more generally, we see there’s a common transformation happening. Getting away from one example, we can think of functions that return probabilities (ie probability distributions) to get at these relationships:",
      "ForegroundDistribution(item A, item B) = transformation(item A, item B) * background( item B)",
      "I haven’t finished mapping this out in my head yet, but this idea is at the root of Bayes Formula. The \"background\" distribution seems to correspond to a Bayesian Prior. It says, with no evidence, what sort of typical background probability distribution can we expect. In the basket analysis article, we argued for a power-law or zipfian distribution for buying patterns. The \"transformation\" in classic Bayesian analysis corresponds to the likelihood function. Here, it’s the relatively likelihood of A, given some other B. You can imagine this as outputting a constant that’s going to shrink or grow the background probability based on the relative co-occurence of A and B. Perhaps it spits out a 1.5 for some A and B pair – growing the foreground percentage. Or spits out a 0.5, shrinking it.",
      "The \"transformation\" function you can imagine in three dimensions. The x and y axis corresponding to items a and b, with the z axis corresponding to how much you’re going to multiply background to get foreground. You can sample P(B | A) (Terminator scoped to Rambo) by looking up Rambo users and counting the raw count of Terminator users. You can similarly lookup P(Rambo) by snagging the total count of Rambo users. From these values you can compute a point-wise value for transformation.",
      "You probably won’t be able to derive this transformation into a formula. But you can use your data to draw a graph. At any given item pairing, for example Terminator and Rambo, you can compute this value. You need not compute this for every pairing. Remember your goal is to figure out what’s typical to guide your evaluation, not find every possible value.",
      "If you sample this transformation, and values tend to stay between 0.9 and 1.1, you know that your foreground and background percentages don’t deviate a great deal. If popular things grow/shrink less than unpopular things, then JLH might work well for you. If there’s wild swings that go against this pattern, you might have a sense that something like JLH might not be appropriate for your data.",
      "I’m actually really excited to pursue these ideas further (and if you play with them, let me know what happens). For example, this isn’t particularly classical Bayesian analysis, where we’re trying to find our confidence in some model parameters (such as the bias of a coin, or weight of die, or distribution of topics in a corpus). Instead it’s more like a regression where we’re trying to determine the relationship between two values (items A and B) and a transformation. Perhaps something like a loess regression that’s not parameterized (ie doesn’t assume the underlying data fits exactly one formula). Perhaps what I really ought to be doing is trying to do Bayesian analysis on the transformation process to find latent parameters in the relationship between arbitrary A and B items. I’d love to hear your ideas! Get in touch or seek me out for a free lunch and learn if you’re curious about exploring these ideas more deeply or if you want to point me at wisdom that already explores these ideas.",
      "Practical Elasticsearch and Data Modeling Considerations",
      "With an agregrations approach, we’re left with a couple of practical considerations for building great recommendations. It’s worth noting them here as areas for further investigation",
      "We can’t use a significance score from the aggregations alongside typical search scoring, so we can’t directly bias by factors like release date with just one query. We can’t simply do \"recommendations\" alongside \"search\" to do personalized search. Unfortunately aggregations and search live in two different universes.",
      "One way to get around this is by also using the sampler aggregation. The sampler aggregation performs aggregations over the top N most relevant documents. So we could, for example, prioritize recent movies in our search results, then treat the top 100 search results as the \"foreground\" set. This provides an indirect relevance influence over the significance calculation.",
      "Also, perhaps there’s a way to integrate significance scoring with the regular query process. I’ve been playing with using More Like This or locality-sensitive hashing via the new Simhash capabilities which can occur in the normal query process. But I have significant reservations about these methods, which I won’t get into here (I apologize, this article is probably the root for 10 other articles :-).)",
      "Graph-based Recommendations with Elastic Graph",
      "Elastic has released an interesting graph product that does the sampling/significance aggregation alongside an ability to navigate to secondary and tertiary significance. For example, in addition to the exercise above, what if we could generate recommendations, notice which genres or taste profiles are significant to me, then find what’s significant across those taste profiles? This is a kind of graph navigation that can provide a lot of value to users and dramatically simplify the creation of recommendation systems.",
      "We’re actually building some interesting demos using Elastic’s graph product for recommendations that go beyond what’s in this article. If you’re interested in seeing a proof of concept of what we’re doing with Elastic’s graph product, feel free to get in touch. I’d be happy to run you through the ideas and proof-of-concept, even as we’re actively developing it. We’d love to hear your feedback!",
      "Search Engines are the future of recommendations",
      "Open source search engines like Solr and Elasticsearch made search extremely simple to implement. Recommendation systems still require integrating multiple distributed systems, learning R, and hiring a huge team of data scientists. It sounds extremely hard. You can standup reasonable search in a few days, but recommendation systems require a great deal more effort.",
      "We’re pushing to change that. In the same way that Solr and Elasticsearch enable pretty-good search out of the box, we want to help build recommendation systems with far total lower cost of ownership than the current regime. Search engines are the ideal place to implement a recommendation system explicitly because they’re easy-to-maintain, simpler to tune, and straightforward to deploy than juggling dozens of machine learning libraries and distributed systems.",
      "If you’re interested in exploring how Solr or Elasticsearch can simplify your development of a recommendation system, get in touch and we’d love to explore this with you. And everyone gets a free hour of consulting through our \"Lunch and Learn\" program!",
      "Special thanks to Mark Harwood at Elastic for his help reviewing this article."
    ],
    "summary_t": "Let’s explore how to deliver great recommendations with Elasticsearch. In this article, we dive into an aggregrations based method for Elasticsearch recommen..."
  },
  {
    "id": "3b6d3b36b875d15482cefc67f7ff66ec",
    "url_s": "https://opensourceconnections.com/blog/2016/09/13/search-engines-are-the-future-of-recsys/",
    "title": "Why I think search engines are the future of recommendation systems",
    "content": [
      "Recommendation systems have often felt like costly endeavours that require machine learning expertise and large amounts of infrastructure. Search, on the other hand, increasingly feels like a commodity with the ubiquity of Solr and Elasticscearch. In reality, as we argue in Relevant Search, search and recommendations are two sides of the same coin.",
      "As search engines improve their ability to implement collaborative filtering, I see a future where recommendation systems aren’t just the domain of sophisticated organizations. Increasingly, I see a shift towards search engines like Solr or Elasticsearch as the only tech needed to implement a recommendation system.",
      "Here’s my opinions on why the future of recsys will be rooted in open source search engine tech:",
      "Devs grok search engine tech – Search engine tech, ie Elasticsearch and Solr, have become extremely familiar to developers. As recommendation features make their way into search engine tech, organizations will get 90% of the way there on collab filtering/personalized search without having to invest in additional infrastructure and skills.\n  \n  \n    Search engines let you mix and match multiple relevance models. Manually tuned relevance works well early on when there’s not statistically significant user behavior to drive results. For small organizations, this can help with the \"cold start\" problem and allow recommendation systems without much user behavior. For larger organizations that can gather more user behavior, search engine technologies allow practitioners to layer in collaborative filtering, personalization, and learning to rank alongside manually tuned relevance.\n  \n  \n    Simpler, more straight-forward recommendation relevance tuning. Solr and Elasticsearch have sophisticated query DSLs that let you directly layer in many ranking factors, including location, content recency, and many other content features alongside collaborative filtering. Tools like Quepid and the practice of Test Driven Relevance let users more simply test relevance solutions.\n  \n  \n    Breadth of applicability of Solr and Elasticsearch: Open source search engines are becoming less about being the best implementation of a search bar, and more focussed on general information retrieval framework. The next generation of user interfaces: voice search, conversational UX, chat bots, etc often have search at the core. Image search, and other forms of \"similarity\" often turn to Lucene-based search. Recommendation systems will be absorbed into the fold, so they can be mixed and matched with many of these other cutting-edge applications.\n  \n  \n    Real-time recommendations: Instead of running offline jobs to find user-item affinities, this can be done in real-time while simultaneously mixing in content properties and other assets mentioned above. The inverted index data structure is really stinking fast and always getting faster. Lucene has been tuned extensively over the years for looking up things given a feature that describes that thing. Or as my coauthor John Berryman puts it, a \"sophisticated token matching system.\" And as I talk about all the time \"tokens\" can be any descriptive feature of your content, not just a word.\n  \n  \n    The next gen of recsys won’t look like the current gen: Recsys today is where search was in the early 2000s. In the early Web, search engines were the domain of extremely sophisticated firms with deep pockets. Solr and Elasticsearch laid waste to those days, letting anyone deliver a basic level of search very quickly. Solr and Elasticsearch will do the same to the next generation of recsys: allowing less sophisticated organizations with much shallower pockets implement 90% of what’s needed relatively quickly without extensive amounts of additional infrastructure or retraining staff.\n  \n  \n    Search is still important: Some say search engines will in fact be replaced by recommendation engines. I disagree. Because search is still an extremely important interaction form. With search, users tell you what you want and you need to respond with relevant matches in real-time. The core component of that is still what Solr and Elasticsearch are good at: matching and ranking content and showing them to the user. Also, most of the supporting functionality of search: autocomplete, faceting, highlighting, etc is still extremely crucial to this interaction and isn’t going away anytime soon. And the personalization that collaborative filtering offers only makes search better.",
      "That’s my listicle (opinioncle?). What do you think? Let’s discuss over at hacker news and you can tell me where I’m wrong. If you’re curious about this topic, check out my post on implementing really good recsys with Elasticsearch. Please invite me to your company’s lunch and learns and I’ll be happy to talk about implementing recommendations with open source search engine technology with your team!"
    ],
    "summary_t": "There’s a trend away from recommendation systems being the domain of wealthy organizations. Increasingly, I see a shift towards search engines like Solr or E..."
  },
  {
    "id": "edfcf42fa2ba72ebb3aae288457b6dcf",
    "url_s": "https://opensourceconnections.com/blog/2016/09/17/expanding-data-frequency-table-r-stata/",
    "title": "Expanding data from a frequency table into case form in R and STATA",
    "content": [
      "I’ve been learning a bit of R this summer. R is a pretty powerful language and environment used for statistical computing. I chose it because it’s open-source (and therefore free!).  I’ve been using RStudio in conjunction with R — RStudio has a more intuitive GUI that’s a little easier for me to use as a beginner.",
      "As I mentioned in earlier posts, part of my project involves regression analysis. This type of analysis is usually done on case data. However, my datasets yielded frequency tables, and I found myself needing a way to expand these tables into case form.",
      "The answer is a function called expand.dft that is part of the vcdExtra package for R. Downloading vcdExtra is simple, but keep in mind that you may need to install a few dependencies to get it to work correctly.",
      "Likewise, using vcdExtra is pretty simple. Make sure you know which column in your raw data contains the frequency numbers, and name that column Frequency for simplicity’s sake.\nI started with a frequency table that looked a little like this:",
      "Gender\n      Enrollment\n      Frequency\n    \n  \n  \n    \n      Male\n      0\n      50\n    \n    \n      Male\n      1\n      25\n    \n    \n      Female\n      0\n      50\n    \n    \n      Female\n      1\n      15",
      "Here’s the code:",
      ">FrequencyTable <- read.csv(\"rawdata.csv\", header = TRUE)\n>Library(vcdExtra)\n>CaseForm <- expand.dft(FrequencyTable, freq=\"Frequency\")",
      "Easy peasy! The end result is a table of 140 rows (50+25+50+15), one for each person represented in the original frequency data.",
      "Another note: I’m working with a large dataset that contains millions of points. Expanding that data to case form, then running a regression on it can be quite a task for a computer that doesn’t have a ton of RAM.  So an alternative, if you are lucky enough to have access to STATA, is to simply open your dataset and type \"expand Frequency\" into the command area to automatically expand the data in your frequency column into instances of case data.",
      "To check whether your data’s been expanded properly, simply ensure that the number of rows in your new dataset are equal to the sum of the cells in your frequency column."
    ],
    "summary_t": "Sometimes you need to convert from case tables to frequency tables.  Here is how I did it with R."
  },
  {
    "id": "1752448d624e3f0fe244d9b61b3eafb3",
    "url_s": "https://opensourceconnections.com/blog/2007/05/15/learn-about-selenium-betech-tomorrow/",
    "title": "Learn about Selenium @beTech tomorrow!",
    "content": [
      "Ill be presenting Selenium for beTech tomorrow in Newcomb Hall at 2 PM!",
      "Im throwing out my old Selenium presentation that has served me well, taking me to such conferences as OSCON in Portland, Oregon, a testing conference in Valencia, Spain, and of course at our own local Neon Guild conference and doing something completely new, focusing on themes and possibilities of using Selenium versus dry technical details. See you tomorrow!"
    ],
    "summary_t": ""
  },
  {
    "id": "b5806e34e29b88afccab6c59b839a292",
    "url_s": "https://opensourceconnections.com/blog/2016/10/05/elastic-graph-recommendor/",
    "title": "Building Recommendation Systems with Elastic Graph",
    "content": [
      "As I’ve written about extensively, I strongly believe that search engines are the future of recommendation systems. The biggest reason being cost: today’s search engines require extensive amounts of expertise and skill to build – usually from scratch. Open source search engines like Solr or Elasticsearch, however, increasingly have capabilities that simplify most forms of user behavior pattern recognition – on top of already existing robust ranking and relevance capabilities.",
      "Today we’re showing off what we mean. We’ve taken one promising product in this arena, Elastic Graph, and applied it to a classic recommendations data set: movielens. Movielens has years of explicit user ratings of movies. It serves as one of the canonical data sets for performing recommendations.",
      "",
      "You can play with the demo here and see the source code here. One thing you’ll notice is the number of settings we’ve exposed: different options for tuning the behavior of Elastic Graph. This really just scratches the surface. Elasticsearch is a ranking machine: customizing the relevance of search and recommendations is where it excels.",
      "After you play with the demo, follow along here if you’re curious how it all works.",
      "How does Elastic Graph deliver tremendous recommendations? Sampling & Significance",
      "This demo implements what’s known as user-to-user recommendations. The documents in the index are users themselves, and the movies they’ve liked. In other words, something like",
      "{\n\t\"liked_movies\": [\"1234\", \"582\", \"581\"]\n}",
      "In the demo, you indicate a few movies you like. Elastic Graph uses search to find users that also like those movies. It then tells you what movies are statistically significant in that set of users. In other words, it recommends to you movies people similar to you also enjoy.",
      "How exactly does this work? This is a combination of three processes, enumerated below:",
      "1. Crafting a query that searches for similar users.",
      "The first job is to create a query that will find users like you. In the case of our demo, we craft a query that simply ORs together the movie ids you like into a big search. Something like",
      "{\n    \"match\": {\n        \"liked_movies\": [\"1234\", \"521\"]\n    }\n}",
      "The most relevant results for this query those users most similar to you (they like many of the movies that you do). This being search, focussed on relevance, those users bubble to the top of the results, followed by increasingly less similar users, down to users who only enjoy one movie that you also enjoy.",
      "Of course, this query may not be the best way to do this similarity, but hey, good news is this is Elasticsearch. You have an extensive Query DSL to use to tweak and tune how similar users are identified. There’s even a whole book for that ;).",
      "2. Limiting to N most similar, instead of all results",
      "With a query like the one above, there’s a chance you’ll get a large number of results. Remember you’re searching users. Like I said, many of these users will match only one movie, others will match many of your liked movies. How do we prioritize users that like many of the same movies you do, and ignore the others?",
      "Well luckily, users are ordered by relevance. The users most similar to you come to the top. Those just barely similar at the bottom (with those not at all similar not even being included in the results).",
      "It’s a good idea to have a cutoff somewhere that eliminates less similar users. Then you use only incorporate these top N most similar users when formulating recommendations. This is precisely what something like the sampling aggregration does and also what Elastic Graph does. When we give Elastic Graph a query like the one above, we specify the \"sample_size\". Here’s our proto-graph request, adding a section for a sample_size. (The variable query below simply being the Query DSL query from above.)",
      "{\n    \"query\": query,    \n    \"controls\": {\n        \"sample_size\": 500\n    }\n\t\t\n}",
      "I hope you’re starting to see why search makes a tremendous graph exploration platform: it focuses on significant and relevant connections using simple, effective, and highly customizable relevance ranking.",
      "3. Statistically significant co-occurrences between you and similar users",
      "Now that we’ve limited to the top 500, what’s next? Well we need to examine the movies they’ve watched and tell you about the ones you haven’t seen yet. Those are your recommendations!",
      "At first blush, the solution seems obvious. Count up the total number of movies in this set. Show you the ones you don’t know about yet: ordered by that count. If \"Forrest Gump\" occurs 20 times and \"Rambo III\" occurs 15, then that’s how we should recommend movies to you.",
      "But there’s actually a big problem with this. In any random sampling of users, looking at the raw counts, we’re probably going to get counts that roughly correspond to what’s globally popular. It’s like presidential polling: if we poll 1000 random voters, we sort of assume that the results will roughly be in line with the overall voting preference.",
      "We actually don’t want a sense of what’s globally popular in this result set: we want a sense of what’s oddly popular to users like you. For example, hypothetically speaking, 80% of sci fi geeks may like Star Trek. The general movie viewing population may only enjoy Star Trek 10% of the time. If you’re like me, you fall in with the sci fi geeks. Which means your recommendations will show you what’s much more interesting to these users when compared to the global population of users.",
      "How exactly does Elasticsearch score this? Well there’s a number of methods, but I would recommend reading either my blog article or the documentation on significant terms aggregration.",
      "More importantly, how to we ask Elastic Graph to perform significance calculation for the results of a search query? To do that, we introduce the \"vertices\" in our request below, indicating the field from the search result to use for the significance calculation. Looking at the complete picture of what this request will accomplish:",
      "query is issued to Elasticsearch, users similar to you are return ranked by relevance\nThe 500 most similar users are taken\nRaw counts (basically a facet) are gathered for the movies in liked_movies, we might note that Forrest Gump occurs 10 times, Star Trek 8 times, and so on\nThe local raw count is compared to the global preference for these movies to measure their significance. We might score Star Trek as very significant (10.5) and Forrest Gump as less significant (1.5)",
      "{\n    \"query\": query, \n    \"vertices\": [\n    {\n        \"field\": \"liked_movies\",\n        \"exclude\": likedIds\n    }\n    ],\n    \"controls\": {\n        \"sample_size\": 500\n    },\n\n}",
      "You’ll also note the exclude line in vertices. This simply excludes movies you like from the significance calculation. Users similar to you like these movies as well, so there’s no point in running them through the significance calculation. Instead, this lets us focus on movies you haven’t seen, but users like you really like.",
      "Tuning Relevance",
      "In our demo, we expose a few extra ways to seed the recommendations graph above. This gets at another core strength of Elasticsearch. You can give Elasticsearch more features of your users (more about what they like, age, demographics, etc).",
      "With movielens, we can extract some information about each user’s preferences and explicitly include it in the seed query",
      "",
      "These include (as pictured above):",
      "Identifying genres you seem to like, and highlighting users that also like those genres\n  Identifying release years you like (specifically broken into half-decades)\n  Using the description text from each movie you like and running a \"more like this\" on movies they like",
      "Incorporating them is as simple as extending the query used above. Perhaps if we included genres you seem to like, we could find similar users with the query:",
      "{\n    \"bool\": {\n        \"should\": [\n            {\"match\": {\n                \"liked_movies\": [\"1234\", \"521\"]\n            }},\n            {\"match\": {\n                \"liked_genres\": [\"science fiction\"]}}\n      ]\n    }\n}",
      "Some of these seem to work better than others (the description text often provides oddly compelling results). We encourage you to experiment and let you know. If you want to see how the query is changing with each setting, check out where the query is built in the source code.",
      "Finding connections",
      "This is graph afterall! One of the fascinating capabilities of Elastic Graph as a recsys platform is its ability to look beyond the current set of recommendations into a secondary tier. For example, if Elastic Graph recommends Rocky to you, Elastic Graph will tell you what’s interesting for Rocky. By default it will tell you this globally: perhaps showing you Rambo, Rocky, Turner and Hootch, Stop or My Mom Will Shoot, and other popular Stallone movies.",
      "But what’s interesting to me is you can guide the exploration contextually: scoping it to items that you likely consider relevant. To do this, we can scope using the query above, perhaps only keeping the liked_movies clause to construct a guidingQuery as shown below.",
      "In the demo we give you the option to toggle global/local graph exploration. Expanding on the graph query above, this becomes",
      "{\n    \"query\": query, \n    \"vertices\": [\n        {\n            \"field\": \"liked_movies\",\n            \"exclude\": likedIds\n        }\n    ],\n    \"controls\": {\n        \"sample_size\": 500\n    },\n    \"connections\": {\n        \"query\": guidingQuery,\n        \"vertices\": [\n            {\n                \"field\": \"liked_movies\",\n                \"exclude\": likedIds\n            }\n        ]\n    }\n}",
      "We introduce the \"connections\" section. Everything under \"vertices\" is the same as above. For each primary recommendation, show me other statistically significant liked_movies but don’t show me what I like. Elastic Graph will come back detailing vertices of additional length and how they’re connected to our recommendations (vertices of depth 0).",
      "The more interesting line is \"guidingQuery.\" This scopes the exploration only to plausible recommendations for me. The end result is for each movie I might be interested in watching, I can see instantly other movies I plausibly would enjoy that sort of \"go down that trail\" of interest. For example:",
      "",
      "There’s a couple of interesting observations here",
      "The Fighter might not be a direct recommendation to me, but it seems that its \"close to me\" in the graph – friends of friends seem to like it\n  I can imagine helping users with a conundrum of choice. I often see a movie I’m vaguely interested in: seeing what else is down that rabbit hole can really help - focus what I want to watch instead of faffing around with high-level recommendations that cover my broad tastes",
      "What else can you do?",
      "One thing we point out in Relevant Search, is that you choose open source search engines because they’re a framework, not a solution. That means that with great responsibility comes great power. You could apply the functionality here to implement additional capabilities. Some I can imagine include:",
      "Personalized Search – use Elastic Graph’s rankings to reweight search results for a user.\n  Social Search – can we reweight search according to direct social relationships?\n  Taste Profiles – bucketing recommendations and search results by specific tastes, genres, or other notable properties",
      "I’d love to hear if you have any neat ideas as well. Don’t hesitate to get in touch.",
      "The future is easier-to-build recsys",
      "I’ve been impressed with how open source search has transformed traditionally hard problems into far simpler endeavours. These days, without having incredible depth in search or analytics, you can get a solution up and running relatively quickly. Decades ago, search was the purview of extremely sophisticated organizations. Lucene-based search (Solr/Elasticsearch) have laid waste to those days. The same thing is happening in analytics. Organizations seem to be replacing tools like Splunk with Elasticsearch/Logstash/Kibana at neck-break speed, resulting in far simpler tools that can be more easily customized by developers.",
      "Recommendation systems are next. If you’re contemplating a recommendation system, you probably think you need to beef up staffing and infrastructure. You’re imagining expensive proprietary tools or an extensive custom development effort. Before you see anything compelling it will be months.",
      "This simple demo was thrown together in 10s of hours with Elastic Graph. It can always be improved, for sure – just like any search application can be improved. More importantly, with Elasticsearch those improvements can come iteratively. With the Elasticsearch Query DSL, relevance tuning, and aggregrations you can make constant tweaks and deep alterations to how Elasticsearch behaves.",
      "Another way to say this is Elastic Graph democratizes recommendations just like Elasticsearch has democratized search and analytics. If you’re a medium-sized organization contemplating a straight-forward recommendation system without the fuss and expense, this is a great place to invest your $$.",
      "If you’re interested in discussing further, I’d be happy to chat about our expertise building recommendations with Elasticsearch. I hope you’ll get in touch if you’d like to discuss how we can help evaluate whether Elastic Graph is a good fit for building a recommendation system for you. If you’d like me to discuss this in-depth with your team, I speak for free at your company’s knowledge sharing events/lunch and lears!"
    ],
    "summary_t": "Graph + Search means easy to implement recommendations. Check out what we’ve done with the Elasticsearch graph plugin."
  },
  {
    "id": "25b000ff1ba30c2d7b38eb778c8bccf9",
    "url_s": "https://opensourceconnections.com/blog/2016/10/19/bm25f-in-lucene/",
    "title": "BM25F in Lucene with BlendedTermQuery",
    "content": [
      "As part of the London hack days Diego Ceccarelli started a BM25F implementation. I began to continue it at Lucene Revolution’s Lucene hackathon. I realized though that when you break down the problem, BM25F can be implemented using existing Lucene bits, including the existing BM25Similarity and the BlendedTermQuery.",
      "BM25 and BM25F",
      "I’ve written about BM25 before. BM25 is an iteration on the classic TF*IDF ranking formula, but with several innovations:",
      "Term Frequency Saturation: We know more times a search term occurs in a document, the more we should consider this document relevant for that search term. But with BM25, we realize at some point you reach diminishing returns. BM25 reaches this point relatively quickly, compared to classic TF*IDF.\n  Smarter document length weighting: A search term occurring once in a short doc is more relevant than a single term occurring in a longer doc (a book). BM25 penalizes/rewards document length relative to a document’s average document length, as opposed to just having a constant multiple based on document length\n  Basically the same IDF calculation: Rare search terms (low doc freq; high IDF) get weighted more heavily than common search terms. The BM25 calculation isn’t that much different than TF*IDF’s calculation",
      "BM25F performs per-field BM25 calculation, but uses the shared document frequency across multiple fields. This document frequency blending is important. For example, sometimes you have scenarios where a common term, say \"cat\" is actually rare in one particular field (say the \"title\" field). But when you broaden out to other fields, you then realize that cat is particularly common, and shouldn’t be scored so highly.",
      "In other words, BM25F basically does",
      "CombinedIDF * ( BM25_title + BM25_description + …)",
      "(note there are all kinds of subtle variants)",
      "Can we implement that using existing Lucene bits? I think so… with a few caveats that Diego has pointed out. You can follow along by looking at this github repo with the sample code from this post.",
      "Per-field BM25Similarity",
      "BM25F lets us configure BM25 parameters per field. Luckily, per-field similarity is pretty easy to configure in Lucene using a PerFieldSimilarityWrapper. We simply need to setup our index accordingly. Notice in the similarity below, k1 and b differ for title and description:",
      "static Similarity perFieldSimilarities =  new PerFieldSimilarityWrapper() {\n      @Override\n      public Similarity get(String name) {\n          if (name.equals(\"title\")) {\n              return new BM25FSimilarity(/*k1*/1.2f, /*b*/0.8f);\n          } else if (name.equals(\"description\")) {\n              return new BM25FSimilarity(/*k1*/1.4f, /*b*/0.9f);\n          }\n          return new BM25FSimilarity();\n      }\n};",
      "Then when we setup the IndexReader",
      "IndexWriterConfig config = new IndexWriterConfig(analyzer);\nconfig.setSimilarity(perFieldSimilarities);\n\nIndexWriter w = new IndexWriter(index, config);",
      "BlendedTermQuery to implement BM25F",
      "Lucene’s BlendedTermQuery forms the guts behind Elasticsearch’s cross_field search. What BlendedTermQuery does is blend global term stats as best as possible across different fields. If the document frequency for cat in title is 3, but the document frequency for cat in the description field is 50, it takes the maximum, 50. This approximates the actual document frequency across both fields (we can’t figure out overlaps very efficiently at query time).",
      "This value is then used for searching both title and description, accounting for the true rareness of the term. To do this, it builds a set of Lucene TermQueries and tells the TermQuery for each field to use the blended document frequency, instead of what’s in the index for that field. So you can imagine that now you have two queries description:cat (\\*with doc freq 50) and title:cat (\\*with doc freq 50)",
      "BlendedTermQuery then gives you two options for combining these queries. You can either as a dismax query (take the max of the underlying field scores) or a boolean query (sum the underlying field scores). In other words, you can pick between one of two ranking functions:",
      "Dismax: The best scoring field wins, with an optional tie-breaker parameter to incorporate other field scores. In other words: max( description:cat (with doc freq 50), title:cat (with doc freq 50), …)\n  Boolean: A summation of the field scores, in other words description:cat (with doc freq 50) + tilte:cat (with doc freq 50) + …",
      "Examining the math above, we really care about the latter form for BM25F as it involves a summation. To implement that, we simply use the following code.",
      "BlendedTermQuery query = new BlendedTermQuery.Builder()\n\t\t\t\t .add(new Term(\"title\", \"cat\"), /*boost*/1.0f)\n\t\t\t\t .add(new Term(\"description\", \"cat\"), /*boost*/1.0f)\n\t\t\t\t .setRewriteMethod(BlendedTermQuery.BOOLEAN_REWRITE)\n\t\t\t\t .build();",
      "Checking the ranking function",
      "So with the BlendedTermQuery above, what we have now is something like the following ranking function",
      "description:cat (with BM25, docfreq=50) + title:cat (with BM25, docfreq=50)",
      "Which is the same as",
      "IDF( docfreq=50) * (description:cat with BM25) +  IDF(docfreq=50) * (title:cat with BM25) +",
      "Here description:cat with BM25 means the non-IDF portion of the BM25 calculation (the term frequency and length part, specific to this field, boosts for this field, etc). The IDF(docfreq=X) is the BM25 IDF formula for a term with a given document frequency.",
      "Now it turns out, this is (pretty much) BM25F! All we need to do to make this BM25F is to factor the IDF (docfreq=50) out to see the formula we have above",
      "IDF( docfreq=50) * ( (description:cat with BM25) +  (title:cat with BM25) )",
      "A few subtle differences",
      "There’s a few reasons this isn’t quite BM25F. First of all Lucene’s boolean query uses a coordinating factor (coord) to reward/punish documents that match all the clauses. So:",
      "IDF( docfreq=50) * ( (description:cat with BM25) +  (title:cat with BM25) )",
      "Is actually",
      "coord * IDF( docfreq=50) * ( (description:cat with BM25) +  (title:cat with BM25) )",
      "If you recall coord is the number of matched clauses / number of total clauses. So if only 1 out of 2 clauses match, coord is ½. This further rewards documents that match more than one clause. Latest versions of Lucene have removed coord",
      "Another thing to point out is that it’s difficult to piece together the true cross-field document frequency at query time. So the maximum is taken as an approximation. The actual blended document frequency isn’t entirely accurate, but we know it ranges somewhere between the max of the two field doc frequencies (when there’s 100% overlap) and the sum (when the fields mention the term in different documents).",
      "And that’s it! Get in touch.",
      "That’s it! Get in touch – I’d love to hear feedback if I missed anything. And if you want to pick my brain for free check out the lunch and learns. I’m about to announce some new topics that might interest your team! And as always, don’t hesitate to reach out about our relevancy services – we just got this great testimonial from Careerbuilder about our services:",
      "OSC’s Solr/Lucene knowledge expanded and greatly improved the abilities of our public job search. They delivered technical excellence at every turn: demonstrating expertise in Lucene internals, relevance models, and data science backed by a solid methodology for improving search relevance. On a deliverables front: they learned our legacy search stack quickly and made high-quality code contributions. OSC marries technical excellence with strategic insight: we highly recommend their experts to any search team."
    ],
    "summary_t": ""
  },
  {
    "id": "65c7771a70d8abb8ecede30908c0ebf3",
    "url_s": "https://opensourceconnections.com/blog/2016/11/22/first-impressions-apache-nifi/",
    "title": "First Impressions of Apache NiFi",
    "content": [
      "My colleague Scott had been bugging me about NiFi for almost a year, and last week I had the privilege of attending an all day training session on Apache NiFi.   NiFi (pronounced like wifi), is a powerful system for moving your data around.  NiFi attempts to provide a unified framework that makes it simpler to move data from the edge of your systems to your core data centers, and makes your Ops folks the core audience.   It offers some unique features to help you feel confident about what is happening to data as it flows through your system, and offers a unique (at least in the FOSS world) visual programming model for moving your data around.",
      "As our instructor, Andrew Psaltis of HortonWorks put it:",
      "As humans, we want to classify something new into buckets we’re already familiar with.  But NiFi doesn’t classify easily.",
      "While NiFi clearly overlaps with systems like Enterprise Service Bus, Processing Frameworks, Messaging Buses, and most clearly ETL, it isn’t just one of them.   NiFi instead is trying to pull together a single coherent view of all your data flows, be very robust and fast, and provide enough data manipulation features to be useful in a wide variety of use cases.",
      "",
      "Below are some of my impressions based on the day of training I took:",
      "Shares many of the best aspects of Camel, but fixes some weaknesses.  I really like Camel and we’ve used it a number of times, and the flow of data through steps is very similar.  However Camel configuration is very developer centric, \"build and deploy\" model.  If you have a problem in production, then fixing it requires shutting everything down, redeploying configurations, and then starting it back up.   I really liked the visual flows that NiFi provides, and the ability to stop and start processors dynamically.   Plus, NiFi has much deeper ability to track what is happening to your data, what they call data provenance.   Hawt.io just doesn’t compare from a management perspective.\n  \n  \n    Simple to stand up.  Unlike many of the various projects that fall under the Hadoop ecosystem, NiFi seems to be very self contained.  My typical approach is to look at how complex the Dockerfile is for a project.  The most popular one mkobit/nifi is just a couple of commands, mostly around downloading the distribution.   NiFi doesn’t depend on a lot of other Hadoop ecosystem.\n  \n  \n    Simple data processing is best in NiFi.  My test project is taking Powerpoint documents, and first extracting the text content, and then converting each slide to an image, and then OCR’ing that image.  (This is to power a highlight on image search interface that is pretty awesome!)   I want to replace my home grown data processing logic with NiFi.   So what is slick is that it was super easy to stand up an HTTPListener that actually receives my PPTX files!   Passing in other metadata as part of the post to the HTTPListener was easy as well.   However trying to send each .PPTX file over to my API for further processing using the HTTPPost turned out to be a bust.  It wasn’t obvious how to connect things, though admittedly I am a newbie at this.  I made progress with the InvokeHTTP processor, but still the exact wiring magic wasn’t obvious.  However, if I am manipulating JSON or CSV data, then it’s much more native feeling set of tools for tweaking the data.\n  \n  \n    For more complex processing, it looks like you can pretty easily create your own Processor, as a block of Java code that is packaged up as a NAR (nifi archive).  I’m going to try out the OCR process as a NAR, and see how that goes.\n  \n  \n    MiNiFi is a light weight version of NiFi.   Most (all?) of the power of NiFi, but without the UI layer.  This is what you want to put on your cash register, or Raspberry PI system, and it comes in both a Java and a super compact C++ version.   This made me think of Beats from Elastic.co.   Beats ties into Logstash, and MiNiFI ties into NiFi.   Logstash is great, but it’s devOps oriented, and doesn’t have the visual layer or the provenance tracking that NiFi has.",
      "The next time I’m looking to move data around, and have a fairly flexible way of bringing data in from multiple sources, then I think NiFi has a lot to offer.   I know a lot of folks like Talend, but it just feels heavy and kind of old.  NiFi, while it’s been around for a while, still feels like it is at it’s growth stage in adoption and features.   Plus the list of already supported processors is really impressive!   Now if they can just make the connecting of processors a bit simpler and visual, and slightly less magic, then nirvana!",
      "Want to dive deeper, checkout this awesome list of NiFi resources!"
    ],
    "summary_t": "Moving data around.  It’s always a pain.  Is NiFi the better mousetrap for this?"
  },
  {
    "id": "96981dd9f07c57890d624399b38b7758",
    "url_s": "https://opensourceconnections.com/blog/2016/11/30/reverse-showrooming/",
    "title": "Traditional Retail: Stop Whining, Start Reverse Showrooming",
    "content": [
      "We’re all familiar with showrooming. It’s the phenomenon where shoppers go to the store, see something they like, then check amazon to see if the item is available. Traditional, brick & mortar retailers love to blame their low sales on this phenomenon. Yet, as a shopper that understands search & recommendations, I rarely find brick & mortar retailers demonstrate the kind of technical innovation needed to counter the showrooming phenomenon.",
      "Today I want to talk about one strategy traditional retailers can use: reverse showrooming. What do I mean by reverse showrooming? Basically that retailers take advantage of my materialistic lust to have an item right fricken now while I’m sitting at home. With reverse showooming I sit in front of my computer/phone and try out Best Buy or Costco to see if I could get the item sooner. It’s reverse because it’s a chance to get in on Amazon’s territory: the user sitting in front of their computer/phone wanting something.",
      "If you’ve ever had a thought \"I want something right now\" and visited a bestbuy.com or costco.com to see if you can get it right away, you’re a prime candidate to be targeted for reverse showrooming. Unfortunately, most traditional brick & mortar create too much friction and optimize in too many directions. They don’t see themselves as immediate gratification engines and instead focus on trying to compete with Amazon.",
      "Here’s the hindrances I’ve had when trying to satisfy my materialistic urges:",
      "The first hindrance is search is no good. I can’t find what I want. If I search for \"surface\" and I get a list of \"surface cleaners\" on your store, I’m likely not going to continue my journey. Many retailers consider their digital storefronts secondary. They focus on SEO or other forms of Internet marketing, but don’t think about the user’s journey through the actual store.",
      "Another related problem is when search is hyperfocussed on giving me the \"one right answer\" to my search. In the case of \"reverse showrooming\" I don’t want the perfect item, I want to just browse the shelves for a little immediate retail therapy. This plays towards physical retail’s more limited inventory. Focus more on recall: letting the user compare and contrast 3 or 4 options. Focus on content-specific filters, that let narrow their priorities in ways that reflect what users prioritize for that content type. An example of this is my experience buying a laptop bag on NorthFace’s site.",
      "Lower-inventory, traditional retail needs to focus on quality over quantity. Amazon is all about quantity. They leave it to the user to do needed research on their extremely vast array of products–and its a lot of work! With reverse showrooming, I’m making a knee-jerk buying decision. Don’t let me hesitate on buying. Don’t leave any chance for having a thought-hiccup that I’m about to spend $250 on a graphics card. Think about the Apple Store: I rarely hesitate on a buying decision. They have a limited, high quality set of inventory that I trust. It’s also like the four star restaurant with five, seasonal items on the menu. The chef has carefully selected the ingredients and crafted a handful of dishes. How can your store become an expert in what it sells? Remember this expertise can pay for itself if it leads to immediate, gratification focussed sales.",
      "How does this relate to e-commerce search? User reviews on anything other than Amazon seem hard to trust. They range from 4 stars to 5 stars. In a search results listing, when comparing/contrasting a few items to purchase those stars seem like a big distraction. When a user then goes to do further research elsewhere, these stars don’t align well to what experts elsewhere say. This causes the shopper to discount the store’s authority on the products they sell. Does Best Buy really know computer graphics cards? Do they really understand computer monitors? Maybe I should go elsewhere…",
      "Another consideration is to define reliable metrics can be used to optimize for your ability to satisfy material lust? Search and recommendations is perhaps unhealthily focussed on optimizing clickthru. The more important metrics are closer to business-level. Optimize for how many \"buy it in store\" products. If you have a delivery service, optimize for this. You can then A/B test a variety of optimizations for these more business focussed metrics, not just clickthru. We focus on finding the right high-level metrics in our search relevance methodology. In our experience, this can be one of the most important and challenging aspects to any search or recommendations systems.",
      "Another consideration is trying to completely separate the Amazon-like \"ship it to me\" user experience from the retail-therapy \"buy it now\" user experience. Perhaps traditional retail should give up on the former, and focus on being a Web-first retail therapy delivery system. This is very important: it speaks to what’s being optimized for. If you’re optimizing in two directions at the same time, you’ll do a poor job at both. In particular the Amazon-like experience is a race to the bottom. It’s optimizing a network of delivery systems for cost. You need to optimize for something else: immediate gratification without hesitation.",
      "If you’d like to discuss how your site can be optimized for reverse showrooming, or any other use case, don’t hesitate to get in touch. We wrote the book on search relevance. We build search and recommendation systems for the best, let us help you!"
    ],
    "summary_t": "Today I want to talk about one strategy traditional retailers can use: reverse showrooming. What do I mean by reverse showrooming? Basically that retailers t..."
  },
  {
    "id": "7d1a9e9be9e64c8096bbb866868cd9dc",
    "url_s": "https://opensourceconnections.com/blog/2016/12/02/solr-elasticsearch-synonyms-better-patterns-keyphrases/",
    "title": "Patterns for Synonyms in Elasticsearch: Keyphrases",
    "content": [
      "As I almost exclusively help folks with Solr/Elasticsearch search relevance, I often encounter the \"giant list of synonyms\" that a client has. This list often creates odd side effects with matching. I want to begin to discuss patterns that I’ve found useful when managing Solr/Elasticsearch synonyms. Here I just use ES for examples, but quite a lot here also applies to Solr with the caveat of dreaded multiterm synonyms .",
      "In this article, I want to talk about thinking about synonyms in terms of keyphrases, not as the individual tokens they represent. Most users think of phrases like \"heart attack\" as a unit, not as the word \"heart\" before the word \"attack\".",
      "First let’s examine the problem more closely. Let’s take a health care example (all code in this gist). It’s not uncommon for folks to start with a synonym list like below. Here we equate all the different names for \"heart attack\" as equivalent:",
      "\"settings\": {\n        \"analysis\": {\n            \"analyzer\": {\n                \"syn_text\": {\n                    \"tokenizer\": \"standard\",\n                    \"filter\": [\"health_synonym\"]\n                }\n            },\n            \"filter\": {\n                \"health_synonym\": {\n                    \"type\": \"synonym\",\n                    \"synonyms\": [\"heart attack, myocardial infarction, mi, cardiac arrest, heartattack\"]\n                }\n            }\n        }        \n    }",
      "This creates a syn_text analyzer which tokenizes text on whitespace then applies the provided synonyms the text. This looks very much like where 90% of my clients start with manually maintained synonyms. The problem is this doesn’t work as most expect. Using elyzer you can see what happens to when this analyzer is applied to the text \"heart attack\"",
      "$ elyzer --es localhost:9200 --index syntest --analyzer syn_text --text \"heart attack\"\nTOKENIZER: standard\n{0:heart}\t{1:attack}\nTOKEN_FILTER: health_synonym\n{0:heart,myocardial,mi,cardiac,heartattack}\t{1:attack,infarction,arrest}",
      "If that’s not infinitely clear, it means that with your text, \"heart attack\" gets turned into a whole bunch of tokens (heart, myocardial, mi, etc) in the 0th position, and a whole bunch of tokens in the 1st position (attack, infarction, …). Which means the text turns into, as you’d expect, the two seperate tokens [myocardial] [infarction]. Perhaps unexpectedly, the phrases [cardiac] [attack], [myocardial] [arrest], and even [heartattack] [attack] now appear next to each other.",
      "These will match phrase queries such as \"heartattack attack\" and \"myocardial arrest.\" Or even single term queries for just \"heart\" or \"cardiac.\" Most developers, thinking of \"heart attacks\" as a unit unto itself that should only match when \"heart attack\" itself is searched for, find this behavior unexpected.",
      "Extracting keyphrases with autophrasing",
      "What you probably expect is that phrases like \"myocardial infarction\" will be treated as a single unit – it’s own keyphrase, not really broken down into constituent parts. Indeed, that’s the strategy behind Lucidwork’s auto phrase token filter for Solr. It catches phrases like \"myocardial infarction\" and ensures they’re treated as a single unit. Perhaps as the single token [myocardial_infarction]. This makes sure that before the synonym step, you don’t inject multiple tokens – only the full phrase as a single token. Thus avoiding weird spurious matches on half of the ‘key phrase’ (such as just attack).",
      "Entity Extraction with Shingle Autophrasing/Keepwords",
      "You can get Solr/Elasticsearch into viewing multi-word chunks as a single token with shingles. A shingle filter lets us group every possible 1 - N set of words as a single token in analysis. So the phrase \"myocardial infarction is no fun\" would get emitted as [myocardial], [myocardial_infarction], [myocardial_infarction_is].... Then following this step you can optionally use a keepwords filter to only keep the phrases you want, perhaps narrowing this text back down to the token just [myocardial_infaction]. As we write about in Relevant Search what we’re really doing is careful feature extraction. This can serve as a poor person’s entity extraction system – or perhaps use the output of an external machine learning system crafted to look for key medical phrases based on statistically interesting phrases.",
      "What does this look like? You’ll want to take your synonyms list and generate two filters, one for synonyms and the other with those synonyms listed as keepwords, as follows:",
      "\"filter\": {\n   \"health_synonym\": {\n      \"type\": \"synonym\",\n      \"tokenizer\": \"keyword\",\n      \"synonyms\": [\"heart attack, myocardial infarction, mi, cardiac arrest, heartattack, acute heart attack\"]\n   },\n   \"keep_health_entities\": {\n      \"type\": \"keep\",\n      \"keep_words\": [\"heart attack\", \"myocardial infarction\", \"mi\", \"cardiac arrest\", \"heartattack\", \"acute heart attack\"]\n   }",
      "Notice we’re setting tokenizer to \"keyword\" for the synonym filter. This will keep the phrases \"heart attack\" as a single token, and also only expand synonyms that it sees as a full token. In other words turning the full token [cardiac arrest] into [heart attack].",
      "You’ll also want to add the shingle filter to generate shingles ranging from length 1-4:",
      "\"4_shingle\": {\n        \"type\": \"shingle\",\n        \"max_shingle_size\": 4,\n        \"min_shingle_size\": 2,\n        \"output_unigrams\": true\n    },",
      "Now, we’re going to string these three together so that first we generate shingles, followed by health synonyms, followed by keepwords. In other words: generate candidate keyphrases by shingling, expand them with synonyms, then cull out any non-synonyms with keepwords. As in the following analyzer:",
      "\"analyzer\": {\n    \"syn_text\": {\n        \"tokenizer\": \"standard\",\n        \"filter\": [\"4_shingle\", \"health_synonym\", \"keep_health_entities\"]\n    }\n},",
      "The keepwords step, in addition to serving as entity extraction, cleans up lots of spurious tokens generated the initial shingles step. Because we’re removing most of the text, this approach is best used on a copy of the text (Solr copyField). In Elasticsearch, we can just use subfields to analyze the text differently:",
      "\"mappings\": {\n    \"article\": {\n        \"properties\": {\n            \"text\": {\n                \"type\": \"string\",\n                \"analyzer\": \"english\",\n                \"fields\": {\n                    \"entities\": {\n                        \"type\": \"string\",\n                        \"analyzer\": \"syn_text\"\n                    }}}}}},",
      "So we’ll have text (the full English, exact text) and text.entities which will contain our keyphrases expanded into synonyms.",
      "I’m stepping over a lot of artful thought you might need to consider for your case. First, I’m doing barebones analysis. I’m not stemming, for example, When you manage synonyms, you should consider whether you’d like to add at least a minimal amount of possessive/plural stemming, so you don’t have to consider many alternate forms in your synonyms. Keep in mind though that the Solr/Elasticsearch stemmers are heuristics. Odd english plural forms like \"shoes\" don’t always get caught, so you need to test and account for alternate forms to some extent.",
      "Anyway, now that there’s two fields, how would they be queried? Well the query strategy here is to query against the plain text and boost by medical entity matches. Something like the following query is a good starting point:",
      "POST syntest/_search\n{\n    \"query\": {\n        \"bool\": {\n            \"should\": [\n            {\"match\": {\n                \"text.entities\": \"heart attack\"\n            }},\n            {\"match\": {\n                \"text\": \"heart attack\"\n            }}]}}}",
      "So the nice thing we’ve done here is pulled out entities that likely matter to our users, and can use them explicitly in a query. This lends a signal to the final ranking function asking: are medical entities the user might talk about, or their synonymous phrases, mentioned in the text? If so – boost!",
      "We could expand this to get more specific about what kind of entities are being talked about, in case you wanted to modify how ranking worked based on those entities. Perhaps diseases ought to be separated from treatments, for example.",
      "Improving on entity extraction and search",
      "One problem with what I’ve presented is that the synonyms are dumped in a different field. So there’s one relevance score in the above query that matches synonyms, the other that doesn’t. The weights of these two queries is hard to balance, and comes with other problems. For example a user searching for the phrase \"bad cardiac arrest\" won’t match the text \"bad heart attack.\" The synonym isn’t expanded in main text, and the non-medical term \"bad\" is stripped out of the text.extracted field.",
      "I’ve found that a better pattern, that I write about in Relevant Search is to have one high-recall \"base\" query. Something that is indeed full of all the problematic noisy synonyms. Then within this, reshuffle the higher precision, secondary key phrase matches query to the top of the search results.",
      "I’m not going to extensively recreate this, but imagine you have a text field with naive synonym expansion like the first example in this article. In other words \"bad heart attack in Paris\" is analyzed as:",
      "$ elyzer --es localhost:9200 --index syntest --analyzer syn_text --text \"he had a blue heart attack\"\nTOKENIZER: standard\n{0:he}\t{1:had}\t{2:a}\t{3:blue}\t{4:heart}\t{5:attack}\nTOKEN_FILTER: synonym\n{0:he}\t{1:had}\t{2:a}\t{3:blue}\t{4:heart,myocardial,mi,cardiac,heartattack}\t{5:attack,infarction,arrest}\n[email protected]$~/ws $ elyzer --es localhost:9200 --index syntest --analyzer syn_text --text \"bad heart attack in paris\"\nTOKENIZER: standard\n{0:bad}\t{1:heart}\t{2:attack}\t{3:in}\t{4:paris}\nTOKEN_FILTER: synonym\n{0:bad}\t{1:heart,myocardial,mi,cardiac,heartattack}\t{2:attack,infarction,arrest} {3:in}\t{4:paris}",
      "There’s our noisy, weird synonyms. Yes this will match many a spurious search term, including just \"heart\" by itself, confusing those heart burn sufferers. But that’s ok, because this is just our starting point for a general working set of reasonably relevant documents.",
      "We use our entities field to bring the good stuff to the top. You then have text.entities as we’ve just introduced. Then you repeat the query, perhaps with a significant boost on text.entities to pull up key medical phrases and their exact synonyms:",
      "POST syntest/_search\n{\n    \"query\": {\n        \"bool\": {\n            \"should\": [\n            {\"match\": {\n                \"text.extracted\": \"bad heart attack\"\n                \"boost\": 10\n            }},\n            {\"match_phrase\": {\n                \"text\": \"bad heart attack\",\n                \"boost\": 10\n            }}\n            {\"match\": {\n                \"text\": \"bad heart attack\"\n            }}]}}}",
      "Here there’s three signals to balance at ranking. First is the base, high recall match. Again, this is low value and shouldn’t be boosted much. Second there’s the two queries pointing at higher precision matches: full phrase match against text and entity match. We boost those higher (the exact boost is a matter of trial and error with something like Splainer or Quepid).",
      "Autophrase with synonym step",
      "Perhaps even better is to just autophrase the key phrases with an additional synonym step. For example turn heart attack into heart_attack and then expand heart_attack into various synonym forms. This simplifies the shingling, but comes with an extra step where you need to list your synonyms. You’ll probably want to generate these filters programmatically from your synonyms list to avoid errors.",
      "Let’s start with the initial synonym step for autophrasing:",
      "\"autophrase_syn\": {\n    \"type\": \"synonym\",\n    \"synonyms\": [\"heart attack => heart_attack\",\n                 \"myocardial infarction => myocardial_infarction\",\n                 \"cardiac arrest => cardiac_arrest\",\n                 \"acute heart attack => acute_heart_attack\"]\n},",
      "Then we follow with appropriate synonyms :",
      "\"health_synonym\": {\n    \"type\": \"synonym\",\n    \"tokenizer\": \"keyword\",\n    \"synonyms\": [\"heart_attack, myocardial_infarction, mi, cardiac_arrest, heartattack, acute_heart_attack\"]\n}",
      "And our analyzer becomes:",
      "\"analyzer\": {\n    \"syn_text\": {\n        \"tokenizer\": \"standard\",\n        \"filter\": [\"autophrase_syn\", \"health_synonym\"]\n    }\n},",
      "This nicely tokenizes \"bad heart attack in paris\" accordingly:",
      "$ elyzer --es localhost:9200 --index syntest --analyzer syn_text --text \"bad heart attack in paris\"\nTOKENIZER: standard\n{0:bad}\t{1:heart}\t{2:attack}\t{3:in}\t{4:paris}\nTOKEN_FILTER: synonym\n{0:bad}\t{1:heart,myocardial,mi,cardiac,heartattack}\t{2:attack,infarction,arrest}\t{3:in}\t{4:paris}\n[email protected]$~/ws $ elyzer --es localhost:9200 --index syntest --analyzer syn_text --text \"bad heart attack in paris\"\nTOKENIZER: standard\n{0:bad}\t{1:heart}\t{2:attack}\t{3:in}\t{4:paris}\nTOKEN_FILTER: autophrase_syn\n{0:bad}\t{1:heart_attack}\t{2:in}\t{3:paris}\nTOKEN_FILTER: health_synonym\n{0:bad}\t{1:heart_attack,myocardial_infarction,mi,cardiac_arrest,heartattack,acute_heart_attack}\t{2:in}\t{3:paris}",
      "When we used shingles, we pulled entities in a different field. There’s no reason you couldn’t use that approach here as well, but here I’m just going to apply this analyzer directly to the text. This demonstrates that we can search for the phrase \"bad cardiac arrest\" and we’ll still match this text:",
      "POST syntest/_search\n{\n    \"query\": {\n        \"match_phrase\": {\n            \"text\": \"bad cardiac arrest\"\n        }}}",
      "As users expect this matches \"bad heart attack in Paris.\" Optionally, this analyzer could be applied to a subfield/copyField to differentiate from exact text and text with synonym keyphrases expanded.",
      "Intent & Entity Focussed – What kinds of things do users search for",
      "One of the biggest questions when dealing with synonyms is not to refer to an industry or language standard for synonyms. Instead you need to map between how users talk about content vs how content creators have written content. This varies tremendously between domains, for example I’m inclined to search a set of laws for laws about a \"dog catcher.\" Yet legislators likely write about \"animal control officers.\"",
      "So when you work on synonyms, don’t worry about a globally pure expression of synonymy. Focus on how your users search, how your content is organized/written, figure out how to map between the two. This isn’t easy, and is often the hard app/domain-specific work you need to do. And as an aside – there’s not usually a generic, machine learning silver bullet to this that doesn’t also require work, domain expertise, monitoring & curation.",
      "In a future article, I’ll expand on these ideas. You’ll see that taxonomies and controlled vocabularies can be incorporated to create semantic search that can order by concept specificity. For example, a search for \"heart attack\" that can first show you heart attack results. If none of those occur, then conditions having to do with the heart, followed by general circulatory diseases, followed by all diseases, and so on.",
      "I’ll also discuss in a future article one concern with these approaches: choosing between query and index synonym expansion. In the above examples, I’m doing both. The strategies here look quite differently at high scale. In those cases, leaving the indexed text largely alone and expanding queries to synonyms at search time is much better – though with other consequences.",
      "That’s not all",
      "These patterns are just the tip of the iceberg on building a good search application. Regardless of your search technology, you need to understand how synonymy works. You need to map between how users seek content and how content describes important entities. And there’s much, much, room to extend what’s been described above that I’ll be writing about in the future!",
      "Anyway, please do get in touch if you need help with Solr and Elasticsearch relevance. Our services focus on improving technical and business outcomes through better search and recommendations."
    ],
    "summary_t": "In this article I want to begin to discuss patterns that I’ve found useful when managing Solr/Elasticsearch synonyms."
  },
  {
    "id": "0475844402fd08a2ee2b68a0058b2762",
    "url_s": "https://opensourceconnections.com/blog/2016/12/23/elasticsearch-synonyms-patterns-taxonomies/",
    "title": "Patterns for Elasticsearch Synonyms: Taxonomies and Managed Vocabularies",
    "content": [
      "Last time on the Young and the Synonomous we discussed how users often think of key phrases like \"heart attack\" or \"cardiac arrest\" as single terms. We demonstrated how to implement keyphrases using Elasticsearch synonyms. In this article, I want to extend this discussion to show you how to build semantic search using curated taxonomies and managed vocabularies.",
      "I want your takeaway to be to stop managing synonyms, and start curating taxonomies and managed vocabularies! You’ll see that for narrow, relatively static domains categorization of user queries and key phrases into a taxonomy can lead to search that anticipate user’s intent in ways most assume can only be done with expensive machine learning techniques.",
      "First let’s define what we mean by these terms:",
      "What’s a Taxonomy?",
      "Taxonomies are hierarchical classifications of things. A well known taxonomy would be the Kingdom, Phylum, Class, Order, … taxonomic rank that we use to organize living things. So an Elephant is in Kingdom Animalia, Phylum Chordata, Class Mammalia and so on. Or as I like to think of taxonomies like a bit more informally as like a directory structure. File Elephant under:",
      "\\Animalia\\Chordata\\Mammalia\\Afrotheria\\Proboscidea\\Elephantidae",
      "Then when you get to the \"Elephant\" level you suddenly become conscious of the fact that there’s many kinds of elephants. There’s actually African Elephants that are different than Asian Elephants. So if we list the taxonomy for an African Bush Elephant, we get:",
      "\\Animalia\\Chordata\\Mammalia\\Afrotheria\\Proboscidea\\Elephantidae\\Loxodonta\\Loxodonta africana",
      "You’ll notice that depending on who you are, you come at the idea of \"elephant\" with your own level of specificity – and that level is mapped directly onto an entity in the taxonomy.",
      "What’s a managed vocabulary?",
      "I see a managed vocabulary (sometimes people say controlled vocabulary) as an extension of a taxonomy that also accounts for synonyms. One prominent example of a managed vocabulary is MeSH. MeSH categorizes diseases, drugs, and other entities in medicine. The entry for headache, gives you both taxonomic classification(s) and alternative names, in this case phrases like \"cranial pain\", \"head pain\", and \"cephalalgia.\"",
      "How do these synonyms fit into to taxonomies? To revisit our elephant example, we might note alternate names for African bush elephant alongside or under the scientific name. Perhaps incorporating them as their own taxonomical entries:",
      "\\Animalia\\Chordata\\Mammalia\\Afrotheria\\Proboscidea\\Elephantidae\\Loxodonta\\Loxodonta africana\\African Bush Elephant\n\\Animalia\\Chordata\\Mammalia\\Afrotheria\\Proboscidea\\Elephantidae\\Loxodonta\\Loxodonta africana\\Elephant, Bush",
      "What do taxonomies and managed taxonomies have to do with search?",
      "A common refrain from my search relevance clients is \"I want to get away from ‘strict matching’ to build semantic search.\" If a user types in \"birkenstocks\" I want to make sure that search can understand they mean \"sandals\" – and by \"sandals\" I of course mean \"shoe\". Conversely, if the user types in \"sandals\" we want to show them \"birkenstocks\" even if sandal is never mentioned.",
      "On the surface of it, this sounds like a problem of direct synonymy. This exact thing equates to this other exact thing. Pure synonyms, however, only increase recall. They make \"sandals\" and \"birkenstocks\" equivalent or \"shoes\" the same as \"sandals.\" This leads to a problematic situation where a search for \"birkenstocks\" brings back \"tennis shoes\" because well \"shoes\" are the exact same as \"sandals\" are the exact same as \"birkenstocks.\"",
      "In other words, direct synonyms often increase recall but decrease precision. Users expect results that match their level of specificity (birkenstocks when searching for birkenstocks; sandals when searching for sandals).  Managed vocabularies help get the value of synonyms without sacrificing precision. This is because taxonomies help by modeling hypernym/hyponym relationships. Sandal is a hypernym (a word with broader meaning) than birkenstock. Birkenstocks are hyponyms (more specific) of sandals. The search engine should get this and rank results accordingly, giving exact matches before hypernyms, and hypernyms before hyponyms.",
      "Perhaps broader than the precision/recall tradeoff is a general pattern in how users search. In my experience, users often start broad (\"laptop bag\") to explore a data set. Most likely, nothing matches at such a broad level of specificity, but there ARE many items that are hyponyms of \"laptop bag,\" including laptop backpacks, messenger bags, kid laptop bags, and so forth. Thus this gives users an option to get the \"30,000\" ft view of the data, or refine with broader/narrower terms to get more or less specific and precise with their terminology. It’s often said that a good search user experience puts users on the \"scent of information\" and using managed vocabularies is a great way to do that.",
      "Building a managed vocabulary from search behavior",
      "Ok, so how do you implement this magic for your search?",
      "Well step one, it’s not magic–there’s skill involved (\"taxonomist\" is a job title after all!). You need to generate and/or recycle a managed vocabulary. Something that captures descriptive terms that map between how content creators describe items and how your searchers describe items. For example, a common problem in legal search is mapping between lay-speak and legalese. My Dad is likely to use the phrase \"dog catcher\" while the law is likely to write about \"animal control officers.\"",
      "In general, surveying dads is not a scalable way to create a taxonomy. Instead, an alternative is to analyze your common & problematic search terms and hunt for key phrases, as discussed in the prior article. If we notice with legal search that users search for \"dog catcher,\" \"police officer,\" \"DA,\" \"real estate taxes,\" \"property taxes,\" \"income taxes,\" we can begin to see how our users structure laws in their own words. We might decide that our users fall into two general categories (1) those that search for topics in criminal law (2) those that search with taxation questions. Within those, there appear to be finer breakdowns – all the way down to individual search terms, such that we end up with an initial managed vocabulary as follows (I’ll start using underscores like I did in the key phrases article):",
      "criminal_law\\legal\\district_attorney\ncriminal_law\\animal_enforcement\\animal_police\\dog_catcher\ncriminal_law\\human_enforcement\\police_office\ntaxation\\real_estate\\property_tax\ntaxation\\real_estate\\property_tax\\real_estate_tax\ntaxation\\income\\income_tax",
      "There’s an art here. We’re deciding ourselves what the right hypernym/hyponym relationships are. Sometimes we create our own artificial categories of items that seem to go together for our users. As we’ll see in the next section, the ranking function we’re going to engineer into Elasticsearch will rank based on how close an item is in a taxonomical hierarchy, preferring hyponyms over hypernyms/siblings when there’s no exact match. So getting this right means also fine tuning precision and ranking: if a user searches for \"property tax\" – should the other real estate taxes come before the income tax?",
      "We’re not quite done yet, next, we reflect on our content to map the phrases used in the content for these entities. For example, we notice that the law talks about \"district attorney\" not \"DA\" and \"animal control officer\" not \"dog catcher.\"",
      "criminal_law\\legal\\district_attorney\ncriminal_law\\legal\\district_attorney\\da\ncriminal_law\\animal_enforcement\\animal_control_officer\\dog_catcher\ncriminal_law\\human_enforcement\\police_office\ntaxation\\real_estate\\property_tax\ntaxation\\real_estate\\property_tax\\real_estate_tax\ntaxation\\income\\income_tax",
      "This is very much an iterative process: get basic keyword search operational. Watch how people search. See if you can structure the key phrases in their search into taxonomies. Rinse repeat. Next up, we’ll see how to transform this into an asset that can be used to improve search relevance.",
      "Using Managed Vocabularies in Elasticsearch",
      "So ok, nothing you’ve seen thus far looks like anything that has anything to do with Elasticsearch. I know, sorry for all the setup. But long story short, we can take a line from our managed vocabulary:",
      "criminal_law\\animal_enforcement\\animal_control_officer\\dog_catcher",
      "and turn it into a this line in an Elasticsearch synonym filter",
      "dog_catcher => dog_catcher, animal_control_officer, animal_enforcement, criminal_law",
      "And done….",
      "Who the what?  Ok let’s break this down. Now from the previous article on keyphrases you’ll recall we talk about how to turn noun phrases into their own tokens. These key phrases represented ideas that really best thought of as tokens in their own right – words that go together naturally. We discussed autophrasing with a synonym filter like so:",
      "\"autophrase_syn\": {\n    \"type\": \"synonym\",\n    \"synonyms\": [\"heart attack => heart_attack\",\n                 \"myocardial infarction => myocardial_infarction\",\n                 \"cardiac arrest => cardiac_arrest\",\n                 \"acute heart attack => acute_heart_attack\"]\n},",
      "The autophrasing here, also applies to all the little phrases in our managed vocabulary, to generate (programatically):",
      "\"autophrase_syn\": {\n    \"type\": \"synonym\",\n    \"synonyms\": [\"dog catcher => dog_catcher\",\n                 \"animal control officer\" => \"animal_control_officer\",\n                   ...\n                 \"income tax\" => \"income_tax\"\n]},",
      "Now here comes the magic. After autophrasing, the next step is to expand leaf managed vocabulary into their hypernyms, in a synonym token filter such as:",
      "\"vocab_syn\": {\n    \"type\": \"synonym\",\n    \"synonyms\": [\"dog_catcher => dog_catcher, animal_control_officer, animal_enforcement, criminal_law \",\n                   ...\n                 \"da => da, district_attorney, legal, criminal_law\"\n]},",
      "For taxonomies, as we’ll see, we also want to only keep those key phrases that we care about, giving us this analyzer that combines the two synonym filters above with a keepwords filter.",
      "\"settings\": {\n        \"analysis\": {\n            \"analyzer\": {\n                \"taxonomy_text\": {\n                    \"tokenizer\": \"standard\",\n                    \"filter\": [\"autophrase_syn\", \"vocab_syn\"]\n                }\n            },\n}",
      "Really these two steps are all you need to get a taxonomy effect. It also so happens that this structures indexes/queries so that vanilla Elasticsearch ranking can be made rank based on taxonomic similarity. How? Simply put, what this synonym mapping does is make the top level terms in the taxonomy very common (criminal_law, taxation). In other words, their document frequency increases tremendously. This makes them lower value terms when scoring. The more specific terms, like \"dog_catcher\" will occur rarely, and matches with them will have a higher score.",
      "Moreover, if you search for \"dog catcher\", hypernyms of dog catcher get second billing as users would expect. This query is expanded into a search for all four search terms in the synonym filter above dog_catcher OR animal_control_officer OR …. If no exact matches for \"dog catcher\" occur (which as we discussed is likely with legal text) search falls back to matches of 3 out of 4 terms or animal control officers. If there’s no animal control officers, then search falls back to just animal enforcement, and so on.",
      "Viola, you’ve built a semantic search system that can seemingly do smart things like fallback to broader categories.",
      "Other techie details",
      "This isn’t open and shut, and there’s still plenty of room for more decisions. You might want to also rank more specific terms (hyponyms) so that items are ranked so that closer hyponyms come above deeper hyponyms. For example, a search for \"shoe\" ought to yield generic \"sandals\" before \"birkenstocks.\" This doesn’t happen as often, but if you did care you could try to use the field length normalization in TF*IDF to get this effect. Field normalization tends to bias results towards shorter fields. \"Shorter\" could mean that \"Shoe\" expanded to 4 taxonomic terms is shorter than \"birkenstock\" with its 6 terms. There’s work here that involves, very roughly",
      "Using keepwords to only keep keyphrases into a separate taxonomical field, thus only considering taxomical terms for length\n  Customize the underlying similarity to count the number of terms – not the number of unique positions – when doing field norms",
      "That’s a google search term salad that should get you started if you’re interested.",
      "I often also create multiple taxonomies because there’s more than one kind of classification that’s going on. A taxonomy for types of people/roles in the law. Another one perhaps oriented at legal problems people might describe. This lets me call these out at query time as different factors to boost. If the user mentioned a specific role, like district attorney, I may want to assert that if this sort of entity occurs in the query, boost it more than other factors. Using keepwords with a copy field or subfield (instead of text you could have text.extracted_roles) lets you query text.extracted_roles with a high boost. The boost will only matter in the off chance that a user actually mentions this type of entity in their search string.",
      "Another topic I noodle with a lot is using a hierarchical classifier keyed into the clickstream to automate this process with machine learning. It’s not as \"silver bullet\" as you think. You’d perhaps never see any of these associations that share no terms (such as dog catcher / animal control officer). Trey Granger and others have done work to use user behavior to augment synonyms. I wonder if it can be used to augment managed vocabularies. I’m sure someone has worked on it.",
      "This seems like a lot of work…",
      "If you’re thinking, this approach seems like a lot of work, you’re right. This is an approach I wouldn’t use for everything. But it’s not as much work as you might think. Consider",
      "There might already be publicly available taxonomies for your domain that can be a starting point (MeSH, etc)\n  You might not need a complex taxonomy if you focus more on the most common or problematic searches\n  You can fall back to general search, which means you’re using this to fine tune specific use cases of search\n  You might have already been trying to patch over search with synonyms, this isn’t that much more work\n  This gives non-techie search managers a great way to manage relevance: a way to control recall (synonyms) AND precision (hypernym/hyponyms)",
      "Finally, if you’re in a field that relies on careful search management via a taxonomy, seriously consider hiring a professional taxonomist. It’s a field unto itself. I’m not even touching on the many many technologies and approaches to managing taxonomies. You should go check them all out!",
      "In a future article, I may talk about the pitfalls and opportunities of tagging items using taxonomies or keyphrases. This is a topic that requires skills beyond a taxonomists. In fact, you may need to hire a librarian! After all, who else understands systematically organizing information by topic?",
      "Would you like to know more?",
      "Get in touch! If this all interests you, and you would like to share your own ideas – don’t hesitate to reach out directly. And as always, let us know if we can help make a smarter search or recommendation system for you – Get in touch it’s what I do with my crack team–day in day out!"
    ],
    "summary_t": "How to build semantic search using curated taxonomies and managed vocabularies"
  },
  {
    "id": "41703c7ad3f0fc24ff71ed34b80c5665",
    "url_s": "https://opensourceconnections.com/blog/2006/07/29/decompressing-from-oscon/",
    "title": "Decompressing from OSCON",
    "content": [
      "Im sitting at TopPot in Seattle slowly digesting all the things that we did and learned at OSCON! Ive got a stack of contacts to follow up with, and feel very invigorated about the state of the open source world.",
      "Our BoF: Scrum War Stories was very well recieved, with 14 particpants and some at times heated discussion! Exactly what you want at a BoF! We helped solve some job difficulties, learned about some new books, and talked about Scrum in the real world. As I promised, Ill have notes posted later this week. I also mentioned our Rugby addon for Scrum project tracking with Trac. I took advantage of Googles Project Host being unveiled at OSCON and did a quick landgrab for \"rugby\". Ill have SVN updated over the next couple of days on our new site at http://code.google.com/p/rugby/.",
      "Ill also post the after action review of OSCON this week."
    ],
    "summary_t": ""
  },
  {
    "id": "200e56a167e4fcc90eb0830c913a4ba7",
    "url_s": "https://opensourceconnections.com/blog/2007/05/24/how-to-ignore-files-in-subversion/",
    "title": "How to ignore files in Subversion",
    "content": [
      "For some reason, adding files and directories to ignore in Subversion is much harder then in CVS.",
      "The syntax, as it you are editing a .cvsignore file is:",
      "svn propedit svn:ignore . –editor-cmd vi",
      "Just add your patterns and then do",
      "svn ci -m `adding new svn:ignore patterns ."
    ],
    "summary_t": ""
  },
  {
    "id": "1ced1e0c01f583c225f76bf94ed0f026",
    "url_s": "https://opensourceconnections.com/blog/2017/01/16/openbadges-dragged-me-through-trough-of-disillusionment/",
    "title": "OpenBadges: Currently dragging me through the trough of disillusionment",
    "content": [
      "For years I’ve been interested in describing what makes someone special by looking at the ambient data that surrounds them.   My first big effort was way back in 2008 and was a simple web app and some screen scraping scripts that collected public data about people in Charlottesville, Virginia, and identified folks who are doing \"high tech\" stuff called HighTechCville.   And yes, \"high tech\" is a rather subjective idea!",
      "Today, as part of making sure that we have the \"right people in the right seats\" for each of our consulting engagements we have explicit criteria for specific technical fields.  These criteria we call Merit Badges, and we have them at various levels for our specific concentrations in Solr, Elasticsearch, Cassandra, and a specialization for Search Relevancy.",
      "I was chatting about our Merit Badge concept with a firm that develops learning paths for non-traditional students and they turned me on to Open Badges:",
      "Get recognition for learning that happens anywhere. Then share it on the places that matter. A digital badge is an online representation of a skill you’ve earned.",
      "I looked at the website and got really excited!   This is exactly what I needed was my first thought.   We currently have on our About Us page some tags that describe what each member of OSC is an expert in:",
      "",
      "How cool would it be if that wasn’t just some static HTML, but actually driven by Open Badges?   We could use Open Badges to issue our internally defined Merit Badges, and maybe even offer them out to the world?   For example, we have a very specific opinion of what makes a Relevance Engineer, so maybe we make that a public badge that we issue via Open Badges?",
      "And I entered the trough of disillusionment.  Open Badges was a brain child of Mozilla Foundation and many others, and there was a lot of initial enthusiasm (dare I say hype even?) in the 2012 to 2015 time frame.   However, now it seems like this wonderfully simple of idea of having Badges that are just images that carry some metadata has bogged down in hard things like how do I store Badges from many providers?  How do I confirm the Badge means what it says?   And the fact that while certain very specific platforms, primarily education related, support it, there isn’t a real simple way of displaying or querying what someone has programmatically.  There is a lot of talk about a backpack which lets you store your badges that you collect from many providers, but the initial one by Mozilla is no longer supported!",
      "I played around with Badgr, which is a reference implementation for issueing and tracking badges.   However, at the end of the day, what I wanted was a very easy way to display my badges that I had earned.  And that apparently is very very difficult!   So, as much as it pained me, I ended up just quickly rolling my own simple tracking system for who has what badges.",
      "It seems that Open Badges, like FOAF, and the various Microformats are all struggling because it’s very hard to have distributed information systems where you have multiple contributors and need to trust the data.   If only Open Badges, FOAF files, and other distributed informations worked the way DNS does…",
      "However, there is an interesting ray of light.   In digging around with Open Badges, I finally wondered if that was how when some passes a Coursera class, it ends up being listed on LinkedIn…   And yep, sure enough, I found the right link in Badgr to pop open a screen in LinkedIn to add a certification.   You can also do it directly.   That is kinda cool!   And then I realized that LinkedIn functions as the aforementioned backpack.    LinkedIn is the aggregator of all your badges, regardless of who issued them!",
      "So to start putting Open Badges and LinkedIn to the test, I created a Solr Merit Badge badge that is issued by OpenSource Connections in Badgr.   I then awarded myself the merit badge ;-).   You can now see it on my LinkedIn profile:",
      "",
      "Eureka!",
      "I’d love feedback or thoughts from others using Open Badges…   Is it worth the integration costs…  How will help me make sure the right consultants are on each project?   Versus just maintaining an internal spreadsheets…   And, is any one interested in a public Search Relevancy certification, powered via Open Badges?"
    ],
    "summary_t": "I try not to get sucked in by hype, but OpenBadges managed to do just that, and has left me in the trough of disillusionment.   However, maybe a light at the..."
  },
  {
    "id": "be93c68d0919ecd751eeb35a8445cc9e",
    "url_s": "https://opensourceconnections.com/blog/2017/01/23/our-solution-to-solr-multiterm-synonyms/",
    "title": "Our Solution to Solr Multiterm Synonyms: The Match Query Parser",
    "content": [
      "You have probably heard us talk about Solr multiterm synonyms a lot! It’s a big problem that prevents a lot of organizations from getting reasonable search relevance out of Solr. The problem has been described as the \"sea biscuit\" problem. Because, if you have a synonyms.txt file like:",
      "sea biscuit => seabiscuit",
      "… you unfortunately won’t get what you expect at query time. This is because most Solr query parsers break up query strings on spaces before running query-time analysis. If you search for \"sea biscuit\" Solr sees this first as [sea] OR [biscuit]. The required analysis step then happens on each individual clause – first on just \"sea\" then on just \"biscuit.\" Without analysis seeing a \"sea\" right before a \"biscuit\", query time analysis doesn’t recognize the synonym listed above. Bummer.",
      "As a team that focuses on Solr relevancy, we often have to solve this problem. One solution we’ve used has been to inject the analysis into the query parsing stage. Indeed that’s what the hon-lucene-synonyms plugin does. A tremendous contribution from Nolan Lawson to the Solr community that tries to \"do the right thing\" for you.",
      "It creates a query parser that",
      "Runs an analyzer specified in your solrconfig.xml (Nolan gives a recommended analyzer)\n  Turns the result into a Solr edismax query",
      "This works for many people. It’s sort of the \"don’t make me think\" option. However, in practice we’ve found it tends to generate rather complex queries. For example, in the README.md, a simple query for \"dog\" with synonyms \"canis familiaris\" and \"hound\" is expanded to this complex beast:",
      "(DisjunctionMaxQuery((text:dog))^1.0 ((+(DisjunctionMaxQuery((text:canis)) DisjunctionMaxQuery((text:familiaris))))/no_coord^1.0) ((+DisjunctionMaxQuery((text:hound)))/no_coord^1.0) ((+(DisjunctionMaxQuery((text:man's)) DisjunctionMaxQuery((text:best)) DisjunctionMaxQuery((text:friend))))/no_coord^1.0) ((+DisjunctionMaxQuery((text:pooch)))/no_coord^1.0))",
      "It gets nastier with the many options that the query parser offers. Such as phrase queries, other edismax features, controlling how synonyms are grouped, etc. It gets particularly complex when you search over multiple fields and want per-field boosts. As a team, we’ve spent an inordinate amount of time debugging weird relevance and performance problems related to these complex queries. Indeed as a result, OSC relevance engineers have done quite a bit of the maintenance on this plugin, adding features, documenting, and fixing bugs.",
      "Hunting for a better ‘match’",
      "Although we value the hon-lucene-synonyms work, we’ve often found a more surgical approach to query construction more useful. Something where we can build a query and understand how it’s going to map to underlying Lucene primitives. This is why the OSC relevance team is excited to announce the match query parser. Increasingly, this query parser is how we solve multiterm synonym problems instead of hon-lucene-synonyms. By itself its not the solution, it’s the lego block to build your own solution.",
      "The match query parser gives the following options",
      "A fieldType you’ve created in your schema to use for query-time analysis\n  A single field to search\n  How to search with that field with the tokens resulting from the analysis",
      "It’s inspired by Elasticsearch’s match query. Instead of a single query parser that attempts to solve every use case, you’re expected to compose multiple configurable match queries together to solve your particular multi term synonym problem. Match query parser also allows you to create query-time only analyzers (via fieldTypes). It removes the limitation of only having one query-time method for analyzing your users queries. The consequence is, you can index text with a standard analyzer, then tell Solr at query time which analyzer to apply to the query string. This has a lot of benefits for indices where you don’t want to, for example, create multiple versions of your text just ofor different query-time synonym/analysis configurations.",
      "Removing this limitation gives you a TON of power. It’s our new favorite way of solving multi term synonyms in a way that lets you manage complexity, relevance, and performance. The downside is, you are responsible for making more of your own decisions. Let’s see it in action:",
      "Match Query Parser in Action",
      "Let’s take the match query parser out for a spin. First let’s tackle the sea biscuit problem head on to see what kind of query it generates.",
      "We’ve created a repository for storing TMDB (The Movie Database) movies to play with the Match Query Parser. In the schema, you’ll note the fields I’ve created using the stock, index-time \"text_general\" analyzer that Solr ships with:",
      "<field name=\"id\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" />\n<field name=\"overview\" type=\"text_general\" indexed=\"true\" stored=\"true\" />\n<field name=\"title\" type=\"text_general\" indexed=\"true\" stored=\"true\" />\n<field name=\"tagline\" type=\"text_general\" indexed=\"true\" stored=\"true\" />\n<field name=\"cast\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n<field name=\"directors\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n<field name=\"genres\" type=\"text_general\" indexed=\"true\" stored=\"true\" multiValued=\"true\" />",
      "However, I’ve also defined a few field types that only have \"query\" analyzers defined. This includes for example:",
      "<fieldType name=\"text_title_phrases\" class=\"solr.TextField\" positionIncrementGap=\"100\" multiValued=\"true\">\n   <analyzer type=\"query\">\n     <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n     <filter class=\"solr.LowerCaseFilterFactory\"/>\n     <filter class=\"solr.SynonymFilterFactory\" synonyms=\"title_phrases.txt\" ignoreCase=\"true\" expand=\"true\"/>\n     <filter class=\"solr.PatternReplaceFilterFactory\" pattern=\"(_)\" replacement=\" \" replace=\"all\"/>\n   </analyzer>\n</fieldType>",
      "First, you’ll notice that this fieldType exists solely to be used at query time. Much like Elasticsearch where analyzers are often created just to be bound to some query.",
      "There’s nothing that surprising in the analyzer abore. The first two steps do a simple tokenization, followed by lowercasing. The synonym step is what’s interesting. Here we create a synonym file intended to transform common key phrases from user searches into a synonomous form. For example, sea biscuit or perhaps huckleberry finn:",
      "sea biscuit => seabiscuit,sea_biscuit\nseabiscuit => seabiscuit,sea_biscuit\nhuckle berry => huckleberry,huckle_berry\nhuckleberry => huckleberry,huckle_berry",
      "This particular synonyms file is targeted at the key phrases you would expect to find in Movie Titles. It’s a synonym file only intended to be applied to the title field. If you recall in our earlier articles about semantic search with keyphrases and taxonomies, actual query logs are a better source for synonyms than a generic thesaurus. Here perhaps we’ve seen (or our code has seen) how users tend to search for seabiscuit and we’ve generate the synonyms file above.",
      "The final step in the analyzer converts underscores from the generated token into spaces. This is just a bit of tidying from the synonym step to generate individual tokens with multiple words. You’ll see how a configuration option in the match query parser lets you treat these as phrases farther down.",
      "So the ingredients are in place. We’ve got fields to search. We’ve got an analyzer to use on the user’s query. Let’s get to work.",
      "Let’s say our user searches for \"party with sea biscuit\" What does it look like to search the title field, after applying the analyzer above? Well using the match query parser means simply specifying it in localparams, like so:",
      "q={!match qf=title analyze_as=text_title_phrases search_with=phrase mm=1}party with sea biscuit",
      "Pretty straightforward (if you know Solr localparams), this means",
      "Use the match query parser\n  Search the title field (qf)\n  Analyze the user’s query as fieldType text_title_phrases (analyze_as)\n  Treat resulting, multiword tokens as phrase searches sea biscuit => \"sea biscuit\" (search_with)\n  Expect at least one of the user’s search terms to match (mm=1)",
      "And what does this generate? A fairly sane query:",
      "(DisjunctionMaxQuery((title:\"party\")) DisjunctionMaxQuery((title:\"with\")) DisjunctionMaxQuery((title:\"seabiscuit\" | title:\"sea biscuit\")))~1",
      "The query is analyzed. The token in the first position, all by its lonesome is just \"party,\" next is \"with,\" finally we have a dismax, pick the best match, for either (seabiscuit | \"sea biscuit\"). Intuitively these two go together in the spot that the user typed in \"sea biscuit\" so they’re treated as a single unit with possible alternatives.",
      "In other words, for each position in the resulting token stream, score using the highest matching token (DisjunctionMaxQuery). Expect at least one \"position\" in the original token stream to match (mm).",
      "Ok, so this seems fairly straightforward. But what about searching another field? Seabiscuit is also a cast member, right? Well that’s easy peasy. We just need to use multiple queries. This can be accomplished several ways (perhaps as a bq on a base edismax query). However here, we’ll use the magic of nested queries to just issue two local params queries:",
      "q=_query_:\"{!match qf=title analyze_as=text_title_phrases search_with=phrase mm=1 v=$userQuery}\" \n   OR _query_:\"{!match qf=cast analyze_as=text_cast_phrases search_with=phrase mm=1 v=$userQuery}\"\n&userQuery=party with sea biscuit",
      "Notice above how two different analyzers are used at search time for two different fields. With this query parser, we could search a single field with as many different analyzers as we wanted. Or we could search multiple fields with the same analyzer if we liked.",
      "And how does the query above look? Well the query above starts to look a bit fugly. But we prefer it fugly, as we want careful, low-level control over how we’re searching. Here’s what the debug query looks:",
      "(DisjunctionMaxQuery((title:\"party\")) DisjunctionMaxQuery((title:\"with\")) DisjunctionMaxQuery((title:\"seabiscuit\" | title:\"sea biscuit\")))~1 (DisjunctionMaxQuery((cast:\"party\")) DisjunctionMaxQuery((cast:\"with\")) DisjunctionMaxQuery((cast:\"seabiscuit\" | cast:\"sea biscuit\")))~1",
      "What’s cool is the extremely pluggable nature of how the query can be sliced and diced using custom analyzers. Want to only search the cast field with actual actors names? Add a keep words list to the end of the analysis chain to filter out anything not in your list of names. Want to expand to broader concepts to increase recall, expanding \"sea biscuit\" to \"horse\" to show users horses as well? Go for it – the world is your oyster with a bit more synonym expansion.",
      "Indeed, you should be able to apply the lessons from our series of articles on building semantic search using keyphrases and taxonomies using Solr, and not that other search engine, to great effect!",
      "Great Power comes Great Responsibility",
      "One of the underlying reasons multiterm synonyms isn’t just \"solved\" is because everyone’s search problem is actually pretty different. In Relevant Search there’s a specific reason why this can be difficult and problem specific (check out the section on field centric vs term-centric search).",
      "What’s more important is to give you the tools to solve your specific problem. That’s what we’ve tried to do with the Match Query Parser. It’s the swiss-army knife, composable approach we tend to find more practical than the \"black box\" approach that \"hon-lucene-synonyms\" gives you.",
      "I hope you find it useful. If you would like to talk about our team, which builds exciting things like the Match Query Parser, can help you solve your specific, gnarly search relevance, recommendations, or other matching problem please do get in touch either through email or through the form below."
    ],
    "summary_t": "You have probably heard us talk about Solr Multiterm Synonyms a lot. This article demonstrates a technique that solves the problem, with fewer performance an..."
  },
  {
    "id": "239c0a7ded8bbfdcf82012b370b32af9",
    "url_s": "https://opensourceconnections.com/blog/2017/02/10/the_conferences_we_went_to_in_2016/",
    "title": "The Conferences We Went to in 2016",
    "content": [
      "Every year sometime during the months of February or March we sneak out of town for our Annual Retreat, an event that gives us a chance to reflect on our year.  This year, I thought it would be fun to plot on a map all of the conferences and meetups that we spoke at as part of our review process.   I thought it would be fun to share that map!",
      "Have an event that needs a speaker?  Check out our list of talks we give.",
      "",
      "Meetups are in dark red, conferences in blue."
    ],
    "summary_t": "During our Annual Retreat, we looked back at the conferences that we spoke at.   I wanted to share ;-)"
  },
  {
    "id": "2c9e06a92071f9263c6c96fb12cd521d",
    "url_s": "https://opensourceconnections.com/blog/2017/02/14/elasticsearch-learning-to-rank/",
    "title": "We're Bringing Learning to Rank to Elasticsearch",
    "content": [
      "It’s no secret that machine learning is revolutionizing many industries. This is equally true in search, where companies exhaust themselves capturing nuance through manually tuned search relevance. Mature search organizations want to get past the \"good enough\" of manual tuning to build smarter, self-learning search systems.",
      "That’s why we’re excited to release our Elasticsearch Learning to Rank Plugin. What is learning to rank? With learning to rank, a team trains a machine learning model to learn what users deem relevant.",
      "When implementing Learning to Rank you need to:",
      "Measure what users deem relevant through analytics, to build a judgment list grading documents as exactly relevant, moderately relevant, not relevant, for queries\n  Hypothesize which features might help predict relevance such as TF*IDF of specific field matches, recency, personalization for the searching user, etc.\n  Train a model that can accurately map features to a relevance score\n  Deploy the model to your search infrastructure, using it to rank search results in production",
      "Don’t fool yourself: underneath each of these steps lie complex, hard technical and non-technical problems. There’s still no silver bullet. As we mention in Relevant Search, manual tuning of search results comes with many of the same challenges as a good learning to rank solution. We’ll have more to say about the many infrastructure, technical, and non-technical challenges of mature learning to rank solutions in future blog posts.",
      "In this blog post I want to tell you about our work to integrate learning to rank within Elasticsearch. Clients ask us in nearly every relevance consulting engagement whether or not this technology can help them. But while there’s a clear path in Solr thanks to Bloomberg, there hasn’t been one in Elasticsearch. Many clients want the modern affordances of Elasticsearch, but find this a crucial missing piece to selecting the technology for their search stack.",
      "Indeed Elasticsearch’s Query DSL can rank results with tremendous power and sophistication. A skilled relevance engineer can use the query DSL to compute a broad variety of query-time features that might signal relevance. Giving quantitative answers to questions like",
      "How much Is the search term mentioned in the title?\n  How long ago was the article/movie/etc published?\n  How does the document relate to user’s browsing behaviors?\n  How expensive is this product relative to a buyer’s expectations?\n  How conceptually related is the user’s search term to the subject of the article?",
      "Many of these features aren’t static properties of the documents in the search engine. Instead they are query dependent – they measure some relationship between the user or their query and a document. And to readers of Relevant Search, this is what we term signals in that book.",
      "So the problem becomes, how can we marry the power of machine learning with existing power of the Elasticsearch Query DSL? And that’s exactly what our plugin does: use Elasticsearch Query DSL queries as feature inputs to a machine learning model.",
      "How does it work?",
      "In short, (the TL; DR)",
      "The plugin integrates RankLib and Elasticsearch. Ranklib takes as input a file with judgments and outputting a model in its own native, human-readable format. Ranklib then lets you trains models either programatically or via the command line. Once you have a model the Elasticsearch plugin contains the following",
      "A custom Elasticsearch script language called ranklib that can accept ranklib generated models as an Elasticsearch scripts\n  An custom ltr query that inputs a list of Query DSL queries (the features) and a model name (what was uploaded at 1) and scores results",
      "As learning to rank models can be expensive to implement, you almost never want to use ltr query directly. Rather you would rescore the top N results such as:",
      "{\n    \"query\": {/*a simple base query goes here*/},\n    \"rescore\": {\n        \"window_size\": 100,\n        \"query\": {\n           \"rescore_query\": {\n              \"ltr\": {\n                  \"model\": {\n                     \"stored\": \"dummy\"\n                  },\n                  \"features\": [{\n                       \"match\": {\n                           \"title\": <users keyword search>\n                       }\n                   }\n               ...",
      "The gritty details: behold the power of this fully functional learning to rank example!",
      "Now beyond the TL; DR;",
      "You can dig into a fully functioning example in the demo directory of the project. It’s a canned example, using hand-created judgments of movies from TMDB. I use an Elasticsearch index with TMDB to execute queries corresponding to features, augment a judgment file with the relevance scores of those queries/features, and train a Ranklib model at the command line. I store the model in Elasticsearch, and provide a script to search using the model.",
      "Don’t be fooled by the simplicity of this example. The reality of a real learning to rank solution is a tremendous amount of work, including studying users, processing analytics, data engineering, and feature engineering. I say that to not dissuade you, because the payoff can be worth it, just know what you’re getting into. Smaller organizations might still do better with the ROI of hand-tuned results.",
      "Training and loading the learning to rank model",
      "Let’s start with the hand-created, minimal judgment list I’ve provided to show how our example trains a model.",
      "Ranklib judgment lists come in a fairly standard format. The first column contains the judgment (0-4) for a document. The next column a query id, such as \"qid:1.\" The subsequent columns contain the values of the features associated with that query-document pair. On the left-hand side is the 1-based index of the feature. To the right of that number is the value for the feature. The example in the Ranklib README is:",
      "3 qid:1 1:1 2:1 3:0 4:0.2 5:0 # 1A\n2 qid:1 1:0 2:0 3:1 4:0.1 5:1 # 1B\n1 qid:1 1:0 2:1 3:0 4:0.4 5:0 # 1C\n1 qid:1 1:0 2:0 3:1 4:0.3 5:0 # 1D  \n1 qid:2 1:0 2:0 3:1 4:0.2 5:0 # 2A",
      "Notice also the comment (# 1A etc). That comment is the document identifier for this judgement. The document identifier isn’t needed by Ranklib, but it’s fairly handy to human readers. As we’ll see it’s useful for us as well when we gather features via Elasticsearch queries.",
      "Our example starts with a minimal version of the above file (seen here). We need to start with a trimmed-down version of the judgement file that simply has a grade, query id, and document id tuple. Like so:",
      "4\tqid:1 #\t7555\n3\tqid:1 #\t1370\n3\tqid:1 #\t1369\n3\tqid:1 #\t1368\n0\tqid:1 #\t136278\n...",
      "As above, we provide the Elasticsearch _id for the graded document as the comment on each line.",
      "We need to enhance this a bit further. We must map each query id (qid:1) to an actual keyword query (\"Rambo\") so we can use the keyword to generate feature values. We provide this mapping in the header which the example code will pull out:",
      "# Add your keyword strings below, the feature script will\n# Use them to populate your query templates\n#\n# qid:1: rambo\n# qid:2: rocky\n# qid:3: bullwinkle\n#\n# https://sourceforge.net/p/lemur/wiki/RankLib%20File%20Format/\n#\n#\n4\tqid:1 #\t7555\n3\tqid:1 #\t1370\n3\tqid:1 #\t1369\n3\tqid:1 #\t1368\n0\tqid:1 #\t136278\n...",
      "To help clear up confusion, I’m going to start talking about ranklib \"queries\" (the qid:1 etc) as \"keywords\" to differentiate from the Elasticsearch Query DSL \"queries\" which are Elasticsearch-specific constructs used to generate feature values.",
      "What’s above isn’t a complete Ranklib judgement list. It’s just a minimal sample of relevance grades for given documents for a given keyword search. To be a fully-fledged training set, it needs to include the feature values shown above, the 1:0 2:1 … included after each line in the first judgement list shown.",
      "To generate those feature values, we also need to have proposed features that might correspond to relevance for movies. These, as we said, are Elasticsearch queries. The scores for these Elasticseach queries will finish filling out the judgement list above. In the example above, we do this using a jinja template corresponding to each feature number. For example the file 1.json.jinja is the following Query DSL query:",
      "{\n   \"query\": {\n       \"match\": {\n          \"title\": \"\"\n       }\n   }\n}",
      "In other words, we’ve decided that feature 1 for our movie search system ought to be the TF*IDF relevance score for the user’s keywords when matched against the title field. There’s also 2.jinja.json which performs a more complex search across multiple text fields:",
      "{\n   \"query\": {\n       \"multi_match\": {\n          \"query\": \"\",\n          \"type\": \"cross_fields\",\n          \"fields\": [\"overview\", \"genres.name\", \"title\", \"tagline\", \"belongs_to_collection.name\", \"cast.name\", \"directors.name\"],\n          \"tie_breaker\": 1.0\n       }\n   }\n}",
      "Part of the fun of learning to rank is hypothesizing what features might correlate with relevance. In the example, you can change features 1 and 2 to any Elasticsearch query. You can also experiment by adding additional features 3 through however many. There’s problems with too many features, as you’ll want to get enough representative training samples that cover all reasonable feature values. We’ll discuss more about training and testing learning to rank models in a future blog post.",
      "With these two ingredients, the minimal judgment list and a set of proposed Query DSL queries/features, we need to generate a fully-fleshed out judgement list for Ranklib and load the Ranklib generated model into Elasticsearch to be used. This means:",
      "Getting relevance scores for features for each keyword/document pair. Aka issuing queries to Elasticsearch to log relevance scores.\n  Outputting a full judgment file not only with grades and keyword query ids, but also with feature values from step 1 (1:, 2:<>...)\n  Running Ranklib to train the model\n  Loading the model into Elasticsearch for use at search time",
      "The code to do this is all bundled up in train.py which I encourage you to take apart. To run this, you’ll need",
      "RankLib.jar downloaded to the scripts folder\n  Python packages elasticsearch / jinja2 installed (there’s a Python requirements.txt if you’re familiar)",
      "Then you can just run:",
      "python train.py",
      "This one script runs through all the steps mentioned above. To walk you through the code:",
      "First we load the minimal judgment list with just our document, keyword query id, grade tuples, with search keywords specified in the file’s header:",
      "judgements = judgmentsByQid(judgmentsFromFile(filename='sample_judgements.txt'))",
      "We then issue bulk Elasticsearch queries to log features for each judgment (augmenting the passed in judgements).",
      "kwDocFeatures(es, index='tmdb', searchType='movie', judgements=judgements)",
      "The function kwDocFeatures finds 1.json.jinja through N.json.jinja (the features/queries), and strategically batches Elasticsearch queries up to get a relevance score for each keyword/document tuple using Elasticsearch’s bulk search (_msearch) API. The code is tedious, you can see it here.",
      "Once we have the fully fleshed-out features, we then output the full training set (judgements plus features) into a new file (sample_judgements_wfeatures.txt):",
      "buildFeaturesJudgmentsFile(judgements, filename='sample_judgements_wfeatures.txt')",
      "The output will correspond to a fully fleshed-out Ranklib judgement list, ala:",
      "3\tqid:1\t1:9.476478\t2:25.821222 # 1370\n3\tqid:1\t1:6.822593\t2:23.463709 # 1369",
      "Where feature 1 is the TF*IDF score of \"Rambo\" searched on the title (1.json.jinja); feature 2 is the TF*IDF score of the more complex search (2.json.jinja).",
      "Next we train! This line runs Ranklib.jar via the command line using this saved file as judgement data",
      "trainModel(judgmentsWithFeaturesFile='sample_judgements_wfeatures.txt', modelOutput='model.txt')",
      "As you can see below, this just basically runs java -jar Ranklib.jar training a LambdaMART model:",
      "def trainModel(judgmentsWithFeaturesFile, modelOutput):\n    # java -jar RankLib-2.6.jar -ranker 6 -train sample_judgements_wfeatures.txt -save model.txt\n    cmd = \"java -jar RankLib-2.6.jar -ranker 6 -train %s -save %s\" %   (judgmentsWithFeaturesFile, modelOutput)\n    print(\"Running %s\" % cmd)\n    os.system(cmd)",
      "We then store the model into Elasticsearch using simple Elasticsearch commands:",
      "saveModel(es, scriptName='test', modelFname='model.txt')",
      "Here saveModel, as seen here just reads the file contents and POSTs it to Elasticsearch as a ranklib script to be stored.",
      "Searching with the learning to rank model",
      "Once you’re done training, you’re ready to issue a search! You can see an example in search.py; it’s pretty straightforward with a simple query inside. You can run python search.py rambo, which will search for \"rambo\" using the trained model, executing the following rescoring query:",
      "{\n\t\"query\": {\n\t\t\"match\": {\n\t\t\t\"_all\": \"rambo\"\n\t\t}\n\t},\n\t\"rescore\": {\n    \"window_size\": 20,\n\t\t\"query\": {\n\t\t\t\"rescore_query\": {        \n\t\t\t\t\"ltr\": {\n\t\t\t\t\t\"model\": {\n\t\t\t\t\t\t\"stored\": \"test\"\n\t\t\t\t\t},\n\t\t\t\t\t\"features\": [{\n\t\t\t\t\t\t\"match\": {\n\t\t\t\t\t\t\t\"title\": \"rambo\"\n\t\t\t\t\t\t}\n\t\t\t\t\t}, {\n\t\t\t\t\t\t\"multi_match\": {\n\t\t\t\t\t\t\t\"query\": \"rambo\",\n\t\t\t\t\t\t\t\"type\": \"cross_fields\",\n\t\t\t\t\t\t\t\"tie_breaker\": 1.0,\n\t\t\t\t\t\t\t\"fields\": [\"overview\", \"genres.name\", \"title\", \"tagline\", \"belongs_to_collection.name\", \"cast.name\", \"directors.name\"]\n\t\t\t\t\t\t}\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
      "Notice we’re only reranking the top 20 results here. We could use ltr query directly. Indeed, running the model directly works pretty well. Albeit it takes a few hundred milliseconds to run over the whole collection. For a larger collection it wouldn’t be feasible. In general, it’s best to rerank a top N results due to the performance cost of learning to rank models.",
      "And that’s the working example. Of course this is just a dumb, canned example meant to get your juices flowing. Your particular problem likely has many more moving parts. The features you choose, how you log features, train your model, and implement a baseline ranking function depends quite a bit on your domain. Much of what we write about in Relevant Search still applies. So I guess what we’re saying is uh still buy the book ;).",
      "What’s next! And get in touch!",
      "In future blog posts we’ll have much more to say about learning to rank, including:",
      "Basics: more about what learning to rank is exactly\n  Applications: Using learning to rank for search, recommendation systems, personalization and beyond\n  Models: What are the prevalant models? What considerations play in selecting a model?\n  Considerations: What technical and non-technical considerations come into play with Learning to Rank?",
      "If you think you’d like to discuss how your search application can benefit from learning to rank, please get in touch. We’re also always on the hunt for collaborators or for more folks to beat up our work in real production systems. So give it a go and send us feedback!"
    ],
    "summary_t": "Announcing the Elasticsearch Learning to Rank Plugin: Bringing machine learning into Elasticsearch to improve search relevance."
  },
  {
    "id": "dadd1f4ad5d1d04cca72325b27ab8cd4",
    "url_s": "https://opensourceconnections.com/blog/2017/02/20/solr-utf8/",
    "title": "Solr UTF-8 Character Handling",
    "content": [
      "Searching for non-ASCII characters can be a challenge. There are a number of reasons for doing so, even in a primarily English corpus:",
      "Accented characters in names and words that have been incorporated. For example, Renèe Pèrez\n  Greek letters, which have been incorporated into mathematical formulae or scientific phrase:  α-linoleic acid\n  Punctuation characters which may change the meaning of the text: α",
      "These characters each have their own special considerations.  Fortunately, Solr provides support for all these cases with a little bit of configuration.",
      "Accented Characters",
      "For the first case, there are two main cases that need to be supported:",
      "Rene and Renè should return different results\n  Rene and Renè should return the same results",
      "Remember, by default Solr does exact matches for each token.  Thus by default, a search for Rene will not match Renè.  The usual solution here is called code folding: replacing the utf-8 characters at index-time with their latin equivalent.  This is provided via the mappingCharFilterFactory",
      "In the schema, add the following lines:",
      "<analyzer>\n  <charFilter class=\"solr.MappingCharFilterFactory\"  mapping=\"mapping-FoldToASCII.txt\"/>\n  <tokenizer ...>\n  [...]\n</analyzer>",
      "That mapping-FoldToASCII.txt contains both the characters to be replaced, and their character replacements.  For our example:",
      "# è  [LATIN SMALL LETTER E WITH GRAVE]\n\"\\u00E8\" => \"e\"",
      "This tells Solr at index-time to replace all è with e, and our search for Rene will return Renè as well.  It is important to remember that this removes the initial è character.  Unless this is applied for both the index and query analyzer chain,  queries for Renè won’t match anything at all!",
      "The mappingCharFilterFactory allows us to specify precisely which characters will be preserved or folded out.  Solr provides the asciiFoldingFilter which will fold down characters to their ASCII equivalent, if one exists.   Thus  è => e will be mapped, although many control and punctuation characters will be folded out.",
      "Greek Letters",
      "Greek letters are common in many document sets, particularly those containing scientific terms and formulae.  In this case, there may not be an obvious character to use in the folding.    Our first thought might be to add a longer character string in the mapping-FoldToASCII.txt file, or one like it:",
      "# è  [GREEK SMALL LETTER ALPHA]\n\"\\u03B1\" => \"alpha\"",
      "However, the mapping char filter is applied as a char filter. This means that it will be applied before the document is tokenized. So when we replace one character (α) with five characters (alpha), there is no way to preserve that mapping of location back to the original document.   If we are returning highlighted results to the user based on the field containing the mappings, then the highlighting will cover too many characters!",
      "The PatternReplaceFilterFactory is a close relative to the MappingCharFilterFactory.  It allows us to specify a pattern of characters to match a corresponding substitution.  In this case, the replacement will be applied after the text has been tokenized, and the tokenFilter will contain the start and end lengths of the original text, even if we change the size of the token:",
      "<analyzer>\n  <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n  <filter class=\"solr.PatternReplaceFilterFactory\" pattern=\"α\" replacement=\"alpha\"/>\n</analyzer>",
      "If our users will be commonly searching for the actual character, the reverse configuration is also possible.",
      "<analyzer>\n  <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n  <filter class=\"solr.PatternReplaceFilterFactory\" pattern=\"alpha\" replacement=\"α\"/>\n</analyzer>",
      "Remember, the important thing is that the same analyzer is applied at both index and query time.",
      "There are two other interesting issues that come up here. These considerations are not special to Greek letters, but are more common with that subset than other special characters.",
      "Encoding",
      "What if the Greek letters in my document are not initially UTF-8 encoded? What then? \nMany larger documents have either been serialized to XML or HTML before being ingested.  For many of these documents, the special characters may be HTML-encoded, rather than the UTF-8 encoding Solr allows searching over.  That \"α-linoliec acid\" may be \"\\α-linoleic acid\". In this case, users will rarely, if ever, search for the html encoding.",
      "If highlighting is not a significant concern, the easy solution in this case is the HTMLStripCharFilterFactory.   This removes all html-syntax from the body before tokenization, including replacing html-encoded characters with their target in the index:",
      "<analyzer>\n  <charFilter class=\"solr.HTMLStripCharFilterFactory\"/>\n</analyzer>",
      "Note that because this is a charfilter, it will negatively impact highlighting.    Additionally, the HTMLStripCharFilterFactory is sensitive to punctuation.  HTML-encoded characters are usually of the pattern \"&[a-zA-Z];\"  however, in many cases the trailing semicolon is not.  In stand-alone tokens, Solr will assume that stand-alone terms missing the trailing semicolon are html-encoded, while embedded terms must provide the trailing semicolon.  Thus",
      "alpha&omega",
      "will be indexed as",
      "alpha&omega",
      "while",
      "alpha&omega;",
      "would be indexed as",
      "alpha&ω",
      "Depending on your familiarity with the greek alphabet, the above encoding may be a surprise.  Isn’t omega Ω?",
      "The answer is simple.  As with the a-z alphabet, non-latin characters also have upper and lowercase syntax.  For the most part, this is handled transparently in the java toUpper() and toLower() libraries.  Furthermore, html-encoding provides syntax to support both cases.   In the previous example,",
      "&omega;",
      "is html-encoded to be lowercase omega, or ω.",
      "Whether this behavior is relevant depends on the text being indexed.  For nutritional content, users may be more interested that the character seamlessly map to ‘alpha’ in their searches.  In a scientific document, ω and Ω may occur in different formulae, and occurrences of the lowercase format may not be relevant to occurrences of the other."
    ],
    "summary_t": "This article investigates Solr support for non-ASCII utf-8 characters."
  },
  {
    "id": "59e8acc87c34bd319216ad05c336a781",
    "url_s": "https://opensourceconnections.com/blog/2017/02/24/what-is-learning-to-rank/",
    "title": "What is Learning To Rank?",
    "content": [
      "As an engineer, artificial intelligence (AI) is cool.  Spaceships and science fiction cool.  There has been a lot of attention around machine learning and artificial intelligence lately.  Some of the largest companies in IT such as IBM and Intel  have built whole advertising campaigns around advances that are making these research fields practical.  How much of this is still cool and fiction? Can these advances can reasonably be used to enhance our applications, right now?",
      "Learning to rank ties machine learning into the search engine, and it is neither magic nor fiction.  It is at the forefront of a flood of new, smaller use cases that allow an off-the-shelf library implementation to capture user expectations.",
      "What is relevancy engineering?",
      "Search and discovery is well-suited to machine learning techniques.   Relevancy engineering is the process of identifying the most important features of document set to the users of those documents, and using those features to tune the search engine to return the best fit documents to each user on each search.  To recap how a search engine works: at index time documents are parsed into tokens; these tokens are then inserted to an index as seen in the figure below.",
      "",
      "At search time, individual queries are also parsed into tokens.  The search engine then looks up the tokens from the query in the inverted index, ranks the matching documents, retrieves the text associated with those documents, and returns the ranked results to the user as shown below.",
      "",
      "Identifying the best features based on text tokens is a fundamentally hard problem.  Whole books and PhDs have been written on solving it.  (Shameless plug for our book Relevant Search!)  Consider a sales catalog:",
      "ID\n      Title\n      Price\n    \n  \n  \n    \n      1\n      Blue shoes\n      $10\n    \n    \n      2\n      Dress shoes\n      $15\n    \n    \n      3\n      Blue dress\n      $20\n    \n    \n      4\n      Red dress\n      $40",
      "As a human, we intuitively know that in document 2, ‘dress’ is an adjective describing the shoes, while in documents 3 and 4, ‘dress’ is the noun, the item in the catalog.   As a relevancy engineer, we can construct a signal to guess whether users mean the adjective or noun when searching for ‘dress’.    Even with careful crafting, text tokens are an imperfect representation of the nuances in content.",
      "Where does learning to rank come in?",
      "From the Wikipedia definition,",
      "learning to rank or machine-learned ranking (MLR) applies machine learning to construct of ranking models for information retrieval systems.  The most common implementation is as a re-ranking function.",
      "This means rather than replacing the search engine with an machine learning model, we are extending the process with an additional step.  After the query is issued to the index, the best results from that query are passed into the model, and re-ordered before being returned to the user, as seen in the figure below:",
      "",
      "Why do this in two parts?   Why not replace the whole search engine with the model?",
      "",
      "Search engines are generally graded on two metrics: recall, or the percentage of relevant documents returned in the result set, and precision, the percentage of documents that are relevant.     As a relevance engineer, constructing signals from documents to enable the search engine to return all the important results is usually less difficult than returning the best documents first.  Intuitively, it is generally possible to improve recall by simply returning more documents.  However, as a human user, if those better documents aren’t first in the list, they aren’t very helpful.",
      "How does machine learning tie into this? Back to our Wikipedia definitions:",
      "Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed.  Evolved from the study of pattern recognition and computational learning theory in artificial intelligence machine learning explores the study and construction of algorithms that can learn from and make predictions on data.",
      "Machine learning isn’t magic, and it isn’t intelligence in the human understanding of the word.  As a practical, engineering problem, we need to provide a set of training data: numerical scores of the numerical patterns we want our machine to learn.    It turns out, constructing an accurate set of training data is not easy either, and for many real-world applications, constructing the training data is prohibitively expensive, even with improved algorithms.  So if our search engine is pretty good at recall, then we don’t need to collect data and train our model on it.   We just need to train the model on the order, or ranking of the documents within that result set.",
      "",
      "",
      "The other reason for narrowing the scope back to re-ranking is performance.  Both building and evaluating models can be computationally expensive.  Because the training model requires each feature be a numerical aspect of either the document or the relationship of the document to the user, it must be re-computed each time.   Since users expect search results to return in seconds or milliseconds,  re-ranking 1000 to 2000 documents at a time is less expensive than re-ranking tens of thousands or even millions of documents for each search.",
      "What’s next?",
      "This article is part of a sequence on Learning to Rank.   If you are ready to try it out for yourself, try out our ElasticSearch LTR plugin!  Watch for more articles in coming weeks on:",
      "Models: What are the prevalent models? What considerations play in selecting a model?\n  Applications: Using learning to rank for search, recommendation systems, personalization and beyond\n  Considerations: What technical and non-technical considerations come into play with Learning to Rank?",
      "If you think you’d like to discuss how your search application can benefit from learning to rank, please get in touch. We’re also always on the hunt for collaborators or for more folks to beat up our work in real production systems. So give it a go and send us feedback!"
    ],
    "summary_t": "We walk through Learning to Rank, which uses machine learning to improve the relevance of search results."
  },
  {
    "id": "f1810cc0284ebaaa87e347a750864ce3",
    "url_s": "https://opensourceconnections.com/blog/2007/05/29/tech-leads-should-lead/",
    "title": "Tech leads should lead",
    "content": [
      "Our tech leads love being eyeball deep in code because they love being coders.  However, sometimes this creates problems because we need them to lead multiple projects.  When tech leads get too deep into the code, two things happen: 1) they cannot pull away to perform the same role on other projects, and 2) they do not maintain perspective on the code that theyre deep into.  Tech leads need to be there to perform code review and to crack the tough nuts.  Other team members are there to code.  IT managers are wise to help their tech leads maintain their distance."
    ],
    "summary_t": ""
  },
  {
    "id": "3103d00d3b996bdb325e67381f647753",
    "url_s": "https://opensourceconnections.com/blog/2017/04/01/learning-to-rank-linear-models/",
    "title": "Learning to Rank 101 -- Linear Models",
    "content": [
      "Many search nerds get an instinct they want to \"learn the right boosts\" to apply to their queries. Search often feels like whack-a-mole, and often folks say \"if I could just optimize the boost on say the ‘title match’ vs the boost on the ‘body match’ I’d be in great shape!\"",
      "This instinct of learning what boost to apply to queries is the instinct behind the simplest learning to rank model: the linear model. Yes! Good ole linear regression! What’s nice about linear regression is that, well, it doesn’t really feel like machine learning. It feels like high school statistics. It’s very easy to understand the model and make sense of it.",
      "In this series of articles, I want to begin to introduce the key algorithms behind successful learning to rank implementations, starting with linear regression and working up to topics like gradient boosting (different kinda boosting alltogether), RankSVM, and random forests.",
      "Learning to Rank as a Regression Problem",
      "For this series of articles, I want to map learning to rank, as you might be familiar from previous articles and documentation to a more general problem: regression. Regression trains a model to map a set of numerical features to a predicted numerical value.",
      "For example, what if you wanted to be able to predict a company’s profit? You might have, on hand, historical data about public corporations including number of employees, stock market price, revenue, cash on hand, etc. Given data you know about existing companies, your model could be trained to predict profit as a function of these variables (or a subset thereof). For a new company you could use your function to arrive at a prediction of the company’s profit.",
      "Just the same, learning to rank can be a regression problem. You have on hand a series of judgments that grade how relevant a document is for a query. Our relevance grades could range from A to F. More commonly they range from 0 (not at all relevant) to 4 (exactly relevant). If we just consider a keyword search to be a query, this become, as an example:",
      "grade,movie,keywordquery\n4,Rocky,rocky\n0,Turner and Hootch,rocky\n3,Rocky II,rocky\n1,Rambo,rocky\n...",
      "Learning to Rank becomes a regression problem when you build a model to predict the grade as a function of ranking-time signals. Recall from Relevant Search we term signals to mean any measurement about the relationship between the query and a document. These are often more generically called features, but I prefer the term signals. One reason is that signals are typically query-dependent - that is they result by taking some measurement of how a keyword (or other part of the query) relates to the document. Some measurement of their relationship. Yet we can take other signals, including ones that are query-only or document-only, such as publication date of an article, or some entity extraction run on the query (like \"company names\").",
      "Let’s consider the movie example above. You might have 2 query-dependent signals you suspect could help predict relevance:",
      "How many times a search keyword occurs in the title field\n  How many times a search keyword occurs in the overview field",
      "Augmenting the judgments above, you might arrive at a training set for regression like below in CSV, mapping grades to signal values.",
      "grade,numTitleMatches,numOverviewMatches\n4,1,1\n0,0,0\n3,0,3\n1,0,1",
      "You can apply a regression process such as linear regression, to predict the first column using the other columns. You can build such a system on top of an existing search engine like Solr or Elasticsearch.",
      "I’m sidestepping a complicated question. Specifically: how do you arrive at these judgments? How do you know when a document was good or bad for a query? Interpreting user analytics? Manually with experts? This often is the hardest problem to solve – and can be quite domain specific! Coming up with supposed data to create a model is fine and dandy, but garbage in, garbage out!",
      "Linear Regression for Learning to Rank",
      "If you’ve learned any statistics, you’re probably familiar with Linear Regression. Linear Regression defines the regression problem as a simple linear function. For example, if in learning to rank we called the first signal above (how many times a search keyword occurs it the title field) as t and the second signal above (the same for the overview field) as o, our model might be able to generate a function s to score our relevance as follows:",
      "s(t, o) = c0 + c1 * t + c2 * o",
      "We can estimate the best fit coefficients c0, c1, c2... that predict our training data using a procedure known as least squares fitting. We won’t cover that here, but the gist is we can find the c0, c1, c2, ... that minimize the error between the actual grade, g and the prediction s(t,o). It’s often as simple as a little matrix math if you want to bone up on your linear algebra.",
      "You can get fancier with linear regression, including deciding there’s really a third ranking signal, a which we can define as t*o. Or another signal called t2, which could be in reality t^2 or log(t) or whatever formulation you suspect could help best predict relevance. You can then just treat these values as additional columns in the dataset for which linear regression can learn coefficients for.",
      "There’s a deeper art to of designing, testing ,and evaluating models of any flavor, I heartily recommend Introduction to Statistical Learning if you’d like to learn more.",
      "Learning to Rank with Linear Regression in sklearn",
      "To give you a taste, Python’s sklearn family of libraries is a convenient way to play with regression. If we want to try out the simple learning to rank training set above for linear regression, we can express the relevance grade’s we’re trying to predict as S, and the signals we feel will predict that score as X.",
      "We’re going to have some fun with some movie relevance data. Here we have a set of relevance grades for a keyword search \"Rocky.\" Recall above we had a judgment list that we transformed into a training set. Let’s take a gander at a real training set (w/ comments to help us see what’s going on). The three ranking signals we’ll be examining include the title TF*IDF score, the overview TF*IDF score and the movie’s user rating.",
      "grade,titleScore,overviewScore,ratingScore,comment:# <docid> [email protected]\n4,10.65,8.41,7.40,# 1366   [email protected]\n3,0.00,6.75,7.00,# 12412  [email protected]\n3,8.22,9.72,6.60,# 1246   [email protected] Balboa\n3,8.22,8.41,0.00,# 1374   [email protected] IV\n3,8.22,7.68,6.90,# 1367   [email protected] II\n3,8.22,7.15,0.00,# 1375   [email protected] V\n3,8.22,5.28,0.00,# 1371   [email protected] III\n2,0.00,0.00,7.60,# 154019 [email protected]\n2,0.00,0.00,7.10,# 1368   [email protected] Blood\n2,0.00,0.00,6.70,# 13258  [email protected] of Rambow\n2,0.00,0.00,0.00,# 70808  [email protected]\n2,0.00,0.00,0.00,# 64807  [email protected] Match\n2,0.00,0.00,0.00,# 47059  [email protected] Gym\n...",
      "So let’s get to cranking out the code! Below we’ve got code for reading an a CSV into a numpy Array. The array is two dimensional, the first dimension being row, the second being column. You’ll see what the funky slicing is doing to the array in the comments below:",
      "from sklearn.linear_model import LinearRegression\nfrom math import sin\nimport numpy as np\nimport csv\n\nrockyData = np.genfromtxt('rocky.csv', delimiter=',')[1:] # Remove the CSV header\n\nrockyGrades = rockyData[:,0]   # Slice out column 0, where the grades are\nrockySignals = rockyData[:,1:-1]  # Features in columns 1...all but last column (the comment)",
      "Great! We’re ready to perform a simple linear regression. What we have here is a classic overdetermined system: way more equations than unknowns! So we need to use ordinary least squares to estimate the relationship between the features rockySignals and the grades rockyGrades. Easy peasy, this is what numpy’s linear regression does:",
      "butIRegress = LinearRegression()\nbutIRegress.fit(rockySignals, rockyGrades)",
      "This gives us the coefficients (the \"boosts\") to use on our ranking signals, along with a y-intercept as below:",
      "butIRegress.coef_  #boost for title, boost for overview, boost for rating",
      "array([ 0.04999419,  0.22958357,  0.00573909])",
      "butIRegress.intercept_",
      "0.97040804634516986",
      "Great! Relevance Solved! (right?) We can use these to to create a ranking function! We’ve learned which boosts to apply to a title and an overview field.",
      "For now, I’m ignoring a bunch of item’s we’ll need to consider to evaluate how good of a fit this model is to the data. For the sake of this blog post, we just want to see generally how these models work. But it’s a good idea to not just assume this model is phenomenal fit to the training data, and always reserve some data for testing! Future blog posts will dive into these topics :)",
      "Using our Model to score queries",
      "With these coefficients, let’s create our own ranking function! We’re doing this for illustration purposes only, sk-learn’s LinearRegression comes predict method which evaluates the model given an input, but it’s more fun to make our own:",
      "def relevanceScore(intercept, titleCoef, overviewCoef, ratingCoef, titleScore, overviewScore, movieRating):\n    return intercept + (titleCoef * titleScore) + (overviewCoef * overviewScore) + (ratingCoef * movieRating)",
      "Using this function, we can arrive at relevance scores for these two movies as candidates for movies for the search \"Rambo\"",
      "titleScore,overviewScore,movieRating,comment\n12.28,9.82,6.40,# 7555\t[email protected]\n0.00,10.76,7.10,# 1368\t[email protected] Blood",
      "Let’s score Rambo and First Blood to see which is more relevant for the query \"Rambo\"!",
      "# Score Rambo\nrelevanceScore(butIRegress.intercept_, butIRegress.coef_[0], butIRegress.coef_[1], butIRegress.coef_[2],\n               titleScore=12.28, overviewScore=9.82, movieRating=6.40)\n# Score First Blood\nrelevanceScore(butIRegress.intercept_, butIRegress.coef_[0], butIRegress.coef_[1], butIRegress.coef_[2],\n               titleScore=0.00, overviewScore=10.76, movieRating=7.10)",
      "Respectively this gives us scores of 3.670 for Rambo and 3.671 for First Blood.",
      "Very close! First Blood narrowly beats out Rambo for the win! This makes sense – while Rambo is an exact match, First Blood was the original Rambo movie! Well we shouldn’t really give our model that much credit, it hasn’t seen that many examples to capture that level of nuance. What is interesting though is that the coefficient for an the overview score is higher than the one for the title score. So at least in the examples our model has seen, more keyword mentions in an overview is the most highly correlated to relevance. We’re already learning a great deal about how our user’s perceive relevance!",
      "It’s been fun to put together this model. It was easy to understand, and produced reasonably sane results. But direct linear combinations of features often fall short for relevance applications. They fall short for the same reason, as our friends at Flax say, direct additive boosting falls short.",
      "Why? Nuance!",
      "In the previous example, we began to see how some extremely relevant movies do indeed have very high title TF*IDF relevance scores. Yet our model sided with the overview field as more correlated with relevance. The reality of when titles match and when overview matches actually depends on other things.",
      "For many problems, there’s not a linear relationship mapping set of relevance grades and the scores of title and overview fields. Context matters. Title matches matter a lot when you want to search for a title directly. But it doesn’t matter in use cases where you’re unsure of the title, or searching with a genre, actor, or other use case.",
      "In other words, relevance doesn’t look like this nice clean optimization problem:",
      "",
      "In reality, relevance is far messier. There’s no one magical optimum, but rather many local optimum depending on many other factors! Why? In other words, relevance looks far lumpier like in this graph:",
      "",
      "You can imagine these graphs (courtesy of Andrew NG’s ML class) as showing the ‘relevance error’ – how far are we from the grades we’re learning. The two theta variables map to say the title and overview relevance scores. In the  first graph there’s a single optimum value that minimizes the \"relevance error.\" – an ideal set of boosts to apply to these two queries. The second is more realistic: lumpy, context-dependent minima. Sometimes a very high title boost value matters – othertimes a very low title boost!",
      "Context and nuance matter!",
      "Getting out of line!",
      "That’s it for now. In future blog posts, I want to work on quantifying exactly where this model falls apart. What measures can we use to evaluate how good the model is? That will be a great stepping off point to examining other methods that can do a better job of capturing nuance.",
      "I write these things so you’ll write to me! If you have any feedback, or if you need to teach me a thing or two, please get in touch and tell me how I’m wrong (I really want to learn from you!). Don’t hesitate to reach out if OpenSource Connections can help you with our relevancy services.",
      "This blog post was created with Jupyter Notebook: View the source!"
    ],
    "summary_t": "Many search nerds get an instinct they want to ‘learn the right boosts’ to apply to their queries. This blog post covers how to do that using a linear learni..."
  },
  {
    "id": "6f3b0cdf1d6ea00de09bca4bdfeacd8f",
    "url_s": "https://opensourceconnections.com/blog/2017/04/03/test-drive-elasticsearch-learn-to-rank-linear-model/",
    "title": "Test Driving Elasticsearch Learning to Rank with a Linear Model",
    "content": [
      "Last time, I created a simple linear model using three ranking signals. Using TMDB movie data, I came up with a naive model that computed a relevance score from a title field’s TF*IDF score, an overview field’s TF*IDF score, and the user rating of the movie. Our model learned the weight to apply to each score when summing – similar to the boosts you apply when doing manual relevance tuning.",
      "What I didn’t tell you was I was using Elasticsearch to compute those ranking signals. This blog post I want to take the simple model we derived and make it usable via the Elasticsearch Learning to Rank plugin. In a future blog post, we’ll load the same model into Solr. This will give you a chance to see a very simple 101 model in action with the two search engine’s learning to rank plugins.",
      "The ranking signals…",
      "If you recall, learning to rank learns a ranking function as a function of ranking-time signals. Classically these are referred to as \"features\" when discussing machine learning models. But I like to use signals to denote that they’re signaling us something about the relationship between a query and document. Plus, selfishly, it’s what we call ranking-time information in Relevant Search to differentiate between the features that exist purely on content or derived from queries.",
      "In this blog post, we’ll use the Python Elasticsearch client library, but mostly I’ll just be showing off the basic queries I use to derive the signals. I’ve already loaded the TMDB movie data locally, if you’d like to have this data at your disposal follow the directions in the Learning to Rank demo README",
      "Onto the action. Below, you’ll see our three queries we use to generate the signal values: titleSearch, overviewSearch, and ratingSearch. The first two are straight-forward match queries. The latter is a function score query that just returns a movie’s rating which has no relationship to the search keywords.",
      "from elasticsearch import Elasticsearch\n\nkeywords=\"rambo\"\n\ntitleSearch = {\n    \"query\": {\n        \"match\": {\n            \"title\": keywords\n        }\n    }\n}\n\noverviewSearch = {\n    \"query\": {\n        \"match\": {\n            \"overview\": keywords\n        }\n    }\n}\n\nratingSearch = {\n    \"query\": {\n        \"function_score\": {\n        \n            \"functions\": [\n                {\"field_value_factor\": {\n                    \"field\": \"vote_average\",\n                    \"missing\": -1   \n                }}\n            ]\n        }\n    }\n}\n\nes = Elasticsearch()\nes.search(index='tmdb', doc_type='movie', body=titleSearch)\nes.search(index='tmdb', doc_type='movie', body=overviewSearch)\nes.search(index='tmdb', doc_type='movie', body=ratingSearch)",
      "If you recall, these three features were gathered for a set of judgments. Judgments let us know how relevant a document is for a query. So Rambo is a \"4\" (exact match) for the keyword search \"rambo.\" Conversely \"Rocky and Bullwininkle\" is a 0 (not at all relevant) for a \"Rambo\" query. With enough judgments, we logged the relevance scores of the above queries for the documents that were judged. This gave us a training set that looked like:",
      "titleScore,overviewScore,movieRating,comment\n4,12.28,9.82,6.40,# 7555\t[email protected]\n0,0.00,10.76,7.10,# 1368\t[email protected] Blood",
      "In that blog post, we used sk-learn to run linear regression to learn which signals best predicted the resulting relevance grade. We came up with a model with a weight for each and a y-intercept. This model was:",
      "coefs = [ 0.04999419,  0.22958357,  0.00573909] # each signals weight\nyIntercept = 0.97040804634516986",
      "Uploading our Linear model to Elasticsearch",
      "The Elasticsearch learning to rank plugin uses a scripting format known as ranklib to encode models. Following the documentation for the ranklib scripting language we know we can encode a linear model that looks like:",
      "## Linear Regression\n0:<y-intercept> 1:<coef1> 2:<coef2> 3:<coef3> ...",
      "So in Python code, we can format our model above in that format:",
      "linearModel = \"\"\" ## Linear Regression\n0:0.97040804634516986 1:0.04999419 2:0.229585357 3:0.00573909\n\"\"\"",
      "Following the documentation for the Learning to Rank Plugin we can upload this model as a ranklib script, and give it a name.",
      "es.put_script(lang='ranklib', id='our_silly_model', body={'script': linearModel})",
      "{'acknowledged': True}",
      "Elasticsearch has acknowledged our upload. Great! Now we should be able to execute a simple query!",
      "How do we construct a query that uses the model?",
      "You almost always want to run a learning to rank model in a rescore query, but the TMDB data set isn’t huge. We can use it directly with only a few hundred milliseconds to evaluate over the whole corpus. This is fun, because it let’s us informally evaluate how well our model is doing.",
      "To query with the model, we create a function that runs the ltr query. Remember the model we built computes a relevance score for a document from three inputs that relate to the query and document:",
      "The keyword’s title TF*IDF score\n  The keyword’s overview TF*IDF score\n  The movie’s rating",
      "To compute the first two, we need to run the titleSearch and overviewSearch above for our current keywords. So we need to pass our model a version of these queries with the current keywords. That’s what happens first in the function below. We inject our keywords into the inputs that are query-dependent. Then we add our ratingSearch that’s only document dependent.",
      "These three queries are scored per document are then fed into the linear model. Remember last time this model is simple: each coefficient is just a weight on each signal’s score. The model is simply a weighted sum of the scores of titleSearch, overviewSearch, and ratingSearch using coefficients as the weight!",
      "def runLtrQuery(keywords):\n\n    # Plugin our keywords\n    titleSearch['query']['match']['title'] = keywords\n    overviewSearch['query']['match']['overview'] = keywords\n    \n    # Format query    \n    ltrQuery = {\n        \"query\": {\n            \"ltr\": {\n                \"model\": {\n                    \"stored\": \"our_silly_model\"\n                },\n                \"features\": [titleSearch['query'], overviewSearch['query'], ratingSearch['query']]\n\n            }\n        },\n        \"size\": 3\n\n    }\n    \n    # Search and print results!\n    results = es.search(index='tmdb', doc_type='movie', body=ltrQuery)\n    for result in results['hits']['hits']:\n        if 'title' in result['_source']:\n            print(result['_source']['title'])\n        else:\n            print(\"Movie %s (unknown title)\" % result['_id'])",
      "Taking the model on a test spin…",
      "With this function in place, let’s run some searches! First a simple title search:",
      "runLtrQuery('Forrest Gump')",
      "Forrest Gump\nDead Men Don't Wear Plaid\nManiac Cop",
      "Hey, not too shabby! How long will our luck go, let’s try another one:",
      "runLtrQuery('Robin Hood')",
      "Robin Hood: Men in Tights\nWelcome to Sherwood! The Story of 'The Adventures of Robin Hood'\nRobin Hood",
      "Wow lucky again. It’s almost like these aren’t lucky guesses but rather a prescient author is selecting examples that they know will look good! Ok, now let’s try something closer to home: a Stallone Movie:",
      "runLtrQuery('Rambo')",
      "Rambo III\nRambo\nRambo: First Blood Part II",
      "Err, not bad, but a bit off the mark… Well this is actually a case, like we talked about before, involving the nuance that the linear model can fail to capture. The linear model just knows \"more title score is good!\". But in this case, First Blood should be closer to the top. It was the original Rambo movie! Moreover Rambo III shouldn’t really before just Rambo.",
      "Still, not bad for 40 odd examples of training data!",
      "Let’s try something where we don’t know the title. Like an example from Relevant Search basketball with cartoon aliens. Here we’re grasping at straws. Hoping \"Space Jam\", a movie where Michael Jordan saves the world by playing aliens at basketball, comes up first:",
      "runLtrQuery('basketball with cartoon aliens')",
      "Aliens in the Attic\nAbove the Rim\nMeet Dave",
      "Sadly, Not even close! To be fair,  we didn’t show our training process examples of this use case. Most of our examples were direct, known-title navigational searches. This just goes to show you how it’s important to get a broad set of representative samples across how your users search for learning to rank to work well.",
      "It also continues to demonstrate how the linear model struggles with nuance. Other models like gradient boosting (ala LambdaMART) can grok nuance faster, and aren’t constrained to our boring linear definitions of functions. We’ll see how these models work in future blog posts.",
      "Next up - Solr!",
      "One of my colleagues will be taking Solr learning to rank out for a test spin for you. A simple linear model is very easy to understand, so it’s fun for these little test-spins.",
      "I’d love to hear from you. If you’d like our help evaluating a learning to rank solution for your business, please get in touch! And I’m always eager for feedback on these posts. Please let me know if I can learn a few things from you!",
      "This blog post was created with Jupyter Notebook: View the source!"
    ],
    "summary_t": "This blog post let’s the simple model we derived in a previous post and make it usable via the Elasticsearch Learning to Rank plugin."
  },
  {
    "id": "306416185a32d71c9c8333536c05b210",
    "url_s": "https://opensourceconnections.com/blog/2017/04/29/choosing-proprietary-vs-open-source-search/",
    "title": "When to choose open source search (and when you shouldn't)",
    "content": [
      "In my role as search consultant, I spend a lot of time helping organizations pick the right search technology. Will it be Solr or Elasticsearch? Algolia or Swifttype? Endeca or Marklogic? The choices can seem endless.",
      "In this article, I want to single out Solr and Elasticsearch specifically for consideration. Why? As open source solutions, they share most of the pros/cons compared to proprietary solutions when making a business decision. Your developers may be pushing eagerly to build something on top of them. You may want to stop paying so much money to a search vendor. With this article, I want to prepare you to make an educated purchasing decision when it comes to open source.",
      "Need Out of the Box Simplicity? Choose Proprietary",
      "Proprietary solutions tend to target domains or use cases. For common search problems, they’ll have 80% of what you need. Big E-commerce firms love Endeca, for example. Endeca comes with powerful merchandising tools that allow merchandisers and marketers to fine-tune how products are showcased. Algolia, has found it’s niche in autosuggest, mobile, and typo tolerance. Marklogic works superbly with documents where structure is very amorphous. Google custom search has worked well for web-like document structures.",
      "If you’re coming from Algolia, Endeca, or Marklogic, the switch to Solr or Elasticsearch can be jarring. You’re suddenly down to a bare-bones framework that lacks the domain-specific bells-and-whistles that might have been serving you well. You can shop for supporting tools. You can buy FindTuner as a merchandising platform. Or buy Quepid for relevance tuning. These can be powerful, but don’t always integrate well with how your team has deployed Solr or Elasticsearch.",
      "Specific User Experience? Choose Solr or Elasticsearch",
      "Yes, Solr and Elasticsearch are bare bones, but that’s a conscious choice. You should think of them as programming frameworks for discovery, not solutions. They require you to make lots of your own choices, because, well, your user experience is unique! With great responsibility comes great power. You can do a lot with these tools. You can build recommendation systems or personalized search or integrate machine learning and semantic search to boost relevance. You can mix and remix all these to deliver precisely what your user experience requires.",
      "It’s often the case that your search user experience differs more than you realize. There’s just one way to implement book search, right? Not really, it’s easy to imagine several obvious variants: Library search differs from Amazon’s e-commerce book search which differs from a searching historic books which differs from the in-store kiosk at Barnes and Noble, which differs from a book search for small children (and on and on…). The discovery process differs tremendously in each of these cases, with radically different user goals, relevance expectations, and business incentives. In my experience, most firms don’t appreciate how unique their use case truly is. Solr and Elasticsearch being frameworks giving you the control means you get to make all the careful decisions to build the right experience.",
      "Reducing Total Cost of Ownership: Proprietary Search",
      "When the use case is understood, and you simply want a capability such as \"e-commerce search\", then you’re probably better off with a proprietary solution that targets your use case. Some companies are a slam-dunk for Endeca. So why increase the cost by building out a development team to reinvent the Endeca wheel? Remember open source is only as free as your developer's time. Have you seen the average Solr/Elasticsearch developers salary these days? The value in open source is not reduced cost, it’s flexibility and innovation.",
      "Reduce Total Cost of Ownership with Hosted Solr/Elasticsearch and Consulting",
      "As an aside, there’s two ways to reduce the cost of ownership with Solr and Elasticsearch that get it closer to proprietary solutions. This gives you the advantages of open source search (innovation) while managing one of the big downsides (development cost).",
      "Hosted Solr and Elasticsearch: The majority of firms shouldn’t be spending their developers time building out search infrastructure. Hosted Solr/Elasticsearch platforms like MeasuredSearch, Elastic Cloud, Bonsai, Websolr, OpenSolr, and the like take a great deal of the infrastructure development away. They also offer operational support, freeing your team from 3AM phone calls.\n  Consulting: Ok yes I’m a consultant, so errm maybe I’m biased. But I really do think you can avoid costly mistakes by leveraging some consulting time during planning and implementation. Get a firm with a long history of focusing primarily on Solr and Elasticsearch (like us, Flax, Sematext). They can cross-train your team, augment your staff, help develop an architecture, guide product decisions, and provide expertise in a pinch when something goes wrong.",
      "Search is your unique value proposition? Choose Solr or Elasticsearch",
      "If small increases in search relevance have big impacts on your business, then go where you have the most control: Solr and Elasticsearch. For example, our case study with Careerbuilder we were able to increase job application rate 3% by careful relevance tweaks. This is a big deal to their business! But it required careful, low-level work to get there. The sort of control almost no proprietary solution will give you.",
      "If something that small isn’t a big deal, and a proprietary solution covers your use case, why burden yourself with Solr or Elasticsearch? Why pull your hair out and build a big development team? You shouldn’t do it to reduce costs. If there’s no upside, don’t invest the incredible amount of effort to bootstrap your own search team and technology.",
      "Search needs to happen at big scale? Choose Solr or Elasticsearch",
      "A close cousin to the last point. Perhaps it’s not so much that search is the core/unique value proposition, or that the user experience is unique, but it happens at large scale. Is Apple’s Spotlight search a core value proposition to the company? Well, maybe no. But how many iPhones exist? Supposedly on the order of 1 billion. Servicing that much search requires incredible investments in custom infrastructure. You need open source to intimately understand the fine-grained code-level details to carefully scale and test search. You may be working on your own fork of the project for scalability reasons. Or you may need project committers that can make contributions to the project to improve your scale use case.",
      "In these cases, you can’t easily rely on a proprietary vendor to help you. You really need as much control and insight as possible, which means open source.",
      "Bottom line",
      "So given all these criteria, where do we stand?",
      "I think to sum up, choose a proprietary solution when it is doing a good job of scratching a particular itch and you’re not innovating. Choose open source when (1) you’re doing something unique (2) search is close to your core value or (3) you’re operating at high scale. If you’re not in these situations, don’t give into developer pressure! Don’t choose open source because you want to reduce total cost of ownership, do it because you want to innovate. Do it because the investment can pay off tremendous, outsized divideds!",
      "Let us help you choose!",
      "Get in touch if we can help you a product assessment or trusted advisor consulting. With hundreds of Solr and Elasticsearch projects under our belt, we can find ways to add business value and avoid costly mistakes. And don’t hesitate to ping me directly if I’m missing anything in this article."
    ],
    "summary_t": "In this article, I want to single out Solr and Elasticsearch specifically for consideration. Why? As open source solutions, they share most of the pros/cons ..."
  },
  {
    "id": "8a532041a6dda80186c5f60d7b887712",
    "url_s": "https://opensourceconnections.com/blog/2017/05/16/debugging-solr-function-queries-splainer/",
    "title": "Using Splainer to Debug Complex Solr Function Queries",
    "content": [
      "In complex Solr relevance work, I find I create a fair amount of function queries to boost & balance results. Solr function queries look like Excel queries from hell. Trying to figure out why something that looks like if(recip(ms(query(...))),query(...),sub(..)) isn’t scoring results the way it should can make your eyes bleed. The debug output is pretty opaque, only showing you the full-on calculation, something like:",
      "{\n   \"match\": true,\n   \"value\": 0.042678382,\n   \"description\": \"1.0/(3.16E-11*float(ms(const(1494960667562),date(release_date)=1994-11-18T00:00:00Z))+1.0)\"\n}",
      "If your function query isn’t working, good luck figuring out which part of the calculation was broken.",
      "This debugging pain is why I want to tell you how I debug complex Solr function queries. I use two tricks. First I parameterize my boosts, breaking them into more manageable parts. So if we’re boosting movies by \"release date\" the classic Solr approach could be broken down into a few parts: msIntoPast=ms(NOW,release_date) and then dateBoost=recip($msIntoPast,3.16e-11,1,1) which could then be used as a multiplicative boost: boost=$dateBoost.",
      "Second, with the parameters broken apart, I can use a Splainer feature to compute the value of each parameter per doc. This lets me dig into parts of the calculation to see what went wrong. Splainer just accepts a little hint to compute something as a function query directly – using Solr computed fields aka \"psuedo fields\"",
      "In the screenshot of a movie search below, we’ve broken out parts of the release date boost for each doc and asked Splainer to compute the values of each \"function query lego block\". We use the \"f:\" prefix in \"Displayed Fields\" to refer to each parameter (ie $dateBoost) that Splainer should compute & display:",
      "",
      "Viola, if for some reason the date boost doesn’t seem like it’s being applied correctly, now I can go look more directly at the function query math. It’s been a very handy tool in relevance work I’ve done, helping me very efficiently pinpoint problems in my solution.",
      "That’s basically it. If you have any ideas for Splainer, the code is here – don’t hesitate to let us know about them. And as always get in touch if you’d like to chat!"
    ],
    "summary_t": "Solr function queries look like Excel queries from hell. I want to tell you how I debug complex Solr function queries using Splainer."
  },
  {
    "id": "c5d86882da96c2a9f770c4237b333435",
    "url_s": "https://opensourceconnections.com/blog/2017/06/26/heatmaps-in-elasticsearch/",
    "title": "Heatmaps in Elasticsearch",
    "content": [
      "Heatmap facets are a powerful demonstration of Solr’s geospatial capabilities. Given a corpus of indexed shapes, a heatmap facet will show you where the results of your query fit on a map. This capability has been around since David Smiley’s initial patch was added to Solr 5.1.",
      "Elasticsearch has its own set of geospatial features, but the way it accomplishes heatmap-ish features is by using geohash grids. On the surface these two approaches seem similar, but they are very different at a low level. Geohash queries don’t support fields other than geo_points, so you’re not going to be able to distinguish between hits in irregular shaped areas (like geopolitical borders). In fact, the only ES query that supports the geo_shape type is a geo_shape query, and Elasticsearch doesn’t come with any aggregations over geo_shapes.",
      "Unlike geohash grids, Solr’s heatmap facets use Lucene’s \"recursive prefix tree\" (RPT) that allows quick categorization of whether a given heatmap cell overlaps an indexed shape. It does this recursively to burrow down from parent cells that have hits until it achieves the desired granularity of the heatmap. Cells at any level along the way that don’t have hits are ignored for the rest of the recursion. The end result is a grid of counts that corresponds to how many indexed shapes fit the query area. Solr can then return that as an array of arrays of integers, or graphically as a PNG.",
      "So in order to generate heatmaps of indexed shapes in ES I created an aggregation plugin called elasticsearch-heatmap. Under the covers it uses the same Lucene API to crawl the RPT and collect hits in a particular shard, but it uses ES’s aggregation API instead of Solr’s facets. In fact it takes the same parameters as the facet query except for \"format\" (no PNGs for now). Here’s what that looks like as a full ES query:",
      "{\n    \"query\": {\n        \"match_all\": { }\n    },\n    \"aggs\" : {\n        \"viewport\" : {\n            \"heatmap\" : {\n                \"field\" : \"location\", \n                \"grid_level\" : 4,\n                \"max_cells\" : 100,\n                \"geom\" : {\n                    \"geo_shape\": {\n                        \"location\": {\n                            \"shape\": {\n                                \"type\": \"envelope\",\n                                \"coordinates\" : [[6.0, 53.0], [14.0, 49.0]]\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}",
      "What that says is, make a grid of at most 100 squares (max_cells), covering the given coordinates, and count how many indexed shapes are under those squares. The output looks something like this:",
      "...\n        {\n            \"grid_level\":4,\n            \"rows\":7,\n            \"columns\":6,\n            \"min_x\":6.0,\n            \"min_y\":49.0,\n            \"max_x\":14.0,\n            \"max_y\":53.0,\n            \"counts\": [\n                [0,0,2,1,0,0],\n                [0,0,1,1,0,0],\n                [0,1,1,1,0,0],\n                [0,0,1,1,0,0],\n                [0,0,1,1,0,0],\n                [],\n                []\n            ]\n        }",
      "The output contains the requested grid_level, and instead of repeating max_cells it shows how Lucene interpreted that as a rectangle. In that calculation it’s allowed to shrink the size of the square to eliminate sections that don’t have any hits, so here the number of cells is only 42. After that it lists the coordinates of the bounding box that encompasses the heatmap, and finally the counts. You’ll see that the last row doesn’t contain any hits, nor does the last column. That’s because those cells were included at a more granular level, so they didn’t get eliminated when the bounds were calculated.",
      "That’s a relatively small heatmap over a really small index. In order to test how well it performs over larger indexes I got some help from Jorge Martinez at Piensa Labs. He built an easy-to-follow performance test in a Python notebook that shows how well the heatmap aggregation performs at various index sizes (10K, 20K, 30K, 50K and 100K documents). The results were encouraging: Requests usually returned in less than 250ms. This is comparable to the results I saw with a Rally test suite I built for a 30k document index.",
      "What’s next?",
      "After I offered the code to Elasticsearch as a pull request some issues were brought up in the discussion that make it unlikely this aggregation will become part of the main project (at least in this form). The first issue is with the output format: It’s different than geohash grids so it won’t be compatible with Kibana. Relatedly, you can’t chain another stock aggregation on top of the heatmap. My design goal for the ES heatmap aggregation was to be as close to Solr’s facet as possible so that it could be a drop-in replacement, so I don’t think I’ll be modifying the output.",
      "The other, deeper issue is that Lucene is changing its geospatial API to use what’s known as the \"Points API\". This has been going on for a while, slowly replacing bits and pieces here and there to maintain backward compatibility. But even though a lot of work has been done at the field level, Solr’s heatmap facet still uses the older API. Elasticsearch is making some interesting plans to reshape its geospatial features as well (some were mentioned in the PR discussion), so the concensus was that it’d be better to hold off. When I spoke to David Smiley about heatmap facets last year at Lucene Revolution he too had some interesting ideas, and I hope we can talk about them in a future Search Disco podcast.",
      "In the meantime, try out the heatmap plugin for all your geo_shape heatmap needs, and definitely reach out to us for this or any other search projects!"
    ],
    "summary_t": "Heatmaps over geo_shape fields in Elasticsearch"
  },
  {
    "id": "e457c49e5a6b8055196004931d6b929e",
    "url_s": "https://opensourceconnections.com/blog/2007/05/30/rails-plugin-fixture_references-has-out-of-date-docs/",
    "title": "Rails plugin ‘fixture_references’ has out of date docs",
    "content": [
      "So I shot about 2 hours today trying to figure out why the use of the fixture_references plugin was causing my fixtures to not load, based on a \"Can not convert String to Integer\" error. I mucked around with my .yml files, thinking it was in my formatting. Finally giving up, I took a quick peek at the changelog, and discovered the API had changed, but the documentation hadnt!",
      "In my fixture I was doing what the docs said:",
      "vehicle_make_id: < %= vehicle_makes(:acura)[‘id’] %>",
      "But the API had changed so it should be:",
      "vehicle_make_id: < %= vehicle_makes(:acura, ‘id’) %>",
      "This is why I hate when people use blogs as documentation sites versus wikis.. Blogs are temporal in nature, and dont lend themselves to updates. Hopefully this will spur the developers to update the docs, after all, the plugin is a really nice plugin!",
      "This was a problem with AjaxScaffold as well, but the creator didnt make the same mistake with ActiveScaffold, the successor project."
    ],
    "summary_t": ""
  },
  {
    "id": "ee63b3bd0c74a1822032a33b97da5804",
    "url_s": "https://opensourceconnections.com/blog/2017/07/04/optimizing-user-product-match-economies/",
    "title": "Optimizing User-Product Matching Marketplaces",
    "content": [
      "The key to really understanding search, recommendations, and other ‘product matching’ systems is microeconomics. Say what!?!? No hear me out. In this article, I want to challenge you to rethink search, recommendations, and other \"matching\" user experiences through this lens. Thinking in these terms, you’ll see can solve interesting problems of marketplace dynamics that often occur in these domains AND how undervalued these interactions are in maximizing buyer and seller utility.",
      "Microeconomics is defined, according to Wikipedia as:",
      "a branch of economics that studies the behavior of individuals and firms in making decisions regarding the allocation of scarce resources and the interactions among these individuals and firms.",
      "In other words, a marketplace is setup that optimizes the relationships between buyers and sellers. There’s often a market underlying the search, recommendations, advertising, and other bits of ‘discovery’ functionality. Obviously e-commerce is one such domain. But the same factors come into play in job search, dating, real estate to name a few domains.",
      "You be the judge…",
      "One way to test search (we’ll stick to search for now) is to create what’s called a judgment list – a test set for evaluating a given search solution. You can think of a judgment list as a tuple of a grade, search keywords, and the content being searched. This tuple makes a statement that when searching with <search keyword>, <document> should is <grade> relevant. Here <grade> ranges from 4 (exact match) to 0 (very poor match for a query).",
      "So a common example we might use is a movie search engine. The movie \"First Blood\" is an exact match for the search keyword Rambo. Whereas \"First Daughter\" would be an extremely poor match. So we might model such a set of ‘judgments’ as this tab seperated file",
      "grade,keywords,document\n4\trambo\tfirst blood\n0\trambo\tfirst daughter",
      "Often identifiers stand in for the actual documents and queries. So in reality, you get something closer to:",
      "4\tqid:1\tmovie1234\n0\tqid:1\tmovie5678",
      "Of course (and bear with me economists…), there’s more than one use case to consider. In a movie search, we might also have searches for actors. With keywords \"Ben Affleck\", for example, we might give a 4 for recent/popular Affleck movies, 3 for older ones, maybe 2 for Affleck movies you’ve never heard of, maybe a 1 for movies with Matt Damon (in case you confuse the two) and 0 for movies not even remotely in the neighborhood of Affleck.",
      "We can add \"Ben Affleck\" to our judgment list as query id 2:",
      "4\tqid:1\tmovie1234\n0\tqid:1\tmovie5678\n4\tqid:2\tmovie9499 # The Accountant\n4\tqid:2\tmovie5638 # Batman vs Superman\n3\tqid:2\tmovie5737 # Good Will Hunting\n1\tqid:2\tmovie3727 # Bourne Identity\n0\tqid:2\tmovie5678 # First Daughter",
      "We can develop such a judgment list over hundreds, thousands, and more keywords to evaluate the performance of a search system we implement. Common metrics for evaluating indivual searches include NDCG, ERR, or MAP. Each gives each search a score from 0-1 (1 being best). We won’t get into how each of these metrics works, but needless to say each measures different aspects of the search experience.",
      "What does this have to do with microeconomics?",
      "So what does any of this search stuff have to do with economics? One function of an economist is to create models of economic behavior from what we know about buyers, products, situations, etc. Using those models, we hope to create systems (from government policies to organizing store shelves) that help buyers and sellers get what they want and need out of the market.",
      "For example, a shopkeeper has one storefront window to attract buyers. That shopkeeper must balance the diverse needs of potential shoppers. For example, if this is a fitness clothing store: the shopkeeper might wish to attract both the fashionable and practical. To get at both sets of users, the window display shows both the no-frills cheap running clothes right next to the expensive, fashionable yoga pants in the store window.",
      "My storefront window is a limited resource I must use to satisfy the broadest range of reasonable shoppers. I must try to maximize potential buyer’s utility with each displayed item. Utility is a fuzzy economics term, that basically means maximize the chance a desired buyer will find something that satisfies their want or need. And different buyers have competing needs: we might need to sacrifice the percieved utility of one buyer a little to do an even better job attracting other buyers we want to cater towards.",
      "A search engine is just like our storefront window: It’s another system with limited resources (programmer time, maintenance of relevance rules, screen real estate, available features/data, etc) that is attempting to maximize a broad range of competing searcher’s needs. Our judgment list stands in as a proxy for a representative buyer - giving a prioritized preference list for a user with a specific keyword.",
      "Indeed the only difference between a judgment list for search and one for recommendations is whether a query id or a user id is used. A judgment list with a set of users’ ranked preferences, who don’t use keywords, can be used to evaluate recommendation systems. Keeping that in mind, everything we discuss here applies to both search and recommendations even if say \"search.\"",
      "Using our limited search engine development resources, the closer we get to our users/queries preference list, the higher the chance that user will gain utility from a specific list of search results. Our relevance metrics: NDCG, ERR, etc really stand in for a measurement of this utility.",
      "All of the work we do as relevance engineers, like the classic whack-a-mole game of test driven relevancy, is about trying to maximize the sum of user utility over a group of users. There’s another way to phrase this if you’re mathematically inclined. If user or search queries were on the X axis, and a relevance measure like NDCG the Y axis – our goal is to maximize the integral of NDCG over the users/queries. In other words, trying to maximize the area under this curve:",
      "",
      "As search/recommendation engineers – what we talk about mundanely as \"matching\" is really a fundamental activity in the modern market. It might in fact be more accurate to think of ourselves as quantitative traders optimizing short term and long term utility for users as they interact with a brand. Certainly this means showing them the right products and hoping they’ll engage. But one wonders if the entire user experience can be rethought in terms of ‘whats the information (product, info, advice, otherwise) that can be put in front of this user by this brand to maximize the user’s long term utility’ (and as we’ll soon see the utility for the brand).",
      "The Lambda Insight : maximizing competing utilities with machine learning",
      "To see how this can work, let’s get a bit more technical. If you’re familiar with machine learning, judgment lists begin to look like a training set. As a training set, we can use a representative set of judgments and have a machine learning system learn mathematical relationships between the relevance grades and possible features – properties of documents, users, queries, or context. For example, if we have a feature that’s a title relevance score and another that’s time the movie was released into the past, we might have a judgment list cum training set that looks like:",
      "grade, query, doc,      timeIntoPast, titleScore, ...\n4\tqid:1\tmovie1234   420       0.5      ...\n0\tqid:1\tmovie5678   14        5.1\n4\tqid:2\tmovie9499   1.5       12\n4\tqid:2\tmovie5638   152       5.0\n3\tqid:2\tmovie5737   90        2.5\n1\tqid:2\tmovie3727   50        11\n0\tqid:2\tmovie5678   71        5.9",
      "We can treat the grades in such a judgment list as labels in classification or as scalars in a regression. This is precisely what learning to rank attempts to do.",
      "The only oddity compared to other machine learning problems, is that this training set comes grouped by query id. Most machine learning data sets don’t come this way. Predicting a stock market price by profit, number of employees, and other features just has one row per company. Here, we’re not trying to maximize our likelihood of predicting a 0 or a 4 per-se – but instead trying to maximize the likelihood any given user or query will come closest to ranked set of preferences. From our earlier fashion store example how do we balance the needs of the fashionista high-end fashion shopper who wants to drop big cash on yoga pants with the practical cheapskate who wants to just spend $10 without completely disappointing one or the other?",
      "One way to solve this is just to ignore the problem. Some learning to rank approaches as we discussed earlier just directly try to predict the grades. Just chop out the column with qid and perform regression or classification. In other words many approaches go from the judgment list above to:",
      "grade, doc,   timeIntoPast, titleScore, ...\n4\tmovie1234   420       0.5      ...\n0\tmovie5678   14        5.1\n4\tmovie9499   1.5       12\n4\tmovie5638   152       5.0\n3\tmovie5737   90        2.5\n1\tmovie3727   50        11\n0\tmovie5678   71        5.9",
      "This approach to learning to rank is called point-wise. It’s simple, and it works ok. The problem with point-wise approaches is that by losing the grouping, we’re not really training directly against user utility. During training, a solution that correctly predicts bad search (can successfully give all the 0s 0s) looks just as good as one that always predicts 4s for 4s. We’d clearly rather have the latter. In reality, we’d prefer to be like Santa: doing our best to give every child their first choice. But failing that, maybe we can give some their first choice and others their second. A point-wise approach doesn’t understand that we’re really trying to optimize often competing lists of ranked preferences with a single model.",
      "This is where pair-wise and list-wise learning to rank formulations come in. They flatten the grouped judgment list a bit differently: taking into the account something about how rearranging an item impacts the overall list for a user/keyword. Training so that the NDCG, ERR, or whatever other utility metric is maximized per user directly rather than indirectly through point-wise methods.",
      "One such method is LambdaMART. LambdaMART is a machine learning approach to learning to rank that uses gradient boosted decision trees. But as cool as they sound, gradient boosted decision trees are actually not the most interesting part of LambdaMART. What’s really interesting in LambdaMART is how it solves the flattening problem we just described. In fact, using the LambdaMART creators’ key insight (what I call the lambda insight) could be used to train with any number of machine learning approaches.",
      "But how does the lambda insight work to translate this table",
      "grade, query, doc,  timeIntoPast, titleScore, ...\n4\tqid:1\tmovie1234   420       0.5      ...\n0\tqid:1\tmovie5678   14        5.1\n4\tqid:2\tmovie9499   1.5       12\n4\tqid:2\tmovie5638   152       5.0\n3\tqid:2\tmovie5737   90        2.5\n1\tqid:2\tmovie3727   50        11\n0\tqid:2\tmovie5678   71        5.9",
      "Into \"Ys\" below that optimize user utility?",
      "grade,  doc,      timeIntoPast, titleScore, ...\nY       movie1234   420       0.5      ...\nY\t      movie5678   14        5.1\nY\t      movie9499   1.5       12\nY\t      movie5638   152       5.0\nY\t      movie5737   90        2.5\nY\t      movie3727   50        11\nY\t      movie5678   71        5.9",
      "Lambda ranking works in two stages. I’m going to use the words ‘NDCG’ in the psuedocode below to make it concrete for search folks. In reality though, NDCG can be swapped out for any utility/relevance metric which is one of the amazing things about this algorithm. And this psuedocode is a bit simplified: LambdaMART works iteratively, to constantly improve an existing model. Here though, we just look at the first pass of the algorithm which is the real core of LambdaMART.",
      "# Zero out all the \"Ys\"\nFor all Judgments A...Z\n\tSet A.Y = 0\n \n# Compute new Y's\nFor each grouped query, sorted in best order:\n    For each pair of judgments A and B where grade(A) > grade(B)\n        1. In a temporary list, swap A and B\n        2. Compute the new, swapped NDCG (which will be a worse NDCG)\n        3. deltaNDCG = oldNDCG - newNDCG (aka how much worse does this swap make the NDCG score?)\n        4. A.Y += deltaNDCG\n        5. B.Y -= deltaNDCG",
      "This algorithm experiments with worsening a user/query’s ranked list of preferences. It computes a delta utility/relevance metric (deltaNDCG) to see how bad it would be to swap these two. Sort of like offering alternate choices to a toddler: \"We don’t have string cheese little baby, would you like yogurt instead. Do I get a cute \"no\" or an emphatic, screeching darth-vader-esque emphatic NOOOOOOOOO!!!!\"",
      "Given the magnitude of swapping out my toddler’s favorite food for something else, steps 4 & 5 are where the magic happens. The Ys are updated based on the magnitude of how bad of a swap this would be. The \"Ys\" here are termed by the author of LambaMART as similar to \"forces.\" They’re also called \"lambdas\" (hence the name) or sometimes \"psuedo responses\". If swapping A with most other items in this ranked list of preferences, then A.Y will grow quite large. For example if A is string cheese, and toddler  really wants string cheese such that no alternative will do, A.Y will go up. I might end up using my limited resources to drive to the store to buy string cheese!",
      "The opposite is also true, if swapping B with most other results makes the results worse, B.Y will be similarly pushed heavily down. Here B might be a food my toddler really dislikes; let’s say tomatoes. It might be worth my limited resources to donate all of our tomatoes, even if it risks arguments with people who sort-of like tomatoes in our home.",
      "Getting this table of \"Ys,\" let’s one run really any regression solution to try to model the Ys. The creators of LambdaMART try several, eventually settling on gradient boosted decision trees for their use case.",
      "LambdaMART itself though continues working to use psuedo-responses to model the remaining error in the model. This iteratively improves the model. If the existing model is very off kilter and the NDCG deltas are also very high, the next tree will try to compensate for where the other models failed trying to act as a ‘patch’. This basically describes gradient boosting — trying to predict the error of other weak learners by laying on another learner to compensate. I’ll cover gradient boosting in a different article.",
      "What do you want for Christmas?",
      "The combination of maximizing a ranked list, to maximize NDCG is similar to Santa’s job of minimizing disappointment. Ranked lists are great for maximizing utility across a broad set of people. Because if Santa can’t get you your first gift, he can get you number 2. And if giving you your second preference helps Santa give a hundred other kids their first preference: he’s probably going to take that opportunity.",
      "Santa could use the lambda insight to build a model that maximized most children’s satisfaction with their christmas presents based on Santa’s limited resources. Santa can do his best to get closest to everyone’s wishlist. If Santa was using a point-wise model, he’d not take into account the ranked preferences: randomly give some children their 3rd choice, possibly completely missing the first preferences, and not getting nearly as close to optimizing utility across all the children.",
      "I suppose if Santa did use the the lambda insight, he’d be operating philosophically under utilitarianism. But mapping economic philosophical disagreements to the world of search and recommendation systems is a disagreement for another time.",
      "What I’m excited about is that the intersection of microeconomics and information retrieval opens up a huge area of cross-fertilization. For example, one thing I’m fascinated about is the idea that maybe we can use this system to maximize ANY utility metric. Indeed, the authors of LambdaMART point out this possibility. The lambda insight can be used with really any utility metric that can compute differences between preference lists. Could our grades be replaced with a direct measure of monetary value? Is there a way to compute utility for a user using econometrics measures that are actually better than relevance measures like NDCG and ERR? I actually don’t know, but it’s an area I’m excited to explore. Luckily OSC founder Eric Pugh has a degree in economics…",
      "The industry’s untackled challenge: two-way search-driven markets",
      "So far, we’ve discussed one way markets where only the buyer’s utility is maximized. The poorly kept secret in search is that the shopkeeper isn’t quite Santa. Selfless Santa only cares about maximizing joy of the world’s children. The shopkeeper has to maximize his utility (i.e. maximizing profit) and isn’t just interested in selflessly serving the interests of users. Even more complex are search systems that act as market places. Where multiple sellers hawk their wares (be they products, jobs, dates, or whatever) to a diverse group of buyers. Here not only must you satisfy your buyers, but also balance the needs of sellers who might also be the paying customer of your service.",
      "So now I lull myself to sleep thinking about problems with search and recommendations (everything here applies to recasts as well) and two-way marketplaces. I’ve come up with a few ideas",
      "Lambdas per seller",
      "First, a seller’s preference can be described also by a judgment list, ordered by the product they’d like to get in front of a neutral buyer’s eyes. Perhaps ordered by the most profitable. Going back to our fashion search example, our fashion line would rather sell you overpriced Yoga pants than cheap running shorts. This can also be expressed by a judgment list:",
      "#Fitness Seller:\n4 qid:999 \tproduct1234 # expensive yoga pants\n1 qid:999 \tproduct4214 # cheap running shorts",
      "Now we wouldn’t just layer this directly as another example into our learning to rank solution. As what that would do would maximize the NDCG for \"Fitness Seller\". We want to maximize the NDCG for ‘Fitness Seller’ when Bob is searching with a keyword ‘running shorts’.",
      "So one idea I’d have is to modify the psuedoresponse calculations by incorporating BOTH the user’s preference AND every potential seller’s preference. In other words, we change deltaNDCG to incorporate multiple utility measures:",
      "deltaNDCGSeller1 = (oldNDCGSeller1 - newNDCGSeller1) \n\ndeltaNDCGSellerN = (oldNDCGSellerN - newNDCGSellerN) \ndeltaNDCG = (oldNDCG - newNDCG) + (weightSeller1 * deltaNDCGSeller1) + (weightSeller2 * deltaNDCGSeller2) …\n+ (weightSellerN * deltaNDCGSellerN)",
      "(Of course as before, NDCG can be replaced by any other utility metric)",
      "As these pseudo responses are forces tugging the search results up and down we can calibrate the \"tug\" used during learning to rank. to incorporate both buyer and (perhaps competing) seller needs. This lets us use features of users, keyword queries, products that can predict the balanced utility ordering.",
      "Simple reranking",
      "Another approach could be to simply have competing models. In Elasticsearch and Solr learning to rank, multiple rescoring phases are possible. As it’s unlikely you’d want to completely screw up what will maximize a buyer’s utility, you can instead offer small tie breaks in a rescoring phase that executes a seller-biased model (or set of models). Perhaps it could incorporate all the seller’s lambda’s in the previous section.",
      "These two ideas are my initial forray, but I’m interested in hearing from you on ideas you might have.",
      "Get in touch!",
      "That’s it for now. Do get in touch with us if we can work with you on a challenging search strategy, relevance or learning to rank problem. We’d love to hear from you!"
    ],
    "summary_t": "We explore the parallels between search, recommendation systems and microeconomics. Proposing a new way to think of search relevance and recsys as a system o..."
  },
  {
    "id": "ced415167cdf14deae292c49b7703155",
    "url_s": "https://opensourceconnections.com/blog/2017/07/05/infrastructure_for_ltr/",
    "title": "Is Your Infrastructure Ready For Learning To Rank?",
    "content": [
      "In previous articles, we introduced Learning to Rank (LTR) for Elasticsearch 1, 2, which uses machine learning to re-score the best documents in a query.  On the surface, it offers a tempting off-the-shelf solution to a chronically hard problem.  But is your infrastructure ready?",
      "Machine learning is fed on data. The learning algorithm is applied to a set of training data, which is used to construct a model.  The model is then used to evaluate new data.  Once in production, user interactions can be leveraged to evaluate and iterate the model.  In order to leverage the value of LTR, the infrastructure must be in place to collect and utilize data. This article will examine the infrastructure needs for a learning-to-rank project.",
      "Step 1: Event Logs",
      "Applied machine learning is fundamentally a data analysis problem.  The success of a machine learning project, then, will be heavily dependent on the quality and quantity of the data available.",
      "In the case of learning to rank, there are three basic questions:",
      "What did users search for?\n  What documents were provided for each search?\n  Were the documents provided good or bad for those keywords?",
      "The answer to the first question is readily available on most server logs as API traffic: what endpoint was hit, with what parameters?  The second will require application-level or orchestration level to record the IDs of the documents provided.  The level of difficulty there is fundamentally a question of the infrastructure available.  Evaluating the suitability of the provided documents for a given query based on user interactions is perhaps as much an art as a science.",
      "Consider the case of an online shopping experience at an online furniture store looking to purchase a bookshelf:",
      "Users searched for a bookshelf\n  Users clicked on one set of bookshelves among many returned.\n  Users continued interacting with the product page.  Perhaps they read a review or checked the dimensions.\n  Users put the bookshelf in the shopping cart\n  Users actually purchased the bookshelf.",
      "In this case, there is a clear, easily measured progression of user interest based on interactions.   Each of those interactions can be logged.  But in order to be used for LTR,  those interactions must be tied back to a given initial search.",
      "In cases where the user’s interactions are not transaction-based, identifying appropriate conversion metrics can be much harder.",
      "Google Analytics is a common platform for collecting data about broader user interactions: In a previous article, Doug looked at Google Analytics in the context of relevancy in general 3.  This can be a great platform for identifying common queries, and metrics like page views, time on page, and bounce rate can tell you a lot about your user experience in general for common queries.  It may or may not be possible to apply these metrics back to individual search results.",
      "Why does tying user events back to a search query matter?",
      "In order to understand why tying user events back to an initial search query matters, let’s take a quick review of what goes into an LTR implementation.",
      "Generating LTR model requires a judgement list: a mapping file which numerically indicates how relevant a given document is for a given query.   Consider our online furniture store, which has several products:\nsmall bookshelf, medium bookshelf, large bookshelf, kitchen table, end table, sofa,",
      "With a very small data set, we can manually construct simple binary judgement mappings: a unit is appropriate (1) or not (0)",
      "Query\n      Product\n      Judgement\n    \n    \n      bookshelf\n      Large bookshelf\n      1\n    \n    \n      bookshelf\n      Medium bookshelf\n      1\n    \n    \n      bookshelf\n      Small bookshelf\n      1\n    \n    \n      bookshelf\n      Kitchen table\n      0\n    \n    \n      bookshelf\n      End Table\n      0\n    \n    \n      bookshelf\n      Sofa\n      0",
      "In most real-world scenarios, however, both the range of user queries and the set of products available will be much too large to construct by hand.   These judgements will be constructed by mapping user events back to the initial source queries.",
      "Step 2: Historical Features Values",
      "Judgement lists only tell part of the story required for training an LTR model.  In order for the model to score documents for new queries or documents, it is necessary to abstract both the query and the document to a feature set.",
      "A feature is a representation of some aspect of the content being searched 4.  In elasticsearch, a feature is a numerical attribute, such as a query score, function score, or even a numerical document value like recency or popularity.   Let’s consider the online furniture store from the previous example, with two features: \"score title\", and \"popularity\".  Let’s let score title be the  frequency of the query term in the title, and popularity the percentage of sales of this item in the past month.",
      "Query\n      Product\n      Title Score\n      Popularity\n      Judgement\n    \n    \n      bookshelf\n      Large bookshelf\n      1\n      .1\n      1\n    \n    \n      bookshelf\n      Medium bookshelf\n      1\n      .3\n      1\n    \n    \n      bookshelf\n      Small bookshelf\n      1\n      .15\n      1\n    \n    \n      bookshelf\n      Kitchen table\n      0\n      .1\n      0\n    \n    \n      bookshelf\n      End Table\n      0\n      .3\n      0\n    \n    \n      bookshelf\n      Sofa\n      0\n      .05\n      0",
      "Selecting the best features is a hard problem - that may be what has lead you to investigate learning to rank in the first place.  In order to train your model, it is necessary to have feature values for each document returned from a user search.  Logging/gathering feature values for model training creates a bit of a bootstrapping problem for your infrastructure, and how to solve it depends a bit on how you plan to train your model.",
      "Considerations with Training",
      "Learning algorithms which process large training sets at once are called batch or offline methods. Other systems, which process examples one at a time are called online methods. 5  As with many real-world implementations, the learning to rank can use either or both scenarios.",
      "Offline training requires some infrastructure beyond the event logging requirements discussed above.  It must be possible to calculate or estimate feature values for the documents included in the training set.  What this means depends a bit on what your features are.  If your features are Elasticsearch queries, then the historical documents need to be indexed to calculate those query scores.  If your training data is a superset of live data, or if your training data set is very large, that may require additional capacity for your Elasticsearch cluster.",
      "This leaves a great deal of flexibility:",
      "Organizations potentially have a lot more training data to work with, because they can use historical data\n  As was already mentioned, selecting the right feature set can be challenging.  By not requiring feature values to be logged with the historical data, offline training makes it easier to hypothesize and test the effects of different features.",
      "There are also some risks implicit to using historical data for training:",
      "The value of a feature at the time a historical query was made might be different than the value of that feature \"right now\".  For example, consider document recency as a potential document feature.   If a document is posted on day 0, a user queries on day 2, that document on day 2 was pretty new.   When collecting features on day 30 the current freshness value might be 30 days, but the value used in training should be 2 days.\n  This same principle applies to text-based similarity scores.  In a lucene-based search engine, basic similarity scores are dependent on document frequency.  If the collection is changing frequently, the similarity score for the same document can and will change over time.    A rare term today might be common next week, and rare again in a month.",
      "What’s Next and Get In Touch!",
      "This article is part of a series on learning to rank.  Topics include:",
      "Basics: more about what learning to rank is exactly\n  Applications: Using learning to rank for search, recommendation systems, personalization and beyond\n  Models: What are the prevalant models? What considerations play in selecting a model?\n  Considerations: What technical and non-technical considerations come into play with Learning to Rank?\nIf you think you’d like to discuss how your search application can benefit from learning to rank, please get in touch. We’re also always on the hunt for collaborators or for more folks to beat up our work in real production systems. So give it a go and send us feedback!"
    ],
    "summary_t": "The success of a machine learning project will be dependent on the quality and quantity of the data available."
  },
  {
    "id": "4e752ce0056edac431fa7091d196c4ad",
    "url_s": "https://opensourceconnections.com/blog/2017/07/10/caching_in_elasticsearch/",
    "title": "Caching In Elasticsearch",
    "content": [
      "Responding to queries takes CPU time, memory, and in unfortunate cases, wall time as well.  Increasing the power of a cluster helps, over-provisioning can be very expensive.  Caching is one of the first tools pulled out of the optimization box.   While older versions of Elastisearch cached everything cachable, newer versions can be quite selective by default.  So what caching does Elasticsearch support, and what is the best way to take advantage of it?",
      "Elasticsearch supports three kinds of caches: the node query cache, the shard request cache, and the field data cache.",
      "Node Query Cache",
      "The node cache is LRU cache shared by all shards on a node.  It caches the results of queries being used in a filter context, and in previous versions of Elasticsearch, was called the filter cache for this reason.  Clauses in a filter context are used to include (or exclude) documents from the result set, but do not contribute to scoring.  Furthermore, Elasticsearch observed that many filters are quite fast to compute, particularly for small segments, and others are rare.  To reduce churn, the node cache will only include filters that:\nHave been used multiple times in the last 256 queries\nBelong to segments holding more than 10,000 documents (or 3% of the total documents, whichever is larger).",
      "Shard Request Cache",
      "The shard level request cache caches query results independently for each shard, also using LRU eviction.  By default, the request cache also limits clauses:\nBy default, only requests of size 0 such as aggregations, counts and suggestions will be cached.  If you think a query should be cached, add the \"request_cache=true\" flag to the request.\nNot all clauses will be cached.  DateTime clauses containing \"now\" will not be cached; \nThe shard request cache is invalidated each time the shard is updated.  This can lead to poor performance in a frequently-updated index.",
      "Field Data Cache",
      "When Elasticsearch computes aggregations on a field, it loads all the field values into memory.  For this reason, computing aggregations in Elastisearch can be one of the most expensive operations on a query.   The field data cache holds the field values while computing aggregations.  While Elasticsearch does not track hit/miss rates, it is recommended to set this large enough to hold all the values in memory for a field.",
      "Monitoring Caching",
      "A number of integrations are available for monitoring Elasticsearch.  Sematext and Datadog being some of the more common ones.  But what if you just need to spot-check during development?",
      "Elasticsearch gives a number of ways to check the cache utilization, but I like the _cat nodes API in this case, because it will give all of the above in one call:",
      "GET _cat/nodes?v&h=id,queryCacheMemory,queryCacheEvictions,requestCacheMemory,requestCacheHitCount,requestCacheMissCount,flushTotal,flushTotalTime",
      "Implications",
      "Separate aggregations from ‘normal’ query processing.  This seems counter-intuitive: why compute something twice, or make two round-trip calls for something that stems from the same clauses?  In practice, aggregations will be more cacheable.  In addition, this will keep aggregations from being re-calculated as users page through data.\n  Distinguish filters from match clauses. Common filters are highly cacheable, and quick to compute; scoring is more  expensive, and difficult to cache.  So caching a more general filter, then scoring a more precise subset is redundant, but helpful.\n  Use reusable filters to narrow the result set before scoring. Similarly, use scripted fields for scoring but not filters.\n  Filters are executed more-or-less in order. ES does some query re-writing, but in general, put the cheap filters first and more expensive filters second.\n  If you must filter by timestamp, use a coarse granularity so the query value changes infrequently.",
      "Further Reading",
      "Elasticsearch’s caching overview\n  This article has a nice overview on using filters for performance optimization.\n  This has some more general speed-ups.\n  This is an older article with some good tips, particularly for new clusters.\n  This has a nice deep-dive into optimizing your aggregations."
    ],
    "summary_t": "What caching does Elasticsearch support, and what is the best way to take advantage of it?"
  },
  {
    "id": "1069514ab8e60e545054257a8b786614",
    "url_s": "https://opensourceconnections.com/blog/2017/07/25/three-steps-to-take-before-replacing-enterprise-search/",
    "title": "Three Steps To Take Before Replacing Your Enterprise Search Solution",
    "content": [
      "Replacing an enterprise search product?",
      "Use this three-step process to create actionable requirements for your team.",
      "The inherent challenge in replacing existing enterprise products is that their functionality and attributes are already cemented into the consciousness and patterning of hundreds or thousands of users. Even when the legacy software is not well designed–and it never is, right?!–it is useful in a significant way that can not be understated.",
      "So where do we start in order to exert meaningful change? Specifically, change that can be documented early in the process and change that inspires key stakeholders to continue with the project.",
      "It’s standard practice to define and prioritize the issues. In that we like to leverage agile methodology to understand and then prioritize the task at hand, user groups are transformed into personas with critical needs and requirements. Yet as an enterprise search product, the software already performs numerous actions for many different user groups. User actions, therefore, are likely to fit within multiple personas. If deciphering the use cases were not challenge enough, combine that with having to appease numerous key stakeholders and it’s clear why it can be quite difficult to find the starting line.",
      "To hold the metaphor above, if requirements are akin to starting the race, the process outlined below is similar to paying your entry free, showing up and lacing your sneakers. Proper preparation will insure that your prioritized business requirements will provide the best kick-off point and, if you’ve really done your work, they will act as guides throughout the build process.",
      "Our three step process for project preparation success:",
      "",
      "Discover",
      "discovery - I can’t stress this enough. The failure of too many projects to count can most often be traced back to a lack of discovery. What is discovery? For us, it’s approaching a project as if we have zero domain knowledge, and then asking the questions–and digesting the answers–that allow us, by the end of the discovery period, to discuss the domain like an old pro. Sure, we know search best practices–that’s why they hired us!–but there are instances where a best practice may be exactly the opposite of what solves the business need. This is why a proper discovery period, one that lasts for a minimum of 2 weeks and as many as 4 weeks, is a preparation requirement.",
      "Pro-tip: Insure that you have access to a product owner who is intimately familiar with the product and users. Ideally this owner has been identified prior to starting discovery. This relationship is mission critical. And don’t even think about a short discovery timetable if you won’t have access to this type of product owner.",
      "Translate",
      "translation - Take the problems that you’ve surfaced and bring them forward as support for your deep understanding of the product. Problems are often more than items that need fixing. They are gateways to creating a deep connection with the users so that you can solve their real problems. As outsiders, we are destined to fail if we are unable to pinpoint both the highlights and shortcomings of the legacy product. But that is not enough. We have been tasked with building a replacement, so we already have the challenge of bringing change (which is always initially resisted!). Translation is the process of speaking the same language as the owners and users. Effective translation creates common ground through clarity, it creates a space to work within that will be co-owned by both you and your client.",
      "Pro-tip: It’s too easy to think you can rush through this step. The translation deliverable is the beginning of your development roadmap, so budget time for it.",
      "Refine",
      "refinement - Propose solutions that are manageable and that DO NOT partially solve each persona’s problem. Instead, refine your information to fully solve one major problem. Prioritize your other refined solutions, but present them in order of importance and propose one solution first. You must gain traction with your client; regardless of the agreed upon scope, people want to see tangible progress. Yes, this concept is ingrained in an agile approach to development, yet we have found that it is too easily missed within enterprise projects. The ability to refine tasks, earn stakeholder approval and then hold to the prioritized list is a game-changer when it comes to long-term client happiness. To be able to point to tangible successes during an extended engagement is critical. While at the outset it might not earn you buy-in with the entire organization, it will create a strong foothold with a critical segment.",
      "Pro-tip: If you don’t work in segments, you are destined to fail. The legacy product simply has too much ingrained inertia. Segments must be thought of in two related ways: segments of the client’s entire team (personas) and segments of the entire solution. A distinct persona may have four or a dozen issues. Map the path to solving one key issue for the important persona and you’ll be well on your way to a positive long-term relationship.",
      "Do Not Pass Go",
      "The client may want you to \"hit the ground running.\" A couple of preliminary meetings have created rapport between you and the client and they believe you \"understand the issues.\" This is a trap. It’s a trap that is not set intentionally but will undermine a project. Do your diligence. Require it, in fact. Show your client that creating space in their budget for analytics is vital to project success. Better analytics is the foundation for actionable requirements. And actionable requirements guide successful search implementations.  You owe it to your team and your client to take these steps."
    ],
    "summary_t": "Replacing an enterprise search product? Use this three-step process to create actionable requirements for your team before you start."
  },
  {
    "id": "04fabbadf552c03eaa3bde8fa5c02a4e",
    "url_s": "https://opensourceconnections.com/blog/2017/08/03/search-as-machine-learning-prob/",
    "title": "How is search different than other machine learning problems?",
    "content": [
      "Learning to Rank turns search into a machine learning problem. In this article, I want to explore what makes search distinct from other machine learning problems. How does one approach search ranking as a machine learning problem vs say classification and regression? We’ll go through a tour of a couple approaches, that will give you an intuition on how to evaluate a learning to rank method.",
      "Measuring Success! (And failure) of search",
      "The fundamental difference comes down to the goals of search versus classic machine learning problems. Or more specifically, how each quantitatively measures success. The accuracy of a stock price prediction system, for example, depends on how far, on average, our predictions are from real company stock prices. If we predicted Amazon to be at $123.57, but in reality it’s $125, we’d say that’s pretty close. If on average, our prediction stayed under $1-$2 error from the actual stock price, we’d probably conclude our system worked well for most needs.",
      "This \"error\" in this case is really what we’d call a residual: the difference between the actual value and a predicted one: actual - predicted. (In reality residual^2 is what’s actually minimized, but we’ll keep it simple here.)",
      "During training time, a regression system uses how we quantify success (or failure) find an optimal solution. We can try different quantitative features of companies, such as number of employees, revenue, cash on hand, or whatever else we think will help to minimize the average error in our stock prices. Naively, it might turn out least-squares regression \"learns\" that the linear formula like stockPrice = 0.01 * numEmployees + 0.9 * revenue + 0.001 * cashOnHand does the best job minimizing the error.",
      "Whether search is good or bad (or somewhere in between) looks quite different. Whereas stock prices are completely about minimizing actual - predicted, search systems try to approximate an ideal ordering for a search query. In other words, the goal is to minimize how \"far\" a list of items is from an ideal, prioritized ordering that reflects how a human searcher perceives good/bad search.",
      "For example, if a user of an e-commerce catalog searches for \"dress shoes\", we might define a rough sense of the following ideal ordering:",
      "Best-selling dress shoes\n  Low-performing dress shoes\n  Other kinds of shoes\n  Ladies dresses (if at all)",
      "With this ideal in mind we can imagine some scenarios and guesstimate how far they are from the ideal. Showing our \"dress shoes\" searcher some kind of shoes before dresses is at least something — but not at all close to perfect. It’d be like predicting Amazon’s price at $150 instead of $125:",
      "Close-ish?",
      "A pair of tennis shoes\n  Meh Dress Shoes\n  A ladies dress",
      "On the other hand, showing the meh dress shoes one position before the best-selling dress shoes before other shoes, etc might be very close, but not exactly the same. This might be comparable to predicting Amazon’s stock price at $123:",
      "Meh pair of dress shoes\n  Best-selling pair of dress shoes\n  Tennis Shoes",
      "As you can see with search, instead of actual - predicted it’s all about getting as close as possible to the best ordering for each user query. NDCG is one such metric that measures how far an ordered set of search results is from an ideal ranking. Other metrics exist that try to measure the \"goodness\" of search results with their own pros & cons. These metrics almost always go from 0 (worst possible search results) - 1 (perfect search results).",
      "Further, these metrics can’t just be a pure diff or edit distance type calculation. As users look more at the top search results, getting these first few right should have priority.  For this reason search metrics often include a position bias that means when the first several results deviate from the ideal, search is considered much worse than if a bottom result deviates. NDCG has a position bias built in.",
      "While there’s a bunch of metrics available (such as ERR, MAP, etc), I’ll just say \"NDCG\" in this article as a shorthand for really any relevance metric.",
      "Producing a ranking function with Machine Learning",
      "A classic regression problem constructs a function, f, that makes direct predictions using a set of features. We try to minimize actualStockPrice - f(companyFeatures). The goal of learning to rank is to construct a ranking function that doesn’t make direct predictions. Instead it’s used for sorting – sorting a set of ideal orderings we’ve been provided as training data. The ranking function we learn takes two inputs, the query and the document, and assigns a score that correctly orders documents for a query.",
      "Restating the above a bit more mathy:",
      "Stock Market: \nFor company x, Produce f(x) that minimizes y - f(x)\n  \n  \n    Search:\nFor document d, query q, Produce f(d,q) that maximizes NDCG of all documents when sorted by f(d,q) descending",
      "Let’s walk through a rough example to see what we mean. As data scientists / search engineers we might suspect the following features of queries/documents could be helpful in our e-commerce catalog:",
      "The TF*IDF score of a keyword in a product’s title — titleScore(d,q)\n  The TF*IDF score of a keyword in a product’s description — descScore(d,q)\n  The number of items sold numSold(d)",
      "A machine learning process might arrive at a document scoring formula like:",
      "f(d,q) = 10 * titleScore(d,q) + 2 * descScore(d,q) + 5 * numSold(d)",
      "That maximizes NDCG across a set of test queries (that is it comes close as possible to user’s ideal ordering as we can get with this kind of model)",
      "Most of the complication in learning to rank usually comes from reformulating the problem so that other machine learning methods can be applied. These tend to come in three categories: point-wise, pair-wise, and list-wise problems which we’ll cover next. We’ll take a brief tour of  a couple of methods, and stop and reflect on the pros and cons of each.",
      "Point wise learning to rank",
      "In search, the \"training sets\" look sort-of like normal machine learning training sets. Point-wise learning to rank uses this fact, so let’s review briefly what a search training set looks like. Search training sets come in the form of graded documents for a query known as judgment lists. Here’s an example of what a judgment list might look like.",
      "Grade,query,document\n4,dress shoes,best_selling_dress_shoes\n3,dress shoes,meh_dress_shoes\n…\n0,dress shoes,ladies_dress\n0,dress shoes,toga_item",
      "As you can see, documents exactly relevant, such as best_selling_dress_shoes are given a grade of 4, while completely irrelevant documents (dresses and random togas) are graded with 0.",
      "Point-wise learning to rank just gives up caring about optimizing per-query ranking directly. Instead, we just try to predict the relevance grade. We use some kind of regression to create a ranking function f(d,q) for document d, query q. Just like the stock price example, we attempt to minimize the residual. We hope we get a 0 for f(toga_item, \"dress shoes\") and a 4 for f(best_selling_dress_shoes, \"dress shoes\").",
      "During training and evaluation our simple error residual y - f(d,q) is minimized (y here being the grade for d,q). In this case suppose our ranking function, f, gives our grade 0 document a score 0.12 and our grade 4 a 3.65. That appears to be doing very well just looking at residuals. If on average our document scores don’t stray on average more than 0.5 away from their grade, then we can hope that per-query NDCG is also being maximized. That is, we hope if we can create a ranking function that just spits out the grades, we should get close to the ideal ordering our users expect.",
      "But appearances can be deceiving. One problem with point-wise learning to rank is that getting the ‘top items’ ordered correctly is usually more important than obscure items on the tail of the judgment list. Basically all the knowledge & position bias inherent in maximizing a metric like NDCG is being ignored.",
      "In practical terms, a model that regularly swaps exactly relevant and meh items, but can accurately predict lower value relevance grades that end up on page 50 isn’t that great. Buyers see marginally relevant items in the first few results and aren’t impressed by the quality of the selections. So they move on.",
      "Even more catastrophic, another problem arises when you consider relationships that only happen within a specific queries. Point-wise approaches wash away the query grouping, ignoring these within-query nuances. For example, one query correlates strongly with the relevance score on a title field while another more with a description field score. Or perhaps a \"good\" title match for one query is a 5, but a \"good\" title match for another query is say a 15. These scenarios aren’t hypothetical: inconsistent document frequency of different matches can create these scenarios. You can learn more in this blog post on linear learning to rank.",
      "So in general, point-wise approaches don’t perform terribly well, and we’ll move on to looking at approaches that don’t wash away the query grouping and instead attempt to directly optimize each query’s ordering with a ranking function.",
      "List-wise, pair-wise, lions, tigers, bears, oh my",
      "Point-wise learning to rank works to minimize the difference between an ideal and actual relevance grade. Other methods define a different sense of error that comes closer to directly optimizing each query’s ideal ordering. Let’s look at two examples of a list-wise and pair-wise learning to rank solution that come closer to incorporating position bias and an ability to capture per-query nuances.",
      "Optimizing the list directly w/ ListNet",
      "List-wise learning feels like the purest form of learning to rank. It defines the error very directly: How far is the current ranking function’s list is from the ideal? One such method, for example, is to look at the permutation probability of a given ordering.",
      "The underlying idea is to define a function that computes the probability that a given permutation of relevance scores is actually what a searching user is looking for. If we think of \"scores\" as grades from the judgment list, the ordering where the 1st results grade is higher than the 2nd, and so on would receive the highest probability. However, it’s possible that even our relevance grades from our judgment list aren’t correct for this user in this place, time, and context. So a single lower scoring item shuffling above a higher scoring item is treated as slightly less likely than perfect relevance sort order — Maybe that’s actually what the user here/now wants! A completely reordered list with the lowest relevance score first seems extremely unlikely, with a permutation probability approaching zero.",
      "Instead of having to compute an error that must look at every possible list ordering, it’s more computationally feasible to approximate permutation priority by only looking at the probability that the first item in the permutation is \"best\" for a search. This is called the \"Top One\" probability. It looks at a single relevance score alongside every other relevance score for a query to compute the probability that item would be first. As you’d expect, higher graded/scoring relevance items would receive a higher top one probability and lower graded items less likely to be the right \"top one\" for this user, context, place, and time.",
      "One List Wise Method ListNet proposes to minimize the cross-entropy between the relevance grades from the training set and weights in a neural network. Sounds like a lot of mumbo jumbo, but what’s really important is to work through the error function that’s being minimized to develop an intuition for how it works, as shown in psuedo-python below:",
      "def error():\n  error = 0\n  For each query\n     For each doc:\n         error -= TopOneP(doc.grade) * log( TopOneP(f(doc, query)))",
      "Here f here is a ranking function we’re optimizing. TopOneP is the \"top one\" probability given the score or grade.",
      "First, let’s look at the first term: TopOneP(doc.grade). If you think about what happens here, if the \"top one\" probability based on the judgment list grade is high, then the second term log(P… will be weighted more heavily. In other words, the first term can be thought of as a priority of the item based on the grade in the judgment list. Higher graded items better have a more accurate top one probability: exactly the position bias we’re looking for!",
      "Now look at the second, log( TopOneP...) term. Recall, log(TopOneP=1) is 0, and log(TopOneP<1) is increasingly negative as TopOneP approaches 0. So if TopOneP gets close to 0, and thus the log increasingly negative, then the entire term gets increasingly negative. More negative is bad, because error-=(big negative number) makes error get more positive.",
      "Taken together, more error is added when (1) the item is important in the judgment list (TopOneP(doc.grade) is high) and when the TopOneP of our ranking function f is really low. Clearly this is bad: something that really needs to be towards the top based on the judgments, really should have f sorting it to the top.",
      "The goal in ListNet is to minimize error by iteratively updating weights in f. I won’t get into that here; what’s more important is the intuition above. You can read more about it and how it uses this definition of error to calculate a gradient (how to change the weights of features) to minimize the error in the paper.",
      "Pair-wise optimization with RankSVM",
      "Pair wise learning to rank works by minimizing the number of out of order results in the search results. A specific metric, Kendall’s Tau measures the proportion of in-order pairs in a search solution. One form of pair-wise learning to rank classifies what makes an item \"in order\" vs \"out of order\" for a query. You might learn, for example, that a bit more title score, but a bit lower total number of sold matters when ordering a particular set of queries.",
      "One popular pair-wise learning to rank approach is RankSVM as introduced in this paper by Thorsten Joachims and also demonstrated in Python in Fabian Pedregrosa’s great blog post who kindly lent me his images for this post:).",
      "Imagine a case where we have two queries. Perhaps our \"dress shoes\" query, but also one for \"t-shirt\". The graph below has the y-axis as, say, the title score feature while the x-axis might be the number of products sold. We’ve plotted each judgment for \"dress shoes\" as a triangle, and each judgment for \"t-shirt\" in circles below. Increasingly \"dark\" shapes are considered more relevant.",
      "",
      "Our goal is to discover a function similar to the w vector in the image above that comes closest to correctly ordering the search results.",
      "It turns out that this task is the same as classifying what makes one item better than another item — a binary classification task appropriate for a Support Vector Machine. If dress_shoes is better than meh_dress_shoes — we can label that ordering as a \"better than\", a \"1\". Similarly, the ordering meh_dress_shoes before dress_shoes is a \"worst than\" relation or \"-1\". We can then create a classifier to predict when one item might be better than another.",
      "This classification task requires a bit more fenegling of the underlying feature values. Think of what \"better than\" means. It means something is different between dress_shoes and meh_dress_shoes that classifies it as better. So RankSVM performs a classification on this delta: itemA’s features minus itemB’s features.",
      "As an example, just focused on the single feature \"numSold\" we might see dress_shoes sells 5000 while meh_dress_shoes sells a mere 1000. So the resulting \"difference\" of 4000 would be one feature used during training. We’d take this difference as an SVM training sample: (1,4000). Here 1 is the \"better than\" label we’re predicting and 4000 is the numSold delta we’re using as a feature in the SVM.",
      "Plotting each pairwise difference in the feature space creates two classes, as shown below. An SVM can be used to find the appropriate decision boundary between the two classes:",
      "",
      "Of course, we don’t need a decision boundary. We need a directional vector which says \"more relevant\" in this direction. So finally, the vector orthogonal to this decision boundary provides linear weights in proportion to the ranking function:",
      "",
      "This sounds like it turns into simple linear regression, after all we’re just arriving at feature weights and a linear ranking function. But if you recall from the point-wise discussion, within a single query you sometimes have within-query dependencies/nuances. Cases where a good title score in one feature is say \"5\" for \"dress shoes\" compared to \"15\" for \"t-shirts.\" What RankSVM does, by focussing classifying deltas within a single query is to point out a rough sense of \"more title score, corresponds to more relevance within a query\".",
      "Graphically, you can see that here, with the same data points above being run using linear regression:",
      "",
      "RankSVM vs List-Wise Approaches",
      "You can see though that RankSVM appears to still create a direct, linear sense of relevance. We know of course reality is often non-linear. With an SVM, you can use non-linear kernels. Though a linear kernel tends to be the most popular. Another drawback with RankSVM is that it looks at pair-wise difference with no consideration of position bias. When items get out of order in RankSVM, there’s no way to prioritize keeping the top items correct while ignoring that the bottom items are inaccurate.",
      "While list-wise approaches tend to be more accurate, and incorporate a position bias, training and evaluating them tends to be computationally expensive. And while RankSVM tends to be less accurate, the model is simple to train and use.",
      "Due to its simplicity, RankSVM can build models for specific users or subsets of queries/users rather easily. One can imagine classifying queries into different use cases. Perhaps for e-commerce there are queries we can definitively say are typos. While others we know are queries for broad category searches (like \"shoes\"). If we classify queries accordingly, we can build models for each type of use case. We can even combine different SVM models. For example, since the model is a set of linear weights, we could take a model tied to a specific user, Tom, and sum it with the model tied to the kind of query Tom is executing, say \"dress shoes\" to show me the dress shoes we feel Tom will likely enjoy.",
      "The takeaway!",
      "The main takeaway is whatever model you choose, you have to understand what the model optimizes for. What error is it trying to minimize?",
      "You see how Point-wise approaches optimize for judgment residuals, and how that can be less than ideal. RankSVM performs a simpler optimization to eliminate out-of-order pairs, but this too has problems as there’s no consideration for position bias. ListNet’s permutation probability and top one probability are interesting by giving leeway to equally valid good answers. I wonder personally if this approach can be used to diversify search results to show many valid hypothesis for this user here and now.",
      "Of course, at the end of the day, the type of model may not matter so much if we don’t select the right features to train our model on! Feature selection, creation, generation, and engineering, not model selection, is often the hard part!",
      "Hope you enjoyed the tour",
      "Hopefully this blog post gives you an intuition for different methods for machine learning. And maybe it turned into a tour of different approaches :). As always, I’m open to feedback in case I got something wrong. Don’t hesitate to get in touch and I’d love to chat!"
    ],
    "summary_t": "In this article, I want to explore what makes search distinct from classic machine learning problems by giving you a tour of some different approaches."
  },
  {
    "id": "7f6e87bd4850c0a4ec76ccbe99a680fb",
    "url_s": "https://opensourceconnections.com/blog/2017/08/07/search-management-minefield-open-source-search/",
    "title": "Solving The Search Management Minefield in Open Source Search",
    "content": [
      "A common theme in software development is the tension between:",
      "Fixing painful bugs now!!\n  Creating tools & practices that let us fix bugs faster (though at some theoretical time in the future)",
      "The same is true in open source search. Item (1) \"Fix bugs NOW!!!\" can be the cry of the search sponsor. So for a very technical search team, the work begins. The techies scramble to try and fix search complaints. They find it frustrating and counterproductive. Feedback is inconsistent, with different answers seemingly coming from analytics, experts, and bosses. Much of the time testing is manual – a hopeless task with millions of search terms. The techies don’t have insight into the domain/business to resolve many fine-grain technical decisions. Even more disastrous, the technical changes tend to have dramatic and broad impact. If you’re not extremely careful, these changes damage the business by creating more bugs. Fixing everything with technical solutions can be a bit like getting a bulldozer to dig a few holes to plant flowers.",
      "Ironically, while Solr / Elasticsearch are open source tools, they are more opaque to the business than targeted proprietary search engines that \"speak the language\" of a domain or business. How many e-commerce store managers miss the Endeca experience, only to be poo-pooed by their technical team as not being with the latest tech? Technical teams, meanwhile, burdened with fixing search tend to move to geekier solutions than what’s actually needed that make it even more obscure to tune search for a domain, not realizing search is getting even further from those who know what’s relevant. We need to moving in the opposite direction: making search more manageable by the non-technical team. I congratulate vendors like HawkSearch, Algolia, and AlphaSense for being more targeted at helping a specific type of domain/user rather than promising broad silver bullets.",
      "Sometimes, open source search teams, prioritize (1) geeking out & fixing \"bugs\" but end up making fixing future bugs much harder. This is because in open source search, we fail to consider how technical and non-technical roles work together to build good search for a given domain. We think of every bug as technical, and end up gardening with bulldozers. Open source search can be a constant cycle of \"fixing\" (in reality creating) bugs. We never taking on the real problem of open source search. We never make resolving bugs easier. We don’t tackle the gargantuan gap between the technology and the domain.",
      "The role of the tech team in open source search is sorely misunderstood. Instead of the open source search team being the group responsible for search bugs, it needs to be the group responsible for enabling non-technical specialists to manage search and their own relevance challenges. The tech team fixing the organization’s search bugs is almost anathema to their true purpose.",
      "Technical and non-technical search shareholders need to work to define a working contract for how search should work. This contract, should",
      "Communicate what’s technically possible/not possible to the non-technical search team\n  Negotiate what assets (synonyms, taxonomies, etc) the non-technical team can maintain to have targeted impacts on search relevance\n  Define a rough sense for how ranking works, without getting too fine grained into jargon\n  Establish criteria for high-quality content to enable other technical/non-technical teams to edit/tune/curate content appropriately (see Google’s SEO guidelines)\n  Coach the whole team to understand how to measure the impact of changes, especially technical and asset changes",
      "The #1 differentiator between a successful relevance project and not-successful relevance project, has not been technical wizardry. Instead it’s been my ability to enable the client to perform this kind of robust & targeted search management. It’s been using technology to enable search management best practices, as defined by people like Martin White. To make it faster for the non-techies to fix the bugs, by giving them powerful tools and workflows.",
      "Given a \"contract\"  it’s easier for the technologists and non-technologists to take a search bug and with some work to escalate it accordingly. Ideally, you would first see if the bug is actually a content issue. A poorly formatted title, or misused tag, would have a narrow and conservative effect and unlikely to upset the search apple cart. But this can often be too fine of a scalpel. Extremely non-technical teams only have this option, limiting the breadth of changes to only careful pruning.",
      "That’s why technologists arm the organization with the second level of escalation, which has a slightly broader impact. Modifying an asset like a synonym or taxonomy entry can impact a set of search use cases at one time, but without the big sledgehammer of modifying the fundamental technology. An entire field of information science is available for us to tap into, that sadly goes ignored by technologists. But the power is there to implement these assets in our organization to give domain experts power tools to control search.",
      "As the search team gets even more sophisticated, the organization needs to break through the murky mystical corners of machine learning to manage that too. The inputs to machine learning models, it’s validation, and careful application of domain expertise need to be owned by the non-techies at the end of the day. Silver bullet promises of machine learning can never get around \"garbage in, garbage out.\"",
      "Finally, when all else fails, the technical and non-technical team can sit together to ask – how can we innovate with technology to create more ways for the non-technical team to impact search? Would adding a new asset help control search? If we introduced, for example, named entity extraction from content or queries, how would non-technical team use the available bells and whistles to solve their own search problems? How would that functionality interact with the other, existing pieces of the search contract?",
      "What I’m describing is an organizational need, not a technical one: search management. Search management is how the organization works together to manipulate search to support the business. Transcending the idea the user’s search bugs are technical bugs, is the work of the organization adopting open source search.",
      "Combining good contract-based search management with the deep customizability of open source search is how you get the power of open source search. YOU get to constantly change what’s possible without being dependent on a vendor. Solr and Elasticsearch aren’t Amazon or Google, they are YourThing(™) where you get to iterate on what the business can do. Your audience as a search developer ISN’T USERS instead it’s your colleagues that have to answer the 3AM calls from the sales team. Your job is to give them tools to make more possible.",
      "This is why I’m passionate about tools like Quepid, Splainer, FindTuner, PoolParty and Querqy. They each solve a targeted niche, helping to create contracts between between domain experts and techies. I see tools like these across many clients: proprietary tools that make search management easier. As proper search management tends to be neglected in the open source search field. I’m eager to see what others come up with. It’s a wide open market and there’s room to make broad impact.",
      "But while that market is developing, be aware of what you’re getting into with open source search. Your entering a brave new frontier where you’ll need to own not just a production search infrastructure, but tooling and management of complex issue specific to your domain. Often these latter topics are what makes a solution complex and powerful!"
    ],
    "summary_t": "Don’t be fooled by fancy technology: search is fundamentally a business problem; not a technical one. Technologists and domain experts need to establish care..."
  },
  {
    "id": "4db9f634d3944a7bb77dc89c38d259ab",
    "url_s": "https://opensourceconnections.com/blog/2017/10/02/elasticsearch-ltr-10rc1-released/",
    "title": "Elasticsearch Learning to Rank 1.0 RC1 Released",
    "content": [
      "We reached a major milestone over the weekend! We released the first release candidate for version 1.0 of Elasticsearch Learning to Rank. While our partners Wikimedia and Snagajob are currently using this to drive search in production, we still need community feedback to help make it bulletproof against the broadest number of use cases. So please try it out.",
      "What is this plugin?",
      "This plugin incorporates common machine learning methods for relevance ranking into Elasticsearch. This includes being able to incorporate ranking models from xgboost and ranklib, two common libraries used in ranking tasks. It also can use simple linear models to optimally weigh query boosts, used in simple linear models or SVM based models.",
      "The functionality is inspired quite a bit by Bloomberg’s Solr learning to rank plugin. It works by",
      "Letting you experiment with features (Elasticsearch queries) that correlate with relevance\n  Log feature values in production for live searches to use in training data\n  Store ranking models by name\n  Rank using the ranking model with a new sltr query",
      "Can I still find the older version of Elasticsearch LTR?",
      "Our original LTR plugin can still be used/found at this branch. Some of our old blog articles use this version of the plugin. This version of the plugin only stores and executes Ranklib models. It doesn’t contain all the bells & whistles of the 1.0 plugin.",
      "What’s next?",
      "If you have ideas for features beyond 1.1, please file an issue to start a discussion. And as always, feel free to get in touch with us at OpenSource Connections if we can be of help."
    ],
    "summary_t": "We’ve released Elasticsearch Learning to Rank 1.0 RC1, which brings machine learning capabilities to Elasticsearch relevance ranking. Beginning the March to ..."
  },
  {
    "id": "aa60cb8047bafd22595c1a4fea872bef",
    "url_s": "https://opensourceconnections.com/blog/2017/10/06/vespa-vs-lucene-initial-impressions/",
    "title": "Vespa vs Lucene: First Impressions",
    "content": [
      "As we learn more about Vespa, we wanted to give our initial impressions when comparing to Lucene-based search (Solr/Elasticsearch). This is based on initial passes with Vespa and our long history with Lucene-based search. Please get in touch to let us know what we’re getting wrong. In fact we’re pretty much writing this so that we can be corrected – Vespa is a beast and will take time to learn :)",
      "1. Tensors Are Awesome! But they aren’t magic…",
      "Vespa gives you a framework for storing and referencing tensors at query time. What threw me with experience with Solr and Elasticsearch learning to rank (ml for relevance ranking) was that I assumed tensors were a model representation from some standard ranking library. For example, in Elasticsearch learning to rank accepts a number of model representations from xgboost or ranklib. Tensors however are just multidimensional data that mean whatever you want them to mean. You can use them for anything. They can represent a ranking model, perhaps. You can then use Vespa’s built in tensor/math operators on to interpret it as a model (Vespa’s docs have a neural network ranking model example).",
      "But Tensors can mean many things, and to me this is Vespa’s power as a ranking engine. They can be a user-item collaborative filtering matrix. They can be some kind of topic model. Tensors seem to be a really powerful feature that the Lucene ecosystem lacks: a resource for storing and using arbitrary multidimensional data resources in ranking. I think this is the main selling point for me of Vespa when compared to the Lucene search engines.",
      "2. Nice concise ranking syntax",
      "I appreciate the syntax behind ranking. Solr and Elasticsearch focus on expressing text-or-math queries than combining them additively with boolean logic or multiplicative with function queries. The \"text\" queries give a lot of fine grain control for what constitutes a \"match\" before the score gets injected. That being said, you have to understand and take apart how queries are combined with boolean/other operators to piece together the underlying ranking function. For example, in Solr what’s the math behind",
      "q={!edismax mm=2 qf=text title}foo bat",
      "Or giant verbose, awkward Elasticsearch queries",
      "{\n    \"query\": {\n        \"multi_match\": {\n          \"type\": \"cross_field\",\n          \"query\": \"foo bat\"\n          \"fields\": [\"text^20\", \"title^2\"]\n        }\n    }\n}",
      "The ranking syntax in Vespa is just math:",
      "first-phase {\n            expression: nativeRank + query(deservesFreshness) * freshness(timestamp)\n }",
      "I can see a lot of appeal to this syntax compared to more Perl-like Solr and overly-verbose Elasticsearch syntax. You can also define your own macros and Java searchers to expand this syntax further.",
      "To me this isn’t quite an issue of power, but of usability. I already know that Lucene-based queries come with an immense level of control and power. Solr and Elasticsearch let you inject scripts or arbitrary math into your ranking. But Vespa \"out of the box\" tells you ‘this is just math’ and you don’t think about boolean logic or anything.",
      "The big unknown will be how \"down and dirty\" you can get with Vespa. Solr/Elasticsearch let you express a great deal of control, including writing Lucene query primitives in a plugin and exposing as a custom query component.",
      "3. Lucene has more customizable text analytics",
      "One big question I had about Vespa was the degree you could configure text tokenization and analysis. So much of the relevance work we do is feature modeling by manipulating terms, expanding terms, etc. Lucene comes with a bevy of libraries for combining tokenizers and token filters to do things like entity extraction or turn tokens into a minhash. You can use this framework to take apart either query strings or text. You can choose to treat name matching different than text for example. Or as my colleague discusses in his Lucene Revolution talk perform query time semantic expansion.",
      "Primarilly what Vespa seems to give you is the ability to add customizable linguistics. You can turn on/off features like stemming. Synonyms seem to be in an altogether different functionality in YQL known as EQUIV, separate from \"linguistics.\" But I don’t see yet the same depth of configurability of analysis chains you find in Lucene-based search.",
      "4. But Vespa Streaming Search",
      "Streaming search is a handy functionality that comes in Vespa. Instead of building an inverted index, Streaming search lets you identify a small subset of data to allow \"grep-like\" search functionality. A common limitation of inverted indexes is in-word searches can be more complex/slower when compared to full-term searches. Because inverted indexes are compact, and the full text data is not, streaming search can only work on a small subset of the whole corpus. So it’s appropriate for a small side-corpus (perhaps a set of phrases for autocomplete?)",
      "5. Community",
      "It goes without saying that it will take time for Vespa to grow the same level of community support and infrastructure as Solr and Elasticsearch. You can get hosted Solr/Elasticsearch from half a dozen cloud providers. There’s client libraries, plugins, developers that you can hire, meetups, and mindshare around Lucene that’s developed over two decades.",
      "Vespa isn’t there yet, but that won’t mean it can’t get there! In some ways now is the time to get in on the ground floor. I’m sure community support will build, and that’s probabily a big part of the reason Vespa was open sourced!",
      "Tentative Initial Conclusions",
      "I’m not sure it’s fair to say that Vespa is like Elasticsearch but 100 times better. They are comparable in power: Vespa is a tremendous piece of technology, but so is Lucene. It’s fantastic to see a major contendor to Lucene come out in the search space. I can see the limitations to Lucene that having a piece of functionality like Tensors exposes.",
      "My initial thoughts are that Vespa could be great for very large-scale search where you can’t do much index-time configuration of text analysis. Where there’s enough training data that technologies like Learning to Rank or recommenders make sense, and most of your ranking work is in machine learning around user behavior and you do little/no baseline relevance tuning or text analysis. You’re going to customize around tensors and their interpretation, not around text analysis or boosting. I’m not convinced Vespa is the best solution for the \"broad middle class\" of search problems that benefit from Solr/Elasticsearch’s customizability around text matching. And I’m not convinced Elasticsearch/Solr are nescsarilly bad for large scale search use cases.",
      "Of course Solr and Elasticsearch are catching up! Solr and Elasticsearch have a learning to rank story by directly accepting models generated by common learning to rank libraries. They also come with infrastructure for feature logging and management needed for learning to rank. But I would love to see something as flexible as tensor manipulation in Lucene search!",
      "Again, this is a working document based on our first impressions. We’d love to hear from you if you have a different perspective!"
    ],
    "summary_t": "Our first impressions of Vespa compared to Solr or Elasticsearch"
  },
  {
    "id": "52efd532d4c0f7b8fca7303867f356f7",
    "url_s": "https://opensourceconnections.com/blog/2007/06/06/file-uploads-with-selenium/",
    "title": "File Uploads with Selenium",
    "content": [
      "A colleague pinged my on how to do a file upload from a Selenium script. My smart ass reply was \"Did you look at the file_upload tag?\". And then I realized that File Upload is one of those things that the JavaScript security model in browsers makes very difficult. You have to explicitly select a file, which makes sense since you wouldnt want to visit a site that had some evil JavaScript on it that magically uploaded your Quicken data file or some other file that is in a well known location.",
      "There are some workarounds, including mucking around with your security model:",
      "http://cakebaker.wordpress.com/2006/03/29/file-upload-with-selenium/",
      "The Selenium FAQ also talks about using SeleniumIDE to work around the JavaScript security model. This is also a good solution when testing across multiple servers as well!",
      "However, the solution I find preferable is to take advantage of the 80% solution/20% effort that Selenium is out of the box, and not test file uploads directly. Instead, add a helper action like /fileupload?file=path/to/my/file.txt that you can invoke from Selenium. Have the helper action do the basic file upload, just like the webpage does, and then return to testing your site post file upload! This is the simplest, most cross platform approach. Its also a very good pattern for working around other gotchas in Selenium."
    ],
    "summary_t": ""
  },
  {
    "id": "8211abcb1d6f17caa602e9515f74d75c",
    "url_s": "https://opensourceconnections.com/blog/2017/11/21/solr-synonyms-mea-culpa/",
    "title": "Solr Synonyms and Taxonomies: Mea Culpa",
    "content": [
      "I screwed up! Mea Culpa. Let he who is without synonyms throw the first rock!",
      "In my talk Taxonomical Semantical Magical Search, I presented an index-time hypernym/hyponym expansion solution. It’s one I’ve blogged about for Elasticsearch",
      "Basically, in this solution, we use synonyms to perform a semantic expansion at index time to broader concepts. For example, we perform the following:",
      "tabby => tabby, cat, feline, animal",
      "This had the feature of inflating the document frequency of animal very highly given it’s low specificity while keeping the document frequency of tabby lower. This works with the strength of TF*IDF based similarity: specific/rare things score higher when they match over common items.",
      "At query time, if a user searches for cat, historically this solution has worked well because Solr/Lucene would translate synonym expansions to OR queries. In other words, the synonym file above is turned into",
      "tabby OR cat OR feline OR animal",
      "As tabby will have a low document frequency, it will score higher with a higher term specificity. Animal will score lower. The result is you have results sorted by semantic proximity to the user’s search.",
      "But, as of Solr 6, they’re not turned into an OR query. Which means the advertised behavior in my talk is not accurate.",
      "Instead of an OR query, they’re turned into a SynonymQuery.",
      "For most people using true synonyms, SynonymQuery does something very useful. It takes all the synonyms at query time and treats them as if they’re the same term with the same document frequency. If there’s 20 occurrences of car, and 40 occurrences of automobile, you’re ok treating them as if they’re the same term, and searching with the document frequency of 40.",
      "However when using the synonym filter for hypernym/hyponym expansion, if animal has a document frequency of 2000 and tabby a document frequency of 20, we use 2000 for the document frequency. Our term specificity is washed away.",
      "I have created this patch to allow one to choose how to handle overlapping query terms (w/ SynonymQuery staying as the default). Your feedback is very welcome. I’d love to get this in to allow the generated query to be configurable! As in my experience a classic mistake with synonyms is thinking two things are synonyms when in reality they have a hypernym/hyponym relationship.",
      "The Work Arounds",
      "There are some workarounds. In all of them, we leave index time semantic synonym expansion alone. The term tabby still expands to tabby, cat, feline, and animal.",
      "The first is to use the match query parser when boosting on synonyms. This is one reason I didn’t notice this issue, as I tend to use match qp (or other custom qparsers) for using synonyms with taxonomies. Only when colleagues unable to use plugins, began using this technique with edismax did I facepalm.",
      "Indeed, not everyone can deal with plugins. The other solution is not pretty, but it involves creating a bq for the parent synonyms, another bq for the grandparent synonyms, and so on. This effectively recreates the original OR query. Albeit with lots of cruft and potentially duplicate field types.",
      "For example, these two boost queries:",
      "bq={!edismax qf=text_parent v=$q}&\nbq={!edismax qf=text_grandparent v=$q}",
      "Where text_parent would have a synonym file that looked like:",
      "tabby => cat\ncat=>feline\nfeline=>animal",
      "And text_grandparent would skip a generation:",
      "tabby=>feline\ncat=>animal",
      "Here the parent query turns into a SynonymQuery one level up from the search term, broadening the least, bringing in only parents & direct siblings. While the grandparent query broadens extensively, bringing in a larger set of related concepts, but scored lower than parent queries due to the much higher document frequency.",
      "This actually has advantages over the original solution, as you can fine tune scoring to decide how much to broaden the query. Perhaps you’d only like to go as far as parents, but want to prompt the user before expanding to grandparents?",
      "The huge downside is maintenance and duplication of fields. A Match Query Parser or Elasticsearch solution would avoid the duplication by allowing a different query analyzer to be used for a field. And as these synonym files are often generated programatically from a taxonomy, the maintenance may not matter as much.",
      "Synonyms are hard cause you probably really have a taxonomy!",
      "Synonyms are one of the hardest thing to deal with in search. In a recent internal knowledge sharing call, our one hour call about synonyms, turned into a two hour call. Then we tacked on another hour to expand on the original two hours. I’m probably going to schedule yet another hour just to talk about this blog post and my screw up!",
      "One big problem is that people assume they have synonyms, when often they really have a taxonomy! Jeans aren’t really exactly the same as pants, they’re a type of pants. If you decide that jeans truly are the same as pants (ie jeans,pants) then you’ll have to explain to customers when they search for \"khakis\" why blue jeans come to the top. Because somewhere else farther down in the synonym file there’s a khakis,pants line!",
      "In reality khakis and jeans are hypernyms of pants. This is a different relationship, requiring different assumptions. Sussing out these relationships is often the interesting & challenging work in search. Especially when searchers and content creators rarely use the same words for items",
      "Well that’s it for now! Perhaps you have some ideas, questions, or feedback you’d like to share with us? We’d love to hear from you – we love empowering other search teams to be their best!"
    ],
    "summary_t": "In my LuceneRevolution talk I neglected to discuss how SynonymQuery impacted index time semantic expansion. I discuss that oversight in detail here, with wor..."
  },
  {
    "id": "c96947a44e2eaec2e2fca58ce2b5b241",
    "url_s": "https://opensourceconnections.com/blog/2018/01/04/why-a-search-relevance-conference/",
    "title": "Why We're Doing a Search Relevance Conference",
    "content": [
      "Our mission is to empower the world’s best search teams. Search teams ultimately generate value for their organizations through better, smarter search. That is: relevance.",
      "Sadly relevance remains maddening! What are you even optimizing for? And how do you know you have the right answer? Then, finally, how do you cut through the buzzword soup like \"cognitive\", \"semantic\", and \"machine learning\" to know what your search team should actually implement?",
      "This is why we’re doing Haystack our search relevance conference!",
      "Haystack is an extension of knowledge sharing we do internally. Every Friday a team member leads a discussion on real technical solutions to search relevance. Many times the talk is around something seemingly mundane like Solr/Elasticsearch synonyms or boosting strategies. Other times, we dive into advanced topics like taxonomies, machine learning, learning to rank, intent classification, or incorporating NLP/personalization into search. Most importantly we support each other as we face the toughest relevance challenges ourselves.",
      "With Haystack we want to open up the invite to practitioners from around the world similarly struggling on hard, meaty relevance problems. We want to hear and share what’s working and not working – the real-life war stories of theory encountering practice. What did that paper actually say, and how did you implement it in your search stack? Was the silver bullet for your team some kind of taxonomy? Or intent classification? Or perhaps learning to rank?",
      "I see this as possibly the inception of a community of relevance practitioners. I’d love to build more open source tooling and infrastructure to support the hardest relevance problems. For example, why do I see so many orgs struggle with instrumenting A/B testing for search? Why isn’t generating search quality data from user events something easier to do?",
      "Hopefully if you come and join us we can talk about our shared pain! Submit a CFP! And let us know if you have ideas for other sessions/panels/hack events etc."
    ],
    "summary_t": "Why are we hosting this crazy Haystack conference? We want to bring together search relevance practitioners to discuss their hardest technical problems."
  },
  {
    "id": "7ac80cf950c7173db316c6f0b6bb5b09",
    "url_s": "https://opensourceconnections.com/blog/2018/01/12/roll-your-own-analytics/",
    "title": "Roll your own search analytics",
    "content": [
      "I recently completed a fairly straight-forward search project in which we were replacing a legacy\nsystem with Solr. The goal for the first release was to just make sure that we had the same recall\nin Solr as in the legacy system, with two secondary goals of updating the UI and maintaining search \nspeed. We were also able to convince the stakeholders that this first iteration should have at least\nrudimentary click metrics built in, and they should live right in Solr.",
      "This isn’t the most common practice with Solr. Usually search metrics are harvested from logs and put into\nsomething like the ELK stack (Elasticsearch Logstash Kibana) or sent to Google Analytics. There’s\ncertainly nothing wrong with that approach, but for me it adds many more hoops I’ve got to jump through\nin order to answer questions about how well Solr is interacting with its users.",
      "But while storing click traffic in Solr is a rare find from my (consultant’s) perspective, isn’t something new in the Solr world. In fact, this approach is part of the \"batteries included\" philosophy behind Lucidworks Fusion and it’s come up several times in the Solr mailing list. Certainly it’s useful for\nmeasuring things like how deep a user has to go in search results, but it can also be used to drive\nsimple collaborative filtering or popularity boosting.",
      "So with that in mind, let’s look at what we chose to measure in my project.",
      "The main set of fields were:",
      "date: When the event occurred\n  type: Some sort of tag indicating the type of event, like:\n    \n      NewQuery (the user started from scratch with a blank Search box)\n      RefinedQuery (they edited the filters or added terms to the query)\n      ViewDetails (this is a click on one of the search results)\n      ZeroResultsQuery (the user queried but we didn’t find anything)\n    \n  \n  user: The logged in user ID",
      "Since we were using Solr for this project we added a few dynamic fields for search events:",
      "filterChanged_b: Was a filter changed\n  pagedChanged_b: Was the page changed\n  numfound_i: How many docs were retrieved\n  page_i: The current page of search results\n  querytime_i: How long did the query take\n  query_s: The query string\n  filter_ss: Any applied filters",
      "(If you’re unfamiliar with the notation, *_b is a Boolean, *_i is an integer, *_s is a string, and\nand *_ss is a multivalued string. These are default settings in Solr.)",
      "We also added fields specific to events triggered by clicking on one of the search results:",
      "docid_s: The ID of a clicked document\n  offset_i: The offset of the document in the search results\n  clicked_b: Did the user click the link in search results to get to the page (there are other ways to get there)\n  title_s: The title of the document (useful for reports)",
      "These values were sent to a distinct endpoint in our application asynchronously via JavaScript,\nwhich added a record in Solr after some value sanitization (an XSS attack in our analytics dashboard would\nbe suboptimal).",
      "After we started getting a nice log of user interaction with our search we needed to share that with the team through a dashboard. Even\nthough the structure of the \"documents\" and answers we wanted were simple Solr queries to construct, only a few of\nus on the team spoke that language. I was going to provide an initial set of queries, but it was important that the\nrest of the team be able to write their own. Solr has the ability to parse a basic dialect of SQL, so we planned our\ndashboard around that. Apache Zeppelin fit the bill nicely and Eric Pugh wrote a nice article about using it with Spark and Solr a couple of years ago.",
      "But for whatever reason I’ve never been able to get Zeppelin to install interpreters through Maven on Windows, which\nis the installation method in the Solr docs. What I ended up doing was adding a new JDBC interpreter by pointing to\na local jar as documented in Zeppelin. Even then I had to copy\nover all of the SolrJ client libraries into Zeppelin’s local-repo directory. Our client’s server was Linux, so this\nheadache was localized to just my development machine.",
      "Once Zeppelin was running the real fun began. If you’re an accomplished SQL writer you’re probably going to get\nimmediately frustrated like I did. The punishment for syntax errors is a Java stacktrace that fills your paragraph area, so experimentation is discouraged. One thing that tripped me up was that Solr query syntax creeps into unexpected\nplaces. For example, to write a constriant over a field in a WHERE clause you do something like this:",
      "SELECT type\nFROM events\nWHERE type = '(NewQuery NewAdvancedQuery RefinedQuery)'",
      "Or to constrain over a range you do this:",
      "SELECT type\nFROM events\nWHERE `date` = '[NOW/DAY-30DAY TO *]'",
      "But it’s not all bad. If your data doesn’t require too much manipulation Zeppelin is still a quick way to build\ndashboards, and our events collection was certainly simple. These are the initial queries we came up with:",
      "Top 20 Zero Results Queries Over 20 Days",
      "SELECT query_s, COUNT(*) AS `count`\nFROM events\nWHERE type='ZeroResultsQuery'\nAND `date` = '[NOW/DAY-30DAY TO *]'\nGROUP BY query_s\nORDER BY `count` DESC\nLIMIT 20",
      "Top 20 Slowest Queries Over 7 Days",
      "SELECT query_s, querytime_i\nFROM events\nWHERE `date` = '[NOW/DAY-7DAY TO *]'\nORDER BY querytime_i DESC \nLIMIT 20",
      "Query Time Over Year",
      "SELECT querytime_i, `date`, query_s\nFROM events\nWHERE `date` = '[NOW/DAY-365DAY TO *]'\nORDER BY `date` DESC",
      "Query Time Over Year",
      "Search Types Over 30 Days",
      "SELECT type\nFROM events\nWHERE type = '(NewAdvancedQuery NewQuery RefinedQuery)'\nAND `date` = '[NOW/DAY-30DAY TO *]'",
      "Search Types Over 30 Days",
      "Average Click Rank Over 7 Days",
      "SELECT avg(offset_i) as average\nFROM events\nWHERE offset_i > 0\nAND `date` = '[NOW/DAY-7DAY TO *]'",
      "As you can see we put in a mix of performance and relevance metrics to watch, and some of them even look good as\ncharts. I can write a set of Solr queries to get similar results. But keeping them organized in\nnotebook-fashion, sharing them with my team, and running them all with one click are why I’ll keep coming back to\nZeppelin.",
      "While you were reading this you probably had your own ideas about what to track and how to visualize, or you’re\ndoing something completely different in your own shop. I’d love to hear about it! Reach out to me on Twitter\nor drop me an email."
    ],
    "summary_t": "Search metrics are crucial, and they’re easy to do."
  },
  {
    "id": "6b68a878d3e94956d106c38e85aa2a03",
    "url_s": "https://opensourceconnections.com/blog/2018/01/29/retaining-customers/",
    "title": "Retaining Customers",
    "content": [
      "In product meetings, when I advocate for customer retention - regular, repeat business from the customer over months/years - I sometimes get blank stares. Teams want to do everything they can to maximize conversions: a customer action with value - such as a product sale/signup/job application/consultation/etc. To many teams customer retention seems softer and too long-term of a phenomena to measure, while driving ‘more conversions now’ has immediate value.",
      "Yet you can’t build a brand on the altar of conversions. Retention – really loyalty or fanaticism – carries the foundational importance. Brand fanatics intrinsically know the value of the brand, and are eager to turn over cash in exchange for that value.",
      "Recently, I was amazed at how readily I turned over cash for an iPhone X without much research. Ironically I had to go through a rather frustrating ‘conversion’ experience: the sales rep. took forever to navigate AT&T’s signup systems over my lunch break. Yet there I was, patiently waiting to turn over cash for another Apple product.",
      "Creating and monetizing such loyalty is the fundamental problem of product development. After struggling to best advise clients, this article captures my thoughts on the problem with what I think may be some promising solutions.",
      "Loyalty Programs?",
      "With loyalty programs, a business regularly tells customers about deals personalized for them. They’ll hopefully reengage, and make the next purchase.",
      "In my experience, loyalty programs can be a crutch. It’s hard for customers to tell the difference between a loyalty program they signed up for years ago and other spammy ads. Increasingly, customers learn to tune out ads. Such loyalty programs are a ‘bad smell’ to me, saying ‘we know ads, but not our customers!’ Ads can be important, but not as a foundational loyalty strategy.",
      "There are exceptions to this rule. Many brands have a customer segment that clips coupons and seeks deals. This may be built into the business model. Even at such places, many customers do not fit into the ‘deal seeking’ group.",
      "Unfortunately loyalty programs are foisted thoughtlessly onto every customer, forcing each to opt-out at every transaction. As an example, electronics retailer, Best Buy, overburdens the checkout process with endless loyalty prompts. Recently I visited to purchase a few iPhone dongles. It took 12 minutes to process 4 customers! Customers grumbled while sitting in line - \"Best Buy is terrible, why do I come here?\" said one. I contemplated just ordering what I needed on Amazon while I waited and leaving. All because Best Buy thought everyone wanted its loyalty program.",
      "The experience needs to be flipped. Instead of foisting it on every customer, force the deal-seekers to opt-in to the ‘cool kids’ club for special deals. There’s psychology at work by flipping the script. From the classic book Influence you can see the outlines of really effective loyalty programs. Exclusivity and scarcity increases the perceived value of an item. \"Sorry we only allow 5 people a day per store to sign up for the ‘cool kids discount club’\". Getting customers to make affirmative commitments to a loyalty club also tends to increase perceived value. People want their choices to be validated. Finally, consider charging for membership to such an exclusive club, further increasing the commitment and perceived value.",
      "These strategies allow the right population to opt-in to a program that stands out from advertising. Most importantly: these programs don’t interfere with regular purchases like those described at Best Buy. In other words, it minimizes interference to the conversion-focused side of the business while creating a way for certain segments to seek discounts from the brand.",
      "Customer Experience",
      "Customer experience means improving the efficiency, reliability, quality of the brand’s interactions with the customer. Great customer experience can be a key to retention.",
      "Some psuedo-conversions related to improved customer experience can be measured. Some activities put the brand at the customer’s fingertips, hopefully improving repeat business. These can look like classic loyalty programs: encouraging the user to register online and save their credit card. Or encouraging downloads of a mobile app. Unlike classic loyalty programs, they’re about simplifying the experience, not about advertising. These activities can be studied, especially as they related to repeat business and higher total customer value.",
      "Other forms of customer experience can be tougher to measure, but still incredibly valuable. An easily accessible Apple store in every major city, is a reliable way of seeking support and looking at Apple products. I’ve been impressed lately by Brooks Brothers that has consistent customer service around buying & maintaining fancy clothing. I can show up with an item that needs mending, and they’ll send it off to their tailor. Circuit City was known for paying top-notch and informative sales people at their electronics stores, lowering wages and driving off the sales staff has been cited as a major reason for the brand’s failure.",
      "Consistent customer experience often matters more than excellence. Consistency allows me to easily assign a value to an experience. I know exactly what I’m getting out of a trip to McDonalds. I’m not getting the world’s best Hamburger. But I’ll get mediocre food quickly and without fuss. Conversely, a local restaurant that has an excellent hamburger 8 out of 10 visits is frustrating. It becomes hard to know what I’ll get on every transaction with that brand.",
      "Step back to understand how your product fits into the customer’s broader life. Recently, I was struck when writing a mobile search study that two pictures emerged. Analyzing Google or Bing mobile search logs reveals simpler tasks than desktop counterparts. You might walk away thinking \"mobile search is simple.\"",
      "But when actual users studies occurred, the context of their mobile searches painted a very complex picture of users on the move, trying to use any tool at their disposal to solve a complex problem. A search for \"What’s the weather?\" really corresponds to a user preparing for some outdoor activity. \"When does the grocery store close?\" is actually a small step of the broader experience of planning meals, which involves many steps with and without technology.",
      "Your brand may be step 5 in an 11 step process to accomplish a larger task that goes beyond what you immediately see in customer data. Understanding what the customer ultimately wants to accomplish, and offering solutions that assists that process, ensures your brand remains part of that experience.",
      "Habituation",
      "There’s an even deeper level of customer retention. Great customer experience can become so good, and so deeply embedded in customer lives, as to be habituated. Consider a recipe app. Instead of ensuring ‘get a recipe’ remains at step 5 out of 11 on a journey to prepare a meal, can we create a product that fundamentally alter how human beings prepare meals? Can we revolutionize the customer’s life so profoundly, that our product becomes habituated in accomplishing some task?",
      "Consider some brands that have great customer retention, and you’ll see habituation front and center:",
      "First of course is Amazon Prime. Amazon Prime has done such an effective job at customer experience, but it goes far beyond just that. Amazon used to be a place to buy things online, but with expensive/slow shipping, it never supplanted the going to the store for immediate gratification. But Prime changed that. With two day shipping for everything, the default reaction shifted to \"go find thing on Amazon and order it.\" It takes 10 seconds.",
      "Amazon prime has fundamentally rewired how we purchase stuff. It’s become habituated & rote to use Prime to get much of what we need.",
      "Another example is illustrative: my wife and I bought ourselves a Peloton bike for Christmas. I was skeptical of this bike with online exercise classes. (Why not go to the gym? Take a spinning class?) But I find myself now with the ability to very efficiently roll out of bed and be in an exercise class. I no longer need to drive to a gym. The convenience really reshapes my day, making me more efficient. I’ve come to depend on the Peloton for fitness needs in my busy schedule.",
      "Human beings want predictable, reliable, ways to achieve what they need in life. They want rote and automatic, they don’t want to ‘think.’ Customer experience can be so great, so deeply understanding of what customers are really trying to do, that it rewires the customer’s life around the brand. Peloton changed my exercise habits. Amazon my buying habits. Google altered my information seeking habits. Facebook and twitter, social & news habits. Text messaging has altered our communication habits.",
      "I don’t exert effort to use these products: I just execute my rote, boring routine. Disrupting the habituated behaviors becomes emotionally upsetting. I want to stay on the rails of automatic, well-known ritual. Take any one of these products away, or harm their customer experience, and my life is thrown out of the whack. I become a fanatic precisely because of what these brands mean in the boring fabric of my life.",
      "Conversely, look at the Best Buy checkout example mentioned earlier. We still have a habit of going to the store to get something very quickly. But Best Buy is harming that habit by foisting endless loyalty programs on customers. Ironically, this hurts retention, and reinforces the value of a service like Amazon Prime.",
      "Beyond the personal rote routine, these habits often dictate how we’re expected to work together in society. Traveling to Scotland in the early 2000s, I noted that directions were easy to follow and locals could help you sketch out how to get from point A to point B. (Remember back when we wrote down directions for people! Or printed mapquest directions!). Now there’s a social expectation amongst human beings that you’ll have a smartphone, and it will have Google Maps. When I went to Cambridge, UK in 2015 I noted how everyone was navigating with their smartphone. Street signs could be inaccurate, but Google Maps got everyone where they needed to go.",
      "Text messaging and social media are other obvious examples. Social communication changes to revolve around the experience of a specific product. If you don’t catch up, you may be locked out of social circles. Twitter is reworking how American politics work (perhaps not for the best!). Facebook may have killed the purpose for the high school reunion by keeping us up to tabs with past friends.",
      "Designing Habits",
      "Product designers should think of themselves as habit designers. This is the hard work of product design. Not arbitrary optimizations in their own end, but defining the way the product changes habits and society, then measuring whether that change is happening. Changing habits is the real disruption: the way that old industries die and new industries supplant them. They create a mote of habituation that customers are loath to break out of.",
      "However, changing habits can also be challenging. Customers cling to their current habits, and for a new habit to supplant an old one, it needs to be a rock-solid customer experience. It’s crucial that the target customer be thoroughly studied. That indeed, for example, a customer population is in need of simplifying meal preparation. And that you know what’s missing from the current habits to allow you to rewire them into a better experience.",
      "Targeting and measuring habituation helps to unify the conversion-focused and retention sides of the business. You hypothesize a segment of the market wants a certain behavior change. The work becomes to understand how and if that market hypothesis is true, if some of the customer population can be ritualized fanatics of your product, and how exactly they’ll turn over cash to realize the brand’s value.",
      "To conclude, when I’m asked about retention now, I start with habituation. What about the product has become rote that absolutely should not be disturbed? How can that habituation be further reinforced through better experience? How can it be monetized? What purely ‘conversion focused’ strategies might threaten the product’s habituation?",
      "Even in my seemingly self-contained world of ‘search relevance consulting’ this matters tremendously. Matching the customer to ‘what they need’ can have little to do with what they’ll purchase, but rather where they are in a habituated journey that they relate to the brand. Both need to be optimized, but the real problem with product design is ‘knowing what to optimize for’. If your only answer is ‘sales’ then you may not really understand your customers or product.",
      "Disagree? Think I’m crazy? Get in touch and chat us up."
    ],
    "summary_t": "Optimizing products to build real brand fanatics - beyond just ‘conversions’."
  },
  {
    "id": "96c7cf9128abe2bae16e3abbf67dc3fc",
    "url_s": "https://opensourceconnections.com/blog/2018/02/12/elasticsearch-ltr-xpack-support/",
    "title": "Elasticsearch Learning to Rank 1.0.1 Released",
    "content": [
      "",
      "Thanks so much to community members who let us know that Elasticsearch Learning to Rank did not work with Elastic Cloud/XPack Security. Today Elastic Learning to Rank 1.0.1 is released that should allow Elastic Cloud users and others using XPack security to take advantage of the plugin.",
      "Read more about ES LTR + Xpack here",
      "A great story of open source teamwork",
      "I love seeing everyone pool time & resources to get something done for the community!",
      "Our friends Erik Berhardson and David Causse from Wikimedia Foundation played a big role in investigating exactly how a plugin should interoperate with XPack Security. Thanks to Skopos Labs who funded some freelancer time to investigate and hunt-down the issue (you can get a lot done in open source if you want to pay for something!).",
      "Finally, special thanks to my colleague Pere Urbon-Bayes. Pere took the ‘football over the goalline’ (I think this metaphor works in all forms of football…). Starting last week, Pere has been helping me co-administer the project. It’s become an important enough project to make sure I’m not a single-point-of-failure :). Pere is a fantastic freelancer who I enjoy working with, someone with deep expertise in the Elastic stack, a passion for relevance/learning to rank, and a love for open source. Thanks Pere for helping all of us out!",
      "Learn how to get the plugin here"
    ],
    "summary_t": "Elastic Cloud users rejoice! Elasticsearch LTR 1.0.1 released with support for XPack Security added."
  },
  {
    "id": "728237f73e93aafe228ec8bfdcb7aa9f",
    "url_s": "https://opensourceconnections.com/blog/2018/02/16/haystack-agenda-announced/",
    "title": "Haystack Agenda Announced!",
    "content": [
      "Hey, you know you want to come to this awesome search relevance conference! We’ll have talks from companies like LexisNexis, Wikimedia Foundation, Snagajob, Lucidworks, and Elastic! Talks range from classic tuning, to measuring search, to semantic search, and learning to rank.",
      "Even better its only $75! And there’s a fantastic machine learning conference the day after",
      "Sign Up Here!"
    ],
    "summary_t": "The agenda for Haystack - the search relevance talk - has been announced!"
  },
  {
    "id": "2aa4fb5f8e61806d5784a01588cdc34d",
    "url_s": "https://opensourceconnections.com/blog/2018/02/20/edismax-and-multiterm-synonyms-oddities/",
    "title": "Solr Multiterm Synonyms: avoiding sow=false surprises",
    "content": [
      "Many Solr teams have gotten stumped on the new sow=false behavior. To review, sow=false (split-on-whitespace=false) changes how the query is parsed. With sow=false the query is parsed using each field’s analyzer/settings, NOT assuming whitespace corresponds to an OR query. This (mostly) resolves the long-term query-time multi term synonyms issues that Solr has been plagued with. But it’s hard to create a universal solution to this problem, and the plethora of Solr field settings can create interesting behaviors.",
      "The primary unexpected behavior we see is at times, the following Solr query produces different query structure depending on a number of factors:",
      "qf=title description&defType=edismax&q=blue shoes&sow=false",
      "Much of the time, we see this query produce:",
      "(title:blue | description:blue) (title:shoes | description:shoes)",
      "This query, as mentioned in Relevant Search, ch 6 is term-centric. It picks the highest relevance score per search term (that’s the | operator), then adds them together. The advantage to term-centric search, is that in biases results towards documents that have more of the user’s search terms.",
      "However, many have noticed, the query can unexpectedly flip to a field centric behavior (same as Elasticsearch best fields). That is the structure flips to:",
      "(title:blue title:shoes) | (description:blue description:shoes)",
      "Here the query chooses the best field that matches the user’s query, not a document that matches both search terms.",
      "Why does this happen? And in what cases?",
      "After some debugging Solr, sow=false follows the following algorithm:",
      "Build field centric queries for each field passed into qf (title, description above) based on each field’s analysis & settings\n  Attempt to weave together each field-centric query into a single term-centric query",
      "The \"weaving together\" is the tough part. Solr evaluates whether to give up on creating a term-centric query only after each field generates a query. This means the specific Solr field configuration (analyzers, settings, etc) aren’t examined like Elasticsearch does in similar contexts to decide whether to allow/disallow term-centric behavior. The downside to Solr’s approach is that sometimes two fields, configured differently, will create consistent query structure some of the time, and inconsistent structures other times. So the flip between term/field centric can occur in surprising ways and depend on query-time factors.",
      "If you’re curious, the logic used to decide whether edismax gives up on the weaving is in this function. You’ll notice this function is rife with escape clauses, where an inconsistent structure across the two queries is detected, and edismax gives up on the term-centric query.",
      "Stopwords, and other settings creating surprising structure flips",
      "As we mentioned, the weaving happens based on each field’s generated query, you can get different results depending on what the user searches for. For example, we’ve noted that the inclusion of a stopword by the user can shift the query to being field-centric. This can happen because the term is a stopword in one field, but not another. So if \"the\" is a stopword in title, but not the description, then suddenly a surprising query structure can be output by the user adding \"the\" to their search.",
      "How do these query structures play with multi-term synonyms?",
      "The query setting sow=false is only one half of the recent solution to Solr multi-term synonyms. The other part is the per-field setting autoGeneratePhraseQueries which instructs the query parser to turn multi-term synonyms (scifi turning into science fiction) into a phrase query (like \"science fiction\"). The setting is placed in the schema on the fieldType:",
      "<fieldType name=\"text_en\" class=\"solr.TextField\" autoGeneratePhraseQueries=\"true\"  >\n     ...",
      "As noted in chapter 6 of Relevant Search term-centric search can respect multi-term synonyms. As long as the query is analyzed consistently across fields, you can get a term-centric search with multi-term synonyms. For example in our Search Relevance Training, we highlight this structure:",
      "Query:",
      "q=best sci fi movie&qf=title overview&defType=edismax",
      "with a query-time \"sci fi\" synonym entry:",
      "science fiction,sci fi,scifi,sci-fi",
      "produces",
      "(title:best | overview:best)\n((overview:\"science fiction\" overview:sci-fi overview:scifi overview:\"sci fi\") | (title:\"science fiction\" title:sci-fi title:scifi title:\"sci fi\")) \n(overview:movie | title:movie)",
      "Notice this is still term-centric. Each dismax clause still picks the best field match per search \"term\". Here a \"search term\" is the outermost synonym phrase seen in the query. The structure between title and overview searches still matches.",
      "With a field with a different query-time analysis, autoGeneratePhraseQueries setting, synonyms, stopwords, or other structure-changing issues, the result could become field-centric.",
      "Multi-term synonyms, there’s still not a silver bullet!",
      "It’s not hard to use your imagination to think of cases where the field has much more complex & ovelapping synonyms than what’s been presented here. Or where toggling different settings creates new, weird, and interesting query structures. (Indeed there’s more settings than what’s been described here!) While the sow=false setting along with auto-phrase query synonyms settings creates generally a saner query, there’s not a better way than considerable trial and error to see how your syntax, field settings, analysis chain, query fields, and user queries transform into Lucene syntax using edismax.",
      "What we recommend",
      "My motto with this stuff is \"Keep It Simple Stupid\". I like the term-centric syntax that comes with autoGeneratePhraseQueries and sow=false. In our experience it provides pretty good default search results. But you must be certain that all the fields being searched are consistently analyzed, with the same stopwords, synonyms, and query settings to avoid sudden field-centric surprises.",
      "But sometimes you want a specialized per-field query behavior. For example, you know a certain tag field should have its own unique synonyms. In these scenarios, some specialized query/index analysis has occurred to target a specific user intent. For these cases, we recommend using boost queries (bq) to apply additional per-field boosts as needed. To other Solr relevance engineers, a bq semantically says in the query DSL that the query is \"attached\" and distinct from the main query. We also recommend getting comfortable with Solr local params syntax to really fine tune the matching logic for those boosts. One of hossman’s man talks on Solr query syntax can be very helpful!",
      "Keep in mind the use cases, data volume, data complexity, languages, and synonym(taxonomy?) issues can be quite different per application & user. So while this is a good pattern, there’s still no \"one size fits all.\" Many of us keep touting so-called cognitive search, but in reality before that’s even possible we’ve got to get the dang synonyms to work!",
      "Shameless plug for search relevance training!",
      "Did I mention we do Solr/Elasticsearch relevance training? We cover topics ranging from organizational models, semantic search, synonyms, and learning to rank :) Get in touch if your team could use training from the team that wrote the book on search relevance. Or if you just want to chat about other Solr/Relevance topics!"
    ],
    "summary_t": "We document some of the behaviors behind Solr’s relatively new sow=false strategy for parsing queries and dealing with query-time synonyms"
  },
  {
    "id": "201412243d15685a508b750c515b027a",
    "url_s": "https://opensourceconnections.com/blog/2018/02/26/ndcg-scorer-in-quepid/",
    "title": "How to Implement a Normalized Discounted Cumulative Gain (NDCG) Ranking Quality Scorer in Quepid",
    "content": [
      "Background",
      "As a search relevancy engineer at OpenSource Connections (OSC), when I work on a client’s search application, I use Quepid every day! Quepid is a \"Test-Driven Relevancy Dashboard\" tool developed by search engineers at OSC for search practitioners everywhere.  I like to think of Quepid as both a unit and system tests environment for search relevancy development. A unit tests environment because the end-user can go into a deep dive of the search engine (Solr or ElasticSearch)-produced Lucene query structure as well as the math behind the Lucene scoring of matched terms and phrases; And a system test environment too as Quepid allows the end-user to monitor progress, or regress, across any number of queries, organized thematically as cases. Which brings us to the topic at hand in this blog: The metric in Quepid to measure progress or regress is a query score, and the queries’ scores combine to produce a top-level case score as shown in Figure 1 below. In this blog, we will use OSC’s favorite Movies index.",
      "",
      "Figure 1: Quepid Query Score and Case Score",
      "Figure 1 shows a Quepid case called Movies, which contains two queries (or searches): Star Trek and Star Wars, with scores 67 and 63 respectively. Quepid scores are always linearly scaled to the 1-100 range (This is not currently configurable). Also shown in Figure 1 is the Quepid case’s top-leve score of 65, which is simply the arithmetic mean (a.k.a., average) of the case’s queries’ scores.",
      "So, what is behind these query scores? In this blog, we will explore and explain the Quepid scorers, which compute the queries’ scores. In particular, we will implement a Quepid scorer that computes a popular ranking quality metric in the Information Retrieval field called the Normalized Discounted Cumulative Gain (NDCG). We will be using the Wikipedia article on Discounted Cumulative Gain as our reference CG, DCG, and NDCG implementations; Keep it on an opened tab next to you!",
      "Recommended prior knowledge:",
      "If you are reading this blog, I am assuming that you have some affinity with, and interest in, the field of search!\n  Some familiarity with Quepid. If you are a current Quepid end-user, it’s even better!\n  Some familiarity with metrics that measure the quality of search results. See this Wikipedia article for some suggested background readings.",
      "Quepid Scorers",
      "Before we dig into the implementation of an NDCG scorer, let’s briefly review what a Quepid scorer is, as well as the default scorer. You can also check out the great Quepid on-line documentation on scorers for a refresher if necessary. Figure 2 shows the anatomy of the default Quepid score.",
      "",
      "Figure 2: Quepid’s Default Scorer",
      "Note: There are two ways to produce Figure 2 on your own. Open any one of your Quepid cases, and:",
      "Either, double-click on any query’s score, and click on Test with an ad-hoc scorer: The ad-hoc scorer is initialized to the default scorer’s settings.\n  Or, click on Custom Scorer on the top-level menu, and click on Add New: The new scorer is also initialized to the default scorer’s settings.",
      "In either case, click the Cancel button to close the window.",
      "As shown on Figure 2, a Quepid scorer is composed of the following parts:",
      "A unique name, preferably user’s friendly, but you can choose any name of your liking. For example, \"[email protected]\" might be a sensible name for an NDCG scorer at rank position 10;\n  A script written in JavaScript (JS) which computes the score for a given query and its results;\n  A rating scale (1 to 10 by default, from worst-result to best-result);\n  And, optionally, the rating labels (Not shown on Figure 2).",
      "Let’s explore the default scorer’s computation, reproduced in Script 1 below:",
      "// Gets the average score over a scale of 100\nvar score = avgRating100();\nif (score !== null) {\n  // Adds a distance penalty to the score\n  score -= editDistanceFromBest();\n}\n\n// (Added by the blog's author) Return the score to Quepid\nsetScore(score);",
      "Script 1: Quepid Default Scorer’s Script",
      "Quepid’s default scorer is basically a 100-scaled Cumulative Gain (CG) calculation in avgRating100(), which is then penalized by the factor editDistanceFromBest(). The JS functions avgRating100() and editDistanceFromBest() are part of the Quepid API that any scorer has access to (See the section What can my code do? in the Quepid scorers documentation for a complete API reference).",
      "More specifically, at the time of writing avgRating100() executes the following computation:",
      "",
      "Equation 1: Quepid Cumulative Gain (a.k.a., QCG)",
      "where:",
      "p is the rank position as defined in the aforementioned Wikipedia article on Cumulative Gain, or depth within the results set, at which the measurement of the results quality is performed. Note that in the Quepid default scorer, p is equal to the number of results displayed as configured by the parameter \"Number of Results to Show\" in the Quepid settings (See Figure 3).\n  number_of_rated_results is the number of results on the Quepid results page that have been rated by the end-user.\n  rating(i) is the end-user’s rating of the result at position i.\n  max_rating is the maximum value in the scorer’s rating scale, e.g., 10 for 1-to-10 rating scale.",
      "As mentioned earlier, the Quepid query scores are always scaled to the 1-to-100 range.",
      "Below is an example of the value produced by avgRating100:",
      "p (Number of results shown in Quepid) = 10\nScale: 1-10\nList of ratings by position in the results set (0 = Position not rated): [10, 8, 9, 0, 5, 1, 4, 0, 0, 0]\nSum of ratings = 10 + 8 + 9 + 5 + 1 + 4 = 37\nNumber of rated results: 6\nmax_rating = 10\n\nscore = (37/6) * (100/10) = 61 (Rounded down)",
      "The penalty factor editDistanceFromBest() quantifies the distance between the list of ratings and what Quepid calls the best ratings (or best docs), which is the list of non-zero ratings sorted by descending order. The distance is calculated using the Levenshtein distance (a.k.a., edit distance) between two character strings:",
      "\"edit distance … is the minimum number of edit operations required to transform s1 into s2. Most commonly, the edit operations allowed for this purpose are: (i) insert a character into a string; (ii) delete a character from a string; and (iii) replace a character of a string by another character.\"",
      "Continuing with the above example, the edit distance is shown below:",
      "best docs = [10, 9, 8, 5, 4, 1]\nedit distance between \n    [10, 8, 9, 0, 5, 1, 4, 0, 0, 0] \nand [10, 9, 8, 5, 4, 1, 0, 0, 0, 0] \nat p=10 = 4   \n\nscore = 61 - 4 = 57",
      "To explore further the above computations, I recommend you create a new scorer, and use the script below to dump various messages and objects in your browser’s developer’s tools JS console (See the console.log(...) statements):",
      "// Explore 'docs'\nconsole.log(docs)\n\n// Explore 'bestDocs'\nconsole.log(bestDocs)\n\n// Gets the average score over a scale of 100\nvar score = avgRating100();\nconsole.log('avgRating100=' + score)\n\nif (score !== null) {\n  // Adds a distance penalty to the score\n  var editDistance = editDistanceFromBest()\n  console.log('editDistanceFromBest=' + editDistance)\n  score -= editDistance;\n}\nconsole.log('score=' + score)\nsetScore(score);",
      "Script 2: Quepid Default Scorer with Debug Messages",
      "",
      "Figure 3: Quepid Case Tuning Settings",
      "Now armed with a solid understanding of the Quepid default scorer, let’s implement our own scorer, using the popular NDCG as an example.",
      "[email protected] Scorer",
      "As mentioned earlier, we will be using the NDCG definition specified in the Wikipedia Discounted Cumulative Gain article. And we will build an NDCG scorer at rank position 10 (a.k.a., [email protected]).",
      "CG, DCG, IDCG, and NDCG",
      "For reading convenience, the math of interest is reproduced below (Thanks to this on-line LaTeX editor). First, the primitive Cumulative Gain (CG), which simply adds the ratings up to a specified rank position:",
      "",
      "Equation 2: Cumulative Gain (CG) at rank position p ([email protected])",
      "Then, the Discounted Cumulative Gain (DCG), which penalizes, or discounts (logarithmically), each rating based on its position in the results:",
      "",
      "Equation 3: Discounted Cumulative Gain (DCG) at rank position p ([email protected])",
      "And finally, the Normalized Discounted Cumulative Gain (NDCG), which normalizes the gain to a number between 0.0 and 1.0, hence making NDCG comparable across queries and search engines. The normalization is accomplished by dividing the query’s DCG with the so-called Ideal DCG (IDCG), which is the DCG of the best possible results based on the given ratings (Same definition as in QCG above):",
      "",
      "Equation 4: Ideal DCG (IDCG) at p ([email protected])",
      "where |REL| is the number of best ratings up to position p (Note: |REL| <= p)",
      "",
      "Equation 5: Normalized DCG (NDCG) at p ([email protected])",
      "NDCG Scorer Implementation in Quepid",
      "Experimentation in JSFiddle",
      "We recommend that you first experiment with, and debug, your new Quepid scorer in a debugging-friendly JavaScript sandbox like JSFiddle or codepen with mocked (or simulated) Quepid objects and API. Our NDCG scorer was first written in this JSFiddle.",
      "Mocked Quepid Objects and API",
      "First, the following objects and API are mocked in our JSFiddle:",
      "Mocked Quepid Objects:",
      "docs: The search results.\n  bestDocs: The best (ideal) rated docs.",
      "Mocked Quepid API:",
      "function docRating(i): Returns the rating of result at position i.\n  function hasDocRating(posn): Returns true if search position i is withing the rank position, and false otherwise.",
      "Using the Wikipedia article data example, the complete Quepid mockup is reproduced in Script 3 below:",
      "// Wrap the mocked Quepid objects and API in a namespace\nlet mockedQuepidApi = {};\n\n(function(context) {\n\n  // List of docs/ratings on page #1 (e.g., 6 with the Wikipedia example)\n  const _docs = [3, 2, 3, 0, 1, 2];\n\n  // List of best docs: Descending order of ratings\n  const _bestDocs = [{\n    rating: 3\n  }, {\n    rating: 3\n  }, {\n    rating: 3\n  }, {\n    rating: 2\n  }, {\n    rating: 2\n  }, {\n    rating: 2\n  }, {\n    rating: 1\n  }];\n\n  context.getDocs = function() {\n    return _docs;\n  }\n  \n  context.getBestDocs = function() {\n    return _bestDocs;\n  }\n\n  // Document's rating accessor\n  context.docRating = function(position) {\n    // from the\n    return _docs[position];\n  }\n\n  // Has a ranking position been rated?\n  context.hasDocRating = function(position) {\n    return position >= 0 && position < _docs.length;\n  }\n\n})(mockedQuepidApi);",
      "Script 3: Mocked Quepid Objects and API",
      "The Quepid API is wrapped in a namespace so that we are able to pass to the NDCG scorer either the mocked or actual Quepid API.",
      "[email protected] Scorer Implementation",
      "Our [email protected] implementation is shown in its entirety in Script 4 below, and is annotated subsequently.",
      "// Scorer's name space to avoid name collision with the Quepid-provided objects.\nvar NDCG_scorer = {};\n\n(function(context) {\n\n  let _quepidApi = null;\n\n  // --------------------------------------------------------------------\n  // JS console logging.\n  // Argument:\n  // obj: Object to log to the JS console.\n  // --------------------------------------------------------------------\n  function log(obj) {\n    // Log messages on the developer's console.\n    var debug = true\n    if (debug === true) console.log(obj)\n  }\n\n  // --------------------------------------------------------------------\n  // Log base 2.\n  // Argument:\n  // num: Number to caculate the log base 2 of.\n  // --------------------------------------------------------------------\n  function log2(num) {\n    return Math.log(num) / Math.log(2);\n  }\n\n  // --------------------------------------------------------------------\n  // Round with precision. Used for logging numbers.\n  // Arguments:\n  // number: Number to round.\n  // precision: Rouding precision.\n  // --------------------------------------------------------------------\n  function myRound(number, precision = 0) {\n    const factor = Math.pow(10, precision);\n    const tempNumber = number * factor;\n    const roundedTempNumber = Math.round(tempNumber);\n    return roundedTempNumber / factor;\n  }\n  \n  // --------------------------------------------------------------------\n  // Retrieve a document's rating.\n  // Arguments:\n  // posn: Document's position (0-based) in the results.\n  // defVal: Default rating value to use if the document has not been rated.\n  // --------------------------------------------------------------------\n  function ratingOrDefault(posn, defVal) {\n    if (!_quepidApi.hasDocRating(posn)) {\n      return defVal;\n    }\n    return _quepidApi.docRating(posn);\n  }  \n\n  // --------------------------------------------------------------------\n  // Build the actual list of ratings.\n  // Arguments:\n  // p: Max rank position (See 'p' in the Wikipedia article)\n  // --------------------------------------------------------------------\n  function actualRatings(rankPosition) {\n    const ratings = [];\n    for (let i = 0; i < rankPosition; i++) {\n      const rating = ratingOrDefault(i, 0.0)\n      log(\"Doc \" + i + \": \" + rating)\n      ratings.push(rating);\n    }\n    log(\"Actual Ratings: \" + ratings);\n    return ratings;\n  }\n\n  // --------------------------------------------------------------------\n  // Build the ideal list of ratings.\n  // Arguments:\n  // p: Max rank position (See 'p' in the Wikipedia article)\n  // --------------------------------------------------------------------\n  function idealRatings(rankPosition) {\n  \tconst idealRatings = [];\n    const bestDocsCount = _quepidApi.getBestDocs().length;\n    for (let i = 0; i < bestDocsCount && i < rankPosition; i++) {\n    \tidealRatings.push(_quepidApi.getBestDocs()[i].rating);\n    }\n    \n    // Padd with 0 (Not rated) if necessary.\n    for (let i = idealRatings.length; i < rankPosition; i++) {\n      log(\"Ideal ratings list: Padding with 0.0 at position (0-based) \" + i);\n      idealRatings.push(0.0);\n    }\n    log(\"Ideal ratings: \" + idealRatings);\n    return idealRatings;\n  }\n\n  // --------------------------------------------------------------------\n  // DCG calculator\n  // Arguments:\n  // docList: List of rated documents.\n  // p: Maximum rank position.\n  // --------------------------------------------------------------------\n  function dcg(ratings, p) {\n    let dcgScore = 0\n    for (var i = 1; i <= ratings.length && i <= p; i++) {\n      const logPosition = log2(i + 1);\n      const dcgAdder = ratings[i - 1] / logPosition\n      dcgScore += dcgAdder\n      log('i=' + i + \" ratings[i-1]=\" + ratings[i - 1] + \"/log(i+1)=\" + logPosition + \" => DCG incr. of \" + myRound(dcgAdder, 3) + \" (DCG=\" + myRound(dcgScore, 3) + \")\")\n    }\n    log('DCG([' + ratings + ']) = ' + dcgScore)\n    return dcgScore\n  }\n\n  // Public API: ndcg\n  // Arguments:\n  // 1. Rank position\n  // For example, for an [email protected], use p = 10\n  //\n  // ATTENTION: Due to current limitation in Quepid (\"bestDocs\" is\n  // 10-deep at most), p must be <= max(Number of results shown, 10)\n  // For example, for p = 10, set the \"Number of Results to Show\"\n  // in the Settings to 10.  \n  // \n  // 2. Rating scale max value: MUST MATCH THE MAX RATING SPECIFIED IN THE \n  // \"Scale for query ratings\" scorer configuration.\n  context.ndcg = function(quepidApi, rankPosition, ratingScaleMax) {\n\n    log(\" RUNNING NDCG!!! \");\n    log(\" *************** \");\n    \n    _quepidApi = quepidApi;\n\n    log('Docs count    : ' + _quepidApi.getDocs().length)\n    log('bestDocs count: ' + _quepidApi.getBestDocs().length)\n\n    // Log the objects for inspection in the developer's console\n    log(_quepidApi.getDocs())\n    log(_quepidApi.getBestDocs())\n\n    log('Rank position (p)=' + String(rankPosition))\n    log('Using scale 1-' + String(ratingScaleMax))\n\n    var myDcg = dcg(actualRatings(rankPosition), rankPosition);\n    var iDcg = dcg(idealRatings(rankPosition), rankPosition);\n    var nDcg = myDcg / iDcg;\n\n    log(\" DCG: \" + myDcg);\n    log(\"iDCG: \" + iDcg);\n    log(\"NDCG: \" + nDcg);\n\n    return nDcg;\n  }\n\n})(NDCG_scorer);",
      "Script 4: [email protected] Implementation",
      "Scorer’s Name Space",
      "The [email protected] implementation is wrapped in a name space (See Script 5 below) in order to avoid object name collision within the Quepid runtime context (See this Dynamic Namespace technique.)",
      "var NDCG_scorer = {};\n\n(function(context) {\n    // NDCG implementation\n    // ...\n    \n})(NDCG_scorer);",
      "Script 5: [email protected] Scorer Namespace",
      "Helper Functions",
      "The following helper/convenience functions are helping with various tasks in the NDCG calculation:",
      "Function\n      Description\n    \n  \n  \n    \n      log(obj)\n      JS console logging in your browser’s developer tools.\n    \n    \n      ratingOrDefault(posn, defVal)\n      Returns the rating of document/result at position posn, using the Quepid api hasDocRating(posn) and docRating(posn).\n    \n    \n      log2(num)\n      Calculats the log base 2 of a number (As used in the DCG).\n    \n    \n      myRound(num)\n      Round a floating point at a given precision (Used for logging only).",
      "Actual Ratings, Ideal Ratings, and DCG and NDCG Functions",
      "let’s now get into the meat of the implementation, starting with the NDCG scorer’s public API ndcg function, reproduced in an abbreviated form (without the logging) in Script 6 below.",
      "context.ndcg = function(quepidApi, rankPosition, ratingScaleMax) {\n      \n    _quepidApi = quepidApi;\n\n    var myDcg = dcg(actualRatings(rankPosition), rankPosition);\n    var iDcg = dcg(idealRatings(rankPosition), rankPosition);\n    var nDcg = myDcg / iDcg;\n\n    return nDcg;\n  }",
      "Script 6: ndcg function",
      "The ndcg function takes the following arguments:",
      "quepidApi: A Quepid API wrapper, which wraps either a mocked Quepid API (See Script 3 earlier), or the production Quepid API. Such a thin wrapper layer allows us to run the same NDCG scorer implementation in either the JSFiddle sandbox environment for debugging, or in Quepid.\n  rankPosition: The rank position to calculate the NDCG for, i.e., the \"p\" in the Wikipedia article.\n  ratingScaleMax: The upper bound of the rating scale, i.e., 10 for a 1-10 rating scale.",
      "The ndcg function performs the following steps:",
      "Saves the passed-in Quepid API wrapper in the private scorer name space variable _quepidApi\n  Calculates the actual ratings’ DCG: dcg(idealRatings(rankPosition), rankPosition) using the scorer private name space function dcg\n  Calculates the ideal ratings’ DCG: dcg(idealRatings(rankPosition), rankPosition);, also the same dcg function as above;\n  And finally calculates and returns the NDCG by dividing the actual ratings’ DCG with the ideal ratings’ DCG.",
      "The scorer’s private name space actualRatings function creates the list of the document’s ratings using our helper function ratingOrDefault, in the order they were returned in the seach results. The scorer’s private name space idealRating function creates the best list of ratings, using the Quepid API getBestDocs, that is sorted by descending rating value, and padded with zeros if necessary to reach a list length of rankPosition.",
      "To see all that in action, go ahead and run the code in JSFiddle (Make sure you open your browser’s developer tools’ JS console to see the logging).",
      "Taking our [email protected] to Quepid!",
      "After debugging our [email protected] scorer in JSFiddle, we’re now ready to take it to Quepid. To do so, we perform the following steps in Quepid (See Figures 4, 5, 6):",
      "Click on Custom Scorers;\n  Click on Add New;\n  Type a scorer’s name, e.g., [email protected];\n  Paste the code that is provided in Script 7 below (The code is annotated following the script);\n  Click on Create; At any time later on, you may edit the scorer by clicking on the pencil icon (See Figure 5);\n  Open your Quepid case;\n  Click on Select Scorer, pick the scorer in the list, and click on Select Scorer (See Figure 6);\n  Re-run the case to update the scores.",
      "",
      "Figure 4: Create a New Quepid Scorer",
      "",
      "Figure 5: Edit an Existing Quepid Scorer",
      "",
      "Figure 6: Select a Quepid Scorer for a Case",
      "// Wrap the Quepid objects and API in a namespace.\n// Simple pass-through required by our NDCG scorer.\nlet quepidApi = {};\n\n(function(context) {\n\n  context.getDocs = function() {\n    return docs;\n  }\n  \n  context.getBestDocs = function() {\n    return bestDocs;\n  }\n\n  // Document's rating accessor\n  context.docRating = function(position) {\n    // console.log('Position ' + position + ' rating: ' + docRating(position));\n    return docRating(position);\n  }\n\n  // Has a ranking position been rated?\n  context.hasDocRating = function(position) {\n    // console.log('Position ' + position + ' rating? ' + hasDocRating(position));\n    return hasDocRating(position);\n  }\n\n})(quepidApi);\n\n// Scorer's name space to avoid name collision with the Quepid-provided objects.\nvar NDCG_scorer = {};\n\n(function(context) {\n\n  // Paste here the NDCG scorer implementation from Script 4.\n  // ...\n\n})(NDCG_scorer);\n\nsetScore(NDCG_scorer.ndcg(quepidApi, 10, 10) * 100);",
      "Script 7: Quepid Scorer [email protected] Code",
      "Let’s point out the following in Script 7:",
      "As required by an NDCG scorer implementation, a wrapper of the Quepid objects and API is created (See the quepidApi name space in Script 7). The wrapper is essentially a pass-through to the Quepid docs and bestDocs objects, as well as the hasDocRating and docRating functions.\n  The score calculation is performed by calling the NDCG scorer’s public API ndcg with the arguments quepidApi (Quepid API wrapper), 10 for the rankPosition, and 10 for the max rating scale value.\n  The calculated score is returned to Quepid by calling the Quepid API setScore(). Voilà!",
      "As a final illustration, Figure 7 shows the execution of our Movies case with the [email protected] scorer. Note the logging messages in the (Chrome) JS logging console, that show the calculation of the score for the search \"Star Trek\".",
      "",
      "Figure 7: Movies Case Scored by [email protected]",
      "Closing Remarks",
      "Well, it’s been a long blog. Thanks for staying with me. Take any part of the code above or in my JSFiddle, and experiment with it in your own NDCG scorer. Or, based on the scorer development methodology I presented in this blog, go ahead and create a Quepid scorer of your own invention. The sky is the limit! And let us know what you think, and don’t hesitate to contact OSC about all search things."
    ],
    "summary_t": "A guide on how to implement, test, and deploy a Normalized Discounted Cumulative Gain (NDCG) ranking quality scorer in Quepid."
  },
  {
    "id": "3a0c797ca716e78c7bac8125bc6d4667",
    "url_s": "https://opensourceconnections.com/blog/2007/06/15/showing-sphereit/",
    "title": "Showing \"Sphere\"it",
    "content": [
      "Several of our recent projects have involved different methods of data sorting, manipulation, and visualization. An example of this is now available with the University of Virginia Librarys BlacklightDL project. One of the issues that we run into is determining similarity and trying to predict whether or not certain results will actually be similar to what is being searched for.",
      "At issue is the balance between the engine interpolating what the searcher wants and, on the other side, making the searcher be as precise as possible with his search terms. On one end is a sample search with one term and a multitude of results (for example, a Google search for the term dog) and on the other end, a search for a long string of text. Most searchers dont want to spend their time either typing in the volumes of text or dont know exactly how to pull out the most pertinent pieces of what they are looking for.",
      "Enter Sphere. Sphere looks at the entirety of the content of a page that you are looking at and searches for blog posts which contain similar information. Rather than looking simply at the links within a page, it searches based on the entirety of the comment. This provides enough detail to create more accurate results without forcing the user to enter in excessive (and potentially inaccurate and superfluous) information.  Alternatively, the user can enter in the link of the page of interest to the Sphere search engine to get similar results based on the full text of the site.  An example search yielded results of similar entries to this page.\nNow, if only Sphere could find blog pages similar to what Im thinking rather than to what Im reading, Id be happy!"
    ],
    "summary_t": ""
  },
  {
    "id": "417d372dd280b2db3b61986cf4904948",
    "url_s": "https://opensourceconnections.com/blog/2018/05/04/reflective-relevance-testing/",
    "title": "Reflective Search Relevance Testing",
    "content": [
      "Many organizations find relevance testing frustrating. Often we start consulting engagements and the first problem we have to solve is \"how do we test search relevance?\" In this article I want to talk about an alternate method you can use to bootstrap search relevance testing and get to the solution more quickly.",
      "Relevance Testing Methods",
      "Search relevance testing usually means one of two things:",
      "There’s maybe a clickstream somewhere that we can use to see what users click on, interact with, or purchase after a search. From this we can see which docs they like/don’t like. Users search for \"Rambo\" and they click \"First Blood\" but not \"Sense and Sensibility\"\n  There’s enough patience and expertise to use a tool like Quepid to gather judgments on which documents are good/bad for a given query. A movie critic tells us that obviously a search for \"Rambo\" should have \"First Blood\" in the first position, followed by Rambo I, etc.",
      "Certainly these methods are valuable. They’re both more-or-less based in traditional information retrieval research in how search should be evaluated. Given that, there’s a lot of know-how in using the data generated by the processes. We are big fans of both approaches, and using them together. But they each come with their own unique pain points with no clear silver bullet.",
      "For (1) many organizations do not have the sophistication or infrastructure to maintain reliable quality data derived from a clickstream. There are inherent problems of bias in clickstream data that you must overcome. Overcoming this bias is doable, but non-trivial. This data can only be massaged so far (users only click, purchase, etc what they see in search results). So getting good test data from a clickstream means applying a bag of tricks to overcome this bias without corrupting the original authenticity of the data. A tough balance to get right. On top of that, you often need more than clicks. Clicks can be noisy, and uncertain indicators. Often user actions with commitment (purchases, etc) have more reliability.",
      "It goes without saying that manually creating judgments (2) is painful. Few have the patience to sit in front of a tool like Quepid or a spreadsheet and record which results were good, bad, ok, etc for a long list of test queries. For this reason, (2) takes skill in prioritizing enough representative queries for experts to grade. You’re always asking yourself questions like -  \"Did we get enough queries that cover searching by movie title?\" A second concern, as Elizabeth Haubert points out in her excellent Haystack talk, is judges can be unreliable. So for some problems, careful analysis of judges for reliability may be needed, and you can detect which judges are more reliable / consistent.",
      "Reflective Testing",
      "Both click-based & expert created judgments have their pros and cons. So they’re often used together. Click-based data comes in large volumes, so has a lot of statistical significant. Expert-created judgments can have more reliability per judgment (if done right) BUT takes a lot of human labor to get right.",
      "Lately, we’ve been using a third method for relevance testing: Reflective Testing. I’m becoming a big fan of it for certain use cases. After all our whole mission is to empower great search teams(™), and what could be more empowering than a relevance testing method that divorces you from some of the morass of click or expert based relevance judgments to get started on relevance work sooner?",
      "Reflective testing uses prominent phrases, features, or other aspects of a document (or related documents) as test queries. An obvious example is taking a document’s title, issuing that to the search engine as a search, and checking whether that document was returned as a first result.",
      "This seems like a hilariously obvious and naive testing method. No duh, right? But it can be effective and liberating mindset for organizations to get into. And it goes beyond just copy-pasting the title. Instead of asking ourselves \"what documents should come back for this query\" let’s turn the question upside down \"what queries are good candidates for this document?\". This goes far beyond just copy-pasting the title.",
      "Instead of asking this question document-by-document, we can abstract some obvious rules for how documents (really the entities/ideas related to document) might go into a users head, down their arm, and (mis)typed out as search queries.",
      "Consider an e-commerce search, with the following metadata:",
      "[\n{\"name\": \"pencil dress\",\n \"color\": \"red\",\n \"brand\",\"ralph lauren\"},\n{\"name\": \"blue jeans\",\n \"color\": \"blue\",\n \"brand\": \"levis\"}\n]",
      "How might we ‘reflect’ some of these documents into reasonable queries that are good matches? By knowing just a tad about our user, and how they search, we can code up some business rules that take as input a document, understand something about its structure, and output query with relative weight. This is the ‘reflective’ comes in, we’re reflecting a document back into queries.",
      "Some obvious good methods might be",
      "The full name as a moderately strong query for the document\n  The color plus the full name as a strong indicator (\"red pencil dress\", \"blue blue jeans\")\n  Just the brand or color, but as a relatively weak query for each document \"levis\" or \"ralph lauren\"",
      "From these weak/strong queries we generate from the document, we end up building a judgment list using all the documents that generate the query. Documents that generate the query weakly would be graded lower for the query than those graded higher. So we might end up with a judgment list that looks like:",
      "Doc\n      Query\n      Grade 0-4\n      Use Case\n    \n  \n  \n    \n      blue jeans\n      blue jeans\n      4\n      Exact Match\n    \n    \n      blue jeans\n      levis\n      3\n      Brand\n    \n    \n      blue jeans\n      pencil dress\n      0\n      (query not generated)\n    \n    \n      pencil dress\n      pencil dress\n      4\n      Exact Match\n    \n    \n      pencil dress\n      ralph lauren\n      3\n      Brand\n    \n    \n      pencil dress\n      levis\n      0\n      (query not generated)",
      "Note this is the same judgment list we’d use if we asked experts to grade queries, just with our document as the primary object of evaluation. If we sort this table by \"query\", we’ll get a classic judgment list we can use for evaluation.",
      "With more sophistication, more query patterns could be extracted to flesh out this judgment list:",
      "Use NLP to figure out the primary noun in the item (\"jeans\" or \"dress\")\n  Mutate the query with common typos\n  Weight common terms relative to the popularity of the item\n  Apply reasonable taxonomy to the color / items to generate queries for semantically proximate terms (\"pants\", \"turquoise\", \"turquoise trousers\") obviously weighted lower",
      "The goal with this method of testing is not to be exhaustive. Indeed, that’s a downside to it. You’re in a sense identifying and regression testing use cases that your solution should get right with gradually increasing sophistication. You can put a number on each use case, and use it in conjunction with other forms of relevance testing to make decisions. It compliments, doesn’t replace, other forms of relevance testing.",
      "Another issue is at some point you just start solving the problem itself. But to me solving and testing simultaneously is ok. I may be disabusing myself of the preconception that relevance testing always requires this untainted ‘pure’ judgment list that has had no engineering applied to it. Afterall, regular unit tests can require test code up to the level of sophistication of the solution being tested. The complexity of test/solution go hand-in-hand.",
      "Moreover, its a kind of ‘solving’ that I think can engage product/business experts in a way that’s closer to how search actually works. Imagine this conversation:",
      "Product person: the person searched for ‘turquoise jeans’ and got no results, they should have matched this document\n  Relevance engineer: Sorry \"turquoise pants\" won’t match here, notice there’s no terms for turquoise to match on in this document\n  Product person: well how do we inject that term, and prioritize this query (and others that fit this pattern) relative to the other queries that might fit this document?\n  Relevance engineer: Let’s collaborate on a business rule. Within this taxonomy, close, but non-exact color matches have this relative weight relative to other possible generated queries for this document\n  (collaboration ensues)",
      "After this conversation the ‘reflective test’ turns into something of a regression test for this business rule, and hopefully can be generalized to other documents.",
      "Can we synthesize learning to rank training data?",
      "One big experiment I am working on is whether you can take this approach and generate a judgment list suitable for training machine learning model? If you recall from an earlier blog post, learning to rank requires labeled judgments. Lots of them! So a big hurdle is setting up the same infrastructure needed for relevance testing covered in the first part of this article.",
      "Can you capture the important queries for each document, turn them into a judgment list, and then use them to train a model that can be used for ranking? In a sense, this reflective work IS feature engineering based on someone’s assumptions about what a ‘good query’ is. It seems crazy, but some of the methods you’d use to generate ‘good queries’ you wouldn’t want to do during runtime. So in a sense you’re capturing computationally expensive patterns (complex NLP stuff) in another medium (a judgment list) so that simpler features (Solr/ES queries) can try to approximate them.",
      "It’s a crazy idea, and there’s a good chance it won’t work, but If you’re curious, check out this github repo for very early experiments. And don’t hesitate to get in touch with questions!",
      "Acknowledgments",
      "Want to call out the great work of Peter Fries and Dan Worley from our team for operationalizing many of these ideas for various OSC clients!"
    ],
    "summary_t": "Exploring a method of search relevance testing that doesnt suffer from drawbacks of clickstreams or expert user judgments"
  },
  {
    "id": "6bfd932f29d417ada40ceb8a71bfc586",
    "url_s": "https://opensourceconnections.com/blog/2018/06/06/why-relevance-training/",
    "title": "Why We Created Search Relevance Training",
    "content": [
      "I am a big proponent of Solr and Elasticsearch search engines BUT…",
      "… they are frameworks. A bag of tools. I can teach you how they fit together in half a day. A bit like chess - the rules are simple, but being a master can take a lifetime! Mastering the relevance ‘game’ is no different. Solving the gnarly, complex, and diverse business problems out there takes a new set of skills desperately lacking in the marketplace.",
      "The fact that most real business problems are gnarly is exactly why I AM a fan of Solr & Elasticsearch. I’m not a fan of commercial/canned solutions with a few knobs and dials, where you hit walls and have no option to customize further and deeper. With Open Source, there’s always another layer deeper you can go to customize to fit your unique and innovative use case.",
      "But what’s usually missing is a cohesive set of practices that tie together the open source toolbox to solve real problems. This is why we wrote Relevant Search.",
      "And this is why we’re announcing Think Like a Relevance Engineer Training! Most Solr & Elasticsearch training  enumerates to you what’s in the toolbox. They don’t teach you the techniques to focus the toolbox on real business problems. It’s not just about the \"What\" but the \"How\" and \"Why\" of business and domain problems.",
      "Because each business is different, organizations need to appreciate how to customize open source search to serve their unique needs. E-commerce remains quite different than job search, which is quite different than enterprise search, which is quite different that Google’s domain of Web search.",
      "This training is for teams that need to really own how their search works. It teaches cutting edge information retrieval with an eye towards what’s practically achievable at scale with the Solr & Elasticsearch toolbox.",
      "I hope to see you in class, here’s the link to sign up:",
      "Explore Upcoming Courses",
      "Want private training?",
      "Don’t hesitate to get in touch to discuss having this training delivered privately for your organization. This is a great opportunity to ‘pick the brain’ of a relevance expert as we perform training."
    ],
    "summary_t": "I love Solr and Elasticsearch, BUT…"
  },
  {
    "id": "d33b043a4e52d87098975c75f3d8cabe",
    "url_s": "https://opensourceconnections.com/blog/2018/06/14/enterprise-data-world-2018-recap/",
    "title": "Enterprise Data World 2018 Recap",
    "content": [
      "In April I went on a pilgrimage to Enterprise Data World to encourage my colleagues in the Data world who typically are focused on issues of Data Governance, Data Quality, and Data Standards, to think more about data from the perspective of the line of business end user who is actually trying to accomplish something with their precious data!  To echo a line from the great keynote by Scott Berkun, data doesn’t actually do anything by itself, it’s what we do with the data that matters!",
      "I’m going to recap a couple of sessions below, and then share my key take aways.  There were were so many sessions, it was almost too hard to pick what to focus on!",
      "Recap of sessions",
      "I started out sitting in on a great talk by Cathy Normand from ExxonMobile on how to enable metadata strategies.    Interestingly she articulated a strong you need to market to your users philosophy.  Too often in the IT world we take the \"build it and they will come\" philosophy, something I am seeing with a current customer project ;-).   She talked about using gamification and going out to meet the end users in their natural habitat and evangelizing the need to provide good quality metadata as key:",
      "It’s a lot of work, but it’s also always changing, and that’s interesting!",
      "I also attended a session on the FIBO group about Knowledge Graph and AI, who had their own sub track at the conference.  FIBO stands for Financial Industry Business Ontology.  I had an interesting conversation with a fellow attendee about it, and it in some ways represented the best and worst of Semantic thinking.   FIBO is very powerful in modeling relationships, but only as long as the relationships being modeled actually reflect the real world!   And the real world, especially in some of the less traditional finance industries, seem to be much messier then these structured semantic worlds.   It reaffirmed my thinking that semantic structures, and the query languages like SPARQL are great for a very narrow well-defined domain, but the reality is most of us work in much broader multi domain worlds, where highly structured relationships like FIBO break down.",
      "I then got to sit in on a talk that finally introduced semantic blockchain…   I knew someone would bring up that buzzword ;-).",
      "I dropped in on a talk that purpoted to demonstrate how easy it is to build a UI using transformation languages like XQuery.  Having burned my fingers on building a complete robust website using XQL, I listened to see if the speaker changed my mind, but he didn’t.   XML transformation languages are not good choices for building robust applications (hint hint Marklogic, TopQuadrant, etc).",
      "There was another interesting talk about doing entity extraction on forms, and how they did specific analyzers to look for \"wet ink\" style signatures etc.   Laid out some nice pros and cons of building it outside of your data platform.   It was also cool to see the shout out for Tesseract OCR.",
      "I really enjoyed Scott Berkun’s keynote talk on creativity and ideas.",
      "I like pyramids, so does Scott Berkun",
      "His keynote felt more topical than most that I’ve seen, and I thought he posited some very interesting ideas, specifically that he feels the phrase the data says should be verboten!   I like that he pushed the message that the data is always interpreted by people.   When we set up a relevance project, we typically are pushing to identify some pretty specific hard KPI’s to measure the impact of the work we are doing.   For example \"Revenue per Search\" or \"Requests for Data Set per Search\", as well as the oldie by goodie \"NDCG\" metric.   I wonder if KPI’s that are generated by data are a case of falling in the the data says trap….?",
      "My Takeaways",
      "One of OSC’s tag lines was Data –> Information –> Wisdom, which mapped nicely on to a pyramid of creativity that Scott mentioned.\n  \n  \n    We’ve seen that as you move up the pyramic, you need more and cleaner data sets to get closer to that, dare I say it, Cognitive AI search platform.   I don’t think we’ll ever really go deep in Data Governance, beyond recognizing that it’s a good thing to encourage good data sets ;-)\n  \n  \n    In learning more about data quality, I’m seeing some tools and ideas to help us better understand the shape of the data, and incorporate the quality of the data as a signal into our relevancy algorithms.  I can see us having boost factors based on quality of data set in making matches.  Or deciding what datasets to use or not based on a analysis of data quality.\n  \n  \n    Enterprise Data World since my last visit in 2014 has become more relevant to search folks, and I’m not sure if the topics have changed over time, or if our focus on data and machine learning has led us closer to the conference’s core topics!",
      "Lastly, I also want to point to a Twitter thread that @HealthcareWen shared.   She captured some great nuggets of information from a number of sessions that I didn’t get to attend.",
      "Comics!  She spent the week drawing amazing comics as speakers talked!",
      "One More Thing",
      "I’m working with a client who has ~60 data sources in various systems, and we’re working with them to start answering questions that they never could previously by bringing together data sets that had never been joined before.   They have a very intuitive sense of \"what the right decisions are\", but correctly, are looking to check those intuitions against actual data to help them refine their decision making.   Plus, find new startling insights  by finding less obvious patterns in the data.   A key challenge is that there is no \"roadmap\" to the datasets.   They are doing some good governance things like establishing a data steward for each data set, but there are no cross data set links.   To figure out how two data sets would work together.   I have a mental picture of a traditional highway map, where the think Interstate lines map the very strong connections between data sets, ie. good primary keys, clean data sets, complete records on either side.   And then lesser lines to represent highways, byways, and then local roads reflecting the less probably connections between the data sets.",
      "I’m thinking this is a graph of knowledge, but how to get that knowledge?   I’ve been involved in too many projects that said \"data lake\", or \"data warehouse\" and spent all the budget moving the data around, and not enough on understanding it.   So how to flip that paradigm?   One of the topics was the idea of Data Virtualization, a new term to me.  here the idea is that instead of directly accessing the data, I work with a virtual data set.   That virtual layer abstracts me form the underlying format, and lets me keep my 60+ data sets in their original platform, whether that is a RDBMS, a CSV flat file, a big data solution, or even a API or website…   It lets me start playing with the data using a common language like SQL.   Denodo is the vendor at EDW who was talking about this approach, and from a \"lets get started\" perspective, there was a lot I liked.    Yes, it has limitations, including I am querying potentially production systems…   I’m intrigued to see if I can put on my Data Cartographer hat and start building that roadmap to the data for other business oriented folks to leverage to ask questions they didn’t think they could before!"
    ],
    "summary_t": "Your precious data is typically an ugly unpolished gem.  Enterprise Data World was all about how to polish it up!  Some thoughts from a Relevancy perspective."
  },
  {
    "id": "dacdb394122e7e1fcd0a97f232402935",
    "url_s": "https://opensourceconnections.com/blog/2018/07/05/sub-phrase-highlighting-in-solr/",
    "title": "Sub-phrase Highlighting in Solr",
    "content": [
      "We wanted Solr to use its ability to do subphrase matching (pf2, pf3) to procedurally generate richer, contiguous highlighting snippets. Unfortunately, we reached a dead end in our efforts to do this without resorting to more complex tactics.",
      "For example, imagine you to searched for q=’mary had a little sheep’. But your documents included:",
      "Mary had a little lamb\n  Little sheeps had a mary",
      "Now of course the first result is the most relevant to the query. It’s nearly (but not quite) a full phrase match for the query. But default Solr highlighting doesn’t do a great job of showing you why. Solr’s highlighting will do:",
      "<em>Mary</em> <em>had</em> <em>a</em> <em>little</em> lamb\n<em>Little</em> <em>sheeps</em> <em>had</em> <em>a</em> <em>Mary</em>",
      "If our queries are long like this, we might want to show the various phrase/subphrase matches in our highlights. Perhaps omitting even single term matches as too spurious. For example, we might prefer something like",
      "<em>Mary had a little</em> lamb\n<em>Little sheeps</em> had a mary",
      "One way to do this is to create bigram/trigram tokens using a shingle filter. This works by placing 2 or 3 phrase tokens in the index. However, for users with very large numbers of documents, having the added index bloat is untenable.",
      "Tricking Solr phrase highlighting?",
      "Solr fast vector and unified highlighters have a parameter called hl.usePhraseHighlighter. One thought is this parameter should allow us to search using pf2 or pf3, then perform highlighting on those subphrase matches. We would hope the following should work:",
      "q=mary had a little lamb&\ndefType=edismax&\npf2=title&\nhl.usePhraseHighlighter=true&\nhl.useFastVectorHighlighter=true&\nhl.method=fastVector",
      "If you turn debug=true on, this query is parsed into:",
      "(+(DisjunctionMaxQuery((title:mary)) DisjunctionMaxQuery((title:had)) DisjunctionMaxQuery((title:a)) DisjunctionMaxQuery((title:little)) DisjunctionMaxQuery((title:lamb))) (DisjunctionMaxQuery((title:\"mary had\")) DisjunctionMaxQuery((title:\"had a\")) DisjunctionMaxQuery((title:\"a little\")) DisjunctionMaxQuery((title:\"little lamb\"))))/no_coord",
      "You would think each of those searched-for phrases would be highlighted. Puzzlingly, this isn’t what happens. Instead Solr persists in doing single term highlights.",
      "What’s odd is you indeed DO see phrase highlighting when you explicitly search with phrase queries. You can override the ‘hl.q’ Indeed, something like the following does indeed work:",
      "hl.q=\"mary had\" OR \"had a\" OR \"a little\" OR \"little lamb\" &\nhl.usePhraseHighlighter=true&\nhl.useFastVectorHighlighter=true&\nhl.method=fastVector",
      "Of course this requires the client to procedurally generate the query, which is less than ideal.",
      "User vs ‘boost’ queries - an important edismax gotcha",
      "If you do some sleuthing in the highlighting code, you’ll learn that the query used for highlighting is controlled by the query parser. Specifically, each Solr query parser overrides getHighlightQuery, which takes parameters such as ‘hl.q’, and decides what query should be used for highlighting. For most query parsers, this would be the same query generated for search:",
      "public Query getHighlightQuery() throws SyntaxError {\n Query query = getQuery();\n return query instanceof WrappedQuery ? ((WrappedQuery)query).getWrappedQuery() : query;\n}",
      "However edismax (and dismax) distinguish between what they call the \"main user query\" – what’s passed to \"q\" – and various add-ons like pf* and boosts. This \"main user query\" the ‘unaltered’ user query is what (e)dismax chooses to return for highlighting. If you’re in the code, the relevant variable in ExtendedDismaxQParser is parsedUserQuery:",
      "@Override\npublic Query getHighlightQuery() throws SyntaxError {\n if (!parsed)\n   parse();\n return parsedUserQuery == null ? altUserQuery : parsedUserQuery;\n}",
      "Indeed, various features in Edismax (qs for example) differentiate the query ‘from the user’ vs all the background boost queries, functions, etc a developer would layer on. It’s important to keep this in mind when working with Edismax. What this means is that at the end of the day, unless quotes are explicitly used by a user, edismax/dismax will give you a term-only query.",
      "Some alternatives",
      "We’ve already talked about one alternative: generating a boolean query yourself and passing it to hl.q. Tokenizing the user’s query on whitespace and doing this, should probably work most of the time. Of course ‘whitespace’ isn’t the only thing a Solr query analyzer would tokenize on, so it’s far from perfect.",
      "Another option is to use a query parser plugin with hl.q/hl.qparser that gives you more flexibility with slicing and dicing the query string. For example, our match query parser lets you specify a query-time analyzer, then turn the result into a phrase query. So you can bigram a query via a shingle analyzer, then turn those bigrams into phrase queries. Maybe this is a good time for me to set aside time to finally incorporate this into Solr :)",
      "Note, this is subtly different than using normal Solr query-time only shingles and setting autoGeneratePhraseQueries=\"true\", as shingles generates single term tokens, and autoGeneratePhraseQueries works with the token graph to try and construct good phrase queries from multiple, connected tokens. Though perhaps there’s a graph-aware shingle-like token filter that generates graphs of adjacent tokens, not just tokens.",
      "What am I missing?",
      "That’s the end of this round of spelunking :) It’s likely I’m missing something obvious in my approach, or missed an obvious solution. Get in touch!"
    ],
    "summary_t": "We share our attempts to implement partial phrase highlighting in Solr"
  },
  {
    "id": "5b15e140fc9bbd0da0105c62e40ef97b",
    "url_s": "https://opensourceconnections.com/blog/2018/08/06/intro_solr_nlp_integrations/",
    "title": "Exploring Solr's OpenNLP Integrations",
    "content": [
      "We’ve focused most of our conversation about machine learning and search on Learning to Rank (LTR), but LTR isn’t the only machine learning tool in the Solr/Elasticsearch toolbox.   LTR uses feedback to balance a variety of signals, and better signals are the key to more relevant search.  Natural Language Processing (NLP) attempts to bring in smarter language models, to start moving from bare text tokens to tokens-with-meaning.",
      "OpenNLP is a java-based toolkit for common natural language processing tasks - tokenization, tagging, chunking, and parsing, among other things.  Since this is precisely the challenge the analysis chains in Solr or Elasticsearch must solve, it seems natural to incorporate the openNLP functionality into Solr.  The idea isn’t actually brand-new.   Lucene had an open issue since 2011 to integrate OpenNLP.  This finally became mainstream with the Solr 7.3 release.  Solr 7.3 also introduced a new update request processor to use the OpenNLP-based entity extraction and language detection.",
      "Using the Solr OpenNLP Filters",
      "There are a few key classes:",
      "OpenNLPTokenizerFactory, which tokenizes and marks sentence boundaries\n  OpenNLPPOSFilterFactory, which rolls in part-of-speech tagging\n  OpenNLPChunkerFilterFactory,  which will replace the POS tags with a ‘phrase chunk’ label",
      "Since Lucene does not yet index token types, in order to make that information available to queries, it is necessary to push the type either to a payload or as a synonym token using either TypeAsPayloadFilterFactory or TypeAsSynonymFilterFactory.",
      "In order to effectively use the tools available, I like to understand what they are doing under the hood.   So I’ve set up an instance of Solr 7.4, and indexed the TMDB movie data, using these tools, plus the following field snippet:",
      "<fieldType name=\"text_opennlp\" class=\"solr.TextField\" positionIncrementGap=\"100\" multiValued=\"true\" >\n      <analyzer>\n        <tokenizer class=\"solr.OpenNLPTokenizerFactory\"\n                   sentenceModel=\"en-sent.bin\"\n                   tokenizerModel=\"en-token.bin\"/>\n        <filter class=\"solr.OpenNLPPOSFilterFactory\" posTaggerModel=\"en-pos-maxent.bin\"/>\n        <filter class=\"solr.OpenNLPChunkerFilterFactory\" chunkerModel=\"en-chunker.bin\"/>\n        <filter class=\"solr.TypeAsPayloadFilterFactory\"/> \n      </analyzer>\n    </fieldType>\n    <field type=\"text_opennlp\" name=\"title_opennlp\" multiValued=\"true\" />\n    <field type=\"text_opennlp\" name=\"overview_opennlp\" multiValued=\"true\" />\n    <copyfield source=\"title\" dest=\"title_opennlp\" />\n    <copyfield source=\"overview\" dest=\"overview_opennlp\" />",
      "The first thing to notice is that each of those classes uses a model, and the tokenizer uses two.  Those models aren’t available from Solr; they must either be downloaded from the OpenNLP project or you need to build your own. Even then only the Language Detector (not used in the analysis chain above) is available on the download page linked in by the Solr documentation.  I found some older models in (I think) the same format after some digging on the OpenNLP Page.  They seem to be compatible for a toy problem, but it is a real concern for taking a system to production.",
      "So what happens when we start indexing some content?  Let’s look at \"George Washingon meets the cat in the hat.\"  On first glance at the analysis page, not much changes, other than our sentence punctuation is preserved:",
      "",
      "Try that again, with the verbose output:",
      "",
      "Compare that to a text_general analysis:",
      "",
      "And the more interesting differences start to show up.  The OpenNLP tokenizer has identified each token as a \"word\", not an \"<ALPHANUM>\", and each token has been assigned a flag; these flags are blank for now, except that trailing \".\", which has been identified as a sentence boundary.   The OpenNLPPosFilter has changed each of those \"word\" tags to a label denoting a more specific part of speach.  OpenNLP uses the Penn Trebank POS labels.  \"George\" and \"Washington\" have been identified as singular proper nouns; \"meets\" is a 3rd person singular present verb; and so on.",
      "Chunking isn’t quite so intuitive.  The idea behind chunking isn’t necessarily to identify phrases in the text in the sense that we would use in the context of multi-term synonyms or shingles.  Chunking identifies a higher-level order in the sentence, but at a fixed content depth.   A chunk tag consists of up to two parts: a prefix, indicating how the token relates to the chunk, and a suffix indicating the role of the chunk itself in the sentence.  So in the above sentence, the first chunk, \"George Washington\" is a proper noun.  \"George\", the first token in this chunk, is tagged \"B-NP\"; \"B\", indicating it is the first term in the chunk, \"NP\" indicating the chunk is a noun phrase.   The second chunk \"meets\", is only a single token long, so it is tagged with \"B-VP\", indicating it is a verb, and there is no subsequent \"I-VP\" term.  The trailing punctuation \".\" is tagged \"O\" - outside a chunk.  Finally, the TypeAsPayloadFilter moves that type string into the payload, so we can use it for subsequent queries.",
      "Using POS tags in queries",
      "Getting that data into the index is only the first step;  using it to improve relevance signals is still a non-trivial amount of work.  While Solr offers two  payload query parsers, only the Payload Check query parser can handle string-based query parsers.  This accepts a list of payload strings, and restricts matches to tokens having the appropriate payload.   This would give some measure of disambiguation - Lucidworks example for how the query parser might be used gives the example of ‘train’ the noun vs. ‘train’ the verb.  But in practice, generating that string of payloads to encompass all variations a noun phrase or verb phrase might will also require some engineering - there is no analysis on payload tokens.",
      "OpenNLPExtractNamedEntitiesUpdateProcessor",
      "Those payloads may not buy so much at query time, but it does lay the foundation field for Named Entity Extraction.  Named Entity Recognition (NER) seeks to locate and classify particular kinds of things - usually the names of people or organizations, but what constitutes an interesting entity is pretty domain-specific.   With Named Entity Extraction, when the model recognizes a particular kind of entity (like person names), then that entity can be copied out of the bulk text bag-of-words to a new field.   Which is exactly what the OpenNLPExtractNamedEntitiesUpdateProcessorFactory does.    This, in turn, opens up the door to a new level of dynamic classification and tagging.  How much the models available from OpenNLP works out of the box for your entities depends on the problem domain.  Mileage may vary, but looking for proper names mentioned in a large-ish chunk of text is pretty powerful.  Extending that to recognize semi-proper names such as products or institutions becomes a much smaller problem.",
      "What’s next?",
      "If you would like to discuss how your search application can benefit from machine learning, please get in touch. We’re also always on the hunt for collaborators or for war stories from real production systems. So give it a go and send us feedback!"
    ],
    "summary_t": "We share our thoughts on the Solr OpenNLP Integrations"
  },
  {
    "id": "b0852d03b4fe34c349b8b428c5e5ad24",
    "url_s": "https://opensourceconnections.com/blog/2007/06/26/continuous-integration-war-stories-meetup-in-nova/",
    "title": "Continuous Integration War Stories Meetup in NoVA",
    "content": [
      "I wanted to let folks know that Andrew Glover and Paul Duvall are hosting a \"Share your Continous Integration War Stories\" meetup in Northern Virginia. Andrew and I met at CITCon, and he and Paul have written \"The Book\" on Continous Integration. Below are the details:",
      "Weâ€™re hosting a â€œShare your Continuous Integration War Storiesâ€ (http://www.stelligent.com/content/view/128/62/) event this Thursday evening at our offices in Reston, VA â€\" it should be a great opportunity to hang out and discuss CI over wine, beer, and other refreshments. In addition, weâ€™ll be raffling off the first signed copy of the CI book Paul authored.",
      "When: Thursday, June 28th from 5:30 PM to 7 P.M\nWhere: Stelligents headquarters",
      "11495 Commerce Park Drive",
      "Reston, VA 20191\nWho: Anyone who is using CI or thinking about CI (and loves wine)",
      "If youâ€™re interested, please RSVP ([email protected]). Hope to see you on Thursday!",
      "Andy writes the very entertaining Disco Blog covering software development and testing practices."
    ],
    "summary_t": ""
  },
  {
    "id": "2b24eeb9eb57273431323a6fa309f8cf",
    "url_s": "https://opensourceconnections.com/blog/2018/10/04/missing-hello-world-for-open-nlp/",
    "title": "The Missing Hello World for OpenNLP",
    "content": [
      "I wrote this back in 2012 for version 1.5.2-incubating, and never published it.  So I’m updating it for the October 2018 version of OpenNLP, 1.9.0.",
      "Visit http://opennlp.apache.org/ and you will discover that the Hello World for OpenNLP example is missing!  Click on the Documentation link and you’re dropped into the deep end of OpenNLP coding details.",
      "SmallBizContracting - An experiment in finding the bull-shitters in the federal space!  We used OpenNLP to power our autosuggest capability. No longer online ;-(",
      "A typical search feature is provide some sort of \"autosuggest\" function.  The initial approach is just to provide snippets of results based on either facet query, or look at the terms that are available.  That can help you complete a single word, but doesn’t help with multiple words.  So for example, if you start typing \"home\", we want to auto suggest the concept \"home health care\".  However, Solr doesn’t know that is a valid concept.",
      "So OpenNLP to the rescue!",
      "We are going to use OpenNLP to parse the sentence:",
      "Avenue HomeCare, a North Carolina state licensed home health care agency of registered nurses, licensed practical nurses, certified home health aides and nursing assistants providing care for people in their own homes.\"",
      "Hopefully we will pull out licensed practical nurses and home health care as logical groupings of text that should be suggested through a technique called Chunking that puts tells you which words go together as a single chunk in a sentence.",
      "Step 0: Download OpenNLP!",
      "Grab a copy of OpenNLP and unzip in your working directory.",
      "Step 1: Build a Model",
      "The first step turns out to be the hardest step.  The magic of OpenNLP comes from it being trained on carefully curated datasets to understand what the structure of your text, so it can do the right things.  These data sets are apparently very difficult to find, and to build your own you need between 10,000 and 50,000 rows of example data!  The resulting trained model is a binary file that contains all the rules required.  You can play with some published models at http://opennlp.sourceforge.net/models-1.5/, however your milage may vary as they are built on old data sets. Note: These are the same models as when I wrote this back in 2012!",
      "Apparently figuring out how to build the model training dataset is the hardest part.  We want to build our own copy of the en-chunker.bin available at http://opennlp.sourceforge.net/models-1.5/.  Googling for \"conll2000 chunking\" leads us to http://www.clips.ua.ac.be/conll2000/chunking/ where we can download the training dataset:",
      "wget https://www.clips.uantwerpen.be/conll2000/chunking/train.txt.gz\ngunzip train.txt.gz",
      "And then we can train our model based on this dataset:",
      "./bin/opennlp ChunkerTrainerME -encoding UTF-8 -lang en -data train.txt -model helloworld-chunker.bin",
      "We now have a model called helloworld-chunker that knows how to group words together in a sentence.",
      "Step 2: Part of Speech Tagging",
      "The chunker works on a sentence that has been tagged with all the parts of speech, that is what it uses to figure out how to group things together.  So back to OpenNLP we go with our sample paragraph to mark it up.  First we need to grab an already pre trained model:",
      "wget http://opennlp.sourceforge.net/models-1.5/en-pos-maxent.bin",
      "The text needs to be whitespace tokenized, so convert the sentence:",
      "Avenue HomeCare, a North Carolina state licensed home health care agency of registered nurses, licensed practical nurses, certified home health aides and nursing assistants providing care for people in their own homes.",
      "into one where there are spaces around the punctuation:",
      "Avenue HomeCare , a North Carolina state licensed home health care agency of registered nurses , licensed practical nurses , certified home health aides and nursing assistants providing care for people in their own homes .",
      "And now use that sentence by firing up the POSTagger and then paste the white spaced sentence into the resulting terminal prompt that is blinking at you:",
      "./bin/opennlp POSTagger ../en-pos-maxent.bin\n\nLoading POS Tagger model ... done (1.431s)\n\nAvenue HomeCare , a North Carolina state licensed home health care agency of registered nurses , licensed practical\nnurses , certified home health aides and nursing assistants providing care for people in their own homes .\n\n\nAvenue_NNP HomeCare_NNP ,_, a_DT North_NNP Carolina_NNP state_NN licensed_VBD home_NN health_NN care_NN agency_NN of_IN registered_JJ nurses_NNS ,_, licensed_VBN practical_JJ nurses_NNS ,_, certified_JJ home_NN health_NN aides_NNS and_CC nursing_NN assistants_NNS providing_VBG care_NN for_IN people_NNS in_IN their_PRP$ own_JJ homes_NNS ._.",
      "You can see the sentence of text now has been marked up with a set of codes describing what part of speech each token represents.",
      "Step 3: Finally do the Chunking!",
      "Maybe this really should be called grouping versus chunking, but this is where we find out how good our training model is.  We take the full marked up sentence and pass it into the chunker, using the model we trained:",
      "./bin/opennlp ChunkerME helloworld-chunker.bin\n\nLoading Chunker model ... done (0.446s)\n\nAvenue_NNP HomeCare_NNP ,_, a_DT North_NNP Carolina_NNP state_NN licensed_VBD home_NN health_NN care_NN agency_NN of_IN registered_JJ nurses_NNS ,_, licensed_VBN practical_JJ nurses_NNS ,_, certified_JJ home_NN health_NN aides_NNS and_CC nursing_NN assistants_NNS providing_VBG care_NN for_IN people_NNS in_IN their_PRP$ own_JJ homes_NNS ._.\n[NP Avenue_NNP HomeCare_NNP ] ,_, [NP a_DT North_NNP Carolina_NNP state_NN ] [VP licensed_VBD ] [NP home_NN health_NN care_NN agency_NN ] [PP of_IN ] [NP registered_JJ nurses_NNS ] ,_, [VP licensed_VBN ] [NP practical_JJ nurses_NNS ] ,_, [NP certified_JJ home_NN health_NN aides_NNS ] and_CC [NP nursing_NN assistants_NNS ] [VP providing_VBG ] [NP care_NN ] [PP for_IN ] [NP people_NNS ] [PP in_IN ] [NP their_PRP$ own_JJ homes_NNS ] ._.",
      "Now, this isn’t the easiest to read, but you look at the content between the [ and ] brackets to figure out what was grouped.  You can see that we have as groups:",
      "Avenue HomeCare\na North Carolina state\nhome health care agency\nregistered nurses\npractical nurses\ncertified home health aides\nnursing assistants",
      "With the exception of a North Carolina state, all of these look like really good sets of phrases to offer up as autosuggest suggestions.",
      "Have fun with !"
    ],
    "summary_t": "In 2012 I first saw OpenNLP, and was both excited by it, but also appalled by the documentation.  I wrote this blog post in 2012, but turns out I never actua..."
  },
  {
    "id": "5130d7ef9fdf129206ff067cf53621ae",
    "url_s": "https://opensourceconnections.com/blog/2018/10/19/thinking-of-ai-for-search/",
    "title": "Think AI will solve your search problems? Ask these questions first",
    "content": [
      "It’s understandable to be frustrated with search. You’ve probably heard someone like OSC lecturing you on relevance best practices, taxonomies, metrics, measurement, and more. You’ve probably thought \"wow that’s a lot of work.\" Indeed!",
      "An understandable question you might then ask is - \"well won’t AI just solve all these problems soon?\"",
      "If only - if anything it creates many many new problems.",
      "While there’s a lot of upside to machine learning in search, I do need to warn you. There’s reasons to be wary of introducing machine learning to your search stack! Consider these questions before pulling the trigger on a solution.",
      "Are you ready for the extra cost and complexity?",
      "Beware of switching to a machine learning solution because it \"makes things easier/cost less.\" Indeed, the opposite is almost always true. Expect it to be harder, more complex, and more expensive (but hopefully with higher payoff). I regularly make arguments to clients why ML solutions should be avoided, trying to help them understand the downsides relative to simpler solutions. Fellow search consultant, Daniel Tunkenlang, lists many of the complexities. Many of the myriad concerns include",
      "Interpretability - it’s often hard to know why a model or solution is doing what it’s doing\n  Maintainability - there’s often many more moving parts in product-ready machine learning solutions\n  Garbage in, garbage out - Gathering, cleaning training data and is very time consuming, as is validation that the model is doing what you expect at scale\n  Expertise - The experience required to do machine learning well is hard to find and expensive.",
      "The reason to take on a machine learning approach is because the potential outcome is worth the higher investment. There’s a big win to me made. But if you don’t need it, don’t invest in it. There’s often a lot of upside in avoiding machine learning.",
      "How will you know if your machine learning investment is successful?",
      "Another characteristic of great search teams: they are obsessed with measurement and feedback. They do this because search is core to their value, and they absolutely must measure how any change impacts that value.",
      "They have two things down pat:",
      "KPIs & Instrumentation - can you tie search to a business metric? This is often easier said than done. But wouldn’t it be great if you could brag about how well your investment paid of to the C-level?\n  Relevance judgments - a ‘judgment’ encodes how ‘good’ a search result is for a query. There are many ways of coming up with judgments. With a good judgment list, you can compute a sense for whether you positively or negatively impact search before going to production",
      "As my colleague Liz Haubert says in her Haystack talk - the key to all of this is having many, independent sources of measurement - not just one. And definitely not one just tied to your vendor. When it comes to data - never trust, always verify!",
      "Case in point, the two Learning to Rank talks at Haystack Europe spent nearly half the time focused on measurement often with no clear obvious answers of what a relevant search result was for a user’s search. Despite being some of the smartest search people I know; Despite having rich user analytics (clicks, purchases, etc) to draw from AND having expert, internal users grade results. Most distressingly in the TextKernel Learning to Rank talk: the measurement approaches pointed in different directions with ongoing efforts to try to understand why!",
      "Never trust, always verify!",
      "Can you evaluate the AI ‘solution’ as a hypothesis?",
      "If I told you your machine learning solution/project/product/idea was going to fail, which position would you rather be in?",
      "A 6 Month, $5,000,000 project was run. The effort had a negative impact on conversions and product sales, and was deemed to be a failure.\n  A 3 week, $10,000 experiment was run. The failed ‘hypothesis’ was abandoned and the team turned to more promising directions for their experimentation",
      "Even better is the scenario:",
      "30 minutes was used to try out an interesting idea on someone’s laptop. The idea was unsuccessful, and everyone decided to go to lunch",
      "The world is rich with ideas that might work. None that are guaranteed to work for you. Ideas might come from what everyone hears that Google is doing. They might come from some crazy academic paper. Or some consulting firm’s blog.",
      "We never ever tell clients that we know how to solve their search relevance. What we have are educated hunches, based on our experience, that should be validated heavily. The real silver bullet is not machine learning, it’s removing barriers to experimentation. It’s to run dozens of experiments quickly - to ‘fail fast’ to find that one gem that’s actually worth investing in.",
      "But \"removing barriers to experimentation\" is hard work. Most importantly hard organizational work. It’s hard to get bosses and stakeholders to \"plan\" against an experiment-driven approach to doing work. (More on how to do that in a future blog article). Needless to say it can take years and discipline. I empathize with the predicament many search people find themselves in.",
      "Sadly, despite this predicament, it can’t change the reality that many orgs take on relevance as a ‘project’ sadly ending up with lots of money spent on disappointing results.",
      "What problem, exactly, are you solving? What’s the hypothesis?",
      "If you care about search, it seems like a good thing to want to progress higher and  higher up the levels of maturity. Or as my colleague Eric Pugh puts it the search relevance \"hierarchy of needs\".",
      "",
      "Moving up the pyramid, you’ll find solutions of increasing complexity. Luckily many are working to end up at \"generally relevant\" - After moderate tuning, there’s modest improvement in the solution. In other words, basic usage of search engine features improves quality enough (assuming you can measure quality!).",
      "It’s the top two levels almost all organizations stop in my experience. Often for good reason - the last 10% under \"contextual\" and \"predictive\" can take years. Is it worth it? Sometimes yes, sometimes not.",
      "The trick is, armed with diagnostic and measurement data, to find those areas where a cutting-edge technical solution makes sense for your domain. Crucially: it’s important to realize that just because a solution works on a problem (even a problem in your domain) it’s very very likely NOT to apply exactly 1-1 to work you’re doing.",
      "In other words, you need to get good at using sound measurement to find hypothesis where search is lacking. Is the key issue challenging domain-specific misspellings, for example? Is it, like in Snag’s use case, that users don’t use a lot of keywords and instead expect hybrid search & recommendations? Is it, like in Wikipedia’s use case, where users hitting search are almost always the weird exceptions where an exact title search doesn’t match?",
      "The trick is you can’t boil the ocean. You have to move up this pyramid strategically. Use insights into how your users behave to find those areas that are worthwhile for investment.",
      "Again we’re back to knowing, really really knowing, and double, triple, quadruple checking that you have the data to understand how users are searching. You should ask a lot of questions anytime you see a graph that just shows \"we tried X and search got better\". How did they measure that? What was their method? Was it confirmed by other approaches?",
      "How confident are you in the training data you need to solve that problem?",
      "Training data are the examples used to show your model the ‘right’ answer.",
      "Consider texting. If you were building machine learning to predict the word being typed in a text message, you would first need many (and I mean a lot) of text messages. You’d want original, poorly formatted text messages. You’d also want corresponding completions or corrections.",
      "Obtaining all of this is itself comes with many interesting decisions. Do you make assumptions about how people text, and use that to construct the training data? Or try to be more \"pure\". Perhaps looking at how users complete or correct strings in texts? Ie If you see \"serach\" corrected to \"search\" by a lot of users that’s a hint it’s a correction, and thus it should be flagged as a correction?",
      "Getting these decisions right are far far more important to a successful machine learning project than picking the algorithm to use. And training data really is just another kind of test data. Again, to hammer home the measurement and testing theme, this is something to obsess over - garbage in, garbage out. Many orgs spend a lot of time on machine learning, only to learn too late that the training data is poor or they can’t obtain the needed training data.",
      "How’s your orgs AI literacy?",
      "In conclusion, it’s easy to become awed by the results of some machine learning demo. I know, I’ve been awed myself! But many of us who have been through it have a secret to tell you: it’s just math. It’s not robots coming to take over the world. Even better - often its shockingly simple math that’s effective, not some obscure deep learning model you’ll find in the latest paper.",
      "That’s good news - once you do learn this stuff, there’s plenty of room for innovation. Just check out my Activate talk. Learning how the math works takes a lot of the intimidation out of machine learning. Just see the range of Haystack talks where some kind of ‘machine learning’ is used - but the term itself is barely mentioned. Instead the approach is just treated as another approach, not intrinsically better because it is associated with machine learning.",
      "The hard work hidden in these questions is organizational discipline about measurement, feedback, testing, and experimentation. That’s the work that takes years - and it transcends technology.",
      "Our mission is to empower organizations to be able to do this. If you’d like help either through training or consulting get in touch!"
    ],
    "summary_t": "This search stuff sounds like work - won’t AI just solve all these problems soon?"
  },
  {
    "id": "ad3add95cd72d3ca4c471a8bad0cdaaa",
    "url_s": "https://opensourceconnections.com/blog/2018/10/21/haystack-eu-recap/",
    "title": "Relevance Engineering is Here to Stay: Haystack EU 2018 Recap",
    "content": [
      "Thanks Flax and Rene Kriegler for partnering to plan a tremendous Haystack EU event in early October. It was gratifying to see the relevance community come together. I love seeing so many good friends and fellow practitioners working together on our mutual struggles in relevance, search, and discovery.",
      "When we originally had the idea for Haystack last year, we wanted to show search teams the honest truth of search relevance work: it’s hard. There’s no silver bullet (not even AI and machine learning). On the flipside, it is \"just work\" as a client of ours once put it. You put the work in, you get results. If you take shortcuts, well…",
      "True Learning to Rank Stories",
      "Nothing highlighted Haystack’s goal more than our two Learning to Rank (LTR) talks. One from Otto / Shopping24, another from Agnes Van Belle of Text Kernel. Both talks pulled back the veil on real machine learning efforts for two very sophisticated search teams. Even after several iterations, both efforts initially showed to be harming more than helping search quality in A/B tests. Though over time progress was made, it took a good deal of work to get there.",
      "Why?",
      "Both talks pointed to a fundamental product challenge underlying both LTR and hand-tuned search relevance: actually measuring what ‘good search’ is for your domain/product. For many, this means a ‘judgment list’ that grades how good documents are for a query. \"Rambo\" is clearly an A+ search result when a user searches for \"Rambo\". But past simple examples, good judgment lists, that operate at scale, generated from humans or analytics requires strategic thought and planning from many aspects in the business. There’s a lot of \"it depends\" and many domain-specific gotchas. The takeaway for me was: be wary of ‘one size fits all’ approaches to this problem.",
      "For both teams, the gotcha of \"position bias\" came up for click-based judgments in different  ways. Basically the idea is the same that I write about here. Top results tend to get clicked more than results farther down the result list. Which means \"self-learning\" search could end up learning bad patterns if your non-machine learning relevance is garbage. Even when you account for position bias, you might still not have enough training data if not enough users actually see a good result for a query. Garbage in, garbage out.",
      "Shopping24’s talk highlights that the long, winding reality of applying Learning to Rank to search depends a lot on measuring what matters to your business (in this case CPC vs Conversion).",
      "Another issue was including more than just user relevance in the \"grade.\" For Shopping24, CPC is another factor that must be included to support their e-commerce business and merchants. René Kriegler started with a blog post I wrote on modifying the LambaMART training algorithm to optimize user-product marketplaces with Learning to Rank, then took it a step further based on an approach mentioned in the original LambdaMART paper for combining two rankers. Torsten’s team also built tooling to allow a floating-point based grade, forking RankyMcRankFace to make FloatyMcFloatFace. This allows fine-grained adjustment of grades to tweak a \"grade\" up or down based on multiple considerations. Hopefully we get a PR for their great work!",
      "Agnes discusses ‘query underspecification’ - where queries don’t get enough judgments to generate meaningful LTR training/evaluation data",
      "Another issue was combining explicit feedback from users and implicit feedback into a judgment list. Agnes Van Belle talked about how TextKernel/Careerbuilder gave users opportunities to give explicit thumbs-up/thumbs-down feedback. She compared that explicit feedback with implicit feedback gathered via a click model. Both had pros and cons, with the explicit being more accurate but underspecified (few users did thumbs up/thumbs down) while implicit had more bulk feedback, but may not cover all the important use cases (see the slide above). Her talk reminded me of Elizabeth Hauberts Talk at Haystack US that discussed combining user feedback and explicit feedback. In Elizabeth’s talk, the goal was to try to figure out a way to get the best of both worlds: the bulk of analytics with the accuracy of thoughtful domain experts.",
      "Testing, measurement, and experimentation: the silver bullets of search relevance",
      "Two other talks showed tools and techniques for gathering judgment lists. Everyone finds the machine learning exciting, but as you may have heard, doing the fun \"building a model\" part is less than 5% of the work. For search, the hard work of wiring data together to get meaningful judgments is a wheel that’s reinvented over and over again at companies. Allesandro Benedetti and Andreas Gazzarini of Sease showed potential for the community to rally around a tool with their work with Rated Ranking Evaluator or RRE. RRE is a fully-formed continuous-integration framework for search. As we have worked on relevance testing tools, RRE was a breath of fresh air. It’s clear RRE was created by two consultants with a lot of experience with relevance who know what’s needed to test relevance. We’ll be using it on a couple of our projects, and look forward to contributing back.",
      "Rated Ranking Evaluator lets you test a complete search configuration against a database of judgments.",
      "The other talk in this vein was Tudock’s Sebastian Russ’s talk on relevance visualizations. Communicating relevance to non-technical audience can be tough. It seems rather abstract, and business users have a hard time understanding the pros and cons of different relevance approaches. Business users can be prone to cherry-picking their pet query instead of looking at a relevance solution holistically. Sebastian showed up several visualizations he had pioneered, using just Excel, to show the impact of different ranking strategies to these users.",
      "Sebastian Russ from Tudock constructs very accessible relevance visualizations from Google Analytics and a few simple tools!",
      "The talks were complimented by Karen Renshaw’s broader perspective. Karen Renshaw manages Grainger UK/Zoro’s search team. She discussed her team’s methodological approach to working on relevance problems.  She gave good specifics on how Zoro tests, sampling from head and tail queries. How it’s unavoidable at time to not fix the \"highest paid person’s\" pet query. Karen showed us how working on search is a journey, and never a destination, no matter what the underlying technology. Most importantly: that relevance exists in a user experience ecosystem alongside the UI and other factors. All must be accounted for in the solution!",
      "Karen Renshaw leads search at Zoro/Grainger Global Online - where she thinks a lot about the process of relevance intersecting the technical and business",
      "Boolean Ninjas need our help too!",
      "Tony Russel-Rose shows off advanced search syntax and tools for improving their workflow.",
      "Tony Russell Rose walked through advanced search syntax use cases. So many industries use advanced search syntaxes: recruiters, paralegals, pharma researchers, and more. In recruiting, Tony pointed out that many of these searchers pride themselves on their skill in crafting queries – calling themselves \"boolean ninjas\". Tony highlighted his tooling to help these searchers craft and execute these verbose query plans. Apparently, over 90% of queries that these users copy-paste into the search box have errors that can change the meaning of the query. Tony helps create something of an IDE for building advanced queries, called 2D search, that does things like suggest query terms, more easily group query clauses, and detect errors. Tony’s talk was a nice reminder of a use case that, while not sexy, is very prevalent and one that every relevance engineer needs to account for: that of the advanced searcher.",
      "See you in 2019!",
      "Keep your eyes peeled for more Haystack goodness in 2019! We hope to keep this community alive and meeting regularly to discuss the challenges in our field. Specifically - how might we continue to collaborate on tools, approaches, and open source software to support each other in our mutual challenges."
    ],
    "summary_t": "I was privileged to participate in Haystack EU 2018 - relevance engineering as a discipline is here to stay, and just about every search & discovery team..."
  },
  {
    "id": "d4b36c05d482b7bef6b7b079446b440b",
    "url_s": "https://opensourceconnections.com/blog/2018/11/09/recap-of-activate-2018/",
    "title": "Recap of Activate 2018",
    "content": [
      "This article is been a bit late, but better late than never. Activate was a fantastic conference, and Lucidworks and the Montreal Sheraton were gracious hosts.  These are a few rambling thoughts on what I learned on my Canadian vacation.",
      "ML ML ML",
      "In previous years, the conference has been titled \"Lucene Revolution\";  it was the biggest open source search conference, and the vibe has historically been very Solr, but fairly generally oriented at text search in general.  As consultants who work with both Solr and Elasticsearch, we are asked from time-to-time to compare the two, and the quick answer is that Elasticsearch is \"Logs, Logs, Logs.\"   The Elasticsearch search engine itself is general-purpose, but Elastic the company has put considerable effort towards supporting time-series and analytics data.",
      "The renaming from \"Lucene Revolution\" to \"Activate\" signals a substantial change in the tone of the conference.  Lucidworks has gathered a number of tools around machine learning into Fusion, and there is considerable energy in the community around augmenting the search experience.  The conference collected a pretty thorough representation of the current state of the art in machine learning for text-based search.",
      "What is the state of the art in machine learning for text-based search?",
      "AI or Machine Learning means different things to different people.  As Beena Ammanath said in day 2’s  keynote speaker, we are very much still in a state of ‘narrow’ intelligence.  The algorithms and models available can solve individual tasks as well or better as a human expert, but are limited to a single focused task.   Right now, those tasks are only at the periphery of the search experience: augmenting the data at ingestion, augmenting user input at query time, and balancing how those signals go together.   That being said, the processes and tools for incorporating machine learning at those offline touchpoints has matured significanlty in recent years.  Activate is Lucidworks’ big conference, and while much of the discussion of tooling focused on Fusion, even out-of-the-box Solr has NLP and LTR support.",
      "# What was missing?",
      "The talks at Activate were excellent. Without downplaying that quality, what wasn’t discussed was almost as interesting as what was.",
      "Machine Learning in a production environment.",
      "With the exception of Malvina Josephidou and Diego Ceccarelli’s excellent talk on the adventures of getting the performance of their large-scale LTR deployment comparable to their pre-LTR performance, there was very little discussion  around the challenges of putting together an ML ecosystem.",
      "Which teams validated their training data? Most, in some way or another, but how?\n  Choosing the correct objective function for training is critical to the successess of the deployment, but there was very little discussion around how to choose an objective function, and why teams selected the implementations they did.\n  How as a data scientist or relevance engineer do we maintain these trained systems in production? What is the life expectancy? There are many, many small Solr installations out there that get to ‘good enough’ and stall; when is ML an option for those teams, and when isn’t it?\n  How do we version control our models? Or does that even make sense?  A model is tied to the data and the analysis configurations at a point in time.",
      "How do we move forward from this narrow intelligence?",
      "During Wednesday’s panel discussion, Grant Ingersoll polled the panel: \"AI means never having to __ again.\"  Only Daniel Tunkelang answered feature engineering, and this feels like the elephant in the room.   The current techniques add tools to generate new features, but in many ways, they aren’t really learning so much as mathematically encoding usage patterns.  Text learning, and  Learning to Rank in particular, are still relying on human product owners and search engineers to identify the key signals regardless of the implementing algorithm.",
      "Jake Mannix of Lucidworks gave a talk on using deep neural nets in Tensorflow to start learning features, and CNNs may yet be the answer for text as well as images.  With enough depth and enough training data, and an explainable model, he was able to pull out text features one character at a time.  Early stages were able to identify stopwords and suffixes, deeper layers were able to aggregate those into linguistic roots and even simple phrases.  I’ve been on the fence about deep learning for search, but this might’ve sold it.  With enough data and enough horsepower, moving those building blocks into a next-generation linguistic model is starting to sound like more than a research problem.",
      "See you in 2019!",
      "Search relevancy tuning is fundamentally a hard optimization task to be replaced by machine learning.  As a community, we are making progress to roll forward with the new technology, but there is a long way to go yet.   Looking forward to seeing everyone next year, and to the progress another year will bring."
    ],
    "summary_t": "We share our thoughts on the Lucidwork’s Activate 2018 conference"
  },
  {
    "id": "1d5d9da0733760288609d73b53dcb5a8",
    "url_s": "https://opensourceconnections.com/blog/2018/11/19/an-introduction-to-search-quality/",
    "title": "An Introduction to Search Quality",
    "content": [
      "Welcome, dear reader, to my first OSC blog post.  Let’s dive in!",
      "While search relevance is often equated with ensuring customers find what they need, that is only part of the picture.  Even if relevance for search results is well tuned and considered great, it may not matter if the experience is poor or search is running slowly.  When considering improvements to search in a product or application it is necessary to have a vision of overall quality, which is a combination of three key areas: relevance, experience, and performance.  This article explains all three at a high level and why they matter as a whole, and offers some tips to diagnose and attack a spectrum of common search quality issues.",
      "",
      "Relevance",
      "Search Relevance is typically called out as an issue when customers or users of a product or application complain that they can’t find the information they need.  Relevance is subjective and therefore its definition is key to understanding what it means to have relevant search.  Formally, we define relevance as a series of metrics that measure whether appropriate documents are appearing in search results for a given query, typically at the top or above the fold of the website or app presenting the results.  These metrics usually include at least precision, recall, and various forms and blend of the two.  There are many other metrics, but these are the easiest to understand and use when approaching measurement of search for the first time.  Only by measuring metrics can we improve on them, and improving them is correlated with more relevant search results being returned for the queries being measured.  If enough queries are measured then we increase coverage of the potential interactions that can happen with search.  It is useful to draw an analogy between this type of coverage and coverage of testing software: the more tests you have, the less likely your customers will experience a bug.  Also, measurement of how customers are using the live search is critically important to know if relevance is performing as expected from testing.",
      "Experience",
      "Search Experience is comprised of the functional aspects of how customers engage with a product in order to find content through search.  Common examples include autocompleting queries, layout of search results and the result data shown, suggesting alternate terms and spelling corrections, offering filters and facets, and highlighting results and documents with the query terms.  Search Experience also includes the format of content - if text in search results has poor grammar or spelling, or documents are displayed incorrectly or inconsistently, it will impact the impression your product has with customers.",
      "Search Experience varies significantly between markets and domains.  These include eCommerce, Job search, Research, Enterprise Search, Application Search, and others.  It is important to understanding how customers expect your search to behave within the context of the domain.  If this is difficult to do because your product is niche, then using tools such as Contextual Inquiry will help to find the ideal workflow and interface that will best help customers when using your product or app.  Just like relevance, one cannot improve an experience unless the improvement is measurable with data.  Capturing metrics for user experience is a well established field and includes click tracking of workflows, heat maps of UI interaction, A/B testing, session length, and many others.  Analytics for search measurement very often overlap with measurement of live search relevance.   Capturing both is key to understanding how customers are experiencing a product and whether that experience is positive.  The more analytics captured and the better the strategy, it is easier to measure which areas reflect good experiences for customers and which areas require improvement.",
      "Performance",
      "Good performance of your product is critical to ensuring customers do not become frustrated with the experience.  It is no mystery that people are short on patience when waiting for an app screen to load or for results to be returned.  Even if relevance is high and functional user experience workflows are excellent, customers will be unhappy if they need to wait several seconds or more every time they search.  Setting KPIs for page load and search response times, striving to meet them, and keeping the search response snappy will in many cases make customers pleased with your product.  Also, being able to instantly return search results can sometimes make customers happy even if relevance is not ideal.",
      "Handy Tips for Common Problems",
      "This cheat sheet can help with a high level diagnosis of problems and where you should focus attention on a product that isn’t meeting customer expectations.  The list isn’t exhaustive but offers insights to the more common search quality issues you might run into.",
      "Customer use of search seems sporadic, and it is difficult to tell if they are finding what they need",
      "Your analytics strategy might be flawed if you are unable to pin down what customers are doing.  Make sure you are tracking ‘search conversations’ that log from start to finish the entire path customers are taking in your product - all the way from clicking on the search bar, running a search, refining the search, clicking or scrolling results, and interacting with individual results.  Implement this strategy in your product and create reports and dashboards for the data that is most important to your product, and add to this over time.",
      "Customers are searching but are not clicking any documents",
      "Make sure you have a relevance testing plan in place.  Gather a broad range of queries, and a set of judgments of what documents should be returned for each query.  Use that to measure precision, recall, and other metrics with a relevance testing tool such as Quepid.",
      "Relevance tests well, but customers aren’t clicking on documents in live search results",
      "The areas to look at here are whether your relevance testing coverage is high enough, and if you are testing with the right queries.  Start by looking through the queries your customers are searching for that don’t result in clicks, categorize them, and look to improve test coverage and improvement on the categories that are performing the worst and work your way up.  Also, investigate whether your test data aligns with customer expectations.  Remember that search is subjective, so if you have the opinion that a result is relevant but your customers do not, find out why and update your judgments accordingly.",
      "Some longer queries are performing well, but short queries don’t seem to be working for customers",
      "You may have what is known as a customer intent problem, which is a problem most search engines have when queries are only one or two terms.  For shorter queries offer various types of results that cover a wide range of content in which the customer might be interested.  Also, give options in the search experience for faceting, filtering, and autocompletion to more specific queries that match common and overly broad terms.",
      "Many queries are returning too few results, and in some cases no results at all, even for short queries",
      "This is specifically a recall problem, and usually happens because the search is not tuned correctly with synonyms or stemming/lemmatization.  Turn on basic language stemming in your search engine, and start curating a dictionary of common search terms and their synonyms, then add it to your search engine query analysis.  This synonym dictionary should be kept current by investigating customer searches that return less than a full page of search results (and especially when no results are returned), and amending where needed.",
      "Customers are clicking on the first or second product or document in the results, but they don’t convert or interact beyond that click",
      "The information you are showing in the results might be misleading or uncharacteristic of the full document.  Investigate search results display and highlighting at first.  Then scrutinize the document and make sure it looks nice and conveys the necessary information clearly.  Get help and feedback from your product team and trusted customers for the problem might be with specific documents and the queries that found them and find for a pattern.  Also, if your product is enterprise search or research, customers may actually be finding what they are looking for when they click on the document.  In that case, start measuring document dwell time and selection and copying of text in your analytics, and see if the data starts telling a better story.",
      "Customers are avoiding search, and are instead locating products or documents through browsing",
      "Investigate performance of search and whether it is returning results quickly.  Do this with a broad range of actual customer queries and gather as much data as you can to investigate overall performance.  Also, take a look at your UX metrics and potentially A/B test with a more prominent search bar.",
      "Customers start searching, but they seem to abandon the search or product before any more measurements are captured",
      "This is almost certainly a performance problem and should be investigated and improved accordingly.",
      "Striking a Balance",
      "Getting all three areas of search quality correct is not easy, and there are often trade-offs. These trade-offs are almost always different between products and can only be found with investigation, experimentation, measurement, and refinement over time.  All the while it is important to monitor search quality as a whole and jump on issues as they pop up to prevent them from getting worse.  For example, using a new plugin to analyze queries to improve relevance might impact performance.  Also, spending all your effort on improving experience can result in relevance languishing as content is updated over time and query trends change.",
      "There are many more problems you can face when offering search to customers, and it is important to look at the whole picture.  By widening your investigations you may be surprised with where they take you.  And as always, try to enjoy the journey, and take the time to celebrate when things improve.  Happy Searching!  Max"
    ],
    "summary_t": "When considering improvements to search in a product or application it is necessary to have a vision of overall quality, which is a combination of three key ..."
  },
  {
    "id": "ad41cff0eb132a52fc04daefd7d8cd57",
    "url_s": "https://opensourceconnections.com/blog/2018/12/07/synonyms-by-any-other-name-part-1/",
    "title": "A Synonym By Any Other Name: From Alt. Labels to Knowledge Graphs",
    "content": [
      "There’s been some discussion lately on what the behavior of synonyms should be in Solr and Elasticsearch. As you know, this is one of our favorite topics.",
      "At the heart of the problem, the a synonym in your ‘synonyms.txt’ isn’t the same as a linguistic synonym. It’s better to think of the search engine’s synonym filter as a relevance engineer’s tool. A way of generating additional terms when another term is encountered. Such a tool can be leveraged to solve a variety of problems, and shouldn’t be used blindly.",
      "From client conversations, what people mean when they say ‘my synonyms are broken’ usually means the search engine’s synonyms don’t match the use case they have in their head. In this article, we’ll enumerate the most common use cases in the field. In future articles, we’ll discuss how to solve each use case using the search engine’s functionality.",
      "An Overview of Common Synonym Use Cases",
      "Below is our framework for how we think about synonyms / synonym-like use cases, based on problems clients are trying to solve:",
      "",
      "In this article, we’ll define each of these. You’ll notice the nested nature of this diagram. Each inner use case is a simpler version of the outer use case. As you move out, complexity, sophistication, and power increase. The exception are shared contexts (such as with word2vec).  As you’ll see terms can share contexts for many reasons unrelated to shared meaning. In these cases, the goal is to munge the data/process to approximate one of the other use cases (such as synonymy).",
      "What are these use cases? In this blog post, we’ll take an inventory of what they are and how they’re used.",
      "Alternate Labels (you say po-ta-to, I say po-tah-to)",
      "Alternate labels are when two terms should be treated as exact equivalent as interchangeable with one-another. Some use cases we see this are:",
      "Acronyms (united states,usa)\n  Weird stems (bacterium,bacteria)\n  Alternate spellings (colour,color)\n  Common misspellings (disappear,dissapear)",
      "In these cases, search users see the terms as almost 100% overlapping in meaning. There’s no ambiguity.",
      "It’s important to understand how strict this is. Tagging two terms as true alternate terms should be done with a great deal of care. This isn’t really a ‘synonym’ as you would find in a thesaurus. You’ll see what we mean in the ‘Synonyms’ section below.",
      "Expected Search Engine Scoring of Alternate Terms",
      "In the case of alternate terms, we want terms to receive exactly the same in scoring. A document containing ‘color’ is just as much about the search term ‘colour’’. As you might guess, alternate terms is the easiest use case. The default Elasticsearch/Solr synonym functionality (SynonymQuery ) works pretty well for this use case.",
      "Synonyms (in the linguistic sense…)",
      "A synonym is defined by Websters as a",
      "word with an identical or nearly the same meaning.\nSounds the same as alternate term! In practice though, if you look at a thesaurus, the closeness can be messy and highly contextual. Look at the synonyms for ‘castle’ you see this problem:",
      "château, estate, hacienda, hall, manor, manor house, manse, mansion, palace, villa",
      "To most, ‘palace’ has a different connotation than ‘castle’. Though we humans see them as ‘nearly the same meaning’. ‘Near’ depends on the search corpus, domain, user, and use cases. A historian sees a castle as a defensive structure, synonomous with a fortress. To the historian, this is quite different from a palace, which is a fancy home for nobility. A tourist, however, sees them as nearly the same kind of place to visit. Maybe not exactly the same, but pretty close to interchangeable.",
      "Most people’s Solr and Elasticsearch synonyms files fit into this category. A messy mish-mash of loosely related terms. You might see, for example:",
      "blue jeans,levis,jeans,denim jeans,denim",
      "Many search teams throw these terms in, don’t think too deeply about how the search engine should work. Then they’re surprised that the search engine doesn’t prioritize them they way they think they should be prioritized. And sadly, every human being and corpus has subtly different senses of how/where these terms should be treated as more similar or less.",
      "Expected Search Engine Scoring of Synonyms",
      "The situation here is pretty messy. It’s also rather domain and use case specific, but there’s a couple of common themes you see in the academic literature:",
      "You want the original term to be ranked higher than the synonym terms\n  You want synonym terms ranked by some approximation of their ‘closeness’ in the current corpus or user’s perception. Usually based on some measure of how likely the terms truly mean something similar in the corpus, or how likely users seem to indicate they have a similar meaning (such as through watching query refinements).",
      "As we’ll see in a future blog article, this fits in with the general field of query expansion. In the query expansion literature, a users search term is expanded and scored relative to its relationship to the original term (via co-occurrence metrics). What I like about this is instead of using word2vec to churn out candidate synonyms, with a lot of data cleaning, this does the opposite. It starts with hypothesized synonyms to seed an exploration, then approximating how they relate via co-occurrence metrics.",
      "Taxonomies",
      "As mentioned in the prior section, search teams will say they want the synonyms ‘closest’ to the user’s query scored higher than the synonyms ‘farthest’ away. A search for ‘dress trousers’ should return literal ‘dress trousers’ matches. But after that, types of dress trousers should be returned  (khakis, slacks) followed perhaps by other kinds of ‘trousers’.",
      "Notice I said types of ‘dress trousers. Of which ‘dress trousers themselves are a type of * trouser. Talk to enough search teams about their synonyms and you’ll pick up that they’re really talking about a *hierarchy of related terms, NOT a bunch of flat synonyms",
      "Instead of :",
      "trousers,dress trousers,khakis,slacks",
      "What they want to express is better organized as:",
      "\\\\trousers\\formal trousers\\khakis\n\\\\trousers\\formal trousers\\slacks",
      "This is what a taxonomy is: a hierarchy of terms organized by how they fit under broader, less specific categories/terms. Because, if you think a bit more about this, you realize that search queries - heck language in general - has a kind of hierarchy to it. When we don’t have a lot of vocabulary, our words are fairly generic. \"Look at those trousers over there\". But eventually as we acquire new expertise and language, we refine into a bit more specificity - \"Look at those khakis over there\". Khakis are a more specific type of trousers. We say that trousers are a hypernym of khakis, they have a broader meaning. Conversely, khakis are a hyponym of trousers - khakis are a type of trousers.",
      "We could flesh this taxonomy out a bit further to include informal trousers:",
      "\\\\trousers\\formal-trousers\\khakis\n\\\\trousers\\formal-trousers\\slacks\n\\\\trousers\\informal-trousers\\jeans\n\\\\trousers\\informal-trousers\\corduroys",
      "Don’t get too hung up on the name of any given node here. It’s best to think of them as concepts. They could easily just have numerical identifiers. The actual terms they correspond to are usually often dealt with in with canonical and alternate terms. Canonical meaning the preferred term for this concept. Alternate terms meaning just what we describe above - words or phrases with interchangable meaning with the main concept.",
      "Very close synonyms in a taxonomy",
      "Often what we see as ‘synonyms’ can be placed somewhere into a good taxonomy. Helping us relate exactly how and where they relate. For example, we might see ‘denim’ as a synonym for jeans, placing both jeans and denim under a broader category.",
      "\\\\trousers\\informal-trousers\\denim-trousers\\denim\n\\\\trousers\\informal-trousers\\denim-trousers\\jeans",
      "Ambiguitiy in Taxonomies",
      "You might also be thinking some of these terms are ambiguous. Corduroys and denim are both also materials, khaki is sometimes a color, which means as terms they might exist under a completely different branch of our taxonomy:",
      "\\\\colors\\brown\\khaki\n...\n\\\\materials\\cotton\\denim\n\\\\materials\\cotton\\corduroy\n...\n\\\\clothing-items\\trousers\\formal-trousers\\khakis\n\\\\clothing-items\\trousers\\formal-trousers\\slacks\n\\\\clothing-items\\trousers\\informal-trousers\\denimtrousers\\denim\n\\\\clothing-items\\trousers\\informal-trousers\\denimtrousers\\jeans\n\\\\clothing-items\\trousers\\informal-trousers\\corduroys",
      "This gives search engines opportunities to discover ambiguities and offer users results with different interpretations of a user’s query term.",
      "Finally, whether some text corresponds to a concept in a taxonomy may have little to do with just the text. It may also involve detecting the part of speech, sense, sentiment, or entity of the underlying text being placed in the taxonomy. For example ‘A dog pants’ is not discussing trousers. But ‘A dog’s pants’ is.",
      "Other hierarchies",
      "In addition to hypernymy/hyponomy - which relate terms by broader and narrower meaning - you often see meronomy (aka partonomy). In these cases, the hierarchy expresses ‘part of’ relationships. A feather is a part of a wing which is part of a bird. A dog is part of a pack. There’s an important distinction here, as meronomy doesn’t relate to meaning. But people do place meronyms in hierarchical taxonomies.",
      "Expected Search Engine Scoring of Taxonomies - Nym Ordering",
      "Alessandro Benedetti, Andreas Galvao of Sease defined a useful structure for thinking of scoring taxonomies that they call Nym Ordering. Something like:",
      "Exact term (lowest doc freq) - jeans\n  Alt labels - ‘jean’\n  Hyponyms (as in child concepts) - denim, blue jeans, stonewashed jeans, bell-bottom jeans…\n  Direct Hypernyms (parent concept) - items labeled just  informal trousers\n  Cousins (terms  with shared parent concept) - other kinds of informal pant (corduroys)\n  Grandparent hypernyms - all forms of /trousers (slacks, khakis)",
      "As a relevance engineer, you can decide how deep to get in the nym expansion (go all the way to great-grandparent hypernyms?) Indeed, a lot boils down to how the taxonomy is designed and/or generated - which itself is a skill and a discipline with its own conferences and specialists. I often say, for most search teams, a good taxonomy will have a much bigger impact than Learning to Rank ever will.",
      "We’ll talk more about using a search engine to score taxonomies in a future blog article. For now I recommend my talk, [Taxonomical Semantical Magical Search] and Max Irwin’s [Haystack Talk on Learning Taxonomies for more information.",
      "Knowledge Graphs",
      "What if we want to move past just hierarchies - to allowing search system to reason about the search query and solve user’s problems? Dogs eat alpo and sometimes sadly squirrels and shoes. These facts might matter if we’re building a search system to help solve common pet problems expressed in the search query \"help! My dog swallowed a shoe!\"",
      "Knowledge graphs are a set of facts. Facts as a graph of relationships between connected concepts, with each node a noun; each connecting edge a relationship (often a verb). For example, we know the fact ‘dogs bite cats’. We can represent that as a list of connections between concepts. Such a triple here would connect concepts ‘dog’ and ‘cat’ with the relationship ‘bites’ as below:",
      "subject, verb, object\n\tdog, bite, cats",
      "More things bite other things:",
      "Cat, bite, squirrel\nCat, bite, dog\nDog, bite, squirrel\nDog, eat, Alpo\n...",
      "We know of other entities that ‘bite’ and ‘eat’ other entities, and can begin to put together a graph that connects concepts together:",
      "",
      "(Confusing terminology alert: you’ll often hear such a thing described as an \"ontology\". We like \"knowledge graph\" because that’s very specific to what it is. And there’s some ambiguity: people often use ontology to also mean a taxonomy)",
      "Knowledge graphs can be used to answer questions about the information. If someone asks \"what animals do dog bites?\". We might be able to translate this (using a tool like Mycroft into a structured query that looks like:",
      "This animal is bitten\n  A dog bit it\n  -> What is it?",
      "We can use a knowledge graph and enumerate the bitten animals:",
      "Squirrel, Cat",
      "Knowledge graphs are often stored in a graph database, which are built to support queries like this.",
      "Indeed, Alexa, Siri, and Wolfram Alpha probably at least in part, works like this, armed with knowledge graphs like Wikidata to support these sorts of queries.",
      "We might also have systems that can infer facts for our knowledge graph from our corpus, and bring that into search. Check out Max Irwin’s Haystack Talk on algorithms that allow that. Now that Max is an OSC employee, perhaps he’ll write a blog post in this series touching on ontologies!",
      "A note on Solr’s Knowledge Graph and Elasitcsearch’s Graph Plugin",
      "Just a quick point to clarify confusion. Some have been confused by Elasticsearch’s and Solr’s graph inference capabilities. These aren’t the same as a formal knowledge graph that we describe here (they lack verbs). Instead these are really interesting in their own right: learning what terms are connected based on co-ocurrences. YOU must decide what those co-occurences mean in your data/for your domain and use case.",
      "Expected Search Engine Scoring of Knowledge Graphs",
      "Knowledge graphs represent arbitrary facts that relate to the domain. How they correspond to some search use case is going to be very domain specific. Indeed the search engine probably isn’t using it, probably a graph database is being used to interpret and solve user questions",
      "These use cases are less about scoring individual documents, but correctly mapping the query into a graph query that can pull back a set of facts from the graph database. This is a slightly different, though very related, problem to search relevance ranking. We’ll discuss this in a future blog post.",
      "Shared Contexts (ie embeddings, word2vec, and the like…)",
      "Tools like word2vec capture statistical rhythms in language that are often the start of discovering relationships between words. Teams often end there, and sidestep the difficulty of thinking about synonymy, taxonomy, knowledge graphs. Can a tool like word2vec eliminate our need to think about the relationships above?",
      "Specifically, many will start their NLP journey by analyzing a corpus for two types of shared contexts:",
      "Co-occurrences: when two terms tend to occur frequently together in the same sentence, document, or other kind of shared container\n  Shared Contexts: when two terms tend to co-occur with the same words",
      "For example, some common co-occurrences for cat and dog might entail",
      "The vet examined the cat and dog\n  I love playing with my cat and dog",
      "Shared contexts connect ‘cat’ and ‘dog’ through the contextual words they occur in",
      "The vet examined the cat and dog\n  Vets love cats and dogs",
      "In the first example, occuring in the same container (doc/sentence/etc) acts as a shared context between ‘cat’ and ‘dog’. In the second example a term (‘vet’) acts as a shared context between cat and dog. If we notice that enough similar words group around ‘cat’ and ‘dog’, we make conclusions that cat and dog are ‘close’ in our corpus: they often share contexts.",
      "Tools like word2vec, Latent Dirichlet Allocation, and Latent Semantic Analysis can be compute a per-word representation that encode the contexts that word occurs in. This representation, a dense vector, is built so that words with similar contexts cluster closely together (their vectors are geometrically close). Words with dissimilar contexts tend to be farther apart. When we represent an item as a dense vector, with the expectation it will cluster together with ‘similar’ things we refer to that vector as an embedding.",
      "These tools are the start of the journey, not the destination. Terms co-occur or share contexts for a variety of reasons, including all of the reasons listed above:",
      "Alternate Terms: Terms share contexts because they truly are alternate representations of the same term (‘CIA’ and ‘C.I.A’ are both connected through context words ‘intelligence’ and ‘espionage’)\n  Synonyms: Terms share contexts because they are synonyms (‘CIA’ and ‘spooks’ are sort of loosely connected through context words ‘intelligence’ and ‘espionage’)\n  Hypernym/Hyponym: Terms share contexts because they have a hypernym / hyponym relationship. \"The shoe store sold wingtips\" vs \"The shoe store sold dress shoes. Here dress shoes and wingtips are connected through shared context ‘shoe store’\n  Statements of Fact: Terms share contexts because they relate to each other through factual statements (such as described above using a knowledge graph). For example \"Dog bites cat\" or \"Shark bites dog\". Here dog connects the term (a verb)  ‘bites’ to ‘shark’. This also might occur a lot with adjectives - ‘red shoe’ and ‘red sky’. Sharing ‘red’ doesn’t mean ‘sky’ and ‘shoe’ are related terms",
      "And other situations we haven’t even discussed yet:",
      "Antonoyms: Certain function words occur in similar contexts, but actually have the opposite meaning. A classic example from chatbots is \"I want to confirm my reservation\" vs \"I want to cancel my reservation\". Cancel/confirm may cluster together due to shared contexts, but are definitely not synonyms!",
      "Using an embedding to emulate any of the use cases we described in this article is possible, but requires work. Indeed it’s very much an open research area! Work we won’t go into here. Many strategies are discussed in the upcoming book Deep Learning for Search by Tommaso Teofili and my Neural Search Frontier talk. And also Simon Hughes talk on Vectors in Search for how to use an embedding in a search engine like Solr.",
      "In conclusion… no cut and dry solutions",
      "I want your takeaway to be to know what problem you’re solving before diving into solutioning! There’s no one ‘cut and dry’ solution to this stuff. Indeed, aside from measurement of relevance, this is an area search teams spend a tremendous amount of time working on.",
      "\"But DOUG I want an easier answer than ‘it depends…’. Put some skin in the game already!\" you say?",
      "OK OK if I must. In many (most?) cases having a good taxonomy is often the best assets a search team can have. More important even than jumping on the Learning to Rank bandwagon. Not everyone needs a full knowledge graph, and usually synonyms don’t come with enough richness to express what search teams intend (many people’s ‘synonyms’ are really badly structured taxonomies). Even e-commerce search is about knowledge management! Is there much difference between an attentive, expert sales person and a librarian after all? So whether you intend to hand-craft a taxonomy or learn a taxonomy from your corpus or user queries, ‘Get thee to a Taxonomy!’ as The Bard says.",
      "That’s an instinct-level reaction, and I’m certainly far from knowing everything about this zaney field. Search is a very diverse field that constantly surprises. So it’s important to understand that each tool has its time and place. In future articles, we hope to write about each of these topics in greater depth.",
      "If you’d like to discuss, disagree, or ask questions I hope you’ll join me over at Relevance Slack or Contact Me Directly."
    ],
    "summary_t": "When people say they have ‘synonyms’ in their search engine, it can turn out to mean a lot of different things…"
  },
  {
    "id": "2716e3629f374cc629434fe207ad2591",
    "url_s": "https://opensourceconnections.com/blog/2007/06/26/hi-my-name-is-michael-and-im-a-programmerdesigner/",
    "title": "Hi, My name is Michael and Im a programmer/designer",
    "content": [
      "Sorry for the AA style header for the introduction blog post. I tend to abuse witty repertoire to set off a style of writing to call my own and yet still find a way to stay informative. My name is Michael, Im a sagittarius, Im originally from an area south of Richmond, and Im what you refer to as the \"new guy\" at Opensource Connections. I specialize in C# on the .net platform, working both on the web and windows side of things. Im also a communications major with a media specialization, which means my skills range from desktop publishing, to web design, audio/visual media, radio, public relations, and marketing. I know it might seem like a bit of a paradox for someone like me, who develops using Microsoft Technologies, to work for an company that specializes in open source software.",
      "However Im a big proponent of open source software and agile style programming, so much so, that Im spending time developing an open source application framework for .Net for my BHAG (big hairy audacious goal(s)) this year. In fact, there is an alpha version of the site and blog for the opensource framework at www.codeaccessory.net, where you can find more information about the BHAG on an ongoing basis as well as code samples. If you have an open sourced project, or even just want a place to write about software, feel free to contact me if youre interested on doing something on the code accessory domain. The framework itself does have a name, but Im waiting for the first alpha version to be released before I reveal the name.",
      "In the mean time expect to see postings from me, about semantic xhtml markup, css, c#, javascript, .net, design principles, code patterns, ruby, php, the current state of my BHAG project and all sorts of User Interface goodness. Lets not forget to throw in the sarcasm drenched humor and random comments like \"fuzzy pants\" (it keeps you reading to see what I type next, doesnt it?). Coming soon: table-less layouts, using code_highligher.js, and ways to free your mind when it comes using CSS to separate presentation from markup/content."
    ],
    "summary_t": ""
  },
  {
    "id": "658793d859da28fe1aa715b71a17e72f",
    "url_s": "https://opensourceconnections.com/blog/2019/01/07/haystack-democratize-relevance/",
    "title": "Why Haystack? Democratizing Search Relevance!",
    "content": [
      "We’re proud to be putting on another Haystack in late April. But why? A few people have asked me why we need another search conference? What’s the purpose behind Haystack?",
      "OSC’s mission is to empower search teams. Instead of having these great ideas, practices, and tools locked up in obscure academic papers or behind closed doors and walled gardens, we want search to be open and free. Because, if search is crucial to your bottom line, you can’t afford to outsource search. You need to understand the practices and principles yourself, to take charge of the search experience, and really own what your users experience. But you can’t do it alone: mid-size companies need to work together as a community to find and explore the menu of approaches.",
      "If search is crucial to your bottom line, no area is more crucial than search relevance. Understanding what your users are telling them with search queries is hard enough. But if you move from search to relevance more generally it gets harder: with the advent of chatbots and personalization and other situational metadata (location, time, etc) users expect search to ‘just work’. They expect \"Simple Made Easy\" without seeing all the complexity behind the scenes.",
      "And no area has seemed more closed off and harder for teams to build a practice around than relevance. Which seems surprising. It seems these search relevance problems should be well solved. Google has been around for decades. Why should it be so hard?",
      "For two reasons:",
      "Academia has been primarily a job and research funnel for Web search giants. Only recently have broader use cases been looked at.\n  Your application’s niche is more unique than you realize: the industry has broadened to cover a long tail niches, use cases, and types of interactions than what academia could ever cover",
      "I like to imagine this situation as a pyramid, like the image below:",
      "",
      "At the bottom of the pyramid are the many many many sites where investing in search or relevance won’t pay off. Many products exist to add a ‘good enough’ search experience to a site. At the top is where Information Retrieval research has been in the last several decades: a strong connection between large scale Web search and academia.",
      "In the middle is probably where you are if you’ve read this far in this article. You have a search experience where investment is called for. Yet when you look at the legacy of search research, you don’t see that much that helps you. The open source search engines help: they have some sane defaults for a search application. They give you a deep toolbox with query primitives, scoring methods, analysis tools, and basic NLP. Yet there’s little guidance in what you should do for Your Application(™). Open source search is a framework, not a solution!",
      "It’s tempting being in this ‘middle class’ of applications to not appreciate how specialized your niche is. It’s tempting instead to think of search as ‘one thing’ with a single relevance solution, that ought to just be plugged in and work for everyone. Maybe read an academic paper, sit through a conference talk, or hear a product pitch that doing this One Cool Trick just makes our search work ‘like Google’. It’s less satisfying to hear that you’ll have to take on hard work, understanding that your search shouldn’t work like Google or Amazon, that it requires product experimentation, and as much engineering as the rest of your product.",
      "Luckily that realization is changing. Organizations realize that search relevance - or some kind of ‘matching’ between users and [products|content|articles|jobs|etc…] is very important. Last year’s Haystack was a testament to this: practitioners coming together to share what solutions worked and didn’t. To give real stories about search and relevance, discuss tools that could be open sourced for the community to use, and share practices and tools that really make a difference.",
      "Some highlights from past Haystacks",
      "Measurement is hard, but foundational! Measurement of search quality is perhaps the hardest problem to solve! But probably the most important. Many times people derive judgments from analytics or human judges, but they don’t turn out to successfully correlate to a successful A/B test. See Liz Haubert’s talk and Peter Fries’s\n  Machine learning isn’t magic. It’s powerful but not necessarily easier than any other method for mid-sized companies just because its a machine learning method. All of Haystack’s Learning to Rank talks (here here and here ) have shared painful lessons learned by some of the smartest people I know\n  Open sourcing is good for business and community! Kudos to Sease for open sourcing Rated Ranking Evaluator. By open sourcing Solr Learning to Rank Bloomberg helped themselves by making their learning to rank toolchain, everyones learning to rank toolchain - and thus more maintainable than if it was proprietary!\n  Decreasing cost of experimentation is the silver bullet. The silver bullet to search isn’t one particular solution, it’s decreasing how hard it is to experiment with solutions against our users quality data - to ‘fail fast’ with relevance ideas. See here\n  Taxonomies, taxonomies, taxonomies - for mid-sized companies, maintaining a taxonomy is a tried-and-true method that isn’t exciting but gets the job done for building semantic-search capabilities (see Max Irwin’s talk )\n  Vectors, vectors, vectors - as more solutions come out for performing more accurate embeddings, search engines need to be able to support vector similarity and filtering for use cases like image search or embeddings",
      "Getting practitioners together to share and discuss how we can improve these use cases was a powerful experience. Separately mid-sized companies will whither and collapse under the weight of search giants like Google. Together, we can build practices and tools that can help our employers make more competitive products.",
      "We need your talks (CFP Ends Jan 30th!)",
      "In conclusion - the community badly needs your talks. We want to hear your real life stories, to help the community grow and learn in relevance! Please visit the Haystack site to put in a talk. Or be prepared with a 5 minute lightning talk!",
      "Submit your Talk!"
    ],
    "summary_t": "Why another search conference? Because search relevance shouldn’t be locked behind obscure papers and walled gardens. It needs a community of practitioners t..."
  },
  {
    "id": "a1c6f7826a7580e4b822b417894856ad",
    "url_s": "https://opensourceconnections.com/blog/2019/02/01/flax-joins-osc/",
    "title": "Flax has joined OpenSource Connections",
    "content": [
      "Flax has joined OpenSource Connections!",
      "On February 1st 2019 Flax’s Managing Director Charlie Hull joined OpenSource Connections (OSC), Flax’s long-standing US partner, as a senior Managing Consultant. Charlie manages a new UK division of OSC who also acquired some of Flax’s assets and brands. OSC are a highly regarded organisation in the world of search and relevance, wrote the seminal book Relevant Search and run the popular Haystack relevance conference. Their clients include the US Patent Office, the Wikimedia Foundation and Under Armour and their services include comprehensive training, Discovery engagements, Trusted Advisor consulting and expert implementation.",
      "Lemur Consulting Ltd., which as most of you will know traded as Flax, continued to operate and to complete current projects in early 2019 but did not take on any new business after January 2019. All future Flax enquiries are now redirected to OSC where Charlie will as ever be very happy to discuss requirements and how OSC’s expert team (which may include some familiar faces!) might help.",
      "We are all very excited about this new development as it will create a larger team of independent search & relevance experts with a global reach. We fully expect to build on Flax’s 18 year history of providing high quality search solutions as part of OSC. We are continuing to run the London Lucene/Solr Meetup and attending and speaking at other events on search related topics.",
      "If you have any questions about the above please do contact us."
    ],
    "summary_t": "Flax has joined OpenSource Connections"
  },
  {
    "id": "7abe916d387b37823970007677aa367c",
    "url_s": "https://opensourceconnections.com/blog/2019/02/28/stop-worrying-solr-elasticsearch/",
    "title": "Stop Worrying about Solr vs Elasticsearch Decisions",
    "content": [
      "I used to fall into a trap that many search teams do. Faced with a seemingly consequential choice, with two products that seem to be fairly similar, it’s easy to fret about whether \"we’re making the best choice\". It’s like staring at purchasing two seemingly similar cars. You worry one will be a lemon - an expensive mistake your team will regret later. Or perhaps you’re not sure you really need that 3rd row of seats or the extra cup holders. Everyone worries they’ll get stuck with a bad choice.",
      "For many, Solr vs Elasticsearch decisions are dealt with this ‘product comparison’ mindset. Teams look at a feature comparison, and try to make heads or tails of the best technical choice, trying to avoid an expensive mistake.",
      "But choosing open source search engines are not like buying cars. Solr and Elasticsearch are not fully baked, ‘products’ to compare blow-by-blow. Instead, these search engines are malleable raw material your own search team’s product is made of. They don’t come with a beautiful paint job with a third row of seats or cupholders, just a bunch of raw parts, a pretty solid engine connected to some wheels. It’s up to you to mould it into something useful.",
      "How people solve search problems using Solr or Elasticsearch helps make it more concrete:",
      "Where does the search engine fit in to your tech? How search relates to adjacent backend technologies changes a lot from team to team. Some orgs keep the search engine at arms length, integrating a lot of cool \"magic\" outside the search engine. Others become Lucene hackers getting under the hood, to really leverage the available data structures\n  How skillfully is it configured? Much like relational databases, how you use the search engine depends on how skilled you are at configuring it. Some things you can get a search engine to do defy a simple feature comparison. A clever query or way of performing text analysis has less to do with the specific search engine and more to do with the skill of the relevance engineer.\n  Extend the search engine with plugins? Of course when all else fails, you can create plugins! You can simply add the ‘3rd row seats’ as you so desire, or remold the search engine to do what you need it to.\n  The hardest work isn’t search engine specific. Really good relevance teams spend more of their time obsessed with experimental methodology (testing/measuring relevance) than even using the search engine.",
      "These areas point at one missing ingredient from the discussions. Team skill. If open source search is ‘raw material’, then teams need skill to mold to their needs. I have never seen a search team fail because they chose Solr when they should have chosen Elasticsearch (or vice versa). Smart teams use both: Wikipedia uses Elasticsearch. Reddit uses Solr via Fusion. Where I have seen teams fall short is because they simply don’t have the skills to mold the putty into the ‘work of art’ that the organization needs. And these aren’t truly ‘Solr’ or ‘Elasticsearch’ skills, but skills in relevance.",
      "How should Solr and Elasticsearch be compared?",
      "The short answer is don’t worry about it, just pick one, and you’ll be fine.",
      "The long answer is to think about differences in communities rather than features. Compare stock car racing vs formula one racing.",
      "",
      "Both are different communities, with different histories, and assumptions for how you should build race cars. Both with different modes of operating, rituals, norms, and believing what car racing should prioritize. Teams choose Solr or Elasticsearch because they feel an affinity to a community’s ethos, not for technical reasons.",
      "One imperfect way to think about Solr vs Elasticsearch is whether the community is optimizing more for the \"contributor\" or the \"user\" community. \"Contributor\" meaning teams that want to pull the direction of the project. \"Users\" meaning teams that wish to download it and ‘make it work’ with no interest in pushing back changes to the main project. Even \"users\" here are still rather advanced, configuring and writing plugins to do their work without pushing changes back to the project.",
      "Solr, for example, hails from of the Apache Software Foundation (ASF). The ASF works to encourage community ownership. ASF wants to create norms and culture to allow many individuals and companies to craft the direction of the project. Consequently, there’s not just \"one vision\" of what Solr is or could be, there’s competing voices, all pushing and pulling the project through contributions and democratic discussion. It’s messy, full of conflict, but it also has the opportunity to get buy in (and resources) from all parties interested in the direction of the project.",
      "On the plus side, Solr’s ‘contributor’ focus means that there’s a lot of opportunity in Solr to really dig into the source code. On the negative side, there’s often lots of \"opportunities\" in Solr to really dig into the source code. The contributor focus means Solr can feel very ‘design by committee’. Features are often at various levels of \"baked\". With one-off, barely working, or highly evolving features making it into Solr releases. These features go through an evolution of dying off, growing, getting fixed, or maybe just orphaned and staying buggy. This is seen as OK… or maybe even a contributor recruiting strategy. Because after all, this is a democratic, open source project! Pull requests welcome. YOU can fix it! As a pure user, however, being surprised by bugs at the 11th hour can be rather frustrating.",
      "Elasticsearch, on the other hand, optimizes for \"users\" of the search engine. Elastic, the company behind Elasticsearch, controls the direction of the project. Generally speaking Elastic have been good stewards of the community and project. Elastic thinks carefully about the where they want the project to go, trying to deploy well thought out, complete features. They eagerly prune the ‘bonsai tree’, removing directions they disagree with. As an organization that wants to steer the direction of the search engine, this can be a frustrating model. It looks very undemocratic. Where you want to take the search engine might go nowhere unless you can convince someone at Elastic to back your play.",
      "But, put yourself in a \"users\" shoes. Someone uninterested in contributing (most orgs) want it to \"just work\". Having Elastic’s single vision pushing the direction of the project helps those users stand something up and simply get it to work. Since most people are users not contributors to the code, having a well thought out API, with only fully-baked features, is a huge advantage to the Elastic stack. It ‘just works’.",
      "More alike than different",
      "At the end of the day, Solr and Elasticsearch have more in common with each other than they differ. As open source projects, they’re raw material, infrastructure - not fully baked solutions. You would never compare a group of car parts to a fully formed car. Of course a car taken off the lot will perform better than your pile of car parts. But enough work can turn those car parts into a really tailored, purpose-built car, or bull-dozer, or whatever weird thing you’re building.",
      "Or more directly, see them both as frameworks for implementing your specific search and discovery solution. But don’t worry too much when your team chooses one, but not another."
    ],
    "summary_t": "Feature blow-by-blows of heavily configurable open source search engines don’t make much sense."
  },
  {
    "id": "2447f77eeaba534e729be800c772ff2e",
    "url_s": "https://opensourceconnections.com/blog/2019/03/13/search-insights-2019-a-new-report-from-the-search-network/",
    "title": "Search Insights 2019,  a new report from The Search Network",
    "content": [
      "Search Insights 2019,  a new report from The Search Network",
      "As many of you will know I’ve now joined OSC as a Managing Consultant - so here’s my first short blog post. Those of you who followed my Flax blog won’t be surprised to hear it won’t be the last!",
      "For the last couple of years I’ve been part of an informal group of professionals working in the search engine space, the Search Network. Two of this group are now my colleagues at OSC (Eric Pugh and Doug Turnbull), others include search managers, taxonomy experts and user interface specialists. Most of us don’t work for big companies and we’re all frequently frustrated by some of the content that masquerades as ‘analysis’ of the sector we inhabit.",
      "So, we decided to do something about it, led by Martin White, author, speaker and all round statesman of enterprise search - we started producing free, un-sponsored reports covering the current position of the search market, recommended strategies for success, new innovations in search and more. In our second annual report Search Insights 2019 we cover topics as diverse as \"Why we (still) need taxonomies (and the taxonomists who nurture them\" and \"Achieving enterprise search satisfaction\" - my own contribution was in collaboration with Doug on \"The rise of the relevance engineer\". You can also read Martin’s story of this year’s report.",
      "The reports are free to download and you may even find a few printed copies at the next search event you attend. If you like them, please spread the word, send on the link to your colleagues, tweet about them or whatever - if you disagree with something, or need some help on a particular aspect of what we’ve written about get in touch with the authors directly. If you want to chat to me or Doug perhaps you can come to one of our Haystack conferences (but be quick, the April event is close to sold out!) - or contact us directly."
    ],
    "summary_t": "OSC have contributed to a free report on search with a chapter on The Rise of the Relevance Engineer"
  },
  {
    "id": "8444665bc6557df551ffbb04d8b8ec42",
    "url_s": "https://opensourceconnections.com/blog/2007/06/29/enhancing-tests-is-the-way-to-join-a-community/",
    "title": "Enhancing Tests is the way to join a community",
    "content": [
      "Yesterday Yonik committed my first contribution to Solr. It was a simple simplification of the test code infrastruture. Some of the tests depend on Java System properties, however if you are not running the tests from Ant, you dont have them… For example, running them from Eclipse. The advice was to pass them in using -D parameters, but that isnt very user friendly! So I did a tweak that set those System properties in the Java code. I posted the patch to the mailing list, and Yonik committed them as revision 551701",
      "What I realized is that one of the best ways to get to know a new community is to contribute to the test code. As an unknown quantity, suggesting major new features or architecture changes wont go over well, but contributing/fixing/updating tests shows that you are eager to learn how the community works, and not just trying to get your cool new feature added. Writing tests for OSWorkflow and OSUser was how I became a committer over at OpenSymphony, only later did I start writing actual product code.",
      "Also, by digging through the unit tests, you really get to learn how the code works, and where the pitfalls are as well!"
    ],
    "summary_t": ""
  },
  {
    "id": "eee4a1ae78e743f318eeadfa0a01a3bf",
    "url_s": "https://opensourceconnections.com/blog/2019/04/10/weather-forecast-cloudy-with-a-chance-of-elasticsearch/",
    "title": "Weather fork-cast: Cloudy with a chance of Elasticsearch",
    "content": [
      "There have been some interesting moves by cloud computing providers over the last few weeks with respect to search and relevance, particularly concerning the Elasticsearch open source stack. As many of you will know, Elastic is the company formed by Elasticsearch creator Shay Banon to offer commercial licenses for his open source search engine and it also offers a hosted version, Elastic Cloud.",
      "Giant cloud provider Amazon Web Services (AWS) also offer a hosted Elasticsearch offering, although historically it has somewhat lagged behind the latest versions of Elasticsearch and prevented various options such as the use of custom plugins. As a user you have other options of course - to run Elasticsearch yourself on a cloud stack or to use specialist providers such as OneMoreCloud’s Bonsai. It really depends on how much of the administration you want to handle, how much control over configuration you need and of course, on your budget.",
      "Elasticsearch isn’t the only open source product Amazon provides as a hosted service, and as a reaction to this several other companies including Redis Labs have been updating their licenses to prevent what they see as a large, rapacious rival from eating their business model, for example by adding the Commons Clause. This prevents services such as Amazon from selling hosted versions of the various open source projects. Reactions to this have been mixed - some see it as an affront to the very idea of open source licensing by restricting one of the core freedoms, some see AWS as a freeloader happily making vast sums from the efforts of others and agree that Something Must Be Done.",
      "In parallel, Elastic have started to intermingle their open source code with commercially-licensed (although available in source code form) modules that provide features such as security restrictions. This hasn’t been universally popular either - the standard distribution of Elasticsearch now contains commercially licensed code and to get the pure, open source only version you need to download a different package. Despite protestations from Shay Banon and others that their hearts remain open source, this move makes it difficult for anyone embedding Elasticsearch into another product or hosting it to operate without breaking the license terms, which would seem like a protectionist move by Elastic.",
      "Amazon’s reaction has been to effectively fork the open source project by releasing their own, purely open source version of Elasticsearch, folding in contributions from other projects (there’s always been a parallel ecosystem of plugins and enhancements for Elasticsearch that cover some of the functionality of Elastic’s commercial add-ons). So, Amazon now have a way to continue hosting Elasticsearch without stepping on Elastic’s toes and they also get some great PR as supporters of open source - but we should lose sight of the fact that this is also a blatantly commercial move against Elastic’s protectionism.",
      "Not to be outdone, Elastic recently announced a more friendly collaboration with Google Cloud, who of course are a major AWS competitor. This is Elastic’s own hosted service running on Google’s infrastructure, and it seems they’re not the only company playing nice.",
      "For the Elasticsearch user, a richer ecosystem cannot be a bad thing - it gives us more choice when choosing a suitable technology stack. However although Amazon have promised to contribute any improvements back to the main Elasticsearch project, it will be Elastic who decide whether to accept these (as they control all contributions) and thus whether the two projects will gradually grow apart. We can’t be sure yet what will happen, and so the future is still a little cloudy.",
      "If you need strategic advice on open source search or relevance don’t hesitate to contact us, and we hope to meet some of you at our Haystack conference in a few weeks."
    ],
    "summary_t": "How Amazon forked the open source Elasticsearch project in reaction to Elastic’s mixing of open source and commercial code, and what it means for users"
  },
  {
    "id": "cf65f271285636fcd7ea7052a9efb82b",
    "url_s": "https://opensourceconnections.com/blog/2019/04/24/test-driven-search-relevance-tuning-for-free/",
    "title": "Test-driven search relevance tuning - for free!",
    "content": [
      "As you may know OpenSource Connections has a commitment to search relevance that goes beyond our expert consultancy and training - we genuinely want to build a community of practitioners and support this however we can. We’re also huge fans of open source software such as Elasticsearch and Apache Solr.",
      "However, we’ve also built a closed source product for test-driven relevance training, Quepid, which we provide as a paid-for hosted service. It’s a tool that we use internally on a regular basis - it’s great for creating test queries for Elasticsearch and Solr and for identifying and resolving relevance issues. We know that many of you have also found it useful and a fair few of you signed up as paying users. We think Quepid is a great way to implement a test-based relevance tuning strategy - but we’d like to encourage even more people to use it.",
      "So, we’ve decided to donate Quepid to the community in two ways: by relicensing the project as open source software and by making the hosted version at www.quepid.com absolutely free to use. From now on there will be no charge to use www.quepid.com and shortly afterwards the source code for Quepid will be released on our Github page  - allowing you to build Quepid into your own relevance testing platform, contribute to the codebase and even run it behind the firewall (a feature many of you have requested).",
      "We’d like to thank all Quepid users for your support of the project over the last few years. It’s been hugely valuable to get your feedback. We expect Quepid to continue to develop and improve, and look forward to your contributions!",
      "If you have any questions about Quepid and how it can help you tune search relevance please do get in touch."
    ],
    "summary_t": "Quepid, the search testing tool, to become open source software from May 2019 - with a free hosted version!"
  },
  {
    "id": "a27910bcd9a66ae7210e3f0513c8baac",
    "url_s": "https://opensourceconnections.com/blog/2007/07/03/getting-business-is-not-an-entitlement/",
    "title": "Getting business is not an entitlement",
    "content": [
      "I just came back from the National Veterans Small Business Conference and Expo. I was really impressed at the outreach that prime contractors and government agencies showed. They are making a good faith effort to live up to the 3% service disabled veteran owned small business (SDVOSB) goals set forth in Public Law 106-50 even though historically, they have had trouble meeting those goals.",
      "I think that it is laudable that the government wants to give back to the veterans who gave their service to the nation. I applaud and admire the set asides not just for SDVOSBs, but for all disadvantaged groups. However, the government, first and foremost, owes a duty of responsibility to all of those it serves, which includes getting the maximum value for the tax dollars that we all pay to it. By not getting the best value out of money that it spends for the goods and services that it procures, the government is doing everyone a disservice.",
      "How does that paragraph not contain a contradiction in terms? It is not when the companies representing disadvantaged groups offer goods and services at a fair value to the government and to the prime contractors who are trying to meet those goals. This does not include exhibiting an entitlement culture, as many of the representatives of small disadvantaged businesses showed during the Veterans Conference. Far too many times, I heard a question along the lines of when was the government going to start holding its people (and, by proxy, its prime contractors) responsible for hitting the goals that were laid out in 106-50.",
      "Prime contractors are in business to make money and to provide value, not to pander to special interests. Rather than griping and whining about how the government is not providing enough of what amounts to be handouts, these small businesses should be going out there and demonstrating and providing valuable goods and services. If they provide something that people want to pay money for rather than a corporate shell which is demanding entitlements, then they will find that profits accrue. Prime contractors did not become large businesses through whining and begging. They got there through hard work and through providing value to others. They did it through the basic tenets of business: find a market niche where demand is greater than supply, and provide supply. Its not easier done than said, or else wed all run behemouth businesses, but, at the same time, no business is going to grow through the increased use of whining and begging and complaining to the government that its not giving them enough entitlements.",
      "To those who attended the conference and went there to network, find opportunities, and create value, I offer applause and thanks that you represent the best of the veterans who have decided to go into business for themselves. To those who attended the conference to badger, harass, and pester government and prime contractor representatives about when they were going to increase entitlements and not provide taxpayers and shareholders with the best possible value for the money, I shake my head in disappointment. You did not ask your drill sergeants for a break or to not make you pass PT or marksmanship tests. Why are you asking for the free ride now?"
    ],
    "summary_t": ""
  },
  {
    "id": "387acd68382295579a55f3a5ce156904",
    "url_s": "https://opensourceconnections.com/blog/2019/05/07/haystack-2019-day1/",
    "title": "Haystack 2019 day 1 - grab your popcorn, it's Relevance Avengers time",
    "content": [
      "You may remember I was ever so slightly enthusiastic about last year’s inaugural Haystack conference in Charlottesville, Virginia - it would perhaps be unseemly to be so effusive again, since I was part of the team running this year’s event after joining OSC earlier this year. However, from what I’ve heard we managed to avoid ‘difficult second album syndrome’ and run another great conference.",
      "This year the venue was a cinema in downtown Charlottesville, which gave us much needed extra space and easier access to the Downtown Mall and its array of restaurants, snack shops and bars. Plus points included reclining seats, an onsite cafe and some very big screens, although we did discover some issues with WiFi coverage (perhaps an aid to concentration however?) and the movie projector didn’t always play nice with presenters’ laptops. We’ll sort this out for next time I’m sure - of course the affected presenters were professionals and coped admirably with the glitches. Also, I’m hoping none of the conference attendees felt they missed out on seeing Avengers Endgame, on show in one of the other theatres…but just in case they did I’ll introduce some of the marvel-lous characters we saw onstage at Haystack.",
      "The first day was introduced by Max \"Ironman\" Irwin of OSC  who gave us a keynote on What is Search Relevance?. Max showed us the three aspects of search quality: performance, experience and of course relevance, and went on to discuss how we can score judgements, cope with disagreements between human raters and fold in user engagement data. He also showed us a list of the speakers to come and welcomed over 140 attendees from the USA and Europe to Haystack.",
      "The next talk I saw was by Alessandro \"Dr Strange\" Benedetti of Sease Ltd. (OK, I’ll stop the Avengers references now before I infer one of our speakers was green and angry) on the Rated Ranking Evaluator relevance testing tool. He showed us the heirarchical model for test queries they have developed and how the open source RRE can be used to run a huge amount of tests on a Solr or Elasticsearch instance as part of the Maven build process, producing a set of relevance metrics. These metrics in turn can be emitted to a spreadsheet, RRE’s own server dashboard or as JSON (RRE also uses JSON for the relevance judgements that must be provided to it).",
      "Tara Diedrichsen & Tito Sierra of LexisNexis followed with a fascinating talk on best practices for gathering human judgements for relevance testing. It’s clear that LexisNexis have put huge amounts of work into this area to help them identify problem areas to focus on and to evaluate new algorithms. I’m pleased they stressed that it’s important to record why a search result is good or bad - this is essential information for relevance engineers who may be unfamiliar with the subject area.",
      "Lunch followed, and conference attendees scattered to the various restaurants on the Downtown Mall - luckily as far as I can tell they all came back afterwards. The next talk I saw came from René Kriegler on Query Relaxation which was fascinating - René showed us various ways to remove terms from a query to increase the number of results and eventually suggested using a neural network to work out the best term to lose.",
      "Unfortunately I missed the next session as I was preparing to run the Lightning Talks, our last session of the day. The Lightning Talks started with a moving tribute to Ted Sullivan by his friend and colleague Eric Hatcher - sadly we lost Ted this year, I was very privileged to be able to meet him at last year’s Haystack.",
      "The talks featured speakers on subjects including Zookeeper on AWS, the new Quaerite relevance test tool, Solr on Kubernetes and the challenges of full text search at the Hathi Trust over 17 million documents. Thanks to everyone who volunteered to speak at such short notice!",
      "You’ll be glad to know we will be releasing the slides for all the main talks and the Lightning Talks very soon, and unlike last year we managed to video all the sessions - so anything you (or I) missed (or simply didn’t understand well enough at the time) will be available to peruse at your leisure. UPDATE - The slides & video are now available here - click the ‘More’ link on each talk to see them.",
      "The first day of Haystack finished with drinks and dinner at Kardinal Hall nearby, during which a few attendees played Bocce (although stupidly I thought it was boules). I’ll write about the second day very soon! If you’d like a richer description of the first day including some of the talks I missed please do read Jettro Coenradie’s blog.",
      "If you missed out on Haystack there will be a European event on October 28th in Berlin (details to be confirmed) - and if you have questions about this or indeed anything search or relevance related, please do contact us."
    ],
    "summary_t": "A brief review of the first day of Haystack 2019, the search relevance conference"
  },
  {
    "id": "1fb6e143205b495319a9683900be6b49",
    "url_s": "https://opensourceconnections.com/blog/2019/05/14/haystack-2019-day2/",
    "title": "Haystack 2019 day 2 - doubling down on relevance with the Relevance Avengers",
    "content": [
      "I’ve already written about the first day of Haystack 2019, here’s a quick summary of the second day. We returned to the cinema, noted that Avengers Endgame was still playing in the next theatre (I believe everyone resisted temptation!), fought off various technical issues with the help of our committed AV team and proceeded with some excellent talks.",
      "Jeremiah Via of the New York Times was the first presentation I attended. Jeremiah described how Elasticsearch is used to index 18 million items at the Times and how they developed both online and offline metrics to improve relevance. The Times’ index contains over 22 million unique tokens and nearly 2 million tags. He stressed the importance of being able to easily iterate through configuration changes - as he said \"improving search is about making lots of little improvements\".",
      "Next up was Tom Burgmans, describing how his team established a relevance focused culture at Wolters Kluwer. I particularly enjoyed seeing a screenshot of their advanced relevance testing tool which showed relevance judgements and also broke down the various contributions to relevance scores - I hope as he did that this tool eventually becomes open source. Wolters Kluwer have also developed a set of loosely coupled reusable search components which help to share knowledge and experience across the organisation. His last point was ‘don’t stop’ - relevance improvement is never finished!",
      "My colleague Bertrand Rigaldies of OSC then talked about Solr query parsers (he noted that there are no less than 29 different query parsers supplied with Solr, including a good few I’d never heard of). He showed how to build a simple proximity query parser (to handle queries like \"‘fish’ within 3 words of ‘chips’\") and stressed that although custom parsers can be very powerful, they are complex to write and one should try to use an out-of-the-box parser where possible.",
      "Lunch followed, attendees again taking advantage of the various outlets in Charlottesville’s Downtown Mall.",
      "John Berryman, one half of the team behind the Relevant Search book and now at Eventbrite, gave an engaging talk on automatic tagging using search logs and machine learning. His system creates a training set from user interactions (the events that users clicked after a particular query) then attempts to predict what tags to apply to other events - the tags being the search queries themselves.",
      "The next session was a panel discussion on Does Learning to Rank Actually Work (my alternative title ‘Learning to Rank - or learning to tank?’ was sadly discarded :) with René Kriegler, Doug Turnbull, Xun Wang (Snag) and Erik Bernhardson (Wikimedia). The audience provided some great questions for the panel.",
      "I sadly missed most of Simon Hughes of DHI’s talk on Search with Vectors but what I did see was very interesting, including how he had built a special query parser for Lucene that stored vectors as payloads. Luckily there’s lots of detail in this Github repository.",
      "The conference ended with thanks to all the speakers, organisers and most importantly the attendees - without whom Haystack would of course not be possible! Thanks to everyone who came and made it such a great event. Haystack will return!",
      "If you’d like a richer description of the second day including some of the talks I missed please do read Jettro Coenradie’s blog. Alessandro Benedetti of Sease has also written about his experience of the event. You can also join many of the conference attendees in Relevance Slack - there’s a #haystack-conference channel.",
      "You’ll be glad to know we will be releasing the slides for all the main talks and the Lightning Talks very soon, and unlike last year we managed to video all the sessions - so anything you (or I) missed (or simply didn’t understand well enough at the time) will be available to peruse at your leisure. UPDATE - The slides & video are now available here - click the ‘More’ link on each talk to see them.",
      "If you missed out on Haystack there will be a European event on October 28th in Berlin (details to be confirmed) - and if you have questions about this or indeed anything search or relevance related, please do contact us."
    ],
    "summary_t": "A brief review of the second day of Haystack 2019, the search relevance conference"
  },
  {
    "id": "50a52c0640fa9bf58342700471a54b63",
    "url_s": "https://opensourceconnections.com/blog/2019/05/16/solr-json-api/",
    "title": "Solr Json API Tutorial",
    "content": [
      "Edismax is the query parser-of-choice for many Solr applications.  The default behaviors are correct for a wide range of use cases.   The syntax has become familiar.  While edismax has its bugs, the ‘happy path’ is pretty stable.  For many teams, there is no compelling reason to look beyond the edismax query parser.",
      "But what happens when we want to give the search engine more guidance?   In this article, we’ll take a look at one case for taking more control over the query structure, and how the Solr Json API lets us do that.",
      "The Scenario",
      "We’ve blogged before about the notion of term-centric and field-centric queries, but first, a quick review.  Edismax constructs a Maximum Disjunction, which means",
      "A query that generates the union of documents produced by its subqueries, and that scores each document with the maximum score for that document as produced by any subquery, plus a tie breaking increment for any additional matching subqueries.1",
      "Consider a query:",
      "q=breakfast comedy\nmm=100%",
      "So long as our fields are relatively simple, and have relatively similar analysis patterns, edismax will usually produce a term centric interpretation. In this case, the disjuction will look something like:",
      "+(((cast_en:breakfast | title_en:breakfast | genres_en:breakfast) (cast_en:comedi | title_en:comedi | genres_en:comedi ))~2))",
      "And we will successfully find comedies in the TMDB database like \"Breakfast at Tiffany’s\" or \"The Breakfast Club\".    But as the complexity of fields in the system grows, chances are there are some queries which will produce different analysis, and Solr won’t be able to marry up the terms neatly.  In this case, we’ll end up with a field centric interpretation, then the disjunction is per-field, and we can end up with something like:",
      "+(((Synonym(title_en:breakfast title_en:first_meal_of_the_dai) title_en:comedi)~2) | genres_precleaned:breakfast comedy)",
      "There are good reasons where this might be the desired result.   Let’s assume for this article that there is a frequent use case where users might want to issue unstructured queries and require all terms to be present, but those terms won’t all occur in the same field.",
      "The Bool query parser",
      "Most edismax users are already familiar with the idea of issuing one blanket full-text query, and layering additional boost queries (bq) and filters (fq) against it.  But there is a catch - boost queries won’t affect recall, and filter queries don’t affect scoring.  What if we wanted to issue multiple baseline queries which affect both recall and scoring?",
      "Enter the BoolQParser.  This lets us construct a Lucene Boolean Query.  The name can be a little misleading.  The outstanding virtue of the BoolQParser is that it layers 4 additional clauses on our queries: must, should, filter, and must_not, so we can construct multiple required clauses.  Filter and must_not are variations on the theme of fq; \"should\" clauses are similar to bq. Must clauses add something new. Like our original ‘q’ term in eDismax, \"must\" clauses contribute to both scoring and recall.  Unlike ‘q’, each clause is potentially its own query.",
      "So we might manually write that ‘breakfast comedy’ query like:",
      "http://localhost:8983/solr/tmdb/select?q={!bool%20must=title_en:breakfast%20must=genres:comedy}&fl=title%20genres%20score&rows=25&debugQuery=true",
      "Even with this limited interpretation, the syntax gets pretty ugly, pretty quickly.",
      "The JSON Request DSL",
      "In 7.1, Solr introduced the JSON Request API.   At the most basic level, this is just reformatting the familiar Solr parameters into a json block, and passing that in the request body.  A few of the terms have been reformatted, so check the reference manual if you are translating queries directly.",
      "http://localhost:8983/solr/tmdb/select?debugQuery=on&defType=edismax&fl=title&q=breakfast%20comedy&qf=title%20genres&mm=100%25",
      "vs",
      "curl \"http://localhost:8983/solr/tmdb/query?\" -d '\n{\n  \"query\": {\n\t\t\"edismax\": {\n\t\t  \"query\": \"breakfast comedy\",\n\t\t  \"fl\": \"title\",\n\t\t  \"qf\": \"title genres\",\n\t\t  \"mm\": \"100%\"\n\t\t}\n\t}\n}'",
      "By itself, this isn’t anything drastic; particularly for teams who push most of the request into the request handler configuration.   Now let’s revisit a more complex query.   Now, we can cleanly specify a term-centric query, with a separate clause for each term:",
      "curl \"http://localhost:8983/solr/tmdb/query?\" -d '\n{\n\t\"query\": {\n\t\t\"bool\": {\n\t\t\t\"must\": [\n\t\t\t\t{ \"edismax\": \n\t\t\t\t\t{  \"qf\": \"title genres\",\n\t\t\t\t\t   \"query\":\"breakfast\"\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t{ \"edismax\": \n\t\t\t\t\t{  \"qf\": \"title genres\",\n\t\t\t\t\t   \"query\":\"comedy\"\n\t\t\t\t\t}\n\t\t\t\t}\t\t\t\t\n\t\t\t]\n\t\t}\n\t}\t\n}'",
      "And if we’d like to boost cases with that (or some other phrase), we get:",
      "curl \"http://localhost:8983/solr/tmdb/query?\" -d '\n{\n\t\"query\": {\n\t\t\"bool\": {\n\t\t\t\"must\": [\n\t\t\t\t{ \"edismax\": \n\t\t\t\t\t{  \"qf\": \"title genres\",\n\t\t\t\t\t   \"query\":\"breakfast\"\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t{ \"edismax\": \n\t\t\t\t\t{  \"qf\": \"title genres\",\n\t\t\t\t\t   \"query\":\"comedy\"\n\t\t\t\t\t}\n\t\t\t\t}\t\t\t\t\n\t\t\t],\n\t\t\t\"should\": [\n\t\t\t\t{\n\t\t\t\t\t\"complexphrase\": {\n\t\t\t\t\t\t\"query\": \"breakfast club\",\n\t\t\t\t\t\t\"inorder\": \"true\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t}\t\n}'",
      "What’s next?",
      "If you would like to discuss how your search application can evolve and grow,  please get in touch. We’re also always on the hunt for collaborators or for war stories from real production systems. So give it a go and send us feedback!",
      "References:",
      "Solr Reference Guide: The Dismax Query Parser.\n  \n  \n    Solr BoolQueryParser\n  \n  \n    Elasticsearch Reference Guide: The Bool Query Parser\n  \n  \n    Solr JSON Request API"
    ],
    "summary_t": "Moving beyond edismax with the BoolQueryParser and Solr’s JSON API"
  },
  {
    "id": "ccf318aa087fc68a403784fcbb0c408c",
    "url_s": "https://opensourceconnections.com/blog/2019/05/16/unreasonable-effectiveness-of-collocations/",
    "title": "The Unreasonable Effectiveness of Collocations",
    "content": [
      "Recently while experimenting with word2vec-based features with Learning to Rank, I was exploring using collocations to improve the accuracy of my embeddings. If you read the original word2vec paper Mikolov et. al. make the observation:",
      "Word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words. For example, \"Boston Globe\" is a newspaper, and so it is not a natural combination of the meanings of \"Boston\" and \"Globe\".",
      "Indeed this is something we note in search engines. And one of our relevance best practices is to identify idiomatic phrases and other entities and treat them uniquely in a search solution. Treating them as a single phrase at query time or a single token at index time. Something like this entry in a synonyms file:",
      "boston globe => boston_globe",
      "Identifying idiomatic phrases can be a simpler way of seeming like your search system understands entities, without building a full-blown entity-extraction system. As always in search relevance, solve the simpler problem first, then move on to the next level of sophistication only if it’s warranted as needed in your evaluation data.",
      "Where do you find idiomatic phrases?",
      "The challenge becomes, where do you find these idiomatic phrases? More than one client of ours has been frustrated by maintaining a list of this manually. Well luckily machine learning, err, let’s just say \"statistics\" to the rescue. In Natural Language Processing (NLP) there is an area that can help us called collocations. With collocations, you can use relatively straightforward statistics to determine whether a given phrase is statistically significant, and occurring more often than random in our corpus.",
      "Chi-Squared Latte",
      "A straightforward, and rather effective way to score collocations is with a chi squared statistic. Chi squared is a fairly robust statistic, that helps overcome the tendency for language to be not normally distributed. You may know about Zipf’s law. That is, the most common words occur much, much more frequently than rare words. This occurs too with two-word phrases. if you zoom into phrases that begin with \"New\" the subsequent terms will also be distributed occuring to Zipfs law.",
      "Given some labels of a day’s weather like \"rainy\", \"not rainy\", \"cloudy\", \"not cloudy\", we might want to know if there’s some relationship between cloudiness and rain.",
      "If we know that out of 365 days, there were 100 rainy days, 265 not rainy days, 125 cloudy days, and 240 not cloudy days. In other words, the probability of rain is 100 days / 365 days or ~0.27. Similarly for cloudy days there’s a probability of ~0.34. And so on…",
      "Let’s suppose we care about understanding the relationship between cloudiness and raininess. We start by assuming they’re completely unrelated. Like the probability of your Uncle Phil showing up and Spongebob being on TV. Completely random, unrelated, independent events occur at a probability P(Event1) * P(Event2). Or P(Uncle Phil) * P(Spongebob).",
      "Applying that here, we can presuppose the events rainy/not rainy and cloudy/not cloudy are unrelated calculating their expected probability. Or simply P(rainy) * P(cloudy) or 0.27*0.34 ~ 0.09. We can repeat this to come up with expected probabilities for the intersection of every event:",
      "rainy\n      not rainy\n    \n  \n  \n    \n      cloudy\n      0.09\n      0.25\n    \n    \n      not cloudy\n      0.18\n      0.48",
      "We want to test if the association is NOT random chance. That is there’s some kind of relationship here. That’s where the chi-squared statistic comes in. Because we actually happen to have observed how many days were actually BOTH rainy & cloudy at our weather station, and we can list those in a similar table:",
      "rainy\n      not rainy\n    \n  \n  \n    \n      cloudy\n      90\n      50\n    \n    \n      not cloudy\n      5\n      220",
      "Using this, we can compute the actual probabilities, and compare them to random chance:",
      "rainy\n      not rainy\n    \n  \n  \n    \n      cloudy\n      90/365\n      50/365\n    \n    \n      not cloudy\n      5/365\n      220/365",
      "Calculating actual probabilities:",
      "rainy\n      not rainy\n    \n  \n  \n    \n      cloudy\n      0.25\n      0.14\n    \n    \n      not cloudy\n      0.01\n      0.60",
      "And intuitively, if we want to know if these things are related, we’d like to know how far these probabilities are from expected probabilities. That’s what chi-squared is, it computes a statistic from the difference of the actual (P) from the expected (E) probabilities of each cell in this table, sums them up. In the equation below, i,j are simply rows/columns of each cell in the table.",
      "",
      "This would give us a chi-squared of",
      "365* ( (0.25 - 0.09)^2 / 0.09 + (0.14 - 0.25)^2 / 0.25 + … ) = 172.65",
      "You can consult a chi squared distribituon which graphs the likelihood of observations occurring independently as a function of the chi-squared statistic. You’ll note the odds of this data happening by chance is infinitesimally small, so we can presume there’s a significant relationship between raininess and cloudiness.",
      "Now, it just so happens to turn out that for these 2x2 tables (called confusion matrices) you can simplify all of that math to a simpler equation. Although you lose much of the intuition of what’s above, it’s a more convenient calculation.",
      "rainy\n      not rainy\n      row sum\n    \n  \n  \n    \n      cloudy\n      90\n      50\n      140\n    \n    \n      not cloudy\n      5\n      220\n      225\n    \n    \n      col sum\n      95\n      270",
      "We can use this equation to compute chi-squared using Counts(C) at each cell, divided by row and column sums:",
      "",
      "Which would be:",
      "365*(90*220 - 50*5)^2 / (140*225*95*270)",
      "Or 172.65",
      "Get to the NLP already!",
      "With this context set, we can start to examine our collocations in the same way. We can examine, out of all 2-word phrases (bigrams) we know some will begin with \"New\" and some will end with \"York\".  So we can repeat the table from above, but over all bigrams in a corpus (N=25156)",
      "w1=New\n      w1!=New\n    \n  \n  \n    \n      w2=York\n      90\n      51\n    \n    \n      w2!=York\n      205\n      25000",
      "Words beginning with New = 295\nWords ending with York = 141",
      "In the same way, we want to know if there’s a significant relationship between a phrase starting with one word \"New ?\" and ending with another word \"? York\".",
      "By random chance, we expect a word ending with New and ending with York to be about (295 / 25346) * (141 / 25346) or 0.00006. As you might guess, two words randomly occuring together as a phrase, in a large corpus with a vocabulary of 100s of thousands, is vanishingly small. But we see in our data it occurs about 90 / 25346 or 0.003, which is much higher than expected! Completing the rest of the math, the final chi-squared value is 4840, very significant!",
      "It’s fun to think through some of the statistics here with some interesting conclusions about text",
      "Most of the corpus is NOT ‘New York’ As you’d expect, most of the corpus is dominated by bigrams with neither w1 nor w2 (the bottom right is always very large). This is different than weather data, for example, where this might not be true!\n  Expected number of bigrams is very small - The expected number of times a term might occur randomly with another term is probably less than 1 bigram. So any occurence of our bigram, \"New York\", is non trivial! (Although it’s recommended to have a minimum value for each cell in chi-squared tests).\n  It’s hard to detect negatively correlated bigrams. Because bigram occurrences are expected to be rare,  we can know \"New York\" is a collocation. It’s hard to know if \"Banana Aardvark\" never occurs in language in a statistically significant way until you get to a lot of text.\n  w1 without w2 is most of phrases starting with w1, the expected number of times a term would occur with w1 but not w2, is basically all of the occurrences of w2 minus a fractional number. So if 141 bigrams end with \"York\" then we would expect about 140+ bigrams to start with New and end with York\n  The last point is a bit more complex, because some words (like New) occur more often. So the expected value of words ending in York, but NOT starting with New might be closer to say 138 139 or so, but still not much less than the total occurrences of words ending in York!",
      "The statistics here are not ground-breaking, but the simple results of knowing whether a query term should be treated as a phrase or not, can be a significant step for a search application. If you’re interested in learning about other approaches to Collocations, I’d recommend the book Foundations of Statistical Natural Language Processing by Manning and Schütz. It just so happens the chapter on collocations is free. Other scoring methods such as Mutual Information are equally as intuitive and produce reasonable results.",
      "Hands on with Movie Collocations",
      "I used Elasticsearch to index bigrams from the TMDB movie dataset. Here the overview field has a subfield (overview.bigrams) stemmed and tokenized into bigrams. We then run a direct terms aggregation to dump the most frequent bigrams. We score using chi-square function, dump each scored bigram into a heap, and output the top N bigrams. It’s worth noting, I do this manually, but in real-life you should explore the significant terms aggregation which includes a chi-squared scoring method, if you wanted significant bigrams out of Elasticsearch.",
      "There’s one subtle difference between using an aggregation for this and what was presented in the last section. Here we are using document frequency, which is not a direct count of the total bigrams in the corpus. In my testing, it doesn’t seem to matter. Indeed I might argue it’s a stronger signal, as it means the bigram is mentioned in very different contexts.",
      "Without further adieu, here were the top bigrams out of the TMDB dataset:",
      "2345200.0 cerebr palsi => cerebr_palsi\n2345200.0 charlton heston => charlton_heston\n2345200.0 dolph lundgren => dolph_lundgren\n2345200.0 dziga vertov => dziga_vertov\n2345200.0 mardi gra => mardi_gra\n2345200.0 sanjai dutt => sanjai_dutt\n2345200.0 tel aviv => tel_aviv\n2345197.999988487 wim wender => wim_wender\n2345193.999768454 sci fi => sci_fi\n2345187.9998823116 são paulo => são_paulo\n2345185.999949258 barack obama => barack_obama\n2345185.999949258 notr dame => notr_dame\n2345181.9998963834 ingmar bergman => ingmar_bergman\n2345179.9999232474 lionel barrymor => lionel_barrymor\n2345169.9999680202 jimini cricket => jimini_cricket\n2345165.999978254 scantili clad => scantili_clad\n2345147.99924611 sherlock holm => sherlock_holm\n2345136.000081872 tourett syndrom => tourett_syndrom\n2345132.0001449804 prakash raj => prakash_raj\n2345106.0003807857 qing dynasti => qing_dynasti\n2345094.000745787 daffi duck => daffi_duck\n2345088.0008596377 rosalind russel => rosalind_russel\n2345078.0009624045 eiffel tower => eiffel_tower",
      "Now this isn’t magic, it’s just statistics! These are all fantastic candidates for good collocations like actor and place names. The next step is to scan through these and remove any candidates that are poor candidates for being treated as a single phrase in the search engine - and as you’d expect, you’ll find more as you move down the list.",
      "Above you see them collapsed to a single term, which seems to imply that these phrases are dealt with at index time. This expansion could also occur purely at query time, by using two synonym filters. One that goes down to the single term, as above. And a second phrase that expands back to the phrase. Such as:",
      "eiffel_tower => eiffel tower",
      "This with query auto-phrasing would turn the search paris eiffel tower to paris \"eiffel tower\" making it seem a bit like the search engine understood that eiffel tower was some kind of entity.",
      "Why not ‘Just Search Bigrams’ and skip all this math?",
      "Something you might ask about - that was a lot of work. Why not directly search bigrams (or use pf2/pf3 in Solr). Certainly the phrase’s TF*IDF (or BM25…) score gets at something like \"statistical significance\" in search! What’s the point in jumping through all these statistical hoops when we can search with sub phrases directly to do the same?",
      "It turns out that TF*IDF (specifically IDF) scores exactly the opposite of what we want, and leads to prioritizing irrelevant phrases over statistically significant ones. In other words gluten free snacks might key in on \"free snacks\" as the most relevant phrase with TF*IDF, not the statistically significant collocation \"gluten free\".",
      "Let’s look at why:",
      "Remember from the table above, most of the action will be between the bottom right (phrases with neither of our words) and the upper left (phrases with both of our words):",
      "High DF, Low TF*IDF:",
      "w1=Gluten\n      w1!=Gluten\n    \n  \n  \n    \n      w2=Free\n      90\n      51\n    \n    \n      w2!=Free\n      205\n      25000",
      "Low DF, High TF*IDF:",
      "w1=Free\n      w1!=Free\n    \n  \n  \n    \n      w2=Snacks\n      20\n      51\n    \n    \n      w2!=Snacks\n      205\n      25070",
      "The upper left is the document frequency of our phrase. For a statistically significant collocation (\"gluten free\"), we want more count in the upper left. Unfortunately, this is the opposite of what TF*IDF would want. TF*IDF scorer rarer phrases (free snacks) as more important. The more common a phrase is, the lower the relevance score that would come out of the search engine. On the flipside, higher document frequency is a positive indicator for collocations! So statistically interesting phrases like \"william shatner\" are going to occur more than \"funny william\". A search for the \"funny william shatner\" would prioritize the bigram \"funny william\" (low doc freq) above \"william shatner\" (high doc freq).",
      "Having a rare, but not really significant phrase, come to the top is quite the opposite of what a user would expect. And brings up an interesting philosophical points about TF*IDF based statistics and the nature of collocations. Common phrases are more likely to be significant collocations - more likely to be idiomatic. They should be treated as a single unit/concepts, with its own measure \"term frequency\" and \"document frequency\". Rare phrases, are less likely to be significant collocations. And should NOT be treated and scored using a term frequency / document frequency.",
      "A big takeaway for me is to avoid combining pf2/pf3/bigrams without disabling TF*IDF or somehow only expanding subphrases that themselves are collocations.",
      "This reiterates the definition of a good collocation from the beginning of the article, a phrase that stands alone as its own concept, that’s not a simple combination of it’s constituent words.",
      "Word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words. For example, \"Boston Globe\" is a newspaper, and so **it is not a natural combination of the meanings of \"Boston\" and \"Globe\". **",
      "A collocation is a word unto itself in the linguistic sense. It needs to be treated as its own concept. Not a simple combination of the underlying terms. I guess this is finally where German has things easier with constantly compounding words into words like bostonglobe :)",
      "Collocate to Charlottesville for some relevance training with me?",
      "I hope you found this useful, if you enjoy learning about relevance, check out our training occurring August if you’d like to hang out! We’ll be doing a reprise of our \"Think Like a Relevance Engineer\" training course as well as our \"Hello LTR\" Learning to Rank training that we did at Haystack. Hope to see you there!"
    ],
    "summary_t": "Using simple statistical methods to find important phrases goes a surprisingly long way to improve search relevance"
  },
  {
    "id": "a37a8851e3a2984e458d23aff1665a4d",
    "url_s": "https://opensourceconnections.com/blog/2019/05/21/summer-of-relevance-early-bird/",
    "title": "Summer of Relevance: Tickets Available!",
    "content": [
      "Did you miss Haystack? This summer, we’ll be fulfilling our mission of empowering search teams, showing you how it’s done with 100% open source tools, by holding a full week of search relevance training.",
      "August 6th and 7th, \"Think Like a Relevance Engineer\"",
      "On August 6th and 7th, we’ll hold public Solr or Elasticsearch \"Think Like a Relevance Engineer\" Training. This training class has been an eye opener for many organizations, setting the groundwork for dramatic relevance improvements in their organizations. Engineers, data scientists, and product managers at Eventbrite, Elsevier, LexisNexis, Spotify, Dice.com, and many others have upskilled their team on how to solve challenging search relevance problems. By spending a day on measurement, methodology, and product concerns  and a second day on relevance tuning techniques, we bring together the technical know-how with the experimental, user-oriented hypothesis-driven mindset needed to improve relevance. Join us! We promise your brain will hurt with what you learn.",
      "View Solr Details\nView Elasticsearch Details",
      "August 8th and 9th, \"Hello LTR\" Learning to Rank Training",
      "On August 8th and 9th, you can continue on to our Hello LTR - Learning to Rank training. This training demystifies Learning to Rank - using Machine Learning to improve relevance ranking. Really there’s no reason for you to fear getting into Learning to Rank. We believe anyone can learn to do this, on their own. Our first class we trained a major US Newspaper publisher, a major e-commerce retailer in the US, and a major music streaming service (Spotify). We gather training data, train models, deploy them to the search engine, and analyze how to improve them with better features and training data. We learn how to incorporate clicks and conversions to generate search training data, and real-life ‘production’ concerns of using machine learning to improve relevance.",
      "View ‘Hello LTR’ Details",
      "Want to do both? Contact me for a discount code",
      "The two classes form a great progression for someone really wanting to go deep in improving their search relevance skills. Please contact us if you’d like $100 off each class.",
      "Quotes from post attendees",
      "\"'Think Like a Relevance Engineer' has helped me think differently about how I solve Solr & Elasticsearch relevance problems\"\n\n    \n    Matt Corkum, Disruptive Technology Director, \n    Elsevier",
      "\"'Hello LTR' training gave me strong conceptual foundation of Learning to Rank and its moving parts, practical tips and learnings on how to implement it in production\"\n\n    \n    Lekha Muraleedharan, Search Developer \n    Spotify",
      "What a positive experience! We have so many new ideas to implement after attending 'Think Like a Relevance Engineer' training.\n  \n      \n      Andrew Lee, Director of Engineering for Search \n      DHI"
    ],
    "summary_t": "Join me for four training days in August, going deep into improving your search relevance and learning to rank skills."
  },
  {
    "id": "eb3c7a598767d26480325ed3c5968951",
    "url_s": "https://opensourceconnections.com/blog/2019/05/28/lets-stop-saying-cognitive-search/",
    "title": "Lets Stop Saying 'Cognitive Search'",
    "content": [
      "I consume a lot of search materials: blogs, webinars, papers, and marketing collateral. There’s a consistent theme that crops up over the years: buzzwords! I understand why this happens, and it’s not all negative. We want to invite others outside our community into what we do. Heck I write my own marketing collateral, so I get the urge to jump on a buzzword bandwagon from time-to-time.",
      "That being said, I want to tell you a dirty little secret. Nobody really knows what ‘cognitive search’ means in any concrete sense. Sit two people down, and ask them what problem ‘cognitive search’ solves, and you’ll get two different answers. Most likely they imagine some kind of silver-bullet solution to a unique, painful search relevance problem they’re experiencing. Problems that require careful, deep thought where there truly isn’t an easy, one-size-fits-all, silver-bullet solution. As this inevitably leads to disappointment, buzzwords like this ultimately disenchant those brought into the community. Let’s go over why it’s important to scrub buzzwords from our language if we want to help each other solve our relevance problems.",
      "1. The ML techniques applied to search are extremely varied",
      "‘Cognitive search’ implies a single machine learning technique. But there’s such a broad range of potential machine learning ‘lego bricks’ that just saying ‘cognitive search’ doesn’t concretely convey how a problem is solved. You might as well say the problem is solved with ‘computers.’ It could mean learning to rank to optimize relevance ranking. It might mean embeddings or knowledge graphs to improve semantic understanding of queries and content. Maybe they use contextual bandits or neural language models?",
      "As many Haystack talks on these subjects attest many of these solutions can just as easily create a disaster as they might be a perfect fit for the problem. The takeaway is if someone says they’re going to apply ‘cognitive search’ to a problem, ask them to get specific about the techniques they use and why they’ll be appropriate to your problems.",
      "2. Expect customization, not turn-key solutions",
      "Search relevance requirements have subtly unique requirements that are hard for users to consciously express. An internal enterprise search engine has not much in common with Google. Even within domains, one job search experience (ie general purpose Careerbuilder) might have little resemblance to another’s (ie hourly-wage oriented Snag). None of which look anything like E-Commerce Search or Patent Examiner Search.",
      "Matching the machine learning lego pieces to the subtle, hard to capture specifics of an application is hard. You should expect a lot of customization. I’ve noticed that search engine product companies have A LOT of professional services. If you care about the problem, you’ll either need a competent internal team or a long-term implementation relationship with a vendor. An honest question is whether you would rather own that customization yourself? Or outsource it to an external entity? If getting it right is core value, tread carefully. Don’t assume because it’s all just ‘cognitive search’ that you don’t need to think very deeply about the problem.",
      "3. ‘Cognitive Search’ is a hypothesis, not a solution. Can you measure it?",
      "The silver bullet to improving search quality is not machine learning, its measurement and experimental methodology. With so many possible ways of applying machine learning to relevance, you need a system for evaluating which will work, and which won’t. It’s not as simple as getting a ‘cognitive search’ engine and saying its done and dusted. Every proposed solution should be approached as a hypothesis, not a guarantee.",
      "Can your organization scientifically study how a hypothesis impacts your business? Do you have KPIs tied to business value, that demonstrate search relevance’s impact on your business? Do you have a way to systematically measure what search results were good/bad/indifferent for a given search query? Have you broken this down by persona, and user segments?",
      "In any progression of a search team, this is the first and best use of data scientists. It’s the foundation for everything else the team does. It’s hard work, but an extremely worthwhile way to increase your pace of innovation with real, concrete machine learning methods.",
      "No more buzzwords, let’s talk about the real techniques used",
      "Instead of talking about vague buzzwords like ‘cognitive search’ let’s pierce the veil. Let’s become aware of the actual techniques that are being used. In the 90s and 00s, regular software development was in the position machine learning is in now. It all sounded vaguely magical if you weren’t a software developer. But over time that’s changed. Product teams and business analysts can more carefully reason about a broad range software techniques. Should an app be built to run entirely client-side in a browser, run in Javascript? Or should it be something built out entirely server-side? Is this a relational database? Or something else? Your average BA might not be able to get deep in the technical weeds, but many know enough to think critically and push back on how these technical decisions impact product needs. We need to get there with machine learning and search relevance.",
      "Everyone needs to be a scientist, not just data scientists",
      "To become a ‘relevance centered’ enterprise, the hardest adjustment I’ve seen in organizations is moving away from a traditional software development mindset to one that’s more scientific and hypothesis-driven. The idea that 93% of experiments will fail is hard to fathom. All that investment, to get that little nugget of success? The reality is, that’s just honest science! In a mature,  hypothesis-driven environment negative results and failures are expected. The goal is to improve the pace and quality of experimentation, where value is gained as much by knowledge gained not just movement of metrics. Fail fast, succeed with confidence.",
      "Changing how our organizations work on software is challenging. So understandably, organizations want to outsource the problem to a ‘cognitive search’ vendor. Organizations, managers, and executives often think ‘my vendor can handle this headache for me’. But really this is just punting organizational issues down the road. If search is core business value, eventually you’ll lose to competition that can act with more scientific agility to attack problems in the marketplace. When you can quickly experiment with real solutions in the marketplace, without disruptive organizational bureaucracy or vendor dependencies, you gain an incredible advantage that’s more fundamental than any machine learning technique. So think carefully about how your team, vendors, and the broader organization fits together to create a fast, experimental pace.",
      "We need Machine Learning literacy beyond the Data Science team",
      "The underlying problem behind buzzwords like ‘cognitive search’ is the lack of machine learning literacy outside of data science teams. A common problem when we consult with search teams is seeing a data science team developing capabilities which the rest of the search team ends up not using. Largely because most of the search team doesn’t know what to do with them. To the search team, the delivered ML model is a black box, impossible to reason about or manage, and usually developed in a siloed data science team without a tremendous amount of external input. Understandably, the data science team is equally frustrated: why aren’t they using this promising new technique? This dynamic really needs to change.",
      "I’m pleased to begin to see job titles like \"Machine Learning Engineer\", tasked to be aware of machine learning sufficiently to think critically about how machine learning works in production. Our conception of ‘Relevance Engineer’ is similar: someone aware enough to use and develop machine learning information retrieval capabilities in production, even if they’re not going to innovate the fundamental science. For example, as engineers, we can all appreciate and learn how LambdaMART works but we don’t all need to be creating brand new learning to rank methods.",
      "These are just initial baby steps, we can’t make progress until everyone has awareness and education of machine learning concerns. In the same way product teams can now think critically about many aspects of software development, so must they now think about how the extremely diverse and large universe of machine learning techniques fits in. It’s not just one thing called ‘cognitive search’ but a huge universe of techniques with tradeoffs and areas for your organization to innovate and grow.",
      "That’s not all folks -",
      "I hope this wasn’t too much of a diatribe! Let me know if you disagree with my thoughts here, eager to hear other viewpoints!",
      "If you do take machine learning literacy seriously, please check out our Summer of Relevance - 4 days of training where we go from basic relevance, measuring search quality, up to advanced machine learning techniques that you can’ implement on your own. As always get in touch if we can be a part of increasing your orgs maturity with search and relevance through consulting or training."
    ],
    "summary_t": "We won’t address machine learning illiteracy in search if we can’t go beyond buzzwords. We need to teach concrete, specific techniques."
  },
  {
    "id": "626468d153ec2e83731dbbd7133af224",
    "url_s": "https://opensourceconnections.com/blog/2019/05/29/falsehoods-programmers-believe-about-search/",
    "title": "Falsehoods Programmers Believe About Search",
    "content": [
      "As much as anyone I’m a fan of resurrecting trends and memes and pretending it’s cool.  In that vein dear friends, I’ve exhumed the venerable \"Falsehoods Programmers Believe\" party from 4 years ago to bring you one about, no less, Search.",
      "Search is a deceptively complex field, where competence is hard-won through training, practice, and experience.  The list stands at a total of 105 falsehoods.  I couldn’t mash up the ole 99-problems meme with this to cull 6 unworthy items, because they are all worthy.  I will leave you with that brief introduction and, of course, the list:",
      "Search engines work like databases\n  Search can be considered an additional feature just like any other\n  Search can be added as a well performing feature to your existing product quickly\n  Search can be added as a well performing feature to your existing product with reasonable effort\n  Choosing the correct search engine is easy and you will always be happy with your decision\n  Once setup, search will work the same way forever\n  Once setup, search will work the same way for a while\n  Once setup, search will work the same way for the next week\n  The default search engine settings will deliver a good search experience\n  Customers know what they are looking for\n  Customers who know what they are looking for will search for it in the way you expect\n  Customers who don’t know what they are looking for will search accordingly\n  A customer using the same query twice expects the same results for both searches\n  Customers only search for a few terms\n  Customers only search for less than some set number of terms\n  Customers never copy and paste a whole document into a search bar\n  Customers balance quotes and parenthesis\n  Customers that don’t balance quotes or parenthesis don’t expect phrasing or grouping\n  You can pass the customer query directly into your search engine\n  You can write a query parser that will always parse the query successfully\n  You will never have to return a query parse error to the customer\n  When you find the boolean operator ‘OR’, you always know it doesn’t mean Oregon\n  Customers notice their own misspellings\n  Customers don’t expect your search to correct misspellings\n  It is possible to create a list of all misspellings\n  It is possible to create an algorithm to handle all misspellings\n  A misspelled word is never the same as another correctly spelled word\n  All customers expect spelling correction to work the same\n  All customers want their misspellings corrected\n  A search should always return results, no matter how absurd\n  If you don’t have any results to show, customers won’t mind\n  When the perfect results are shown to the customer, they will notice it\n  You don’t need to monitor search queries, results, and clicks\n  Customers won’t get nervous that you are logging their search activity\n  Search queries are not affected by GDPR\n  Looking at the data, it is always possible to tell whether a customer found what they were looking for\n  Customers will click on what they are looking for when they’ve found it\n  You can build a search that works like Google\n  You can build a search that works like Google sometimes\n  You should use Google as a target for your search\n  Customers don’t mind if your search doesn’t work like Google\n  Customers don’t expect your search to work like Google\n  Customers won’t compare you to Google\n  A bad search, no matter how minor nor how rare, will never reflect poorly on your product\n  Since Google doesn’t use facets, customers don’t need them\n  Facet hit counts are always correct\n  Facets have no impact on performance\n  You can just cache queries to get performant facets\n  Personalized search is easy\n  Learning to rank is easy and just requires a plugin\n  You have enough data for learning-to-rank\n  Over time, you can curate enough data for learning-to-rank\n  You don’t need to spend lots of time formatting content for it to work well in your search engine\n  Text extraction engines will always produce text that doesn’t need to be post-processed\n  All your markup will be stripped as you expect it to be\n  Content is well formed\n  Content is mostly well formed\n  Content is predictably well formed\n  Content, sourced from a database and templates, are formed the same\n  Content teams treat search as their top priority\n  Manually changing content to improve search is easy\n  Improving content can be automated with reasonable effort\n  Queries for ‘C programming’ and ‘C++ programming’ will produce different results\n  Queries for ‘401k’ and ‘401(k)’ will produce the same results\n  Tokenization as it works out of the box is right for your content and queries\n  Tokenization can be changed to meet the needs of your entire corpus and all queries\n  Tokenization can be changed to meet the needs of most of your corpus and most queries\n  Tokenization can be conditional\n  You should roll your own tokenizer\n  You will never have a debate about tokenization\n  Regular Expressions for tokenization is a good idea\n  Regular Expressions have minimal performance impact\n  You will never have a debate about regular expressions\n  You should remove stop words\n  You should not remove stop words\n  You know what the list of stop words should be\n  Stop words will never change\n  When you find the stopword ‘in’, you know it doesn’t mean Indiana\n  It’s easy to make certain things case sensitive\n  Case sensitivity is a good idea\n  Synonyms are easy\n  Synonyms will improve recall in the way you want\n  Synonyms have the same relevance in all documents\n  Synonyms for Abbreviations and Acronyms always work as you expect\n  Synonyms can be extracted from your corpus with natural language processing\n  Using Word2Vec for synonyms is a good idea\n  Stemming will solve your recall problems\n  Lemmatization will solve your recall problems\n  Lemmatization dictionaries are static\n  Languages don’t change\n  Natural language processing (NLP) tools work perfectly\n  Incorporating NLP into your analysis pipeline is straightforward\n  Search queries are complete sentences and can be accurately tagged with parts of speech\n  Showing a list of search suggestions is easy\n  Suggestions should just use the out of the box search engine suggestions\n  Suggestions should incorporate customer query logs\n  Customers would never type anything offensive into your search bar\n  Customers would never try to hack you through your search bar\n  Customers don’t need highlighting to find what they’ve searched for\n  Default highlighters are good enough for all your content and queries\n  Making a custom highlighter isn’t too difficult.  It’s just matching strings right?\n  Making a custom highlighter that is better than the default version will take less than a year\n  Turning on caching will solve your performance issues\n  Customers don’t expect near real time updates\n  30 second commit time is short enough for everyone"
    ],
    "summary_t": "Search is a deceptively complex field, where competence is hard-won through training, practice, and experience.  In that vein dear friends, I’ve exhumed the ..."
  },
  {
    "id": "8dcd2dd1945c330a8f5f575044d9d6da",
    "url_s": "https://opensourceconnections.com/blog/2019/06/11/zookeeper-resiliency-aws/",
    "title": "Zookeeper Resiliency for Solr Cloud in AWS, using Auto-Scaling Groups",
    "content": [
      "We’re very happy to present a guest blog by Alex Addison, Senior Developer at LexisNexis UK. Alex’s team will be presenting a talk on some of their work at the next London Lucene/Solr Meetup on July 4th.",
      "Here at LexisNexis we use Solr extensively, and I’m on a team building some new production-ready infrastructure around it. We ran into a problem in how they interact with AWS’s resiliency tool, the Auto-Scaling Group. Here’re the solutions and workarounds we considered, and some guidance and code to help you solve the same problem.",
      "Introduction to Solr Cloud and Zookeeper",
      "For those who know what SolrCloud is and how it uses Zookeeper, feel free to skip this section. For the rest of you, Solr is a search engine, with a cloud mode which helps with scaling, load-balancing, and all of the problems which come with large data-sets and distributed computing. To enable all of this, it uses Zookeeper for cluster coordination and configuration. You just start Solr with a parameter pointing it at each of your Zookeeper servers, and away you go.\nOf course, this means you need a Zookeeper cluster (known as an ensemble) as well as your Solr Cloud cluster, but that’s not a problem, 3 Zookeeper servers running on small, cheap server instances can handle quite a large SolrCloud cluster, and this setup allows you to separate concerns of search and cluster management.",
      "What’s the problem?",
      "To ensure your Zookeeper ensemble keeps running in AWS, you should keep it in an auto-scaling group with at least 3 instances in separate Availability Zones. EC2 instances can fall over or be terminated, sometimes without warning, and without Zookeeper SolrCloud can’t index data and stay up to date. New EC2 instances created by the Auto-Scaling Group are likely to have different IP addresses and DNS names, so Solr will no longer know where to find all of its’ Zookeeper servers. Some useful discussion can be found in a blog post by Credera here.",
      "Potential solutions",
      "We considered a number of solutions:",
      "Manual\n    \n      But manually do what? Also, I’m not too keen on waking up at unsocial hours to do this.\n    \n  \n  Tell Solr when the Zookeeper ensemble changes\n    \n      There doesn’t seem to be a mechanism for this without taking the Solr server down, and looking at the Solr code, I think it would be quite time consuming and difficult to implement into Solr.\n    \n  \n  Re-start Solr when the Zookeeper ensemble changes\n    \n      We could do this, and it would probably be fine, but it could take a long time and is either painstaking manual work, or you have to work out how to automate knowing that a SolrCloud server is up to date and ready; you probably don’t want to just tear all of your SolrCloud servers down at the same time.\n    \n  \n  Put all of the Zookeeper servers behind a load balancer\n    \n      Solr’s designed to know about each of its’ Zookeeper servers, not one at random (which might change) - and I’m told Solr and Zookeeper have slightly weird connections, so it might simply not work. In any case, the point here is resiliency, and we’re adding another thing to go wrong.\n    \n  \n  Put each Zookeeper server behind its’ own load balancer\n    \n      Solr and Zookeeper have slightly weird connections, so it might simply not work. In any case, the point here is resiliency, and we’re adding another thing to go wrong.\n    \n  \n  Give each Zookeeper server a persistent IP address or DNS record which can survive re-creation of the instance, and which is attached to the new instance as it starts\n    \n      One way of attaching an IP address or DNS record to an instance as it starts is using a Lambda. Fortunately, there’s a guide on how to do that.\n      Alternatively, we could give the EC2 instances permissions and allow them to pick up a nominated IP address or DNS record.",
      "Our Standards",
      "It’s worth noting at this point that we think the right thing to do here is an automated approach - waking people in the middle of the night if a Zookeeper server fails is not where we want to be. We’re heavily into infrastructure-as-code and for this project we’re using Terraform. We don’t have access to make changes in production unless we’re on the support rota, and even then, access is given sparingly.",
      "A neat solution, two ways",
      "Here we use ENIs (an additional Elastic Network Interface per Zookeeper Server). So far as we are aware, you could equally well use DNS records - but we haven’t tested it (so Java/Solr might cache the DNS resolution, for example). We used ENIs partly because we were using Terraform, partly because we didn’t see any need to clutter up our DNS namespaces, and partly because the instructions and sample code available to us were working with ENIs. Instructions and code are provided here as examples, with no guarantees of quality or fitness for purpose.",
      "Use a lambda to attach a network interface to a zookeeper instance",
      "Depending on exactly how you set up your infrastructure, this might look substantially more complicated, as AWS’ online console does a number of steps under the hood to save you time. For this you will need: a Zookeeper server, set up and running in an ASG (Ideally using Exhibitor or similar). This is simplest if you have one server per ASG, but can be adapted to work for several per ASG. Do this for each ASG.",
      "Create a new ENI\n  Attach the new ENI\n  Create a Lifecycle hook for the Lambda to run on instance launch\n  Create a Lambda with suitable permissions and code to re-attach your network interface when a new instance is created - example code is here\n  Check it all works!",
      "Use EC2 startup script to attach a network interface to a zookeeper instance",
      "While this is simpler than creating a lambda, it requires that you set it up when you set up your Zookeeper servers, so it can’t be retrofitted into existing ensembles as easily. When you create an ASG, there is an option to provide a user data script. If you’re using an Amazon Linux based image, you should be able to use this one-liner (with your own ENI id in it): aws ec2 attach-network-interface --device-index 1 --network-interface-id eni-0123456789abcdef --instance-id $(ec2-metadata --instance-id | cut -d \" \" -f 2) --region $(ec2-metadata --availability-zone | sed 's/placement: \\(.*\\).$/\\1/').\nIn order for this to succeed, the EC2 instance is going to need some permissions:",
      "{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:AttachNetworkInterface\",\n                \"ec2:DescribeInstances\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n        }\n    ]\n}",
      "Conclusion",
      "In hindsight it seems obvious that persistent addresses for Zookeeper servers are a useful way to connect Solr to them. Going a step further and automating how they are assigned simplifies ongoing maintenance and removes manual steps which could be error prone. For new Zookeeper ensembles I’d suggest you attach the addresses as part of the instance startup script.",
      "Credits",
      "This write-up covers the work of many people. Credit is due to:",
      "Team Columbo in LexisNexis UK, including several members of OSC.\n  Several DevOps specialists; especially Andy.",
      "Thanks Alex!"
    ],
    "summary_t": "A guest blog from Alex Addison on how LexisNexis UK are building production-ready infrastructure for Solr using AWS autoscaling groups, Terraform and Zookeeper."
  },
  {
    "id": "778dfca4f8a9b6c52d8d00d28ee399fa",
    "url_s": "https://opensourceconnections.com/blog/2007/07/05/why-contributing-back-to-the-community-is-important/",
    "title": "Why contributing back to the community is important",
    "content": [
      "As a general rule, our company does not design and develop small websites. However, there is one exception. We run the website for the Middlebrook Horse Show, run by the Middlebrook Ruritans. Why do we spend our time on this website? We spend time working on and supporting this website because it is a charitable organization, and we want to give back to the community from which we come.",
      "Were it not for the local community, we would not be in the position that we are in. Schools, infrastructure, and friends all come from our surroundings, and we feel obligated to give in some small part back to the community that gave so much to us.",
      "While it is a simple site, the Middlebrook Horse Show site also gives us a chance to test out concepts in Ruby, which is always a good thing!",
      "Generally, a high end services company does not want to dilute its brand by taking on simpler or cheaper work. Mercedes does not build competitors to the Yugo for a good reason. However, we feel that providing a simple service for a good organization in our community helps us give back and be better community citizens–a concept more important than profit motive.",
      "See the site at http://www.middlebrookva.org."
    ],
    "summary_t": ""
  },
  {
    "id": "96e77fa9fddbf2e4581e9b9e81f31368",
    "url_s": "https://opensourceconnections.com/blog/2019/06/26/catching-mices-in-berlin-for-ecommerce-search/",
    "title": "Catching MICES in Berlin for e-commerce search",
    "content": [
      "MICES is a one-day, free to attend event held in Berlin just after the Berlin Buzzwords conference - it’s a combination of talks and less formal discussion sessions, run by René Krieger and Paul Bartusch and hosted by mytoys.de in their offices. The e-commerce search community is still relatively small but MICES is helping to build it in a friendly and collaborative way, encouraging people to share their experiences and expertise. I’ll give a brief overview of the talks here but videos and slides should be available soon on the conference website.",
      "René welcomed us all to the conference and talked briefly about the importance of subject matter expertise in improving search and made a plea to ‘include the seller perspective’.",
      "OSC is a sponsor of MICES and this year Eric Pugh, co-founder of OSC, gave the first talk after a brief introduction from René. Eric gave an overview of the Search Maturity Model we use at OSC to help our clients measure their ‘search maturity’:",
      "Eric also talked about how we regard search as a ‘team sport’ - and how it’s important to identify the gaps in your team and the best areas for effective future investment.",
      "Next was Jens Kürsten & Andreas Wagenmann of Otto Group on offline evaluation of a search application. They showed how in e-commerce search there must be a balancing act between what the user asks for and the business wants to show them. Initially their system opted for high recall over high precision, which would lead to a lot of irrelevant results (e.g. fashion results for the query \"tv\"): they used data on product performance drawn from user interactions to improve precision and also mentioned the need to normalise similar queries into a single query.",
      "Andreas Wagner of Searchhub.io talked about optimising findability, and how traditional textual relevance alone is not enough to improve the user’s perceived relevance, with many search results that drive high conversion appearing to be ‘bad’ (irrelevant) results. He mentioned how diversity of results is important to expose products to the user that may drive improved revenue, and how he had developed a composite model that combined measures of ‘sellability’ with findability.",
      "Peter Thomas of Attraqt talked about how search teams need to respond to a rapidly evolving search landscape: his recommendations included unlocking silos, sharing data, allowing teams to set their own goals and to be allowed to learn from making mistakes. He also recommending training (something OSC can help with) and overall a collaborative learning approach.",
      "Diego José de Calazans & Georg Wolf of MediaMarktSaturn talked about search quality as a business goal, and repeated some themes from previous talks about how to find that sweet spot between business requirements and relevance. They used metrics such as NCDG but acknowledged the high manual effort required to gather human judgement data. Customer interactions gave a way to perform automatic search optimisation, but they also mentioned how factors such as advertising campaigns can affect this data.",
      "Cagdas Senol of Zalando was the first speaker after lunch, and talked about how they have developed a way to sort their source data before indexing. He also mentioned his open source testing framework for Painless scripts, used by Elasticsearch. Roman Grebennikov of Findify was next with an amusing and interesting talk on building a personalised search framework using machine learning. He mentioned how on an e-commerce site you may have a very small time window to learn about the customer and the risk of position bias causing positive feedback loops.",
      "Our last speaker was Angel Maldonado of Empathy.co who gave a wonderful and inspiring talk on how good search experiences aren’t just about algorithms but should also inspire joy and create trust. The latter point was well illustrated by what called the ‘Netflix pattern’ - if you are using an algorithm personalised to the user, tell them and allow it to be turned off, rather than just imposing it.",
      "After the formal talks, we then split into groups for various roundtable discussion sessions on a variety of topics including best practices for gathering judgment data. MICES then came to and end with a drinks reception and a few of us then went out for pizza in the evening sun.",
      "Eric and I would like to thank René, Paul, all at Mytoys and everyone else who made the event possible. It was great to meet so many of the European e-commerce search community and hear about some of the fascinating work being done in this area of search. We’ll be back!"
    ],
    "summary_t": "A review of the Mix Camp E-Commerce Search 2019 held in Berlin, maturity models, offline/online evaluation, ranking & emotion in search"
  },
  {
    "id": "6d0fb33e6f9d17da7b68a8fa1f73bff0",
    "url_s": "https://opensourceconnections.com/blog/2019/07/25/2019-07-22-quepid-is-now-open-source/",
    "title": "Quepid is now an Open Source project!",
    "content": [
      "Today we have flipped the switch to release Quepid as an open source project, licensed under the Apache License v2.0.  Come check out the source at http://github.com/o19s/quepid.",
      "What is Quepid? Why should you care?",
      "If you haven’t heard of Quepid, it’s a means for the whole organization to tweak Solr and Elasticsearch relevance. In the early 2010s, search teams would ship a beautiful and fast Solr or Elasticsearch application but lo and behold, users would begin to complain that they couldn’t find what they needed - which after all is the whole purpose of the search engine!",
      "Nobody on the product/technical team wants to think about this mystical ‘relevance’ bit. It was and is a hard problem to approach. It requires a lot of trial and error, fiddling with obscure relevance features like \"TF*IDF\" or the \"edismax query parser\". It means understanding how the search engine dealt with (or didn’t deal with) synonyms, taxonomies, and knowledge graphs.",
      "Most importantly, it means getting into the heads of the users, frankly something our technologists were not equipped to do. We have to tap into domain or business expertise to really understand what the application’s audience considered relevant. In Quepid’s first use, we worked on medical queries and none of the search engineers knew that ‘Myocardial Infarction’ was a fancy term for a heart attack. Or that ‘ct’ was an abreviation for about half a dozen medical terms.",
      "This cartoon captures why we created Quepid in 2013 and remains accurate today if you don’t have tools like Quepid:",
      "",
      "Quepid became a tool for bootstrapping the relevance process with a few simple queries, and a few ratings on what a good / bad result was for that query. In classic search parlance, this is what you’d call a ‘judgement list’. Using a judgement list, we could then put a number on the quality of each query - from simple ‘unit test’ style tests we would code up to classic judgment list metrics like NDCG.",
      "Armed with this data, you have the ability to tweak and tune your search engine configuration. Quepid itself is a sandbox for changing the Solr or Elasticsearch query and seeing the result. It acts like a kind of search team IDE - with a history of all the query patterns that have been tried and an ability to go back to a good version after you decide you’re in a rabbit hole.",
      "But the real power of Quepid is you’re doing all this side-by-side with a domain expert. As you play with the search engine configuration, you can sit and explain ‘why’ a result was coming back in a more understandable way than getting into voodoo about BM25 or boosts, as shown in the screenshow below:",
      "",
      "Most organizations get lost in relevance, and Quepid continues to be a big part of making the process transparent, easier to get started before  worrying about staffing a massive data science competency and measuring relevance. Quepid is like using an IDE for search alongside a set of unit tests, a kind of ‘editor’ that helps the whole team iterate on relevance. We use it extensively in our consulting services to help organizations go from about \"0 to 25\" with relevance, with iterative tuning that helps them get up and running quickly. We hope you can help continue with us on the Quepid journey!",
      "Why did we open source Quepid?",
      "We see teams that need to bootstrap the relevance process, and Quepid is a great way to do that. Quepid’s not the tool for a 10000 query judgment list for testing relevance. Quepid’s a tool that is a gateway to starting the process. It’s the babystep to a gradual, iterative improvement. We wanted to make that accesible to everyone to democratize relevance!\n  \n  \n    Every organization (including us!) creates their own relevance judgement list testing and tuning platform, and we don’t want to keep reinventing plungers.  Let’s rally around one platform for collecting judgements and then using that to tweak and tune Solr or Elasticsearch relevance.\n  \n  \n    Our number one request was \"Can I download and install Quepid in my datacenter?\".  As a closed source commercial tool, we couldn’t figure out a reasonable economic model for providing a downloadable version of Quepid that met our expectations.  Open sourcing Quepid means that we can ask the community to contribute to Quepid being deployable in many environments.\n  \n  \n    Recently a bevy of open source relevance tools are being created. Imagine if ratings created in Quepid fed directly into RRE to measure search metrics and Quaerite to tune query parameters?\n  \n  \n    The clue is in the (company) name!",
      "What does this mean for me?",
      "We’re going to continue to provide the free hosted version of Quepid at http://www.quepid.com, so if you depend on that service, rest assured it will continue to be available.\n  \n  \n    Want to run Quepid in your data center?  Have at it!  Improving how Quepid is deployed will be an area of focus now that the project is open sourced. There’s already a way to build a Docker image of Quepid.\n  \n  \n    Have a custom search API?  Using a different search engine than Solr or Elasticsearch?  We originally created Quepid to scratch our specific itches, and that legacy has continued on to today. By moving to a community driven effort, we open the door to Quepid fitting a broader set of use cases. Maybe you could develop a plugin to connect Quepid to a commercial search engine you’re migrating from, and then compare the ratings with those coming from Solr or Elasticsearch?\n  \n  \n    We chose the Apache licence because it is a very business friendly license to encourage people to reuse the Quepid source for their own purposes, and removes impediments from deploying it in your enterprise.  Great relevance is good for everyone!",
      "I want to thank the over 1100 users of Quepid, as well as the 15 contributors who’ve made 2951 commits over the life of Quepid as a internal OSC project.   I especially want to thank my colleagues Doug Turnbull for the initial idea for Quepid, and Charlie Hull who saw the value of Quepid to not just OSC’s clients, but the world, and led the effort to make it a public hosted service.",
      "Come and join us in Relevance Slack - there’s a #quepid channel to share your experiences and to get community support."
    ],
    "summary_t": "Quepid is now available as an ASL2 licensed open source project!"
  },
  {
    "id": "16fccc58a9b786073290a5aacd534039",
    "url_s": "https://opensourceconnections.com/blog/2019/08/13/hello-ltr-sandbox-for-learning-to-rank/",
    "title": "How You Can Use Hello LTR - The Learning to Rank Sandbox",
    "content": [
      "Learning to Rank just seems hard. Applying Machine Learning to relevance in Solr or Elasticsearch seems non-trivial, and it seems to require a lot of crufty code and plumbing. With Hello LTR we have come up with a series of code and notebooks that attempt to simplify the process. You should be able to ‘bring your own data’ to the notebooks and experiment with Learning to Rank rather easily yourself! Let’s walk through the process with one of the most exciting data sets we could think of - OpenSource Connection’s own blog search!",
      "What is Hello LTR?",
      "Hello LTR is a set of iPython notebooks that demonstrate Learning to Rank in Solr or Elasticsearch. Within Hello LTR comes a client library for performing the typical Learning to Rank workflow - nearly identical between the two search engines. Specifically,",
      "Configuring and Indexing a corpus, using a specific index configuration\n  Load Learning to Rank Features into the search engine\n  Given a Judgment List, extract each feature’s values for each query-document pair, and output a training set\n  Train an offline LTR model using RankyMcRankFace, providing stats on features that worked well / didn’t work\n  Upload that model to the search engine\n  Issue searches using the model",
      "We’re continuing to work to streamline this process in the Hello LTR notebooks, so you can mix in any data and judgments into a sandbox for experimenting with Learning to Rank and relevance for your data set. Certainly, as you go through this, we would welcome your feedback. We know there’s still a lot of work to do, and we welcome community members to help improve and democratize this stuff!",
      "A tour through OSC’s Blog Search via Hello LTR",
      "Let’s poke around this notebook which sets up a Learning to Rank task around OpenSource Connection’s blog. This notebook captures much of the search functionality in our blog, and is a fairly approachable example with a simple corpus and set of judgments. If you want to see other examples, the many other examples focus on TheMovieDB corpus.",
      "Launching Solr or Elasticsearch",
      "Under the docker folder in hello-ltr is a folder for Elasticsearch or Solr. A shell script launch_es.sh or launch_solr.sh builds the docker containers and loads the search engine. With docker installed on your system, simply launch those from the command line, and you should see either system spin up:",
      "$ ./launch_es.sh\nSending build context to Docker daemon  6.656kB\nStep 1/4 : FROM docker.elastic.co/elasticsearch/elasticsearch:6.4.1\n ---> 86c6e080644a\nStep 2/4 : RUN bin/elasticsearch-plugin install -b http://es-learn-to-rank.labs.o19s.com/ltr-1.1.0-es6.4.1.zip\n ---> Using cache\n ---> 2a2c88b6298c\n...",
      "Launching and Setup Notebooks and Datasets",
      "With all the hello ltr dependencies installed you should be ready to launch jupyter:",
      "$ jupyter notebook",
      "Jupyter with a selection of notebooks should be available for you to use. We’ll walk through the OSC blog Jupyter notebook, which you can hack up with your dataset as needed. So click on osc-blog.ipynb in the listing.",
      "",
      "The first cell simply downloads the datasets we’ll use, including a dump of OSC’s blog (blog.jsonl) and a judgment list (osc_judgments.txt) along with the other datasets.",
      "from ltr import download\ndownload();",
      "The corpus and the judgments are the primary input into the Learning to Rank process. The rest is search engine and Learning to Rank configuration and experimentation.",
      "If you wanted to use your own corpus and judgments, simply place them in the data/ directory manually, you’ll see how those files are used and how they’re formatted in a bit, starting with the corpus itself, which we’ll get to in the next cell:",
      "import json\n\narticles = []\n\nwith open('data/blog.jsonl') as f:\n    for line in f:\n        blog = json.loads(line)\n        articles.append(blog)\n\nlen(articles)",
      "Here we just load the corpus (a jsonl file - a text file where each line is a json object). We create a list of objects, each object is a blog post, such as:",
      "{'title': \"Lets Stop Saying 'Cognitive Search'\",\n 'url': 'https://opensourceconnections.com/blog/2019/05/28/lets-stop-saying-cognitive-search/',\n 'author': 'doug-turnbull',\n 'content': ' I consume a lot of search materials: blogs, webinars, papers, and marketing collateral …'\n excerpt': ' We won’t address machine learning illiteracy in search if we can’t go beyond buzzwords. We need to teach concrete, specific techniques. \\n',\n 'post_date': '2019-05-28T00:00:00-0400',\n 'categories': ['blog', 'Relevancy', 'Learning-to-rank'],\n 'id': 2883614620\n}",
      "Configure Search Engine and Index the Corpus",
      "First we create a client object that fulfills the Learning to Rank interface for a specific search engine, here we will use Elasticsearch:",
      "from ltr.client import ElasticClient\nclient=ElasticClient()",
      "The notebooks would be nearly identical for Solr or Elasticsearch (you can see various examples in hello-ltr of both search engines being used). The only differences are where we configure the search engine or use search engine syntax is needed to create Learning to Rank features.",
      "Next we rebuild the index using the corpus:",
      "from ltr.index import rebuild\nrebuild(client, index='blog', doc_type='post', doc_src=articles)",
      "Here rebuild deletes the provided index and then rebuilds it using a local config file.",
      "Rebuild looks for configuration with that index’s name under the docker/elasticsearch or docker/solr directories. For Elasticsearch, this means docker/elasticsearch/<index_name>_settings.json, which stores the typical JSON body you would use when creating a new Elasticsearch index. For Solr that  means a Solr Config set folder name <index_name> with the expected folder structure, containing your solr_config.xml, schema, etc. If you browsed to docker\\elasticsearch you’ll notice blog_settings.json already provided in the Hello LTR codebase.",
      "For your own dataset, you can copy the existing configuration or place a new index configuration with your index’s name in the Solr or Elasticsearch folder. If you want to modify your Elasticsearch or Solr configuration to tweak analysis or query settings, you would need to rebuild the containers and index via the notebook by repeating the command above.",
      "Finally, after configuration, the documents are indexed to Solr or Elasticsearch. Hello LTR expects there to be a field called id on each document. It uses id as the document’s primary key (_id in Elasticsearch). As you would expect, each document’s field needs to be appropriately configured for the corresponding Solr or Elasticsearch configuration.",
      "Configuring Learning to Rank Features",
      "As you may know, features in learning to rank are templated Solr or Elasticsearch queries. Of course this includes the traditional TFIDF based scoring (BM25 etc) on fields you’ve crafted via analysis. It also includes the ability to add in numerical features like dates, etc, combined in arbitrary formulations with text based features, and more lately vector-based features that can capture an embedding-based similarity from an external enrichment system built with word2vec, BERT, or whatever the latest hotness is.",
      "For the blog, there’s a handful of relatively simple features to explore. Here’s two. One is a constant score query, which returns a 1 or 0 depending on if the term matched. The second is a BM25 score in the content field. Other queries search with phrases, create a stepwise function around the age of the post (intuition being newer posts are more relevant), etc.",
      "config = {\n    \"featureset\": {\n        \"features\": [\n            {\n                \"name\": \"title_term_match\",\n                \"params\": [\"keywords\"],\n                \"template\": {\n                    \"constant_score\": {\n                       \"filter\": {\n                            \"match\": {\n                                \"title\": \"\"\n                            }\n                       },\n                       \"boost\": 1.0\n                    }\n                }\n            },\n           {\n                \"name\": \"content_bm25\",\n                \"params\": [\"keywords\"],\n                \"template\": {\n                    \"match\": {\n                       \"content\": {\n                          \"query\": \"\"\n                        }\n                    }\n                }\n            },\n          ...",
      "These features are loaded, configuring a feature set in the search engine with the name ‘test’:",
      "from ltr import setup\nsetup(client, config=config, index='blog', featureset='test')",
      "For your problem, if you think a specific set of features would be a good selection, you can express them here, and configure them into a featureset with it’s own name.",
      "Logging Features to Create a Training Set",
      "Logging feature values is one of the most complex engineering aspects of building a Learning to Rank system. In Hello LTR, we have a function which takes as input, a judgment list, and outputs a training set with every feature’s value for that judgment.",
      "A judgment list grades how good a document is for a query. Some documents will be very relevant for a query. Others we mark as not relevant for a query.",
      "The judgment list is expressed as an ‘stub’ RankSVM file format. This file format, common to learning to rank tasks tracks the grade in the first column. In our example, we use the standard of a 0 meaning most irrelevant and a 4 meaning perfectly relevant for the query. The second column is a unique identifier for the query, prefixed with qid. A comment with the document identifier follows. For example, here’s a snippet from the blog judgment list for one of the queries:",
      "# qid:1: solr\n4    qid:1     # 4036602523    solr\n3    qid:1     # 2916170833    solr\n2    qid:1     # 3986527021    solr\n2    qid:1     # 3440818580    solr\n0    qid:1     # 2926247873    solr\n0    qid:1     # 3065216762    solr\n0    qid:1     # 14036114    solr\n0    qid:1     # 1765487539    solr",
      "In the file header, query id 1 is associated with the keyword solr (# qid:1: solr). Farther down the list, a series of documents are graded for qid:1 (solr). Document w/ id 4036602523 is assigned a grade of 4 (perfectly relevant) for Solr on the line 4    qid:1     # 4036602523    solr. Document 14036114 is assigned a grade of 0 (very irrelevant) for Solr on the line 0    qid:1     # 14036114    solr",
      "The task of logging is to use the keywords for the query id (in this case solr) and compute the value for each feature for that graded document. This way model training can learn a good ranking function that maximizes the likelihood relevant documents will return towards the top for a query. We want to provide features that will help our model make decisions on when a document is relevant or irrelevant.",
      "We see one feature from above is title_term_match which is a 1 or 0 based on whether a search term occurs in the title. Perhaps title_term_match is a 1 for the line 4    qid:1     # 4036602523 and perhaps title_term_match is a 0 for an irrelevant document 0    qid:1     # 1765487539    solr. We repeat this process for every feature in our feature set, for every line for the ‘solr’ query.",
      "Ideally, what we’d like to build is a training set that looks like:",
      "4    qid:1     title_term_match:1      content_bm25:12.5    ...   # 4036602523\n0    qid:1     title_term_match:0    content_bm25:0    # 1765487539",
      "The RankSVM format expects features to be identified by ordinals, starting with feature id 1. So we need to transform these into a 1-based index of feature in the original feature list. Hello LTR does that for you and does the bookkeeping to keep them straight!",
      "4    qid:1     1:1      2:12.5    ...   # 4036602523\n0    qid:1     1:0    2:0    # 1765487539",
      "There’s a lot of considerations for logging feature scores to consider in a live production system. What Hello LTR is focuses on the ‘sandbox’ use case: you have a corpus, a judgment list on that corpus, and you’d like to experiment with Learning to Rank features. In this case, we simply batch up every doc id for each query to the search engine, and ask for logged feature values. See the documentation linked above for how this happens.",
      "Anyway, all the bookkeeping and plumbing you just learned about happens in just a few lines in Hello LTR. With the input file osc_judgments.txt, features from our featureSet test will be logged out and written to osc_judgments_train.txt in the right file format:",
      "from ltr.log import judgments_to_training_set\ntrainingSet = judgments_to_training_set(client,\n                                        index='blog',\n                                        judgmentInFile='data/osc_judgments.txt',\n                                        trainingOutFile='data/osc_judgments_train.txt',\n                                        featureSet='test')",
      "Train a Learning to Rank Model",
      "With a training set prepared, you can now go through the process of training a model. Here we’re simply performing the training process and loading a model into the search engine for us to experiment with in the code below:",
      "from ltr.train import train\ntrainLog = train(client,\n                 trainingInFile='data/osc_judgments_train.txt',\n                 metric2t='[email protected]',\n                 featureSet='test',\n                 index='blog',\n                 modelName='test')\n\nprint(\"Train [email protected] %s\" % trainLog.rounds[-1])",
      "The train function invokes RankyMcRankFace via the command line, and provides an opportunity to pass a variety of parameters that are passed along to RankyMcRankFace. The method returns a log (trainLog), parsed out of the command line output of Ranky with a lot of good information on the model training process.",
      "A couple things to note about train:",
      "trainingInFile is the training set you prepared in the logging step with the funky format 4    qid:1     1:1      2:12.5    ...   # 4036602523 etc\n  A variety of LambdaMART hyperparameters are exposed (number of trees and leafs of each tree). In a future post we can go deeper into how to select these hyperparameters\n  RankyMcRankFace can optimize for a number of classic relevance metrics such as Precision, ERR, NDCG, and DCG. Here we optimize for [email protected] by passing this as an argument for metric2t parameter\n  The model is stored in the search engine at modelName=test. This model is associated with featureSet test to understand how the model’s features bind to a set of search queries at search time",
      "The model output is also interesting, you can examine the impact of each feature via trainLog.impacts, which stores a dictionary of each index ordinal and how much error is reduced in the model:",
      "{'2': 64.48048267606816,\n '3': 33.63633930748523,\n '7': 31.319488313331828,\n '8': 2.72292608517665,\n '1': 0.014245484167042312,\n '6': 0.007610925647204436,\n '4': 0.0,\n '5': 0.0}",
      "Feature impacts are a bit complicated to interpret in these kinds of models, but this still gives you a hint that feature 2 (content_bm25) matters most followed by feature 3 (title_phrase_bm25) and feature 7 (excerpt_bm25`). Pretty interesting!",
      "Training a good LambdaMART model is an art. Here we just want to get you started with the \"Hello World\" sandbox - there’s a lot we’re not discussing here about doing this well to help you get started. There’s additional functionality in ltr.train, such as feature_search, which performs a brute-force search for the feature mix that best optimizes [email protected] (or whatever metric you choose) based on k-fold cross validation.",
      "Search with your model!",
      "Finally, we can execute a search using the model, using the provided display:",
      "blog_fields = {\n    'title': 'title',\n    'display_fields': ['url', 'author', 'categories', 'post_date']\n}\n\nfrom ltr import search\nsearch(client, \"beer\", modelName='test',\n       index='blog', fields=blog_fields)",
      "According to the model I trained, the following posts are most relevant to a search for \"beer\". As you might expect, a lot of events and something secret Google is doing to foster collaboration… hmm!",
      "Holiday Open House at OSC &#8211; Come Share in the Fun \n1.3306435 \nhttps://opensourceconnections.com/blog/2013/12/11/holiday-open-house-at-osc-come-share-in-the-fun/ \njohn-berryman \n['blog', 'solr'] \n2013-12-11T00:00:00-0500 \n---------------------------------------\n4 Things Google is doing internally to foster collaboration and innovation \n-0.032763068 \nhttps://opensourceconnections.com/blog/2008/03/13/4-things-google-is-doing-internally-to-foster-collaboration-and-innovation/ \narin-sime \n['blog', 'Opinion'] \n2008-03-13T00:00:00-0400 \n---------------------------------------\nTrip Report: Shenandoah Ruby User Group \n-0.032763068 \nhttps://opensourceconnections.com/blog/2008/04/30/trip-report-shenandoah-ruby-user-group/ \neric-pugh \n['blog', 'Code', 'Community', 'Speaking'] \n2008-04-30T00:00:00-0400",
      "Eric Pugh, John Berryman, and Arin Sime seem to be the resident beer experts. Not unexpected.",
      "Where to go from here",
      "I encourage you to try to plugin your own data into Hello LTR! Try it out and please let me know how it goes. We love learning about unique problems in this space!",
      "Hello LTR is also a training course we offer using a lot of this code to build and explore Learning to Rank models with search relevance experts. Please get in touch if you would like a 2-day hands on Learning to Rank course for your search team, and keep an eye out for public training"
    ],
    "summary_t": "Hello LTR is a framework that lets you plugin your data to easily build and test Learning to Rank models in Solr or Elasticsearch. Try it Out!"
  },
  {
    "id": "5a1a2142393f2a6884c7265f478e4931",
    "url_s": "https://opensourceconnections.com/blog/2019/09/05/diversity-vs-relevance/",
    "title": "Diversity vs. Relevance in Search Systems",
    "content": [
      "I want to share something powerful I’ve learned from my journey into designing a successful search ranking systems; an area I don’t think most search practitioners are really thinking about:",
      "There’s a tension between diversifying the result set to meet possible user needs vs. improving the relevance of the results against a very specific need we think the user has.",
      "I want to discuss how we might consider optimizing for both: most importantly focusing on measurement of these concerns.",
      "Users come to search with a goal in mind. They hunt for what they want, expressing their needs in natural language keywords. Much like any conversation, the initial ‘statement’ is an imperfect expression of their needs. Users might start early with ambiguous keywords (like ‘waffle maker’). When search seems to need clarification, they step back, think how to clarify what they’re saying, and reformulate their language to something more specific (‘waffle maker’ => ‘blue waffle maker’). Eventually as less and less ambiguity is left in the user’s query they have a very clear specification of what they need (‘blue double waffle maker for commercial kitchen’).",
      "We realize it’s a fallacy, especially in these queries early in the conversation, to consider all users that share the same keywords as having the same goals/needs. \"Waffle maker\" is so broad, encompasses so many needs, that it could mean anything from an Olaf Waffle Maker from the movie Frozen to something more like a George Foreman Grill. All are 100% relevant to the keywords (though perhaps not to the user’s need).",
      "On the other end of the spectrum, when a user gets more specific (‘blue double waffle maker for commercial kitchen’), the focus switches to relevance to that intent. We see more of the user’s goal - can we give results close to that target? And we do literally mean proximity. The vector space model of information retrieval presumes can can put the user’s query in a vector space (like x/y coordinates) and we can get ‘close’ or ‘far’ from that target.",
      "For example, with a search for ‘blue double waffle maker for commercial kitchen’ we might show first an exact match (‘blue double commercial waffle maker’). We might follow with a red commercial waffle maker. Or a blue non-commercial ones. Which attributes are more important / make most sense to drop becomes a part of relevance tuning. By prioritizing certain attributes over others, if we are missing some (say the color) then perhaps this user might still be satisfied with something imperfect for their need, but good enough (like a ‘red commercial double waffle maker’).",
      "The search results diversity measurement gap",
      "We don’t have canonical measures of search result diversity. And that’s a huge problem. Especially because queries on the ‘diversity’ end of the spectrum are where users often start. Instead of results that reflect a range of possible intents/next steps, search teams frequently flood the SERP with repeated, similar looking search results. Instead of a range of waffle makers, we disappoint our users with nothing but Olaf waffle makers.",
      "O laf the puns up you search nerds",
      "We have pretty good methods for measuring relevance. We can use judgment lists along with classic search metrics like NDCG and tools like Quepid /  RRE / Quarite to measure how well a query does at returning a relevant result sets. If you’re unfamiliar - judgment lists come from a number of places, like clickstreams, conversions, crowdsourcing, or expert raters. The crowdflower ecommerce search relevance dataset is one example. It has crowdsourced judgments for several e-commerce site’s queries. Each document is graded from 1 (very irrelevant) - 4 (extremely relevant) for that query.",
      "For example, check out this search for a very specific type of Brett Favre (US Football Star) jersey:",
      "",
      "(If you’re curious about this weird query: it turns out that the New York Jets used to be called the Titans – confusingly now the name of a different NFL team. On Sept 28, 2008, while Brett Favre played with the Jets, the Jets played in old blue NY Titans jerseys. )",
      "I can validate that, indeed, the judgments make sense. A successful search system would perform a ranking of results where the item closest to the user’s specification comes first, followed by gradually ‘farther away’ items. Most classic relevance metrics, like NDCG or Precision can measure what we need: whether the search engine is doing its job placing items close to the users intent (blue Favre jersey) above lesser candidates (the green ones).",
      "On the other hand, the relevance measurement regime fails for queries that need the ‘diverse’ treatment. Here’s a handful of judgments for our friend ‘waffle maker’:",
      "",
      "You can see the diversity measurement problem here. Notice we have a set of waffle makers, all very different, but each perfectly relevant for the ‘waffle maker’ query. From a relevance mindset, a page full of ‘4s’ would be the ideal outcome. Using a precision, we can compute the precision of the page that contains 3 very different ‘waffle makers’: (4+4+4) / 12 = 1 - perfect! We can also compute the precision of a page with 3 Olaf waffle makers:  (4+4+4) / 12 = 1 - also perfect!",
      "No matter the ‘relevance’ metric we chose, it won’t measure result diversity.",
      "Achieving relevance in these ‘diverse’ queries is a necessary but not a sufficient condition. It’s good that the search engine returned waffle makers. Certainly a teddy bear or television in the search results would be bad. However, relevance here is not enough to build a good search experience.",
      "Diversity measurement is hard, let’s go shopping",
      "As you can see, when trying to measure what ‘success’ looks like, the diversity end of the spectrum doesn’t play by the same rules as relevance.",
      "With the Brett Favre query above, a user expressed a very specific goal, and we might expect them to convert if we were successful at satisfying their goal. The item that performs / converts well might arguably most relevant.",
      "But these early queries like ‘waffle maker’ shouldn’t be held to that standard. That’s not these query’s role. They start, they don’t complete the user’s mission. A successful search system doesn’t, and probably shouldn’t, simply optimize for ‘conversions’ this early in the conversation.",
      "With diversity, the user needs to see the diverse range of options possible, to select a new direction, reformulate their query. Users need to feel confident they still have the scent of information for their conscious or unconsious needs. A page full of Olaf waffle makers is unlikely to achieve that goal.",
      "I’m reminded of my colleague René Kriegler’s slide from his original MICES keynote discussing e-commerce search:",
      "",
      "Set aside waffle makers for the moment, and let’s think instead about a very expensive purchase. One where you’re not likely to get lucky and see exactly what you want on the first query. Something like TVs.",
      "If you buy a television, you’ll do a lot more research. You’ll undergo a long buying decision-making process. You’ll try to formulate the problem and gather information. Purchase would only come after many, many searches.",
      "The diversity vs relevance spectrum doesn’t correspond neatly to René’s \"Buying Decision Process\". For example you can imagine you might have a ‘relevance’ query for a specific kind of television and simply be gathering information. But it does serve to illustrate that users go through a process where, especially earlier, we might expect more of the diverse sorts of queries. Maybe Amazon isn’t crazy for both making it easy to purchase AND a great place to do research into products.",
      "Some Diversity Measurement Approaches",
      "I won’t pretend to have all the answers. But with diversity,  you want to answer the question: \"Do the results I provide help further the user’s search towards a satisfactory result?\" or maybe \"do the results (or UI) give user’s visibility over the possible clarifications of their search.\" Is there enough ‘scent’ for most users? Spitballing, here are two ideas that I hope trigger discussion and reaction in the community.",
      "Gini Coefficient (or Inig Coefficient?)",
      "Gini coefficient: how equally distributed over a well known set of categories (most important facets) is the data. This has been the most common statistic I’ve seen used to measure diversity. It’s appealing because (a) some search teams can pinpoint which kind of ‘facet’ might be most important for a query (like size for shoes; color for waffle makers) and therefore (b) can know which categories make most sense to diversify over.",
      "The intuition behind the Gini Coefficient is hard to explain without a graph. Let’s walk through an example.",
      "In the graph below, we have 3 colors of waffle maker (Red, White, Blue). The graph below is cumulative. So it’s not a graph over the number of results per colors instead each column adds to the column to the left. The first value, just \"R\" counts all the Red waffle makers. The second \"R,W\" are the Red+White waffle makers. Finally the last column \"R+W+B\" represent all of our waffle makers.",
      "",
      "The other trick is that the first value R is the smallest subset, W is the second smallest, and so on. So it’s both sorted and cumulative. This guarantees the graph looks like this kind of ‘elbow’ curve. If you want to sound fancy at parties, this is a Lorenz Curve.",
      "The ideal Lorenz Curve is a straight line. In this curve, ever column adds equally to the total, and there’s perfect equality over each of the classifications. In other words, it’s not very curvy at all, it’s just a line:",
      "",
      "The classic Gini Coefficient goes from 1 (most unequal) to 0 (most equal). But I think it’s more useful to go from 0 (least diverse result set) to 1 (very diverse result set), so we don’t get confused on which metrics ‘higher is better’. So really what I’m about to tell you is really 1 - Gini Coefficient which I will cleverly name the Inig Coefficient.",
      "Quite simply we will compute the area of our result set’s distribution (the top curve) divided by the area of the ideal (the line). You can see that more clearly in the graph below. The blue area would be divided by the red ‘ideal’ area. Close to 1, they overlap and we say we have a maximally diverse result set, where each category has equal representation. Close to 0, we are dominated by one of the categories.",
      "",
      "I computed this in the spreadsheet using the area of each trapezoid segment for the blue curve. Summing that up, I arrive at the blue curve’s ‘area’. Then I compute the area of the red triangle. Dividing the first value by the second gives us our Inig coefficient.",
      "Here’s a pretty unequal case, Inig coefficient of 0.3333…",
      "",
      "A more equal distribution, Inig coefficient of 0.86",
      "We can of course restrict Inig to top N results to get [email protected], or over whatever top N makes the most sense.",
      "I like this metric as it’s fairly intuitive. However it does require understanding the correct categories for a given query to diversify for! Not all search teams quite know for every query what the right categories to cover to ensure a diverse query set from the user’s point of view.",
      "Min Refinement Precision At Root Query (aka ‘Working Backwards’)",
      "If we know the top refinements for our diverse query we can grade the results of the original query using relevance judgments from the common refinements. We might take minimum (or perhaps mode) of all of the refinement queries’ precision when graded against the original query’s results.",
      "For example, let’s say we want to evaluate \"waffle maker\", using it’s top 2 refinements \"red waffle maker\" and \"extreme waffle maker\". \"Doug’s Red Waffle Maker\" and \"René’s Red Waffle Maker\" are both a grade of \"4\" for query \"red waffle maker\". And \"Waffleator 9000\" and \"Wafflecannon\" are both grade of \"4\" for query \"extreme waffle maker\".",
      "A ‘waffle maker’ result set that only returned red or extreme waffle makers would be problematic for our ‘diversity’ criterion. One that returned a mixture of results from top refinements (both ‘red’ and ‘extreme’) would be better (at least that’s what this metric presumes!).",
      "So this is a nice and diverse ‘waffle maker’ set:",
      "René’s Red Waffle Maker\n  Waffleator 9000\n  Doug’s Red Waffle Maker\n  Wafflecannon",
      "[email protected] against ‘red waffle maker’ = (4 + 0 + 4 + 0) / 16 = 0.5\n[email protected] against extreme waffle maker’ = (0 + 4 + 0 + 4) / 16 = 0.5",
      "Min Refinement Precision = 0.5",
      "However this situation would be problematic:",
      "René’s Red Waffle Maker\n  Liz’s Red Waffle\n  Doug’s Red Waffle Maker\n  Red shiny chrome waffle maker",
      "[email protected] against ‘red waffle maker’ = 1\n[email protected] against ‘extreme waffle maker’ = 0",
      "Min Refinement Precision = 0 <- problem!",
      "Our metric might show here that users looking for that ‘extreme waffle experience’ might not see anything reflected in the initial ‘waffle maker’ query. They might give up, writing off the search experience as not having the information scent for their kind of waffle maker. Without the ‘extreme’ waffle makers in the mix, they might not have teh lightbulb go off that they themselves need to clarify their intent with the term ‘extreme’, so they might give up.",
      "I like this metric as it doesn’t require us to understand the categories to diversify over. Unfortunately, this metric is somewhat biased by the relationship between diverse result set and refinements. Users may never refine their queries if they aren’t offered a diverse result set. Or they may refine them in a particular way given the result set.",
      "It’s also probably important to think about how to define the top refinements. Should you cluster top refinements, but eliminate candidates that overlap too much? And it may be important instead of picking top N refinements, to instead pick the top refinements as scored by statistical significance.",
      "A bit on ranking function design: Ranking Intents, Not Just Documents",
      "To measure relevance and diversity effectively, we may need to reflect a bit more about a solution framework to this diversity/relevance dilemma.",
      "I propose we might want to reformulate our thinking of a search ranking into roughly two stages, with each stage having a different emphasis depending on where the query lives in the relevance-diversity spectrum:",
      "What are the user’s likely intents? (user preferences conscious or unconscious, categories of documents, clusters of documents, etc)\n  Given those intents, what are the most relevant documents that role up to each intent?",
      "In the first step, we have a probability distribution or a ranking of intents. We might evaluate this ranking independently of step (2). Perhaps it relates to distribution over categories or reflection of refinements. But likely it goes deeper into personas, user segments, non-facetable categories (like proximity), and perhaps latent features of document text. You might have more context on the user or query to know which intent is more likely (though probably you don’t want to go too far with this assumption).",
      "In the second step, we hold an intent constant and rank against it. We presume we know what the user wants and aim to rank candidate documents close to that target (just as in our Brett Favre example above). This is what search engines are built for",
      "I think likely there’s an important relationship to taxonomies here that might help us with this problem.",
      "At the ‘top’ of the taxonomy, you have broad categories - like ‘waffle makers’. When a search corresponds to a broad topic, you want to gather a healthy, diverse set of candidates from the next layer down (extreme waffle makers / chef waffle makers / etc). You can even use the taxonomy to explicitly say \"we have a lot of waffle makers - what kind do you want?\". But if you’re farther down in the taxonomy, searching for obscure forms of Brett Favre memorabilia, you are trying to get as close as possible while still being true to the user’s intent. You ask yourself whether it is OK to violate the users ‘color’ preference? Or show jerseys of other famous players? We know in these situations the results won’t be 100% relevant. But maybe they would be grateful for the 90% option. Where to cut that off is a tricky problem to solve and is rather domain & user specific.",
      "To restate the original problem, how would we measure whether we had a correct probability distribution over intents for a broad query? Perhaps a taxonomy (generated or otherwise) WOULD help.",
      "Perhaps the key question becomes:",
      "Does the taxonomy cleanly separate one set of intents from another?",
      "In other words, do extreme vs chef waffle makers overlap a lot. If so, it’s not a useful ‘next step’ for our users. If they tend to segment users of early queries successfully, then we might say it’s a good taxonomy - and therefore can be used to present a diverse result set.",
      "As an example, a ‘clean’ breakdown of ‘trousers’ might be something like",
      "Informal Trousers\n    \n      Jeans\n      Corduroys\n    \n  \n  Formal Trousers\n    \n      Khakis\n      Cuffed Dress Pants",
      "A bad taxonomy would be:",
      "Trousers with Two Legs\n    \n      Clown Trousers\n      Jeans\n      All Other Trousers\n    \n  \n  Trousers with One Leg",
      "The first taxonomy cleanly divides the ‘trousers’ space. We can cover more of that space more confidently and efficiently. We might monitor that users navigate/refine into each category in closer to equal terms (see Inig metric above). Whereas the bad taxonomy, everyone would go into \"Trousers with Two Legs\" and probably next \"All Other Trousers\" - not a good Inig over user next steps.",
      "Likely Intent Ranking Systems - First Class IR systems with Search and Recommendations",
      "To conclude, there’s a ton more to discuss on this topic, and I hesitate to begin writing another book via this post :) In particular, I haven’t discussed HOW to perform diversity. Daniel Tunkenlang has a blog post on results clustering. Certainly the search engine’s grouping feature is an option. Explicit UI’s built for diversity are another option.",
      "I’m also interested in connections to other kinds of Information Retrieval. I wonder whether ‘results diversity’ exists as a kind of third system in between recommendation systems (no user intent expressed) and relevance (hard user intent expressed). It’s like this is a 3rd modality - soft intent - rarely addressed by the IR community. It seems likely a modality needing it’s own systems. If search engines care about the ‘hard intent’ end of this spectrum, collaborative filtering systems focus on the ‘no intent’ side. What’s the ideal system for the ‘soft intent’ problem?",
      "Such a third ‘soft intent’ IR system seems appropriate for ranking intents, not documents. Further, it seems very much tied to the broad topic of intent detection, which for most search teams is an increasingly complex piece of functionality sitting between the query and the search engine. Perhaps we need to think of this component as a first class IR system, not some add on to search.",
      "Anyway, as this is my initial blogging foray into this topic, I certainly hope to ‘diversify my response set’ by hearing from you and your opinions on the topic. Get in touch if you’d like to provide feedback - I’m eager to discuss! Come find me at Relevance Cornucopia🦃, Haystack EU, or Activate if you’d like to discuss in person."
    ],
    "summary_t": "Search systems struggle to balance diversifying the result set to match likely intents vs ranking against one well understood intent."
  },
  {
    "id": "f03411b36744a5f0c50e772ac1465fe8",
    "url_s": "https://opensourceconnections.com/blog/2006/08/01/battle-of-the-scaffolds-aug-9th/",
    "title": "Battle of the Scaffolds Aug 9th",
    "content": [
      "Join Eric Pugh of OpenSource Connections August 9th at RubyJam as he pits two competing scaffolds designed for quickly ripping out a CRUD interface: AjaxScaffold and Streamlined. Learn the strengths and weaknesses of each. See the design choices the developers took to minimize the code intrusion of their tools into your application. See how much fun tabluar editing of data is when you have Ajax goodness!",
      "6:30 PM, Room 160 at Darden Business School"
    ],
    "summary_t": ""
  },
  {
    "id": "8f93ad381c0cf33173611bab4ef408db",
    "url_s": "https://opensourceconnections.com/blog/2007/07/09/eric-pugh-chosen-to-present-at-oreilly-open-source-convention/",
    "title": "Eric Pugh Chosen To Present at OReilly Open Source Convention",
    "content": [
      "Eric Pugh has been chosen to present at the 2007 OReilly Open Source Convention on using and writing Rails plugins.",
      "OSCON is the worlds largest open source convention, with over 2,500 participants.  It will be held July 23-27, 2007.",
      "Erics presentation will be held on Thursday, July 26 at 11:35 AM. More information on his presentation can be found on the OReilly page."
    ],
    "summary_t": ""
  },
  {
    "id": "98293dccd3e64bf48706c1023c71c2f6",
    "url_s": "https://opensourceconnections.com/blog/2019/09/11/announcing-relevance-cornucopia/",
    "title": "Announcing Relevance Cornucopia🦃 - How Natural Language Search completes Learning to Rank",
    "content": [
      "In August’s Summer of Relevance we took 19 attendees through intense, small-class search relevance training workshops - including \"Think Like a Relevance Engineer\" and \"Hello LTR - Learning to Rank Training\" to turn them into elite relevance engineers! Now the week of November 11th, we’ll be doing another week-long training extravaganza again at Relevance Cornucopia🦃!",
      "In Summer of Relevance, we spent a week helping attendees cut through the ‘magic’ of smarter search, to teach best practices in relevance measurement & tuning. In Learning to Rank training, we taught them how to best build and deploy machine learning optimized search ranking using 100% open source search components.",
      "We had fun, but there’s a missing piece to how we think through machine-learning search relevance. Do you see what’s missing in the diagram below?",
      "",
      "See the thing is Learning to Rank can take a set of features, and try to build an optimal ranking function. But what it CAN’T do is get underneath the natural language meaning of the user’s query, really grok the query’s intent or meaning in a document. Learning to Rank can’t get better than the features you feed it (just a bunch of numbers). And if those features are just about strict, exact term matching - you won’t get the most you could out of it.",
      "You might say Learning to Rank is a bit like eating the Thanksgiving turkey without any of the fixins!",
      "The missing ingredient in this is adding NLP to your search system. And that’s why we’ve added Natural Language Search to our training lineup at Relevance Cornucopia 🦃. Learning to Rank is built to optimize a ranking function given arbitrary features. Natural Language Search creates creates features that make it possible for LTR to optimize the ranking based on deeper understanding of the user’s language. It’s like Yams and Marshmallows, you don’t want one without the other :)",
      "That all sounds fun and exciting, but you really shouldn’t be getting into either until you’ve had the foundational \"Think Like a Relevance Engineer\" class. In this class, given before LTR and NLP classes, you’ll learn about relevance measurement, tuning, best practices, and making search accountable to the business. In particular, in the diagram above the \"Hypothesize, experiment, and measure\" cycle of doing relevance work is absolutely critical, and is either an organization’s foundation for success, or why an organization fails.",
      "We offer this course for both Solr and Elasticsearch.",
      "In addition, we’ll be your hosts in beautiful Charlottesville, and look forward to entertaining you in our lovely downtown during the evening. There’s a lot of fun things to do in Charlottesville area, including enjoying beautiful fall foliage in Shenandoah National Park, exploring wineries, Monticello, the University of Virginia and more!",
      "If you’re interested, check out the details below. Or Get in touch with any questions.",
      "For Beginning Relevance Engineers",
      "Take the \"Think Like a Relevance Engineer\" course Tuesday & Wednesday of Relevance Cornucopia. Don’t worry there’s still an option to add Learning to Rank or NLP the next two days",
      "View Solr Details\nView Elasticsearch Details",
      "For the Advanced Relevance Engineer",
      "Either after you’ve taken \"Think Like a Relevance Engineer\" or for the more advanced practitioner we have our Natural Language Search and Hello LTR courses.",
      "Hello LTR Details\nNatural Language Search Details",
      "Want to do both? Contact us for a discount code",
      "The two classes form a great progression for someone really wanting to go deep in improving their search relevance skills. Please contact us if you’d like $100 off each class.",
      "Quotes from post attendees",
      "\"'Think Like a Relevance Engineer' has helped me think differently about how I solve Solr & Elasticsearch relevance problems\"\n\n    \n    Matt Corkum, Disruptive Technology Director, \n    Elsevier",
      "\"'Hello LTR' training gave me strong conceptual foundation of Learning to Rank and its moving parts, practical tips and learnings on how to implement it in production\"\n\n    \n    Lekha Muraleedharan, Search Developer \n    Spotify",
      "What a positive experience! We have so many new ideas to implement after attending 'Think Like a Relevance Engineer' training.\n  \n      \n      Andrew Lee, Director of Engineering for Search \n      DHI"
    ],
    "summary_t": "Our next training event, Relevance Cornucopia🦃, we’ll add the missing piece to the manual tuning & learning to rank picture: Natural Language Search. Com..."
  },
  {
    "id": "6563031cf6996e7c170e059cfb8b546f",
    "url_s": "https://opensourceconnections.com/blog/2019/09/23/what-happens-at-a-lucene-solr-hackday/",
    "title": "What happens at a Lucene/Solr Hackday",
    "content": [
      "For the last few years I’ve run free Lucene Hackdays around the same time of year as the largest conference in our open source search sector, Activate (previously known as Lucene Revolution and run by Lucidworks). The idea is that if people are in town for a conference anyway, they might like to get together with other Lucene/Solr enthusiasts and work together on some problems. This year, as Activate was in Washington DC, OSC organised a hackday kindly hosted by the American Geophysical Union at their new (and highly eco-friendly!) new headquarters building. Around 20 people attended including several Lucene/Solr committers and four of us from the OSC team.",
      "I’ve listed below some of what we worked on, with links to related JIRA issues and other supporting material. On the day we used Relevance Slack to communicate (check out the #lucene-hackday channel) - if you want more information this is the best place to ask. Thanks to everyone who came and hacked, it was a lot of fun! Note that we didn’t have time to work on everything but I’ve listed all the ideas anyway in case someone else is inspired to continue. Also, the following is based on my notes taken during the day and the shared GoogleDoc we used, so do please forgive any inaccuracies or mistakes.",
      "Things we worked on",
      "1. Andy Webb suggested two ideas:",
      "a. add support for BooleanSimilarity to Solr, for situations when you don’t want document/term frequency to influence scores (ref LUCENE-5867): \"I’ve raised SOLR-13751 which links to https://github.com/apache/lucene-solr/pull/867 - just running ant test now. It’d be great to get further feedback/comments on this - this is my first Solr PR!\"",
      "b. Solr admin UI tweaks such as alphabetical ordering of collections in the drop-down lists. Bob Bunch took this on: \"Here are a couple of patches (master branch; I’m not set up to contribute yet).  This fixes:",
      "Collections page, order of collections\n  Collections drop-list, order of collections\n  Cores drop-list, order of cores\"",
      "diff --git a/solr/webapp/web/index.html b/solr/webapp/web/index.html\nindex 7fe1381763c..11157402269 100644\n--- a/solr/webapp/web/index.html\n+++ b/solr/webapp/web/index.html\n@@ -182,7 +182,7 @@ limitations under the License.\n                         ng-model=\"currentCollection\"\n                         chosen\n                         ng-change=\"showCollection(currentCollection)\"\n-                        ng-options=\"collection.name for collection in aliases_and_collections\"</select\n+                        ng-options=\"collection.name for collection in aliases_and_collections | orderBy: 'name'\"</select\n               </div\n               <p id=\"has-no-collections\" ng-show=\"collections.length==0\"<a href=\"#/~collections\"\n                 No collections available\n@@ -208,7 +208,7 @@ limitations under the License.\n                         ng-model=\"currentCore\"\n                         chosen\n                         ng-change=\"showCore(currentCore)\"\n-                        ng-options=\"core.name for core in cores\"</select\n+                        ng-options=\"core.name for core in cores | orderBy: 'name'\"</select\n               </div\n               <p id=\"has-no-cores\" ng-show=\"cores.length==0\"<a href=\"#/~cores\"\n                 No cores available",
      "diff --git a/solr/webapp/web/partials/collections.html b/solr/webapp/web/partials/collections.html\nindex 198030c744f..8c9cebc81c7 100644\n--- a/solr/webapp/web/partials/collections.html\n+++ b/solr/webapp/web/partials/collections.html\n@@ -88,7 +88,7 @@ limitations under the License.\n           <input type=\"text\" name=\"alias\" ng-model=\"aliasToCreate\" id=\"alias\"</p\n \n         <p class=\"clearfix\"<label for=\"aliasCollections\"Collections:</label\n-          <select multiple id=\"aliasCollections\" ng-model=\"aliasCollections\" ng-options=\"collection.name for collection in collections\" class=\"other\"\n+          <select multiple id=\"aliasCollections\" ng-model=\"aliasCollections\" ng-options=\"collection.name for collection in collections | orderBy: 'name'\" class=\"other\"\n           </select</p\n@@ -382,7 +382,7 @@ limitations under the License.\n   <div id=\"navigation\" class=\"requires-core clearfix\"\n     <button id=\"add\" class=\"action\" ng-click=\"showAddCollection()\"<spanAdd Collection</span</button\n     <ul\n-      <li ng-repeat=\"c in collections\" ng-class=\"{current: collection.name == c.name && collection.type === 'collection'}\"<a href=\"#~collections/\"</a</li\n+      <li ng-repeat=\"c in collections | orderBy: 'name'\" ng-class=\"{current: collection.name == c.name && collection.type === 'collection'}\"<a href=\"#~collections/\"</a</li\n     </ul\n     <hr/\n     <button id=\"create-alias\" class=\"action requires-core\" ng-click=\"toggleCreateAlias()\" ng-disabled=\"collections.length == 0\"<spanCreate Alias</span</button>",
      "diff --git a/solr/webapp/web/partials/cores.html b/solr/webapp/web/partials/cores.html\nindex 1615769564f..4dccc313449 100644\n--- a/solr/webapp/web/partials/cores.html\n+++ b/solr/webapp/web/partials/cores.html\n@@ -217,7 +217,7 @@ limitations under the License.\n \n   <div id=\"navigation\" class=\"requires-core clearfix\" ng-show=\"hasCores\"\n     <ul\n-      <li ng-repeat=\"c in cores\" ng-class=\"{current: core.name == c.name}\"<a href=\"#~cores/\"</a</li\n+      <li ng-repeat=\"c in cores | orderBy: 'name'\" ng-class=\"{current: core.name == c.name}\"<a href=\"#~cores/\"</a</li\n     </ul\n   </div>",
      "2. Bob Bunch also suggested:",
      "a. To work more on monitoring JVM full garbage collects and shard/core status at scale. (we didn’t get around to this)",
      "b. Exposure testing - we resolved a major bug in Gus’ nifty cloud.sh script - SOLR-11492. The -c option (in conjunction with a restart) caused a very bad scenario, even on *nix platforms! (Bob’s Macbook got so warm it started restarting itself and one of the USB ports stopped working for a while!)",
      "3. Anshum Gupta suggested:",
      "a. Drive forward Mark Millers’ Gradle effort (a huge project to change Lucene/Solr’s build system - Mark has put months of work into this and it promises to shorten build times considerably). A group of people worked on testing the Gradle build system to identify any issues and to become familiar with the new system.",
      "b. SOLR-13527  (we didn’t get around to this)",
      "c. Get GitHub to run ‘ant precommit’ as an action for all pull requests - SOLR-13753 - this has been committed to Github here.",
      "4. Bertrand Rigaldies suggested:",
      "Return the offset of snippets in Solr’s highlighter. A large team worked on this. We began with setting up a Highlighter formatter using the original highlighter in Solr.  An example showing this approach is here.",
      "",
      "David Smiley began implementing an \"extended\" mode to the unified highlighter that offers similar functionality - SOLR-1954",
      "The innovation in the \"extended\" unified highlighter that David prototyped was in returning the passages’ metadata out-of-band of, or in addition to, the formatted snippets (See screenshot below).",
      "",
      "5. Eric Pugh and Peter Fries worked on upgrading the Clade taxonomy tool from Python2 to Python 3. (Clade was a project created by Flax to use Solr to classify data into a taxonomy).",
      "6. Joe Ye worked on the Marple index inspector tool to allow it to read older versions of Lucene indexes. Joe had worked on this at a previous Hackday but had lost all his work when his laptop was subsequently stolen!",
      "Things we didn’t get a chance to work on",
      "The following suggestions were made but we didn’t have time to look at them. If you have a particular interest in an issue, or can help, you could either contribute to the JIRA ticket or perhaps contact the author directly via Relevance Slack.",
      "7. Andy Hind suggested:",
      "a. A fix for MLT using corpus size - SOLR-13752",
      "b. Identify known vulnerabilities in dependencies while building Solr using Gradle.",
      "8. Gus Heck suggested: Hardcoded timeouts - SOLR-13457",
      "9. Michael Gibney suggested: Facet caching/singlePass multi-domain facet collection (relatedness) - \nSOLR-13132",
      "10. Shyamsunder Mutcha suggested: JSON Facet API Java objects - \"I don’t see a feature to write JSON facet queries as Query objects.\"",
      "Summary_t",
      "As usual, we had considerably more ideas than time but everyone worked hard on what we selected - thankyou all for your efforts. Perhaps more importantly everyone got a chance to meet or catch up with others in the Lucene/Solr community, discuss problems and potential solutions and give some insights into how they use Lucene/Solr.",
      "We’ll be running more Hackdays - watch this space! If you’re interested in supporting or running a Hackday and empowering your search team, get in touch."
    ],
    "summary_t": "How we hosted and ran a Lucene Hackday in Washington DC the week of the Activate conference, and what attendees considered and achieved during the day"
  },
  {
    "id": "e00c11aa35d8df60cf2eaf5a1b595f05",
    "url_s": "https://opensourceconnections.com/blog/2019/10/24/it-s-okay-to-run-tika-inside-of-solr-if-and-only-if/",
    "title": "It's okay to run Tika (and Tesseract!) inside of Solr ;-) If and Only If....",
    "content": [
      "Recently I saw this post on solr-user mailing list asking about running Tika for text extraction in Solr, which if you follow the thread led to chorus of people saying:",
      "In any case, you may want to run Tika externally to avoid the\nconversion/extraction process be a burden to Solr itself.",
      "Indeed, this advice is captured in the Solr Reference Guide.",
      "So I want to tell you when you are safe to ignore this advice!  And yes, this is the minority opinion.",
      "The common wisdom around Tika comes out of the experience of folks who are processing massive volumes of varied document types, where they have no influence over how these various documents were generated.  Indeed, it is really an adversarial experience…  \"Am I going to receive a zip bomb that will blow up my environment?\", \"Does this .scrpt file represent a virus?\", or even \"Who the hell created a 1 GB PDF document?\".",
      "In those situations all the standard advice makes sense, and I would point you to the batch mode parameters that are specifically around increasing the robustness of Tika:",
      ">> java -jar tika-app-1.22.jar --help\n\nBatch Options:\n    -i  or --inputDir          Input directory\n    -o  or --outputDir         Output directory\n    -numConsumers              Number of processing threads\n    -bc                        Batch config file\n    -maxRestarts               Maximum number of times the\n                               watchdog process will restart the child process.\n    -timeoutThresholdMillis    Number of milliseconds allowed to a parse\n                               before the process is killed and restarted\n    -fileList                  List of files to process, with\n                               paths relative to the input directory\n    -includeFilePat            Regular expression to determine which\n                               files to process, e.g. \"(?i)\\.pdf\"\n    -excludeFilePat            Regular expression to determine which\n                               files to avoid processing, e.g. \"(?i)\\.pdf\"\n    -maxFileSizeBytes          Skip files longer than this value",
      "However, what about the rest of us?  The folks whose use case looks like:",
      "I have a smallish number of documents, say 1000’s of PDFs? Not > 100,000 PDF’s\n  My organization created the documents, so there won’t/shouldn’t be any adversarial Zip Bombs.\n  I only index a couple of documents a day, so load really doesn’t matter.\n  I don’t want to stand up yet another service…   Solr does everything I need.\n  If Solr craps out for some reason, well, I can restart it, and everything will be okay.  <– Very common Disaster Recovery plan ;-)",
      "I’d also like to point out that as we extend the types of workloads that we use Solr for, for example all of the exciting Streaming Expressions type jobs, that we need to think more about how to make Solr really robust to handle misbehaving processes of all kinds, not just Tika.   So let’s improve the ExtractingRequestHandler ;-)",
      "Oh, and I’d love some more eyes on my effort to put together a demo of using Tesseract and Tika inside of Solr: https://github.com/o19s/pdf-discovery-demo/tree/crazy_tika_tesseract_inside_of_solr"
    ],
    "summary_t": "Don’t be afraid of running Tesseract and Tika in your Solr cluster if…"
  },
  {
    "id": "de2ebff30bc455641d3461c9a35a9eac",
    "url_s": "https://opensourceconnections.com/blog/2007/07/20/come-join-us-at-oscon/",
    "title": "Come join us at OSCON",
    "content": [
      "Were going to be presenting and exhibiting at the OReilly Open Source Convention in Portland, Oregon next week, July 23-27.",
      "Eric will be presenting on using and writing Rails plugins on Thursday, July 26 at 11:35 AM – 12:20 PM in room Portland 255 at the Oregon Convention Center.\nWe will also be hosting a Birds of a Feather session on Scrum War Stories Wednesday, July 25 at 7:30-8:30 PM in room D136 at the Oregon Convention Center.",
      "If youre going to be there, let us know, and well be on the lookout for you!",
      "See you in Portland!"
    ],
    "summary_t": ""
  },
  {
    "id": "6a69710724f323700725ce6d31a33a4e",
    "url_s": "https://opensourceconnections.com/blog/2019/11/05/understanding-bert-and-search-relevance/",
    "title": "Understanding BERT and Search Relevance",
    "content": [
      "",
      "There is a growing topic in search these days.  The hype of BERT is all around us, and while it is an amazing breakthrough in contextual representation of unstructured text, newcomers to natural language processing (NLP) are left scratching their heads wondering how and why it is changing the field.  Many of the examples are tailored for tasks such as text classification, language understanding, multiple choice, and question answering.  So what about just plain-old findin’ stuff?  This article gives an overview into the opportunities and challenges when applying advanced transformer models such as BERT to search.",
      "What’s BERT and why is it important?",
      "BERT, which stands for Bidirectional Encoder Representations from Transformers is a deep learning architecture developed by Google for NLP.  It is one of several approaches that leverages transformer architecture.  Transformers address a gap in previous architectures such as recurrent and long short-term memory neural networks, with the key difference being the focus on maintaining attention during training using a bidirectional encoder.  Plenty of articles have been written about this recently. So, I won’t dive into the details of how or why this works, but I’ve added links at the bottom for further reading if you want to learn more.  The important part for the practitioner is that pre-trained models and open source libraries have been released to the public for use by anyone.  You don’t need to train your own model and you can use these as they are, or for transfer learning (referred to as fine-tuning in BERT).",
      "What happens when you use these models is what we’ll focus on here.  When you, for example, pass a document’s text through a pre-trained model using a transformer network, you get back a tensor, which is comprised of a vector representation for each token.  One pre-trained large uncased model for BERT uses a feature vector of 768 floating point values.  768 features for a token, yielded from such a sophisticated model, contains a highly accurate dense contextual representation of the meaning of that token that can be further used.  Also importantly, if the document has 234 words in it, you’ll get a tensor with the dimension of 234x768.  If your document has 172 words, your tensor is 172x768, and so on.",
      "Traditional search in inverted indexes such as Lucene maintains zero context for each token - as all the words are all usually analyzed in isolation.  Relevance engineers spend lots of time working around this problem.  Without linguistic context, it is very difficult to associate any meaning to the words, and so search becomes a manually tuned matching system, with statistical tools for ranking.",
      "How can we use BERT for search?",
      "So what can you do with this tensor information?  Well that’s the question at hand for many search engineers these days.  We’ve been given this immensely powerful tool, and are trying to figure out how we can apply it to make relevance tuning easier and less prone to silly language issues we commonly face.  Before you jump out of your chair shouting this is a solution looking for a problem, remember, the problem is that search in its current form has no connection to language and meaning.  So we need to see how these two match up for a better future together.",
      "The main area of exploration for search with BERT is similarity.  Similarity between documents for recommendations, and similarity between queries and documents for returning and ranking search results.  Why?  Because search relevance can be phrased as a similarity problem.  What documents are most similar to what the user is trying to convey with their query?  If you can use similarity to solve this problem with highly accurate results, then you’ve got a pretty great search for your product or application.",
      "Commonly, the approach is to use a nearest neighbor algorithm.  This takes two or more vectors, and calculates the distance (or similarity) between them in an efficient manner.  To use a simple example, let’s say we’ve got two people - Alice, standing 5 meters away from a water fountain, and Bob, standing 7 meters away from the water fountain.  Who is standing closest to the fountain?  Alice of course.  Now Carrie joins the group and stands 8 meters from the fountain.  So who is Carrie standing closest to? Alice or Bob?  Well, that’s easy - it’s Bob.  That’s similarity with a vector of one feature.  We can also have a vector of two features, such as latitude and longitude coordinates, and use that calculate who is standing closest to the fountain and who are standing in groups close to each other.",
      "When we need to do this with more features, such as 3, 10, or even 768, across thousands or millions of words and documents, things get complicated.  So we turn to approximate nearest neighbor algorithms to efficiently calculate this similarity as fast as possible across all these dimensions.",
      "And that brings us to where much of the recent practical development has been focusing on: Efforts to apply this type of nearest neighbor calculation to documents that we index in a search engine, and also the queries that people use in the search bar.  Because if you can represent all your documents as rich sets of vectors that contain embedded meaning, and cross reference those with a query also represented by a rich set of vectors, you can see which documents are most similar to the query!",
      "Making progress, with some obstacles",
      "There’s some great stuff happening to make the above nearest neighbor tools available for use inside of search engines.  Several approaches are being built to allow arbitrary vectors to be indexed inside of Lucene, and Vespa already supports indexing vectors.  However, the sizes noted above are not really practical.  For example, vectorizing short (three or four sentence) overview text data from 28000 movie documents balloons the representative size from 5MB to 5GB!  That’s a whopping 1000x increase in size! And 28000 really isn’t very many documents.",
      "There are investigations underway to distill these huge document tensors into a more maintainable size.  Areas of research include fine-tuning by using another model on top of BERT for a smaller representation, or just trying to average some of the dimensions together.  The problem with these approaches is as you shrink or compress the size, you lose more of the context that the original model provided.  So careful testing for each dataset needs to be performed to ensure the accuracy stays reasonable as the representation is compressed.",
      "This is also a very different way of doing things than most search teams are used to.  Handling large models that need GPUs for effective speed, and querying across vectors instead of terms, requires a shift in technology, infrastructure, and practice.  Also, debugging such a system becomes exceedingly difficult.  These models are black boxes, and explainable AI is an open area of research that is out of reach for most practitioners.  When you get a strange result from Lucene, you can dig down and see exactly why the result was returned.  But when a document or query yields a tensor, knowing why that tensor was produced and what it means is more or less impossible.",
      "The problem with queries",
      "All the above is wonderful, but there’s another problem - how people search.  Think yourself about how you search, when approached with a need to get information from a website or from a web search engine like Google.  Do you type an elaborately crafted sentence as if you were asking another person?  Of course not.  You type one or two words, typically a noun phrase, for what you want to find.  Don’t feel bad - everyone does this, even me!  People usually don’t give enough context to search engines for most of their queries.  But you can’t blame people for this problem - we’ve been doing this for years because that’s how search engines usually work.",
      "When you perform a similarity between those short queries and lots of documents, you are faced with the same age-old information retrieval problem:  ambiguity.  No matter how advanced your technology is, if people don’t provide enough context to search on, your engine is going to have a difficult time returning exactly what they were thinking.  You will get documents back that reflect the meaning of the terms better, but they might not be ranked the way you expect.",
      "Tradeoffs and next steps",
      "Search is full of trade-offs.  How much time can you reasonably spend to get search right?  How much money is worth improving search for 10% of your queries?  Everyone has deadlines.  Everyone has budgetary restrictions.  Lots of smart people are working hard to make this technology cheaply available and accessible to search teams who can use it to benefit customers.  Keep an eye out for more updates, as this represents the biggest shift search has seen in years, so you’re likely to see many more articles, tutorials, software, and training soon.",
      "If you’re interested in learning about and potentially applying some of the techniques described above to empower your search team, please contact us!",
      "Further reading",
      "These links below provide more in depth information for the concepts explained above.",
      "Academic papers on BERT and attention models",
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "Attention Is All You Need",
      "The Annotated Transformer",
      "Articles explaining BERT in simpler overviews",
      "BERT explained",
      "Demystifying BERT",
      "Understanding searches better than ever before",
      "Libraries for using BERT and other transformers",
      "Huggingface Transformers",
      "Investigations of BERT’s true practicality",
      "NLP’s Clever Hans Moment Has Arrived",
      "Arbitrarily dense vector search",
      "https://github.com/o19s/hangry",
      "https://github.com/castorini/anserini/blob/master/docs/approximate-nearestneighbor.md",
      "https://arxiv.org/abs/1910.10208",
      "https://github.com/jobergum/dense-vector-ranking-performance",
      "Tensors in Vespa",
      "Attributions",
      "Image by Pete Linforth"
    ],
    "summary_t": "There is a growing topic in search these days.  The hype of BERT is all around us, and while it is an amazing breakthrough in contextual representation of un..."
  },
  {
    "id": "442c11c71fed098464de1cc84c08dda6",
    "url_s": "https://opensourceconnections.com/blog/2019/11/17/haystack-eu-2019-berlin-conf-notes/",
    "title": "Haystack Europe 2019, Berlin, Germany, Conference Notes",
    "content": [
      "Greetings! I was very fortunate to attend OpenSource Connections (OSC) Haystack Europe 2019 conference in Berlin, Germany, on October 28th, and below are my notes from the conference. Thank you again to OSC and my colleagues for sending me over the Atlantic to attend a great conference on search relevancy.",
      "Venue",
      "First, let me recognize that our event partners Plain Schwarz had secured us a bright and comfortable room in ExRotaprint, a converted print works now owned and run by a collective. Lunch was served by the ExRotaprint cafe and was healthy and very tasty.",
      "Keynote",
      "I was honored to give the conference’s keynote address. In the keynote, I wished Lucene its 20th birthday, and showed my deep appreciation of the open source search technologies that we use and enjoy every day. I also looked back at six decades of Information Retrieval (IR) research & development to retrospect on where we, search practitioners, came from; what the major research milestones have been during the long journey covered thus far; and, where we are today with state-of-the-art IR. I finished the keynote by bringing attention to the latest and renewed interest in thinking of search as a vector distance (similarity) calculation in a dense vector space, which provided a segue into the day’s extremely high-quality talks on advanced search topics such as automatic filters, AI-powered search, knowledge graphs, Markov chain-based query rewriting, relevant facets, reinforced learning, and multi-objective Learning-to-Rank (LTR).",
      "Talks",
      "All the talks were of high quality, and delivered by experienced and passionate search practitioners. The talks covered topics that are  important to search professionals who try to improve search relevancy on a daily basis, from user intent detection, to the leveraging of knowledge graphs, to query reformulation, to relevant facets, and advanced learning-to-rank techniques.",
      "My great thanks to all the speakers for sharing their experiences, insights, and visions.\nBelow, I provide a quick summary of what impressed upon me at each talk.",
      "See the conference’s program here. Click on any talk on the program page for the talk’s abstract as well as links to the talk’s slides and recorded presentation.",
      "Improving precision of e-commerce search results to generate value for customers and business",
      "Jens Kürsten and Arne Vogt from Otto addressed the classic issues and challenges with high-recall queries containing a lot of \"noise\", and with balancing the end-users’ and the business relevancy requirements. Jens and Arne presented their hypotheses that two issues turn end-users away: a) poor relevancy (\"Effectiveness\" issue); and b) the amount of work necessary to reach relevancy (\"Efficiency\" issue); and showed that boosting on product category helped alleviate both issues. Jens and Arne shared their KPIs, and both offline- and online-testing methodologies to reach that conclusion.",
      "Balancing the Dimensions of User Intent",
      "Trey Grainger from LucidWorks treated us to his invigorating vision of AI-powered search, where the confluence of Search, AI, Machine Learning, Deep Learning, and Data Science in general provides fertile grounds for solutions to help with many search problems. In his talk, Trey focused on user’s intent understanding, the holy grail of search practitioners! Trey broke down user’s intent understanding into: a) Content Understanding; b) Domain Understanding; and c) User Understanding, on a scale of increasing sophistication in the techniques used to infer understanding. Content Understanding is basically the \"dumb\" matching documents based on the query terms. Domain Understanding can be inferred using a spectrum of techniques from synonyms, to taxonomies and hierarchies, to ontologies, and finally to knowledge graphs. And finally, User Understanding can be inferred using various Collaborative Filtering techniques leveraging user-item, item-item, and query-item relationships gathered from usage. I bought Trey’s AI-Powered Search \"MEAP\" book, and I look forward to enjoying Trey’s visions of our future.",
      "Query Intelligence: Understanding User Intent",
      "Erica Lesyshyn from EBSCO Health delivered a convincing presentation on the use of knowledge graphs for content enrichment as well as query segmentation and re-writing. Erica shared the pros and cons of her approaches, and in particular noted that although knowledge graph-enhanced queries can be effective at correctly inferring the end-user’s intent, they can also hurt precision with the increased recall. Ha, the joy of the Precision/Recall yin yang!",
      "Search to Search recommendations (Collaborative Synonym and Spell corrections)",
      "Sadat Anwar and Matthieu Pons from Rebuy showed an innovative technique for query expansions with synonyms and spelling corrections using Markov chains. After trying somewhat unsuccessful text-to-numbers embeddings techniques such as word2vec and prod3vec (both with the Python library gensim), Sadat and Matthieu experimented with some success with Markov-chains based models to generate synonyms. The technique is interesting in that it shows us that despite the buzz and the hype around neural network-based applications in search, sometimes a good old probabilistic model such as Markov chains can provide effective mechanisms for traditional search issues such as synonyms \ngeneration.",
      "Relevant Facets",
      "Lucian Precup and Radu Pop from Adelean presented an often overlooked but critical aspect of the search results: How relevant are the facets? And how can we make them more relevant? Lucian and Radu explained with deep insights how relevant facets can provide a \"holistic\" view of the search results, as well as help with disambiguation of query terms, and overall, provide an effective support to find the \"needle in the haystack\".",
      "Ranking article comments using reinforcement learning",
      "Lester Solbakken from Verizon Media gave us, Lucene aficionados, another chance to have another look at the still mysterious, just open-sourced, and exciting search engine Vespa. Using Vespa’s native support for Machine Learning-based ranking models (aka, \"tensors\" in Vespa lingua), Lester shared with us how to relevancy-rank the numerous comments associated with News articles in Yahoo using reinforcement learning techniques.",
      "How to Kill Two Birds with One Stone: Learning to Rank with Multiple Objectives",
      "Alexey Kurennoy from Zalando shared his research on the training and execution of a learning-to-rank model based on multi-objectives optimization. As Jens and Arne also told us earlier in the morning, it is often the case in e-commerce applications that relevancy is based on competing requirements between the end-users’ information needs and the business’ numerous and complex objectives (financial, inventory management, marketing, etc.) Alexey addresses this multi-faceted relevancy problem as a multi-objective optimization problem, and accordingly, trains a Learning-to-Rank model using an \"objective function\" as the model’s loss function.",
      "Post-Conference Social",
      "After the conference a large number of the attendees visited Café Pförtner, a nearby bar and restaurant for further networking over dinner and drinks. The café is set in a former bus garage and you can even sit down to eat in a converted bus!",
      "Closing Remarks",
      "As I mentioned in the keynote address, I marvel as the levels of creativity and ingenuity that search practitioners exhibit in their relentless quest to bridge the gap between end-users’ information needs and the search results. I am so excited to have joined the search field a few years ago, and I look forward to continue helping push the envelope of the possible.",
      "The Haystack conference will return - in the USA and Europe - in 2020! Contact us if you’d like to know how OSC can help you build relevant search."
    ],
    "summary_t": "My notes taken during the Haystack EU 2018 in Berlin, Germany, on October 28th."
  },
  {
    "id": "a35dde5d7c8cbcb5944e8a87f69dadcb",
    "url_s": "https://opensourceconnections.com/blog/2019/11/22/it-s-time-for-tika-tuesdays/",
    "title": "It's time for Tika Tuesdays!",
    "content": [
      "What is Tika Tuesdays?",
      "Over the past few months I’ve finally accomplished the long time personal goal of being able to easily search PDF documents with in context hit highlighting using only open source projects with minimal coding effort!   I believe this is a common use case that many different organizations could benefit from, but to date has been too difficult for most organizations to embrace.",
      "This short series in the run up to Christmas 2019 will share some of the lessons (both good and bad!) that I’ve learned.   I will publish a new blog post every Tuesday, and link them all back to this one as they are published.",
      "To get you started, here is one inspired by the Solr mailing list that provoked some eye rolling from long time Tika folks:",
      "It’s okay to run Tika (and Tesseract!) inside of Solr ;-) If and Only If….\n  Using Tika and Tesseract outside of Solr\n  Using Tika and Tesseract as an API exposed by Solr\n  Tesseract 3 and Tika\n  Parsing Tika & Tesseract formatted HOCR output inside Solr ingestion pipeline"
    ],
    "summary_t": "It’s time for Tika Tuesdays!   Three years ago I started messing around with OCRing documents with Tika, and today that process is relatively straightforward..."
  },
  {
    "id": "d0b23b818dd1c960c7dd31a09a54402f",
    "url_s": "https://opensourceconnections.com/blog/2019/11/26/tika-and-tesseract-outside-of-solr/",
    "title": "Tika Tuesday: Using Tika and Tesseract outside of Solr",
    "content": [
      "Extracting content from file formats using Tika as a standalone service is the traditional architectural approach, and what my most recent project is built around.  You can try out a demo online at http://pdf-discovery-demo.dev.o19s.com:8080/.",
      "This article walks you through the steps we followed to run Tika, both as a server process and using it as a command line tool.",
      "If you want to follow along locally, follow the Quickstart in the README to run the demo.  To get an overview of all the steps, follow the Text Extraction instructions.  The key bit is an extraction script that calls Tika to extract from a PDF the information we need.",
      "A couple things that are interesting:",
      "It’s super simple to swap between Tika the Command Line App and Tika the Server Process.  The nice thing about using the tika-app.jar is that all your dependencies for parsing are packaged up into one 78 MB file.  Very easy to include that in your project.  However, if you are going for scale, then you might want to run a cluster of Tika server processes with a load balancer in front, and then you would want to swap to making a curl command against a deployed Tika server.\n  \n  \n    Deploying Tika server in dockerized world is super simple: https://github.com/o19s/pdf-discovery-demo/blob/master/docker-compose.yml#L47.  However, I do wish the Apache Tika project had a official image that was released every time Tika was released. ;-)  I also wish that for non Docker setups, there was a nice set of service scripts provided to manage starting/restarting Tika.\n  \n  \n    I’m very happy to report that in Tika-1.23, you can now configure the PDF and OCR Parsers via a single tika-config.xml file.  In 1.22 and earlier, you needed to have on the filesystem a tika-properties directory that was included in the classpath.  You can see an old commit where this was done: https://github.com/o19s/pdf-discovery-demo/tree/6f5b37305dd863a73af4617db64cbe853c5ecd2a/ocr/tika-properties/org/apache/tika/parser.   It was awkward!  Now you can use your tika-config.xml to set everything.\n  \n  I learned about the magic header parameters that you can send to the Tika server to configure your parser.  This is an alternative to either the properties file configuration or the tika-config.xml configuration.  It’s cool, but also more magic…  For example, the parameters don’t follow any over all pattern of naming them, so you need to be very careful to not misspell them:\n    curl -T ./path/mypdf.pdf http://pdf-discovery-demo.dev.o19s.com:9998/rmeta --header \"X-Tika-OCRLanguage: eng\" --header \"X-Tika-PDFOcrStrategy: ocr_and_text_extraction\" --header \"X-Tika-OCRoutputType: hocr\"\n    \n  \n  \n    Parsing out the HOCR output looks daunting at first, till you realize you just care about <span class=\"ocrx_word\"> tagged content.  Check out https://github.com/o19s/pdf-discovery-demo/blob/master/ocr/extract.ps1#L63 to see both the HOCR pulled out of the XML as well as the raw text pulled out.\n  \n  \n    You can store lots of different data in your payloads!  We have the bounding box from HOCR, but also store the page number as well in the payload.  Base64 encode it all to store in Solr.\n  \n  We did a crazy thing to allow us to do traditional highlighting of snippets of text in our SERP page, but then link each snippet to the payload based highlights in the PDF document, even though there was no explicit connection between the two.  We track the offset of our highlights, and pass that along in the front end, in order to give the front end additional data to narrow down the payload highlighting.  We did this via custom formatter which injects into the HTML <em/>tag additional data that lets the front end figure out how to make a connection between a snippet highlight and a payload highlight:\n    <em data-num-tokens=\"1\" data-score=\"1.0\" data-end-offset=\"2110\" data-start-offset=\"2079\">HELOCs|NiAzNTEgNTAyIDQwNSA1Mjc=</em>",
      "Learn more by looking at the Solr Payload Component from https://github.com/o19s/payload-component and the Offset Highlighter Component from https://github.com/o19s/offset-hl-formatter.",
      "Read other posts in the Tika Tuesday series here."
    ],
    "summary_t": "In which we explore how to deploy Tika and Tesseract as a stand alone service.  And tie it into Solr via Payloads…"
  },
  {
    "id": "6d5ee605997660e7931400fb454ece7d",
    "url_s": "https://opensourceconnections.com/blog/2019/12/03/using-tika-and-tesseract-as-an-api-exposed-by-solr-via-extractingrequesthandler/",
    "title": "Tika Tuesdays: Using Tika and Tesseract as an API exposed by Solr",
    "content": [
      "Don’t want to deploy a separate Tika server?   But need Tika server-like capabilities and you already have Solr?  Then this is the solution for you!   What I am going to show has been available out of the box in Solr for quite a while, but has been very opaque to configure, so hopefully this demystifies the process.",
      "First we figured out the magic incantation to configure Tika from inside of Solr, which is via the parseContext.config parameter and a specific XML formatted file:",
      "<entries>\n  <entry class=\"org.apache.tika.parser.pdf.PDFParserConfig\" impl=\"org.apache.tika.parser.pdf.PDFParserConfig\">\n    <property name=\"extractInlineImages\" value=\"true\"/>\n    <property name=\"ocrStrategy\" value=\"OCR_AND_TEXT_EXTRACTION\"/>\n  </entry>\n  <entry class=\"org.apache.tika.parser.ocr.TesseractOCRConfig\" impl=\"org.apache.tika.parser.ocr.TesseractOCRConfig\">\n    <property name=\"outputType\" value=\"HOCR\"/>\n    <property name=\"language\" value=\"eng\"/>\n    <property name=\"pageSegMode\" value=\"1\"/>\n  </entry>\n</entries>",
      "You might be tempted to think that this is the same file format as a tika-config.xml that you may have seen before, and you’d be wrong ;-).   While visually very similar, this file is loaded by ParseContextConfig, which is part of the Solr extraction contrib module.   Yes, there are many different ways to specify configuration settings for PDF extraction and Tesseract OCR!",
      "We then tweaked the default /update/extract request handler to refer to the parseContext.xml file.  We want any fields that we don’t already have defined in schema.xml to be prepended with the name attr_, which triggers dynamic field generation.  So if the field from Tika is Creator, it becomes in Solr a text field called attr_creator.",
      "Here is our config file:",
      "<requestHandler name=\"/update/extract\"\n                class=\"solr.extraction.ExtractingRequestHandler\" >\n  <str name=\"parseContext.config\">parseContext.xml</str>\n  <lst name=\"defaults\">\n    <str name=\"lowernames\">true</str>\n    <str name=\"uprefix\">attr_</str>\n    <str name=\"multipartUploadLimitInKB\">20480</str> Limit to 20 MB PDF\n  </lst>\n</requestHandler>",
      "Because PDFs can be really big, we also needed to bump the size on the requestDispatcher:",
      "<requestDispatcher handleSelect=\"true\" >\n  <requestParsers enableRemoteStreaming=\"false\" multipartUploadLimitInKB=\"20480\" formdataUploadLimitInKB=\"20480\" />\n</requestDispatcher>",
      "You can now hit Solr via:",
      "curl 'http://localhost:8983/solr/documents/update/extract?literal.id=doc2&commit=true&extractOnly=true' -F \"[email protected]/alvarez20140715a.pdf\"",
      "and get back from Solr the Tika processed content in a relatively easy to process structure that is very similar to what Tika server returns!",
      "Read other posts in the Tika Tuesday series here."
    ],
    "summary_t": "In which we deploy Tika and Tesseract as a API in Solr, exposed via the /extract handler!"
  },
  {
    "id": "aa03f227513aa2869c533d528098a5dc",
    "url_s": "https://opensourceconnections.com/blog/2007/07/25/subversionits-not-just-for-geeks/",
    "title": "Subversion…it’s not just for geeks",
    "content": [
      "We met Joe \"Zonker\" Brockmeier on the train last night going to Kelt (if youre at OSCON and you like Irish pubs, hop on Max and go to Kelt). He saw our OSCON shirts, which have the moniker Ruby + SVN == Committed on the back and asked if there was a Ruby front-end for Subversion. We replied that there wasnt; it is a geek play on words. He told us that he used Subversion for editing his journalistic writing. \"Wow,\" I thought, \"a non-geek way to use Subversion. Brilliant!\" Of course, we followed that up with a visit to the Irish pub and had Guinness-cooked foods. Brilliant!",
      "So, what other uses are there for Subversion besides just checking in code?"
    ],
    "summary_t": ""
  },
  {
    "id": "54c00afa2bf8c35421d095068f20d854",
    "url_s": "https://opensourceconnections.com/blog/2019/12/04/what-should-your-search-document-be/",
    "title": "What Should Your Search Document Be?",
    "content": [
      "In a search engine, the \"document\" is the basic unit of indexing and retrieval. It’s the \"result\" on the search results screen when a user enters a query. Many teams make the mistake of selecting whatever entity makes sense to themselves as backend developers to make the \"document\". Often this works out, but many other times we don’t take a nuanced view of what the document should be. What is the real \"thing\" the users want back?",
      "I wrote in Relevant Search that users don’t care how your backend data systems work, they don’t want some searchable view of your RDMS. They want a view specific to search, that structures the data in a way that satisfies their information need - answers their question, solves their problem, finds the best job, or forwards them along a more complex decision making process.",
      "Comparing Grubhub (food delivery) and Kroger (grocery) is an interesting example of this tension around a similar information need (mmm food…).",
      "On Grubhub, most people search for restaurant names, and then explore that restaurant’s menu. However, increasingly there seems to be movement to searching for food items directly. Here’s my search results for my local area, for the query \"Shrimp Tempura\":",
      "",
      "We wouldn’t really make this decision without more data (so we can’t really critique Grubhub…). But just as a fun thought experiment, what do you think this search UI should return as a document? Should it return",
      "Restaurants, because the product is really about checking out from restaurants?\n  Individual food items, with more detail on each item?\n  Both?\n  It Depends?",
      "Compare to what we expect in a grocery search UI, we are hyperfocused on single items, as in Kroger’s UI:",
      "",
      "Arguably, there may be domain-specific concerns at play in these examples (Grubhub: I’m checking out from one restaurant among many; but in grocery there’s a more traditional e-commerce workflow). So Grubhub is a bit more in a pickle.",
      "Notice the problem with this query on the Grubhub site. I’m not that convinced the results are relevant for my query. Whereas in Kroger, it looks clearly like Shrimp Tempura. There’s clearly a picture of Shrimp Tempura! I’m sold that the search knows what I want. I can kind of see that on Grubhub these restaurants offer these menu items, but it’s not as easy sell.",
      "Perceived Relevance - It’s about selling the user this is the right ‘document’ for them",
      "What is a \"document\" is about convincing users in the Search UI that we understand the thing they are asking for. This is perceived relevance: Does the \"document\" returned relate to my query? Does the search UI appear to know what I want? Does it look relevant to my query?",
      "E-commerce search results give lots of information on why you would want to purchase one product or another, for example on Amazon, a search for blue shoes, you’ll notice a clear picture (all blue), badges, reviews, brands, and other facts that might be relevant to my decision making process. It says \"we think you want this sort of thing, and here’s why\"",
      "",
      "Of course, it’s not just about showing me blue shoes. It’s about all the unspoken bits of my information need (that I’m an Amazon Prime member, and what I really want are sneakers!). This is why diversity is so important for head queries we often can’t fill in the rest of the user’s specification, so we need to present a range of options, and give users whatever information that helps clarify their information need so they can take another step. (also notice how the irrelevant ads detract from my belief the search engine knows what I want…)",
      "Bringing perceived relevance into our Grubhub vs Kroger example, Kroger has an easier time convincing us that what was returned was actually relevant. All the results from Grubhub, despite their relevance, don’t look as relevant. It may be harder to take the next step. The fact that restaurants are the documents for my foot-item query is a big part of this - a better type of document returned for my query might have convinced us more of the relevance of the results.",
      "Teams come to us all the time with what they say are \"relevance\" issues. Sometimes what’s really happening is the search UI isn’t selling anyone on the relevance of the returned documents. The choice of the search document may not appear to relate to the query in the search system. The items may actually be relevant, in a roundabout way. But they don’t look the part, they don’t move forward the user’s journey, and if they don’t do that, they’re not as useful \"documents\" in our search system.",
      "Porque No Los Dos? (why not both kinds of documents?!)",
      "",
      "In the future, in your search system, will it make sense for there to be one kind of \"document\"? What if I go to an e-commerce search experience and ask a question about products:",
      "> Which kind of running shoe should I buy for trail running?",
      "We’re not there yet for e-commerce:",
      "",
      "Would the right \"document\" in this case be a set of products? Or would it be best to return some kind of community curated answer (as exists on many Amazon products)?",
      "We’d still need to keep supporting our regular e-commerce use cases, but alongside perhaps another kind of document. We’d have to look at the user’s intent (which will often be ambiguous) and return one or many sets of results that corresponded to the interpretation of the user’s intent. Google, as I’m sure you know, must support many kinds of results for this purpose:",
      "",
      "There’s 3 kinds of results here, products, regular web pages, question/answer pairs.",
      "So for our friends at Grubhub, maybe there’s a future where both restaurants and food could be search results, depending on the user’s query?",
      "We’re seeing more and more teams split up their content into many indexes or types of documents to satisfy different kinds of information need, with an intent classification system in between. In other words, there’s a big \"It Depends\" box before a query gets to a search engine. What should the document be? What kinds of things should be returned? It depends on what we think the user wants. It’s not about just one search engine and one kind of result anymore. It’s potentially moving to all-federated-all-the-time based on how we interpret the user’s intent.",
      "In other words, it all comes back to the world’s most important search relevance pun:",
      "",
      "And if you put that pun into action, like any good puns, you interpret it many different ways, to hit different processing centers of your brain:",
      "",
      "Here with potentially different information needs / use cases behind an ambiguous query, we need to think about the likelihood the user wants a specific kind of response. We can use what we know about the user and other contextual factors (location, time, etc) to weigh the probability of the likely response types.",
      "The future of search is not 10 hot links, it’s conversational, multi-modal UIs that can navigate between the many possible intents of the users.",
      "In other words, we are teaching search engines puns. Prepare for more search dad jokes. I’m sorry.",
      "If you have thoughts on this article, please get in touch! We’d love to work with you on your tough search & relevance problems."
    ],
    "summary_t": "We explore what the basic unit of your search system should be. What should your document be? What ‘result’ in the SERP UI best satisfies your users?"
  },
  {
    "id": "1cbe3eba10ed1dffe105d9296be25320",
    "url_s": "https://opensourceconnections.com/blog/2019/12/06/newbie-guide-to-indexing-data-with-Solr-classic-schema/",
    "title": "A noob’s guide to indexing data with Solr’s classic schema",
    "content": [
      "Intro",
      "I joined OSC a month ago as their first data-scientist, so I’ve been drinking from a firehose trying to get up to speed on Solr. After breezing through the first two Solr tutorials, I felt pretty good about tackling an internal set of Solr challenge tasks. However the transition from the friendliness of start -e to standing Solr up myself was a little rough. This post is my newbie guide for setting up classic schema as of Solr v8.3.0.",
      "Why classic?",
      "Managed schema is the default in Solr v8.3.0 and is used in all of the tutorials, because it makes setup easier by handling the field definition through the Solr Admin UI, which is simpler and safer than editing complicated .xml files by hand. But the classic schema is still useful because it is explicit and forces you to interact with the nuts and bolts which is useful if you are learning how the engine works.",
      "Setting up classic",
      "To transition back from the managed schema to the classic schema of yesteryear, the Solr docs say you need to do two things:",
      "Rename managed schema to schema.xml\n  Add <schemaFactory class=\"ClassicIndexSchemaFactory\"/> to solrconfig.xml",
      "Seems simple enough, just start my local Solr server and create a new core using the _default configset.",
      "bin/solr start\n\nbin/solr create_core -c new",
      "Navigate into the newly created solr-home in my VS-code (not pictured) and make those changes.",
      "Reloading my core to pick up the new changes.",
      "curl ‘http://localhost:8983/solr/admin/cores?action=RELOAD&core=new_core’",
      "Trying to post some example data from films.json.",
      "bin/post -c new example/films/films.json",
      "Boo, error-message: \"This IndexSchema is not mutable.\"",
      "But that Googling that error-message led me to this post on StackOverflow and the third step:",
      "Set update.autoCreateFields:false on line #1197",
      "<updateRequestProcessorChain name=\"add-unknown-fields-to-the-schema\" default=\"${update.autoCreateFields:false}\"\n   processor=\"uuid,remove-blank,field-name-mutating,parse-boolean,parse-long,parse-double,parse-date,add-schema-fields\">\n    <processor class=\"solr.LogUpdateProcessorFactory\"/>\n    <processor class=\"solr.DistributedUpdateProcessorFactory\"/>\n    <processor class=\"solr.RunUpdateProcessorFactory\"/>\n  </updateRequestProcessorChain>",
      "Those lines are related to the managed schema’s auto-update functionality for fields that were not explicitly named.",
      "Trying my post operation again….",
      "Boo, more errors: [doc=/en/45_2006] unknown field 'directed_by'.",
      "Ok so I add these field definitions to schema.xml.",
      "<field name=\"directed_by\" type=\"text_general\" indexed=\"true\" stored=\"false\" multiValued=\"true\"/>\n<field name=\"initial_release_date\" type=\"text_general\" indexed=\"true\" stored=\"false\" multiValued=\"false\"/>\n<field name=\"genre\" type=\"text_general\" indexed=\"true\" stored=\"false\" multiValued=\"true\"/>\n<field name=\"name\" type=\"text_general\" indexed=\"true\" stored=\"false\" multiValued=\"false\"/>",
      "Reloading my core and re-retrying my post … woohoo! No more error-messages!",
      "Outro",
      "This was not easy for me, navigating my Solr noob-ness through partial documentation, but I learned a lot about what is actually happening behind the scenes. I’m looking forward to getting back to my data-science comfort zone of making plots and exploring insights. Thanks for reading and I hope your classic schema set up goes smoother than mine!"
    ],
    "summary_t": "How to convert the _default managed schema in Solr v8.3.0 to the classic schema of yesterday"
  },
  {
    "id": "9b13b19aa6a89b7656d85f0d372dc68e",
    "url_s": "https://opensourceconnections.com/blog/2019/12/09/demystifying-ndcg-and-err/",
    "title": "Demystifying nDCG and ERR",
    "content": [
      "Welcome back, dear reader!  In this post, we unwrap the mystery behind two popular search relevance metrics through visualization, and discuss their pros and cons.  Our subjects for this exercise are Normalized Discounted Cumulative Gain, and Expected Reciprocal Rank, commonly acronymified as nDCG and ERR.  We’ll start with some refresher background, visualize what these metrics actually look like, and paint a picture of how each can be either helpful or misleading, depending on the situation.  Afterwards, you’ll have a better understanding of their behavior and which ones to use when (and why).",
      "Assumptions",
      "Note that, while some basic things are explained, this is not an introduction to these metrics - so I’ll assume you’ve at least heard of nDCG and what it’s used for!  So if you’re new to relevance measurement, you probably want to start with something like the book \"Relevant Search\", or at the very least the Wikipedia article on search metrics.",
      "As a formality, we’ll stick with the relevance grading scale of poor=1, fair=2, good=3, and perfect=4.  We’re also only going to look at the grades of the top 4 result positions, and assume each of those results has a grade.",
      "We’re going to also assume that your results are listed one at a time on each row, and not on a grid.  There are varying opinions on how best to measure grid results, but that’s beyond the scope of this post.",
      "OK!  No more assumptions, let’s get to it…",
      "Background",
      "nDCG has been the go-to metric for measuring relevance in a typical search results list since its first introduction in SIGIR 2002.  In the original ERR paper, improvements over its predecessor were mightily touted.  However, though being around for about a decade, ERR is surprisingly underused.",
      "The similarities shared between the two metrics are, as noted in the title, the discount and cumulative functions of relevance.  These respectively mean that a result is ‘worth’ less the lower on the result list appears, and the grades of all the documents in the list contribute to the score for a query’s relevance.  When I spoke at Haystack in April, I showed a simple breakdown of DCG while diagraming the components of the formula:",
      "",
      "To get a DCG score, just follow these steps!",
      "For all the results\n  Award a result by its relevance\n  Punish a result by its rank\n  Add all the result scores",
      "Step 3 in the above diagram is the ‘discount’, and step 4 is the ‘cumulative’.  Together they provide the motivation to get relevant documents to the top (and the irrelevant documents to the bottom) to acheive a higher overall score. nDCG has an additional trick up its sleeve - it uses the ideal list of documents to normalize the score between 0 and 1:",
      "",
      "While this is helpful (or not, depending on your math chops), it’s hard to tease out the full picture.  For one thing, it doesn’t use any sort of maximum grade.  Grades can be anything, and this can be troublesome when performing apples-to-apples comparissons between results.  We’ll see why later, but first let’s dive into Expected Reciprocal Rank, and its purported improvements over nDCG.",
      "The first thing ERR incorporated as a fix, was that you must outright declare what the maximum possible grade is.  This helps by not needing to calculate an ‘ideal’ score, but it also can be misleading because it assumes there are always relevant documents available.  Another addition is the cascading model of relevance.  When a user sees a search result they like, they are satisfied.  Starting with the understanding that a user will lose trust in your search engine the longer it takes them to be satisfied is the cascade.  It works by keeping a running tab of this ‘trust’ a user has, and punishes a query’s score when it’s not satisfying a user quickly.",
      "",
      "Showdown",
      "Whew!  With that out of the way, time for the fun part.  Let’s look at how these two actually differ in the real practical world.  To do that, we’re going to map out every possible combination of grades for the top 4 positions, get their scores, and plot them.  The Y axis is the resulting score based on its respective X axis grades of the top 4 results.  For example, given the first four results with grades 2, 4, 3, 2, that gives us a standard nDCG of 0.7697.  The Y axis is 0.7697, and the X axis is \"2,4,3,2\".  To make things more interesting, we’ll also look at different discount models.  We can change the way lower scores are punished, and it is useful to see how this impacts the metric.  The standard for nDCG is 1/log2(r+1), and the standard for ERR is 1/r (where ‘r’ is the position rank).",
      "",
      "Noting specifics, for the standard discount model of nDCG, right away we start off at about 0.45.  This means that for the top four results, you’ll never have a score lower than that.  You can also see that lots of the possible combinations for nDCG yields a perfect score of 1.0.  Why is this?  Well, If your top four results are 1,1,1,1 nDCG will say that’s a perfect score because the ideal sort is the same.  We’re going to actually list out the full table below, and you’ll see more of that harsh truth there.",
      "But first, let’s show the visuals for ERR and compare:",
      "",
      "Fascinating.  You can clearly see the juxtaposition of nDCG favoring higher scores, and ERR with a more balanced growth.  You can also clearly see the dropoff cliff in the ERR scores as soon as the top result becomes a 4 (perfectly relevant).  For many of the discounts, ERR heavily favors queries that give a perfect first result.  This is not surprising - because the user will be satisfied immediately, making the other results inconsequential.",
      "You may have also noticed something interesting when examining the discount functions.  Did you happen to catch 1/(r^0.18)?  I arrived at that function through human learning (trial and error), looking for a good discount that provided a more gradual dropoff when the first result was not perfect.  While this makes for a more balanced metric however, it can be seen as going against the cascading model’s purpose.  With the far more drastic cliff of 1/r (the green line), there will be a much clearer signal for an irrelevant top result.",
      "Pros and Cons",
      "The visualizations above (and the data tables below), gave us an interesting glimpse into the behavior of these two formidable metrics.",
      "nDCG",
      "nDCG is a great metric when you’ve done your best job at grading, and don’t mind a high score when you have nothing better to offer.  Remember, nDCG will return a perfect 1.0 for any result set that has the grades in order of highest to lowest in the resultset.  When using nDCG, I always recommend using the global ideal rather than the local ideal.  This means that when you know a better document exists is out of your measurement scope (like the 10th document in an [email protected] measurement), use that as part of your ideal and avoid just sorting the top 4.  Also, for learning-to-rank, consider just using DCG without the normalization!  If the goal is a higher number, nobody cares that it’s between 0 and 1.  nDCG also has no way to indicate what the maximum score is.  To get around this, sometimes it might suit well for the ideal to be all perfect scores for the positions as a best theoretically possible relevant set, as the ideal denominator.",
      "ERR",
      "The default ERR is a great metric for providing a good signal whether the top result is relevant.  Some practitioners will argue this is all it’s good for, but If you are serving content that needs more flexibility, you can also tune the discount function for when this is not the case and you want more forgiveness.  One interesting thing about ERR is that it never returns a perfect 1.0 score, and it will always assume that the score can be better, which is a main contrast with nDCG and one I happen to like.",
      "Conclusions",
      "In this authors opinion, I prefer using ERR and modifying it to your needs for most cases.  It is more advanced than nDCG and may be more complex to explain, but it’s more closely aligned with how people actually behave and react - people do get frustrated with search engines that don’t show relevant documents at the top, so it’s a good idea to use a metric that models that frustration.  There are those that argue for information needs with multiple good results (such as exploratory search), that ERR doesn’t accurately reflect this, but there are ways to customize ERR to build the desired measurement - the paper itself has a section on diversity for such occasions, which usually goes overlooked.",
      "Thanks for reading, and see you next time!",
      "Papers",
      "nDCG\n  ERR",
      "Code",
      "The code used to create the above plots and the data tables below can be found at https://github.com/o19s/metric-plots",
      "Data Tables",
      "Here are the colorized data tables for nDCG and ERR, as visualized above.",
      "nDCG",
      "Position Grades\nDiscounted Scores\n\n\n1\n2\n3\n4\n1/r^0.18\n1/r^0.5\n1/log2(r+1)\n1/r\n2/2^r\n\n\n\n111111111\n11120.9190.7910.750.6330.548\n11130.860.6580.6010.4430.333\n11140.8230.5830.5190.3470.228\n11210.9350.8230.7810.6730.613\n11220.9220.7970.760.6390.538\n11230.8710.680.6260.4680.352\n11240.8330.6010.5380.3660.244\n11310.8860.7110.650.5050.429\n11320.8860.7110.6540.5050.408\n11330.8850.710.660.5040.379\n11340.8480.630.570.3970.272\n11410.8560.6470.5770.420.339\n11420.8590.6520.5850.4240.333\n11430.8620.6590.5970.4320.325\n11440.8670.670.6140.4440.311\n12110.9570.8780.8380.7550.742\n12120.9390.8390.8040.7050.641\n12130.8820.7060.6520.5050.408\n12140.8390.6150.5530.3850.274\n12210.950.8640.8280.7380.692\n12220.950.8640.8330.7390.674\n12230.8970.7360.6840.5380.44\n12240.8520.6380.5770.4080.295\n12310.9040.7510.6950.560.493\n12320.910.7630.710.5730.493\n12330.9040.7520.7050.560.451\n12340.8630.6610.6020.4350.316\n12410.8690.6750.6070.4540.378\n12420.8760.6860.6210.4650.381\n12430.8770.6890.6280.4680.368\n12440.8780.6930.6380.4740.348\n13110.9260.80.7410.6290.619\n13120.9190.7870.7340.6150.577\n13130.910.770.7230.5940.517\n13140.8640.6670.6080.450.351\n13210.9260.8020.7480.6330.606\n13220.9290.8090.7580.6410.6\n13230.9190.7880.7440.6170.538\n13240.8730.6840.6270.4680.368\n13310.9270.8050.7570.6390.586\n13320.9290.810.7650.6450.582\n13330.9330.8180.7770.6560.576\n13340.8880.7140.6590.5020.399\n13410.890.7190.6560.5110.444\n13420.8940.7260.6660.5190.445\n13430.90.7390.6820.5340.448\n13440.8970.7330.6820.5280.415\n14110.9060.7560.6880.5650.559\n14120.9040.7520.6880.5610.541\n14130.9010.7460.6870.5550.51\n14140.8960.7380.6860.5450.464\n14210.9080.760.6950.5710.556\n14220.9110.7670.7040.5770.554\n14230.9070.7590.7020.570.523\n14240.9020.7490.6980.5580.476\n14310.9120.7680.7070.5810.55\n14320.9140.7730.7150.5860.548\n14330.9180.7820.7270.5970.546\n14340.9110.7680.7190.5810.497\n14410.9160.7780.7240.5960.541\n14420.9180.7820.730.60.54\n14430.9210.7880.7390.6080.538\n14440.9250.7990.7540.6220.536\n211111111\n21120.9710.9330.9310.9020.846\n21130.9030.7630.7270.6150.521\n21140.8520.6480.5940.4440.333\n21210.9830.9580.9550.9340.897\n21220.9770.9440.9410.9130.86\n21230.9150.7870.7530.6410.547\n21240.8630.6690.6160.4650.353\n21310.9250.8090.770.670.606\n21320.9280.8150.7780.6750.6\n21330.9180.7930.760.6450.538\n21340.8730.6870.6370.4850.368\n21410.8820.7070.6490.5120.437\n21420.8870.7170.660.5210.439\n21430.8860.7150.6630.5190.419\n21440.8860.7140.6660.5160.39\n221111111\n22120.9910.9790.980.9710.953\n22130.9250.810.7770.6750.6\n22140.8690.6820.630.4840.381\n222111111\n222211111\n22230.9390.8380.8050.7070.636\n22240.8820.7050.6530.5070.404\n22310.9440.8510.8150.7260.68\n22320.950.8630.8290.740.688\n22330.9370.8340.8030.7010.613\n22340.8880.7190.6690.5230.414\n22410.8970.7380.6810.5490.482\n22420.9040.7510.6950.5620.489\n22430.9010.7450.6940.5560.465\n22440.8970.7380.6920.5460.429\n23110.9580.8850.850.780.775\n23120.9570.8830.8510.7780.76\n23130.9410.8480.8190.730.67\n23140.8880.7230.6730.5360.445\n23210.9640.8970.8640.7950.787\n23220.9680.9050.8740.8050.792\n23230.9510.8680.840.7550.699\n23240.8980.7410.6920.5560.465\n23310.9560.8810.850.7730.736\n23320.960.8890.8590.7820.742\n23330.9580.8860.860.7790.723\n23340.9090.7650.7180.5830.491\n23410.9120.7710.7190.5950.535\n23420.9170.7810.730.6050.541\n23430.9210.7890.7410.6140.539\n23440.9130.7740.7310.5960.492\n24110.9270.8070.7520.6490.644\n24120.9290.810.7570.6530.64\n24130.9220.7970.7490.6370.6\n24140.9130.7790.7360.6140.54\n24210.9330.8180.7650.6620.655\n24220.9370.8270.7750.6710.66\n24230.930.8130.7650.6540.618\n24240.920.7920.7490.6290.556\n24310.9320.8180.7680.6620.639\n24320.9360.8260.7770.6710.643\n24330.9380.830.7840.6760.636\n24340.9270.8070.7660.6480.574\n24410.9320.8180.7730.6630.615\n24420.9350.8240.780.670.619\n24430.9360.8270.7850.6740.614\n24440.9380.8320.7950.6810.606\n311111111\n31120.9820.9590.9590.9450.915\n31130.9580.9050.9030.8650.793\n31140.8950.7510.7170.6070.51\n31210.9890.9750.9730.9630.944\n31220.9840.9640.9630.9490.92\n31230.9620.9120.910.8720.802\n31240.9020.7640.730.620.523\n31310.9750.940.9360.910.862\n31320.9720.9340.9310.9010.846\n31330.9680.9250.9220.8850.818\n31340.9140.7860.7530.6440.546\n31410.9210.8020.7650.6680.603\n31420.9230.8050.7690.6710.6\n31430.9260.8110.7770.6760.595\n31440.9170.7920.760.6480.538\n321111111\n32120.9940.9860.9870.9830.973\n32130.970.9310.930.9010.846\n32140.9070.7750.7430.6370.548\n322111111\n322211111\n32230.9770.9450.9440.9180.871\n32240.9160.7920.7590.6540.567\n32310.9850.9630.9610.9430.912\n32320.9860.9660.9630.9460.914\n32330.9810.9530.9510.9260.881\n32340.9250.8110.7790.6760.588\n32410.9310.8240.7880.6960.639\n32420.9350.8320.7960.7040.643\n32430.9370.8350.8020.7070.636\n32440.9260.8120.7810.6740.574\n331111111\n33120.9950.9890.990.9860.978\n33130.9870.9720.9730.9620.939\n33140.9270.8180.7870.6920.62\n332111111\n332211111\n33230.9920.9820.9830.9750.96\n33240.9340.8310.80.7070.636\n333111111\n333211111\n333311111\n33340.9450.8540.8240.7340.669\n33410.9480.8620.8290.7470.706\n33420.9510.8670.8350.7530.709\n33430.9560.8770.8460.7640.716\n33440.9430.8480.8190.7240.642\n34110.9590.8890.8550.790.788\n34120.9580.8870.8550.7890.781\n34130.9570.8850.8560.7870.767\n34140.9420.8520.8250.7410.682\n34210.9620.8940.8620.7970.794\n34220.9640.8990.8670.8020.796\n34230.9620.8960.8670.7990.782\n34240.9470.8620.8350.7520.695\n34310.9660.9040.8740.810.804\n34320.9680.9080.8780.8150.806\n34330.9710.9150.8860.8230.811\n34340.9550.8790.8530.7740.721\n34410.9590.8880.8590.7870.754\n34420.960.8910.8630.7920.756\n34430.9640.8980.8710.7990.761\n34440.9620.8940.870.7950.742\n411111111\n41120.9890.9770.9780.9710.956\n41130.9730.9410.9410.9210.881\n41140.9510.8910.890.8480.77\n41210.9940.9860.9850.980.97\n41220.990.9790.9790.9720.957\n41230.9750.9440.9440.9240.884\n41240.9540.8960.8940.8530.775\n41310.9840.9630.9610.9480.921\n41320.9810.9580.9570.9410.91\n41330.9780.9490.9490.9290.89\n41340.9580.9040.9010.860.785\n41410.9710.9320.9280.8990.847\n41420.970.9290.9250.8950.84\n41430.9680.9240.9210.8870.826\n41440.9650.9170.9130.8740.801\n421111111\n42120.9960.9920.9930.9910.986\n42130.980.9560.9560.9410.91\n42140.9580.9050.9040.8670.797\n422111111\n422211111\n42230.9840.9640.9640.9510.924\n42240.9620.9140.9120.8760.81\n42310.990.9770.9750.9660.948\n42320.990.9780.9760.9670.949\n42330.9860.9680.9670.9540.927\n42340.9660.920.9180.8830.817\n42410.9760.9440.940.9160.872\n42420.9770.9460.9420.9180.873\n42430.9750.940.9370.9090.858\n42440.9710.9310.9280.8940.831\n431111111\n43120.9970.9930.9940.9920.987\n43130.9910.9810.9820.9760.963\n43140.9690.930.9290.90.846\n432111111\n432211111\n43230.9940.9880.9890.9850.976\n43240.9730.9370.9360.9090.858\n433111111\n433211111\n433311111\n43340.9790.950.9480.9250.881\n43410.9860.9660.9630.9470.918\n43420.9860.9670.9640.9480.919\n43430.9870.9690.9660.950.92\n43440.9820.9560.9540.9320.889\n441111111\n44120.9970.9940.9950.9930.989\n44130.9930.9850.9850.980.969\n44140.9860.9690.970.9580.934\n442111111\n442211111\n44230.9950.990.9910.9870.98\n44240.9880.9740.9750.9650.944\n443111111\n443211111\n443311111\n44340.9930.9840.9840.9770.963\n444111111\n444211111\n444311111\n444411111",
      "ERR",
      "Position Grades\nDiscounted Scores\n\n\n1\n2\n3\n4\n1/r^0.18\n1/r^0.5\n1/log2(r+1)\n1/r\n2/2^r\n\n\n\n11110.1590.1360.1270.110.106\n11120.1590.1360.1270.110.106\n11130.1590.1360.1270.110.106\n11140.1590.1360.1270.110.106\n11210.2490.1990.1820.1470.133\n11220.2490.1990.1820.1470.133\n11230.2490.1990.1820.1470.133\n11240.2490.1990.1820.1470.133\n11310.430.3260.2920.220.188\n11320.430.3260.2920.220.188\n11330.430.3260.2920.220.188\n11340.430.3260.2920.220.188\n11410.790.580.5110.3660.298\n11420.790.580.5110.3660.298\n11430.790.580.5110.3660.298\n11440.790.580.5110.3660.298\n12110.2570.2140.1970.1660.162\n12120.2570.2140.1970.1660.162\n12130.2570.2140.1970.1660.162\n12140.2570.2140.1970.1660.162\n12210.3350.2690.2450.1980.186\n12220.3350.2690.2450.1980.186\n12230.3350.2690.2450.1980.186\n12240.3350.2690.2450.1980.186\n12310.4910.3790.340.2610.234\n12320.4910.3790.340.2610.234\n12330.4910.3790.340.2610.234\n12340.4910.3790.340.2610.234\n12410.8040.5990.530.3880.329\n12420.8040.5990.530.3880.329\n12430.8040.5990.530.3880.329\n12440.8040.5990.530.3880.329\n13110.4520.3720.3380.2790.276\n13120.4520.3720.3380.2790.276\n13130.4520.3720.3380.2790.276\n13140.4520.3720.3380.2790.276\n13210.5060.410.3710.3010.292\n13220.5060.410.3710.3010.292\n13230.5060.410.3710.3010.292\n13240.5060.410.3710.3010.292\n13310.6140.4860.4370.3440.325\n13320.6140.4860.4370.3440.325\n13330.6140.4860.4370.3440.325\n13340.6140.4860.4370.3440.325\n13410.830.6380.5680.4320.391\n13420.830.6380.5680.4320.391\n13430.830.6380.5680.4320.391\n13440.830.6380.5680.4320.391\n14110.8410.6860.6190.5030.503\n14120.8410.6860.6190.5030.503\n14130.8410.6860.6190.5030.503\n14140.8410.6860.6190.5030.503\n14210.8470.690.6230.5060.505\n14220.8470.690.6230.5060.505\n14230.8470.690.6230.5060.505\n14240.8470.690.6230.5060.505\n14310.8590.6990.630.510.508\n14320.8590.6990.630.510.508\n14330.8590.6990.630.510.508\n14340.8590.6990.630.510.508\n14410.8830.7160.6440.520.516\n14420.8830.7160.6440.520.516\n14430.8830.7160.6440.520.516\n14440.8830.7160.6440.520.516\n21110.2710.2510.2430.2290.225\n21120.2710.2510.2430.2290.225\n21130.2710.2510.2430.2290.225\n21140.2710.2510.2430.2290.225\n21210.350.3060.2910.260.249\n21220.350.3060.2910.260.249\n21230.350.3060.2910.260.249\n21240.350.3060.2910.260.249\n21310.5060.4160.3860.3240.296\n21320.5060.4160.3860.3240.296\n21330.5060.4160.3860.3240.296\n21340.5060.4160.3860.3240.296\n21410.8180.6360.5770.4510.391\n21420.8180.6360.5770.4510.391\n21430.8180.6360.5770.4510.391\n21440.8180.6360.5770.4510.391\n22110.3560.3190.3040.2770.274\n22120.3560.3190.3040.2770.274\n22130.3560.3190.3040.2770.274\n22140.3560.3190.3040.2770.274\n22210.4240.3670.3460.3050.295\n22220.4240.3670.3460.3050.295\n22230.4240.3670.3460.3050.295\n22240.4240.3670.3460.3050.295\n22310.5590.4620.4280.360.336\n22320.5590.4620.4280.360.336\n22330.5590.4620.4280.360.336\n22340.5590.4620.4280.360.336\n22410.830.6530.5930.470.418\n22420.830.6530.5930.470.418\n22430.830.6530.5930.470.418\n22440.830.6530.5930.470.418\n23110.5250.4550.4260.3750.372\n23120.5250.4550.4260.3750.372\n23130.5250.4550.4260.3750.372\n23140.5250.4550.4260.3750.372\n23210.5720.4880.4550.3940.387\n23220.5720.4880.4550.3940.387\n23230.5720.4880.4550.3940.387\n23240.5720.4880.4550.3940.387\n23310.6650.5540.5120.4320.415\n23320.6650.5540.5120.4320.415\n23330.6650.5540.5120.4320.415\n23340.6650.5540.5120.4320.415\n23410.8530.6860.6260.5080.472\n23420.8530.6860.6260.5080.472\n23430.8530.6860.6260.5080.472\n23440.8530.6860.6260.5080.472\n24110.8620.7280.670.5690.569\n24120.8620.7280.670.5690.569\n24130.8620.7280.670.5690.569\n24140.8620.7280.670.5690.569\n24210.8680.7320.6730.5720.571\n24220.8680.7320.6730.5720.571\n24230.8680.7320.6730.5720.571\n24240.8680.7320.6730.5720.571\n24310.8780.7390.6790.5760.574\n24320.8780.7390.6790.5760.574\n24330.8780.7390.6790.5760.574\n24340.8780.7390.6790.5760.574\n24410.8990.7540.6920.5840.58\n24420.8990.7540.6920.5840.58\n24430.8990.7540.6920.5840.58\n24440.8990.7540.6920.5840.58\n31110.4960.4810.4760.4660.463\n31120.4960.4810.4760.4660.463\n31130.4960.4810.4760.4660.463\n31140.4960.4810.4760.4660.463\n31210.550.5190.5090.4880.48\n31220.550.5190.5090.4880.48\n31230.550.5190.5090.4880.48\n31240.550.5190.5090.4880.48\n31310.6580.5960.5750.5320.513\n31320.6580.5960.5750.5320.513\n31330.6580.5960.5750.5320.513\n31340.6580.5960.5750.5320.513\n31410.8740.7480.7070.620.579\n31420.8740.7480.7070.620.579\n31430.8740.7480.7070.620.579\n31440.8740.7480.7070.620.579\n32110.5540.5290.5180.50.497\n32120.5540.5290.5180.50.497\n32130.5540.5290.5180.50.497\n32140.5540.5290.5180.50.497\n32210.6010.5620.5470.5190.512\n32220.6010.5620.5470.5190.512\n32230.6010.5620.5470.5190.512\n32240.6010.5620.5470.5190.512\n32310.6950.6280.6040.5570.54\n32320.6950.6280.6040.5570.54\n32330.6950.6280.6040.5570.54\n32340.6950.6280.6040.5570.54\n32410.8820.7590.7180.6330.597\n32420.8820.7590.7180.6330.597\n32430.8820.7590.7180.6330.597\n32440.8820.7590.7180.6330.597\n33110.6710.6230.6030.5670.565\n33120.6710.6230.6030.5670.565\n33130.6710.6230.6030.5670.565\n33140.6710.6230.6030.5670.565\n33210.7030.6460.6220.580.575\n33220.7030.6460.6220.580.575\n33230.7030.6460.6220.580.575\n33240.7030.6460.6220.580.575\n33310.7680.6910.6620.6070.595\n33320.7680.6910.6620.6070.595\n33330.7680.6910.6620.6070.595\n33340.7680.6910.6620.6070.595\n33410.8980.7830.7410.6590.635\n33420.8980.7830.7410.6590.635\n33430.8980.7830.7410.6590.635\n33440.8980.7830.7410.6590.635\n34110.9050.8120.7710.7020.702\n34120.9050.8120.7710.7020.702\n34130.9050.8120.7710.7020.702\n34140.9050.8120.7710.7020.702\n34210.9080.8140.7740.7030.703\n34220.9080.8140.7740.7030.703\n34230.9080.8140.7740.7030.703\n34240.9080.8140.7740.7030.703\n34310.9160.8190.7780.7060.705\n34320.9160.8190.7780.7060.705\n34330.9160.8190.7780.7060.705\n34340.9160.8190.7780.7060.705\n34410.930.8290.7870.7120.709\n34420.930.8290.7870.7120.709\n34430.930.8290.7870.7120.709\n34440.930.8290.7870.7120.709\n41110.9440.9420.9420.9410.94\n41120.9440.9420.9420.9410.94\n41130.9440.9420.9420.9410.94\n41140.9440.9420.9420.9410.94\n41210.950.9470.9450.9430.942\n41220.950.9470.9450.9430.942\n41230.950.9470.9450.9430.942\n41240.950.9470.9450.9430.942\n41310.9620.9550.9530.9480.946\n41320.9620.9550.9530.9480.946\n41330.9620.9550.9530.9480.946\n41340.9620.9550.9530.9480.946\n41410.9860.9720.9670.9580.953\n41420.9860.9720.9670.9580.953\n41430.9860.9720.9670.9580.953\n41440.9860.9720.9670.9580.953\n42110.950.9480.9460.9440.944\n42120.950.9480.9460.9440.944\n42130.950.9480.9460.9440.944\n42140.950.9480.9460.9440.944\n42210.9560.9510.950.9470.946\n42220.9560.9510.950.9470.946\n42230.9560.9510.950.9470.946\n42240.9560.9510.950.9470.946\n42310.9660.9590.9560.9510.949\n42320.9660.9590.9560.9510.949\n42330.9660.9590.9560.9510.949\n42340.9660.9590.9560.9510.949\n42410.9870.9730.9690.9590.955\n42420.9870.9730.9690.9590.955\n42430.9870.9730.9690.9590.955\n42440.9870.9730.9690.9590.955\n43110.9630.9580.9560.9520.952\n43120.9630.9580.9560.9520.952\n43130.9630.9580.9560.9520.952\n43140.9630.9580.9560.9520.952\n43210.9670.9610.9580.9530.953\n43220.9670.9610.9580.9530.953\n43230.9670.9610.9580.9530.953\n43240.9670.9610.9580.9530.953\n43310.9740.9660.9620.9560.955\n43320.9740.9660.9620.9560.955\n43330.9740.9660.9620.9560.955\n43340.9740.9660.9620.9560.955\n43410.9890.9760.9710.9620.959\n43420.9890.9760.9710.9620.959\n43430.9890.9760.9710.9620.959\n43440.9890.9760.9710.9620.959\n44110.9890.9790.9750.9670.967\n44120.9890.9790.9750.9670.967\n44130.9890.9790.9750.9670.967\n44140.9890.9790.9750.9670.967\n44210.990.9790.9750.9670.967\n44220.990.9790.9750.9670.967\n44230.990.9790.9750.9670.967\n44240.990.9790.9750.9670.967\n44310.9910.980.9750.9670.967\n44320.9910.980.9750.9670.967\n44330.9910.980.9750.9670.967\n44340.9910.980.9750.9670.967\n44410.9920.9810.9760.9680.968\n44420.9920.9810.9760.9680.968\n44430.9920.9810.9760.9680.968\n44440.9920.9810.9760.9680.968"
    ],
    "summary_t": "In this post, we unwrap the mystery behind two popular search relevance metrics, and discuss their pros and cons.  Our subjects for this exercise are Normali..."
  },
  {
    "id": "cff7e50a56da02d42b10ff88f54a20df",
    "url_s": "https://opensourceconnections.com/blog/2019/12/10/tesseract-3-and-tika/",
    "title": "Tesseract 3 and Tika",
    "content": [
      "Tesseract 4 is a major upgrade to this venerable OCR library, incorporating neural networks and lots of other great improvements, but not everyone has upgraded to it (including one of our customers), so we investigated using Tesseract 3 instead.   It turns out that the HOCR support in Tesseract 3 is identical to Tesseract 4, which means that Tika doesn’t mind that it’s an older version.",
      "Later on I found out the first integration of Tesseract with Tika was with Tesseract 3, so it shouldn’t have been a surprise it worked so well!",
      "Want to try out Tesseract 3 inside of Tika?  Checkout https://github.com/o19s/pdf-discovery-demo/tree/master/tika-server-tesseract docker image, which is based on the https://logicalspark.github.io/docker-tikaserver/ project.  I got lucky and built the docker image on an older Linux distro, so it dragged in Tesseract 3 versus 4!",
      "Read other posts in the Tika Tuesday series here."
    ],
    "summary_t": "In which we deal with learning that sometimes you don’t get to use the latest version of Tesseract…"
  },
  {
    "id": "73a9bb64e5780a02c49ae60583579721",
    "url_s": "https://opensourceconnections.com/blog/2019/12/11/what-is-a-relevant-search-result/",
    "title": "What is a 'Relevant' Search Result?",
    "content": [
      "Five years ago, I wrote an article called What is Search Relevance?. Back then, I had to shout to convince people to even notice whether search results were accurate or not. On OSC’s app development projects, I often urgently raised my hand trying to get anyone to take seriously the thought that \"look these search results clearly have nothing to do with this query!!\"",
      "Now I feel the awareness battle has been won. After many more years of my personal experience, I want to take a more thoughtful look than my previous article. Let’s dig into what we mean. What is relevance really? What do we mean when we say this or that result is relevant or irrelevant?",
      "Two People Walk Into a Search Bar",
      "It’s amazing how two people can take a simple query, say \"Rocky\" in a movie search, look at a document, say \"Rocky Horror Picture Show\" and argue over the movie’s relevance or irrelevance. Recently, I asked a Meetup audience of about 50 folks to rate the relevance of \"Rocky Horror Picture Show\" was for a \"rocky\" from 1-4. I got surprisingly varying results, as shown in the graph below:",
      "",
      "If you dig into why this is so, you get various explanations and justifications:",
      "Rocky is mentioned in the title, perhaps the user hasn’t typed their full query in yet? Both begin with \"rocky\" …\n  Sure it’s not as relevant as \"Rocky\" movies, but it’s still one of the few movies that mention the term \"Rocky\" in the title, so it’s at least a \"2 or 3\"!\n  The user clearly wants the movie \"Rocky\", there’s little ambiguity, so show \"Rocky\" movies, Rocky Horror is clearly irrelevant\n  Anecdotally you get different perceptions when you use Rocky (capital R) vs rocky (lowercase r) with the former’s capitalization seeming to signify the proper noun",
      "To me (the person who formulated the query), \"Rocky Horror Picture Show\" was clearly not relevant. I knew that I meant Rocky movies. So I was frankly surprised to see such a wide distributions on what I perceived to be an \"obvious\" query.",
      "What I just did was ask the audience to provide a judgment on the query and document, whereby a grade is assigned indicating the relevance of that document for a query. We can create a judgment list if we accumulate many grades for many query document pairs, such as grading:",
      "\"Rocky III\" for q=Rocky as a 4 (very relevant))\n  \"Happy Gilmore\" for q=Rocky as a 1 (very irrelevant)\n  \"Run Lola Run\" for q=movies in Berlin a 3 (moderately relevant)",
      "And so on… And as many reading may know, with enough of this data you can give a search solution’s listing a score on it’s overall relevance with metrics like nDCG and ERR",
      "Best Method for Judging Search Result Relevance?",
      "Judgments gathered by raters explicitly are known as expert judgments. If we have a query, we might go about sitting real live human beings to review a complete search result and ask them \"how relevant is this?\". After fully reviewing, each expert rater assigns a grade (such as 1-4 above) to the item for the query.",
      "As you might guess, this is rather time consuming to do well. Many search applications interpret user behavior on the site (clicking on search results, watching movies, purchasing products, etc) into an implicit grade for a document in that query. We’ll call these implicit judgments. If users seem to watch \"Rocky Horror Picture Show\" a lot after entering the query \"rocky\", we may decide to give that a grade of 3. This implicit grade is based on our own assumptions of how users behave when they feel an item is relevant/irrelevant.",
      "This brings us back to the central question of this article - What is a relevant result? Which item should be brought back to the top? Should we trust the well thought out opinions of live human beings, carefully considering whether an item is relevant? Should we trust our imperfect measures of real users behavior?",
      "As you might suspect both have their flaws.",
      "This panel of experts (sometimes not even representative of users) is not really being forced to solve real problems. Even when thinking carefully, and recruited from the user population, can they really put themselves in a live user’s shoes, with all their many subtle considerations? With the problems users are trying to use the application for? Sitting in front of a query \"Notre Dame\" isn’t the same as the stressed out high school student researching for their homework the latest fire, as they frantically search for good citations for their paper. That student may need a quick recap, not a detailed writeup. A subtlety we might miss if not looking at real user behavior.",
      "Implicit judgments themselves are imperfect models. It’s often assumed a click is a signal for relevance. Is it? Clicks often happen to inspect an item further, but not finally decide the item is relevant. A conversion is another indication of relevance, but the deeper you go in the funnel, the less data you might gather. Without a conversion, you make even more complicated assumptions (did they read the article? Do they stay on the page for a while?). Even more troublesome, users only interact with what’s presented to them, especially those items presented higher in the search listing. Your search UI may only have interaction data with 5 results for a query - what if the relevant item is buried on page 2 and no one clicks on it?",
      "Of course there are a lot of methods to mitigate these issues, and every search team has it’s bag of tricks. I’d recommend Max Irwin’s Haystack Keynote if you’re curious about these tricks. But I want to dive deeper into what this notion of ‘relevance’ really means.",
      "What is the \"thing\" we are chasing in these measures?",
      "Information Needs are Almost Always Incomplete",
      "In search, you might hear about this notion of an information need. The idea is that a user comes to search with a conscious and unconscious specification for what they want. The queries they enter are an imperfect formulation of that information need.",
      "To get at what users really want in their heart of hearts, here’s a list of classes of information needs:",
      "Informational: Some users need factual answers to their questions (such as in question answering, \"When was the Battle of Waterloo?\")\n  Compare / Contrast: Some users need to compare / contrast options (such as purchasing shoes or comparing jobs to apply to)\n  Research: Users intently gathering a set of items to solve and deepen their understanding of a complex topic, question, or problem (gathering articles to write a school report on a topic)\n  Browsing: Some users simply want to passively browse or window shop (what strategy games are on the video game store Steam today)\n  Known-Item: Some users want to lookup an item by name (such as a lookup by name or SKU, or a name in your phone’s contact list)",
      "If you reflect on these use cases users aren’t always conscious of exactly what they want. Users gain awareness of what they want as they search. You start by researching the US Civil War, unsure what you’ll write your research paper about. You may then see results, become more aware of what you want, and focus deeper in on a specific aspect of some specific battle close to your interests or problem you need to solve. Reflect also on shopping, often you start with an imperfect need, like a laptop bag, only to become more aware as you search that what you really want is a \"rugged, large laptop backpack suitable for overnights.\"",
      "It’s not just that users are not conscious of what they want, they are learning as they go through the search process. By refining queries, seeing what comes back, and being enticed by options:",
      "Search users are quite often \"in process.\" If users knew exactly what they wanted, this would often be just a database lookup by an id, or turn into a pure question answering problem. Nor are they completely passive, which would fall into the realm of recommendation systems. They are on a spectrum that is neither 100% passive nor 100% knowing exactly what they want, as illustrated on the diagram below:",
      "",
      "Relevance is about forwarding the process",
      "In reality, this user’s \"process\" may last months. An ongoing health issue could involve extensive searches over a long period of time. An expensive product search requires a lot of decision-making and research. For this reason, I prefer to focus on what forwards this process or journey they are on.",
      "In the diagram above, I show a spectrum of search use cases, depending on how aware & committed users are to what they want. Users all the way to the right are close to knowing exactly what they want. Users all the way to the left are browsing / exploring some curiosity of theirs, barely a step away from a recommendation system problem.",
      "In search, we help users along this process. We hope passive browsers might be delighted by something they see, and want to take further steps in a new direction, getting closer to taking action, answering their question, or solving their problem. More concretely, they start with an imperfect notion, and gradually get closer to knowing the exact question they want to ask, item they want to buy, kind of job they want to apply for, or articles they want to gather.",
      "Relevance is about deciding whether or not to verb the item",
      "In many contexts, what’s relevant, is also about what users are going to ‘do’ to the item. Many search systems are built with an explicit action in mind. E-Commerce is about buying things, Job search applying, and so on. Similar to the object of a sentence, a search query could also be restated as \"I want to buy __\" or \"I want to apply for ___ job\", where the blank is filled in by a search query. The blank might be \"blue shoes\" or \"software engineer\" respectively. Knowing whether I want to \"buy\" the item, means my decision on relevance, is going to be dictated by factors that lead me to making a good buying decision. Amazon, for example, gives us a lot of information to help us decide whether to buy an item, as shown in the screenshot below showing search results for \"blue shoes\":",
      "",
      "Not all search systems have very clear action. But for those that do, this verb is clearly part of the relevance conversation. Search users on the right half of the process above are more committed. They have a clear sense they want to do. Even if they are not fully conscious of everything they want, they are on a mission. They will ultimately take a final action or be dissatisfied unable to find an item they want to (verb).",
      "However, it’s also the case, less committed users might not have a clear action they want to take, or that we could readily measure.. Even on more transactional systems like e-commerce or job search, users may start out browsing. For these browsing users, on the far left side of the spectrum above, the \"verb\" might change to something a bit less committed. Something like \"I want to bookmark about __ jobs\".",
      "A word of warning to search teams that simply want to optimize conversions. Supporting the non-committal side of the spectrum is really important. If your search system becomes a resource for more passive browsers, it could also be their system for purchasing products when they learn what they want. Amazon is both a resource for passive product research and buying products. This is one reason while in a Best Buy you’ll look at Amazon. Not just to buy, but for the depth of product information and reviews.",
      "It’s not just relevance, it’s about contrasting options that forward decisions",
      "If most relevance is about being in the middle of a process, moving through the process is about making decisions and evaluating options. This too dictates what we consider ‘relevant’. Different options that contrast with each other help users make decisions on next steps, move through the process, and get closer to the exact thing they want.",
      "For example, passive/browsing job searchers would want to know options at a very high level. They want to see that for a software engineer job in their area, the major themes are .Net and Python jobs. And that most jobs are located downtown. Answering these questions might not just be search, but many other analytics/discovery elements that help users understand the lay of the land, and decide whether to make further steps.",
      "When a job seeker gets more committed to a type of job, they still want to contrast different details. They know they want a \"Java programming\" job. They see one job with no commute; another that seems to pay more, but requires a long drive. Both are relevant. However highlighting their key differences are more useful than their similarities to a user making a decision.",
      "In these situations, and other search systems like research, e-commerce, and many others, we present users contrasting options to choose from that ought to be both relevant, but different in important ways. The search UI’s job becomes doing it’s best to highlight these differences. A search system that shows five identical Java programming jobs, downtown, won’t help you feel like you made a good decision.",
      "We won’t feel these results are relevant, if they’re not useful as a whole. They won’t feel relevant if they don’t sufficiently differ with each other to give a good overview of all the options scoped within the explicit keywords provided. If search is about the user’s next step on the process, and I can’t understand the most relevant next steps I might take, then search has failed at it’s goal.",
      "This is where classic relevance breaks down, and where it’s good to look at other systems of evaluation. Not just individual results relevance, but the performance of the search results page itself in helping users make additional decisions. This gives a broader point of view, helping to answer:",
      "Do the results display in a way that emphasizes their qualities relevant to the user (ie perceived relevance?)\n  Do I get a good overview of the different options / paths to follow scoped to this keyword?",
      "It’s not just about putting a search results page of \"4s\" in front of the user, it’s about the best \"4s\", that provide good forks in the road. Such a \"fork\" might mean new topics to explore, product options to consider, or additional factors when buying a house or applying for a job.",
      "So finally what are ‘relevant’ search results?",
      "I think when you put this all together, you can get a picture of what a ‘relevant’ result is to me, at this stage in my career, after fighting too many relevance battles :) To me returning a relevant result increasingly is only half the battle. It’s just as much about a relevant page that serves the user, helping them learn what they don’t know about the options. Options they may ultimately want to (verb) - buy, apply for, etc. Helping them with their own evolving definition of \"relevance\".",
      "There will be keywords that need more of browsing treatment / measurement / and optimization. Often these are head queries like \"shirt\" on an apparel site. What’s \"relevant\" might be a broad range of options in this category, almost a curated page just for this topic. When do you see users having a better idea of what they want \"blue dress shirt\", the pool of relevant possibilities becomes smaller, but the needed information to make a decision more specific to that area (‘dress shirt’). A useful search result page knows those options, and contrasts the relevant items most appropriately..",
      "Probably the most important thing is to study and map your user’s \"journey\" through the search process for your domain from less committed browsers to known-item searchers. Every search application will have different classes of information needs on this spectrum that need their own system of measurement & solutions. They’ll each have a way of demonstrating to the user the ‘next step’ to take, whether a conversion, or simply a further refinement and increased education of the options available on their topic.",
      "Sorry it’s not that easy! Users have so many, increasingly complex demands of search. I’m sure in a few years, my thinking on this will evolve further. If you’d like to help that evolution and show me what I missed, please do reach out to contact me"
    ],
    "summary_t": "A relevant search result helps users take further steps, make good decisions, and supports their ability to formulate better, more informed, queries."
  },
  {
    "id": "bf89d6860a9664c18afb9f3c772255a1",
    "url_s": "https://opensourceconnections.com/blog/2019/12/17/parsing-tika-tesseract-output-inside-of-solr-via-statelessscriptupdateprocessorfactory/",
    "title": "Tika Tuesdays: Parsing Tika & Tesseract formatted HOCR output inside Solr ingestion pipeline",
    "content": [
      "In our first couple of posts, we ended up doing a lot of the processing work outside of Solr. It gave me a chance to polish my PowerShell skills, which was cool, and gave me a nice appreciation for having a scripting language that works on both Windows and Unix systems!",
      "However, sometimes you want the search engine to be a black box. I take documents and put them into the black box, and now they are searchable.  We added some additional complexity by moving to a parent/child relationship for the PDF’s because each page ended up being it’s own document in Solr. This meant another parsing script that dumped more intermediate format documents.",
      "What if we could do everything inside of Solr? What if we could take the output from Tika with the Tesseract generated OCR content, and then convert that to a set of parent/child documents that  are indexed into Solr?",
      "Time for one of my favorite Get out of Jail Free cards from Solr, the awkwardly named StatelessScriptUpdateProcessorFactory which would let us put all that parsing logic into a script run inside of Solr. I’ve used this in the past a couple of times, but would it work with the extraction code?",
      "We started with setting up a custom extract end point, but this time included a update.chain parameter:",
      "<requestHandler name=\"/update/speeches\"\n                  class=\"solr.extraction.ExtractingRequestHandler\" >\n    <str name=\"parseContext.config\">parseContext.xml</str>\n    <lst name=\"defaults\">\n      <str name=\"uprefix\">attr_</str>\n      <str name=\"multipartUploadLimitInKB\">20480</str> <!--Limit to 20 MB PDF-->\n      <str name=\"update.chain\">process-speech-from-extracted-text</str>\n    </lst>\n  </requestHandler>",
      "The update.chain is what lets us override the normal execution flow, and inject the call to our Scripting step:",
      "<updateRequestProcessorChain name=\"process-speech-from-extracted-text\">\n    <processor class=\"solr.StatelessScriptUpdateProcessorFactory\">\n      <arr name=\"script\">\n        <str name=\"script\">process-speech.js</str>\n      </arr>\n    </processor>\n    <processor class=\"solr.LogUpdateProcessorFactory\" />\n    <processor class=\"solr.RunUpdateProcessorFactory\" />\n  </updateRequestProcessorChain>",
      "This then lets us call a custom Javascript script (though we can use other languages like Ruby etc), to deal with the text.",
      "I’ll let you go through the process-speech.js script yourself.   The big thing is that the Extract handler, for some odd reason, gives us not XML content, but the XML content with none of the wrapping < or > tags!  So we can’t use the XML parsing logic that we’ve used previously, instead we do lots of string splitting!",
      "Other things to note:",
      "We are able to invoke any Java methods we want by prepending Packages. to the class name, like this example of base64 encoding:\nlogger.info(\"Here comes some base 64: \" + Packages.org.apache.solr.common.util.Base64.byteArrayToBase64(id.getBytes()));\n  \n  \n    Shockingly, we can create a brand new Solr input document:\nvar childDoc = new Packages.org.apache.solr.common.SolrInputDocument();\nand then add it to our parent document via calling the Java method on the object:\ndoc.addChildDocument(childDoc);\n  \n  \n    The process-speech.js script is parsed at startup, so if you have syntax errors, or non compatible Javascript, then you will get an error.  While not positive, I believe the version of Javascript supported by the Rhino engine in Java (or maybe called Nashorn?) is Javascript 6, so stay with the simplest Javascript syntax you can.\n  \n  \n    It’s nice that you can deploy this script via your regular SolrCloud friendly deploy scripts, and it would be interesting to see what other use cases there might be for this.",
      "Read other posts in the Tika Tuesday series here."
    ],
    "summary_t": "In which we take advantage of Solr’s scripting update capability to turn Solr into an OCRing blackbox!"
  },
  {
    "id": "10c8caa058a843881712a098d0cef9e3",
    "url_s": "https://opensourceconnections.com/blog/2019/12/18/bert-and-search-relevance-part2-dense-vs-sparse/",
    "title": "What problem does BERT hope to solve for search?",
    "content": [
      "I’m sure if you run in search or NLP circles, you’ve heard of BERT. It’s what Google famously used to improve 1 out of 10 searches, in what they claim is one of the most significant improvements in the company’s history. Even my favorite neural search skeptic had to write a thoughtful mea culpa.",
      "As Max Irwin wrote about in his overview The T in BERT stands for Transformer, which a freakishly accurate deep-learning based language model, able to predict with alarming accuracy what the ‘next word’ should be. We want to start getting into the nitty gritty. We’ll show you what you need to know to be a successful search relevance engineer in this brave new BERT future.",
      "First part of this deep dive, before we jump to the cool Transformer architecture, it’s important to first understand the background & motivation for BERT. What does BERT solve that traditional search engines, like Lucene/Solr/Elasticsearch, don’t solve? What does BERT do that word2vec wouldn’t do for a search application?",
      "Let’s start by first understanding the basics of where search engine technology is now, it’s roots and recent evolution, te see where BERT fits into the landscape.",
      "Sparse Vectors, The Inverted Index and You!",
      "Search engines like Lucene come with a baked in TF*IDF scoring method (such as BM25). This is how we score a ‘similarity’ between a query and a document. The idea is pretty intuitive, with every document, the relevance score of a search term match in that document in proportional to:",
      "Term Frequency - how often does that  search term occur in this doc? More occurrences - more relevant.\n  Inverse Document Frequency - how rare is the term? Rare terms are more specific to the user’s intent.\n  Inverse Document Length - how many terms does this document have? If \"Skywalker\" occurs once in a tweet it’s more about skywalker than once in a book",
      "TF*IDF based try to measure the ‘aboutness’ of the text. Higher TF*IDF, means the text in more ‘about’ the term. Terms are usually words, but as I write in Relevant Search in practice they might be any discrete attribute, including words with parts of speech, concepts in a knowledge graph, pitches in a song, attributes of an image, or even the pixels in an image themselves.",
      "This is famously known as the classic ‘bag of words’ vector space model. Each term in the corpus is a dimension in some N-dimensional space. If we take a piece of text, we could compute a TF*IDF score for every term in our corpus. We could compute a sparse vector where each dimension corresponds to a single term. In the table below, several terms from movie scripts are given BM25 scores. If we were to fully flesh out this sparse vector representation, we would arrive at possible hundreds of thousands of terms, with most terms getting a score of 0 for a document (as it doesn’t occur in that doc - that’s what makes it ‘sparse’!).",
      "…\n      Hogwarts\n      Luke\n      Skywalker\n      Space\n      Prison\n      Cat\n      Car\n      ….\n    \n  \n  \n    \n      Star Wars\n       \n      0.0\n      3.5\n      12.5\n      4.4\n      0.0\n      0.0\n      0.0\n       \n    \n    \n      Empire Strikes Back\n      …\n      0.0\n      3.2\n      13.4\n      4.1\n      0.0\n      0.0\n      0.0\n      …\n    \n    \n      Cool Hand Luke\n       \n      0.0\n      1.4\n      0.0\n      0.0\n      9.4\n      0.0\n      0.5",
      "A query also is a bit of text we might represent in the sparse vector space:",
      "…\n      Hogwarts\n      Luke\n      Skywalker\n      Space\n      Prison\n      Cat\n      Car\n      …\n    \n  \n  \n    \n      q=luke skywalker\n      …\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      …",
      "If you multiplied this query vector, element wise with each doc, (aka a dot product), and came out with a number, you’d get something like this set of scores:",
      "Score\n    \n  \n  \n    \n      Star Wars\n      16\n    \n    \n      Empire Strikes Back\n      16.6\n    \n    \n      Cool Hand Luke\n      1.4",
      "Viola, you’d get some kind of similarity or relevance score, where the Star Wars movies ranked higher for Star Wars character Luke Skywalker than spurious mentions of Luke.",
      "This is great. Of course, search is not some robotic, dictionary lookup system. It involves dealing with the messiness of natural language. This includes a lot of inference and context which human beings bring to the problem. Relevance engineers and search teams know this well, and work to mitigate the problem by augmenting the indexed terms by turning those terms into elements in taxonomies, entity recognition, manually adding synonyms, and other ‘classic’ workarounds.",
      "As we’ll see in a future article, BERT attempts to help us with many of these problems. As perhaps (spoiler) a slightly better starting point for relevance than TF*IDF for ‘aboutness’. But before we get there, we need to understand how search has been augmented with dense-vector approaches from systems like word2vec.",
      "Dense Vector Search",
      "Dense vector representations attempt to turn a sparse vector into something less precise. For search, this helps us ‘open up the window’ for terms close in meaning to be grouped together.",
      "Instead of thousands of terms in our language, if we could represent the documents above with major topics or themes, we might create a denser vector for the movie space, down to perhaps a few hundred themes, or in the table below, 3 dimensions :) :",
      "‘Space’-ness\n      ‘Prison’-ness\n      ‘Sword-fight’-ness\n    \n  \n  \n    \n      Star Wars\n      0.9\n      0.01\n      0.8\n    \n    \n      Empire Strikes Back\n      0.9\n      0.10\n      0.8\n    \n    \n      Cool Hand Luke\n      0.01\n      0.9\n      0.0",
      "And similarly for the terms themselves, here we add a term that doesn’t occur literally in the movie dialog, ‘spaceship’",
      "‘Space’-ness\n      ‘Prison’-ness\n      ‘Sword-fight’-ness\n    \n  \n  \n    \n      luke\n      0.5\n      0.55\n      0.5\n    \n    \n      skywalker\n      0.9\n      0.10\n      0.8\n    \n    \n      spaceship\n      0.9\n      0.01\n      0.2",
      "We can perform two dot products per movie (luke_vector * movie) + (skywalker_vector * movie) to get a score.",
      "First the term ‘luke’",
      "Luke Dot Product\n    \n  \n  \n    \n      Star Wars\n      0.9*0.5 + 0.01*0.55 + 0.8*0.5 = 0.8555\n    \n    \n      Empire Strikes Back\n      0.9*0.5 + 0.10*0.55 + 0.8*0.5 = 0.905\n    \n    \n      Cool Hand Luke\n      0.01*0.5 + 0.9*0.55 + 0.0*0.5 = 0.5",
      "Repeating for Skywalker:",
      "Skywalker Dot Product\n    \n  \n  \n    \n      Star Wars\n      0.9*0.9 + 0.01*0.1 + 0.8*0.8 = 1.45\n    \n    \n      Empire Strikes Back\n      0.9*0.9 + 0.10*0.1 + 0.8*0.8 = 1.46\n    \n    \n      Cool Hand Luke\n      0.01*0.9 + 0.9*0.1 + 0.0*0.8 = 0.099",
      "Summing each term together, we can get a pretty good ‘dense’ relevance score:",
      "Score\n    \n  \n  \n    \n      Star Wars\n      2.3\n    \n    \n      Empire Strikes Back\n      1.451\n    \n    \n      Cool Hand Luke\n      0.599",
      "Even cooler, we can repeat the process, and even get a good score when someone searches for ‘spaceship’, even though Star Wars movie dialog doesn’t actually use the term ‘spaceship’:",
      "‘Spaceship’ score\n    \n  \n  \n    \n      Star Wars\n      1.21\n    \n    \n      Empire Strikes Back\n      2.365\n    \n    \n      Cool Hand Luke\n      0.018",
      "(Note I’m cheating a little, as usually we’d normalize these vectors or divide by the magnitude of each vector to to perform true cosine similarity, but the intuition is the same)",
      "This is rather neat, as we get a kind of similarity of a search term, with a document regardless of whether that term is directly mentioned or not.",
      "Word2Vec",
      "This is pretty much what word2vec does. However, we don’t tell word2vec what each dimension of the dense-vector encoding means. Word2Vec starts by giving every term a random vector:",
      "dimension 0\n      dimension 1\n      dimension 2\n    \n  \n  \n    \n      luke\n      0.2\n      0.6\n      0.3\n    \n    \n      skywalker\n      0.6\n      0.2\n      0.9\n    \n    \n      spaceship\n      0.3\n      0.7\n      0.2",
      "Word2vec then looks at little windows of text, a window of perhaps 6 words:",
      "Luke skywalker flew a spaceship",
      "Skywalker blew up the spaceship",
      "Word2Vec notice the tuples (skywalker, spaceship) and (luke, spaceship). With this ‘positive’ example of luke and spaceship sharing context, word2vec shoves the two terms a little closer together:",
      "dimension 0\n      dimension 1\n      dimension 2\n    \n  \n  \n    \n      luke\n      0.25\n      0.55\n      0.35\n    \n    \n      skywalker\n      0.55\n      0.25\n      0.85\n    \n    \n      spaceship\n      0.3\n      0.7\n      0.2",
      "This process is repeated over and over using the whole corpus. Each time a term is viewed sharing a context with another term (for example ‘confirm’ and ‘accept’ seem to share ‘reservation’ a lot - as in ‘confirm my reservation’/’reservation accepted’) we tweak it closer together. The process gradually pushes and pulls, bubbling words with shared context close and others farther apart.",
      "Ambiguous terms like Luke, those with high document frequency, will get pulled in a lot of different directions, and will end up towards the ‘middle’ of many different contexts. Less ambiguous terms (ie ‘skywalker’) will be pulled into their own little universe of Star Wars stuff. Some terms that occur only once or twice, will still stay pretty close to their random values. In fact, it’s not uncommon to omit terms that are too rare.",
      "Indexing Dense Vector Representations",
      "One challenge with dense-vector encodings is search engines, like Lucene, don’t yet have a consistent method of storing, matching, and scoring queries and documents using dense vectors. The inverted index data structure is optimized for sparse vector matching & scoring. This is why recently you may have seen an explosion of methods for performing approximate nearest neighbor or ANN algorithms. These algorgithms index vectors, support queries to find close neighbors, and score them in a way that approximates the kind of similarity we performed here. All without having to manually perform millions of multiplications for every document in the index. Be sure to support the various ANN algorithms being developed by search engines:",
      "Lucene 9004\n  ANN in Vespa\n  Hangry\n  Answerini’s method",
      "The Conundrum: Sparse vector (precision) vs word2vec Dense vector (recall)",
      "Selecting a dense vector strategy is usually a tradeoff. By loosening up our ‘match’ definition, and opening up recall, we might score highly for terms actually not appropriate for this context. These little contextual and use-case specific exceptions are where search teams get into trouble with word2vec. For example, a ‘dense vector’ representation for \"Spock\" might be:",
      "‘Space’-ness\n      ‘Prison’-ness\n      ‘Sword-fight’-ness\n    \n  \n  \n    \n      spock\n      0.9\n      0.01\n      0.01",
      "Which is nearly identical to ‘spaceship’ in the examples above. Thus ‘Star Wars’ would be equally relevant for a spock and a spaceship query. Pulling back ‘Star Wars’ for ‘spaceship’ intuitively makes sense. However, users might object to ‘Star Wars’ being considered a match for a character in a completely different science fiction franchise.",
      "If we were to only perform direct, ‘sparse vector’ term matches, we would strongly err towards precision. We would only pull back direct term matches. Of course, this feels too restrictive, as we’ve seen in the ‘spaceship’ use case, where we’d like to open up the recall a bit. Even here, without direct term statistics to score on, the balance gets tricky. We might score a topically related document (Empire Strikes Back) just as highly for one that actually uses the term spaceship, like First Spaceship on Venus.",
      "A lot of engineering goes on to balance recall in precision. Either leveraging dense vector strategically to perhaps improve recall. Or to muck with the inverted index (synonyms, taxonomies, knowledge graphs, etc) to make it strategically less precise, in ways that we feel users will tolerate for this domain.",
      "BERT gets at what both sparse and classic dense representations miss",
      "So the state of the world of search has been:",
      "Not very precise dense vector representations\n  Lots of work to augment sparse vector systems to balance precision/recall\n  Relevance based on lots of (semi-)manual feature engineering, balanced manually, or with learning to rank",
      "If you step back and think about it, neither sparse nor dense methods capture what it’s like to read and understand a document. As you read a document, you have an evolving mental state that captures not just a small window, like word2vec, but the full subject matter and context of the document. If you were to stop half way in reading a news article, and predict what comes next, you probably could. You probably are trained enough in the language of how news is written, you have understood the context of what is being written about, that you could write prose that more-or-less looked like a news article on that topic.",
      "As we’ll see Transformers model this task. BERT uses transformers to give us a dense vector prediction of what kind-of-word is appropriate to come next at a given point. Not just to complete simple sentences like word2vec could, but to model and flesh out the likelihood, given some context, of what next words are most likely.",
      "Recall earlier, we mentioned how TF*IDF was trying to approximate a notion of ‘aboutness’ of a term mentioned in a document. What if by predicting what the article might be written about directly, we could simulate a dense vector that modeled ‘aboutness’ directly? Could it help overcome these precision/recall tradeoffs? Are there aspects of the current heavy feature engineering regime that we could overcome?",
      "I’m still figuring out these questions myself, as we all are. More on my thoughts time. I’m sure to make some ‘bold’ predictions on the future of all this stuff for us ‘Average Joe’ non-Google search teams!",
      "As always if you like this article, want to share feedback, or tell me what I got wrong. Please get in touch."
    ],
    "summary_t": "Traditional search engines and word2vec have their limitations for search relevance. Pt 2 of our series on BERT."
  },
  {
    "id": "75380b0b85ad7d1769d3ef9b3b360e86",
    "url_s": "https://opensourceconnections.com/blog/2020/01/14/why-you-should-submit-a-talk-to-haystack-the-search-relevance-conference/",
    "title": "Why you should submit a talk to Haystack, the Search Relevance Conference",
    "content": [
      "Most of you will be aware of our Haystack conference, which we currently run in April in Charlottesville, USA and in October in Europe. We’re now in our third year of Haystack events and we’ve recently opened the Call for Papers for our next conference, which closes on 31st January. The event simply wouldn’t be possible without those who offer to present a talk and we’re immensely grateful to those who do.",
      "So why should you consider submitting a talk? It can be a scary prospect for those unused to presenting at conferences (and even for some who have done it before): will my talk get chosen? Will people listen? Will anyone find it remotely interesting? I’m going to do my best to assuage those fears and encourage you to submit your relevance story.",
      "Firstly, we try very hard to create a high-quality programme for Haystack through the use of a blind-as-we-can-make-it submission process. One of our team receives every submission and removes as much information as they can that might identify the speaker and their employer. We then ask a small group of search relevance community members (not OSC employees) to rate the anonymised talks. Once we have a list of the top-scoring talks (and a list of backups in case for some reason a speaker has to drop out) we inform all the submitters and put together a programme - the person running the process makes sure we have some subject diversity (e.g. not just 8 talks all about Elasticsearch Learning to Rank, much as some might enjoy that!) and that we don’t have more than one talk from any one organisation. This hopefully means that our programme is based on the quality of the submissions - we also don’t currently take any direct sponsorship for the event, so there’s no way of guaranteeing a talk slot by sponsoring. We know this process isn’t perfect but it’s as good as we can make it, given we’re a relatively small team - and we do try and learn from any past mistakes. Some OSC staff even submit talks - and they are treated exactly the same as all the others, some get through the process, some don’t.",
      "The great thing about presenting at Haystack, a relatively small and informal event is that everyone is there for the same reason - to learn about search and relevance. Some attendees may be more technical than others, some may be more interested in certain aspects of the programme, but you can be pretty sure if they choose to come to your talk that they’re interested in what you’re going to say. It’s an informed but friendly crowd. One thing to note is that we’re as interested in the process around search relevance tuning as the technical details itself - how you create and manage a search team for example, as Karen Renshaw talked about at Haystack EU 2018. So it’s not exclusively technical talks that we’re looking for. If you’re looking for inspiration, do check out the past events - there’s slides and/or videos of all the past talks (yes, we’ll video your talk and add it to the collection).",
      "However, this begs the question: what will presenting at Haystack do for me (and my employer)? Well, by talking at Haystack you can demonstrate what cool and interesting projects you’re working on, why others (clients, partners, customers, potential employees) should be interested in your work and importantly you can help others facing similar challenges. They might even have some feedback that could help with your current and future projects. It can also build your personal profile as a relevance expert!",
      "At OSC we firmly believe in growing and supporting the relevance community, which is why we created Relevance Slack, which now has over 650 members (it’s a great place to hang out between search conferences), run Meetups and of course Haystacks. Interacting with each other, being open about the challenges, working together to solve them and publishing the results for others to learn from is at the core of how we think at OSC, and Haystack is the chance to do that in person. You’ll meet some great people and have some amazing and inspiring conversations too.",
      "We’re very much looking forward to the event - help us to make it the best it can be, and send us your submission today!"
    ],
    "summary_t": "Will my talk get chosen? Will attendees find it interesting? A chance to answer some of the questions you might have about submitting a talk to Haystack, the..."
  },
  {
    "id": "7265820616cc4aceeb0ab1370f04da91",
    "url_s": "https://opensourceconnections.com/blog/2007/07/26/ruby-on-windows-textmate-clone-and-ides/",
    "title": "Ruby on windows (textmate clone and IDEs)",
    "content": [
      "Ive been plowing through a lot of ruby editors for doing rails development on a windows box. I know some would say it is a lost cause. However, I do tend to do a lot of .Net development. Despite the fact i spent like 4 years in a mac lab in college, and you can run things like parallels on a mac, you always run into those extreme cases during development that can cripple your development process and spend hours, or even days to fix the issue. Sometimes you can find that you have a road block that you cant get past due to the operating system and interoperability. So rather than burning hours ( a precious resource ) fixing these special case problems, it would be best to stick with what works for now.",
      "Well Ive tried Ruby In Steel, which was not half bad, but it crippled any Intellisense in the .rhtml files and it just seemed to make visual studio somewhat clunky and it didnt provide as much Intellisense as you would expect from visual studio. As far as an IDE goes, Id have to say the Komodo IDE has the edge right now, as it also provides working with the prototype library as well as rails. There is also a Komodo Editor that has less features, but is a free download.",
      "However for those people looking for text mate on windows, I ran across the text mate blog that pointed out E-TextEditor. This editor makes use of existing text mates bundles. Now, it would be nice if you could use ctrl+space instead of tab and if the HTML bundle would have better designed snippets. But after some major tweaking of the colors and some of the bundles, its not a bad editor and will probably replace my use of notepad++, which makes use of scintilla.",
      "Sooner or later, I should make an attempt to export the color settings for both Komodo and E-TextEditor and post them, since decent color settings for a black background are hard to create."
    ],
    "summary_t": ""
  },
  {
    "id": "85bf6f07981c13d7fc8860aa6aa0ba88",
    "url_s": "https://opensourceconnections.com/blog/2007/07/28/knowing-when-to-shut-down-a-project/",
    "title": "Knowing when to shut down a project",
    "content": [
      "Property law (at least, as taught to me by Elizabeth Scott) seeks to have owned property utilized at its highest value use. Thus, according to the basics of property law, raw land has the lowest value, since its the least utilized in comparison to houses, factories, etc.",
      "The same can be said for all sorts of things Internet. One example is parked domains. According to Domain Tools, there are 71,083,043 registered domains, but 217,385,494 deleted domains. While there is value in appreciation, particularly of the potentially higher demand sites–the example of sex.com comes to mind–many sites are simply either parked or have become link farms. However, as a property rights advocate, I believe that if someone has paid a fair value for a piece of property, be it land or internet domains, then that person has the right to do with that property what they want.",
      "This does not extend to dead projects, though. There are currently 153,954 projects registered on",
      "SourceForge, and a long tail truly exists for these. Take the 950th most popular project, the Jedi Code Library. It averages about 100 hits a day. This is for a project that is in the top .5%ile of the existing projects on SourceForge.",
      "While a project like Jedi Code Library certainly is still fulfilling a need, there are many, many projects down the charts which are either no longer serving the needs of the users or, more likely, never fulfilled those needs. Keeping them on the SourceForge servers is certainly fine (I do not deign to recommend to the SourceForge team what they should do with their disk space), for projects that have become overcome by events, the owners should consider winding down the project. This will serve two purposes. It allows the better, or more relevant projects, to rise to the top, get more viewership, and, therefore, serve the community better. Secondly, it allows the developers who are on the team still piecing together the code to keep it running long past its expiration date to free up valuable time to work on other, potentially more useful projects.",
      "The projects are not property, per se, but the time of the developers is a valuable resource. Tie up projects which have served their useful life and put them in a state where patches will no longer be applied. Refocus developers who are donating their time to projects where the hours can have a great impact. Sometimes houses need to be razed so that newer, more livable houses can be built. The same can be done with projects which have run their useful lives, where the hours spent have passed the point of diminishing returns."
    ],
    "summary_t": ""
  },
  {
    "id": "0ca4b6c5cfd6fa3d69d41832b77f30d5",
    "url_s": "https://opensourceconnections.com/blog/2007/07/31/is-poker-the-newest-vc/",
    "title": "Is Poker the Newest Venture Capitalist?",
    "content": [
      "Some deep little part of nearly everyone I know has that inner burning desire to start up their own company. They dont know what they want to do, how theyre going to do it, or even why they want to, but the desire is there. Most of it has to do with the notion of getting out of the day-to-day grind of the 9 6 to 5 11 job, the Dilbert boss, the monkey-like sycophantic coworkers whose seeming purpose in life is to say \"yes\" to requests as many times as possible during work hours. Theres also the dreams of making it big, getting rich quick, and living the life of luxury.",
      "Similar aspirations drove us to start OSC. However, it wasnt a hasty decision, and weve learned some lessons along the way.",
      "Have a backup source of income. The guys at ClearContext played poker on the side to pay the bills. Others have spouses who work and can afford to be a single income family. Others rode Enron to the top and got off at the right time. Unless you get some sympathetic angel to fund your income or you have an idea that is an immediate hit and provides revenue right away (which can be done if youve doing the on-the-side work for a while), youre not going to be cashing a paycheck for a while.",
      "**Make sure that your partners all trust each other. ** Most startups are a slog. Most dont work. Theres a lot of sacrifice and a lot of risk involved. If youre adding to the risk and the stress by working with people you dont trust and believe in, then youre exponentially increasing the probability that youre going to be one of the majority of failures.",
      "**Figure out your make or break timing. ** It can be mentally lucrative to think that prosperity is right around the corner. Dont be the Herbert Hoover of your startup. Its much easier to continue to sink time and money into a failure than it is to admit that it didnt work and its time to move on. Set that break point up front and stick to it. Learn your lessons to apply the next time rather than grind yourself into a bankrupt oblivion.",
      "Network like your life depended on it. It does. Figuratively. The life of your company certainly does. Unless you happen to be the one in a million viral idea that catches on with the world, youre going to have to go out and find customers. The only way to do that is to pound pavement. The best idea in the world that sits in your head will always have an audience of one.",
      "Yes, I was inspired to write this because theres a successful start-up out there which funded itself out of poker winnings, for which I am endlessly jealous, but it also caused me to think about how we ran for a while. If it werent for the things above, we would have been forced to take rash risks which would have most likely ended up with us being a failure and going back to real jobs."
    ],
    "summary_t": ""
  },
  {
    "id": "04d4c50e9eab5bfb0be9d11fecde1f14",
    "url_s": "https://opensourceconnections.com/blog/2007/08/09/beware-of-missing-gems-in-rails/",
    "title": "Beware of missing Gems in Rails!",
    "content": [
      "Ive been getting the incredibly cryptic message undefined method options for []:Array (NoMethodError) from Mongrel on startup when moving from one laptop to another.",
      "I googled around, and finally realized that it was due to missing gems. I like to think that this kind of error is just due to Rails immaturity. The Rails community hasnt decided how to manage reusable code. You can either use plugins, gems, or one of the various hybrid tools that make gems into plugins! It does highlight that until we have tools to verify that our dependencies are available, versus blindly throwing a cryptic misleading exception, that the best approach is to \"Vender Everything\".",
      "Im going to take a stab at putting all our Gems that dont have native code into our /plugins directory, hopefully reducing the amount of hunting around the source to identify what dependencies there are that new folks have to go through. And those that cant go in the /plugins directory I am wrapping in a bit of code to put out a pretty message:",
      "begin\nrequire `cache\nrescue RuntimeError\nraise \"Please install ruby-cache gem\"\nend",
      "Reducing the time it takes for a new developer to get up and running on a project means less frustration, less hand holding, and a better experience for everyone!"
    ],
    "summary_t": ""
  },
  {
    "id": "0baaec81319c138a04992833cc77f64a",
    "url_s": "https://opensourceconnections.com/blog/2007/08/16/beware-of-dashes-in-rails-plugin-names/",
    "title": "Beware of – (dashes) in Rails plugin names!",
    "content": [
      "Continuing in the vein of my last blog about Beware of Missing Gems in Rails! comes my discovery of – (dashes) being verboten in plugin names. About a month ago I looked at using Selenium On Rails in an app since I liked using it in the first one I worked on. However, I kept getting an error message:",
      "The error occurred while evaluating nil.empty? from",
      "./script/../config/../vendor/rails/activesupport/lib/active_support/dependencies.rb:202:in 'load_file';\nfrom ./script/../config/../vendor/rails/activesupport/lib/active_support/dependencies.rb:94:in 'require_or_load';",
      "So I gave up, thinking that it was because of some weird interaction with another plugin since I had so many installed for the project. Today I came back to it with a nice simple project, installed things and had the same error! Argh… So I joined the mailing list, looked around and found a post mentioning that in \"Edge\" Rails, which is now Rails 1.2.3 the name of the plugin needed to be changed from selenium-on-rails to selenium_on_rails! I’;ve looked around at some other plugins and realized that they all use _ characters in the name! transaction_migration, annotate_models etc.",
      "Amazing how Rails can make life difficult over the most innocuous things. I am fairly sure this is never an issue in the Java world! Of course, every plugin would be implementing a very complex IDynamicActiveReloadablePlugin interface :)"
    ],
    "summary_t": ""
  },
  {
    "id": "7b636a1f0f0a03f2fbf19e7f8c0bd24b",
    "url_s": "https://opensourceconnections.com/blog/2007/08/30/sucking-in-footer-from-remote-sites/",
    "title": "Sucking in the header and footer from a remote site",
    "content": [
      "How do you share a common header and footer across multiple apps? For example, you might be prototyping a new app, and want to borrow the main sites header and footer. Obviously you can do crazy stuff like have JavaScript pull something via the client, or maybe have iframes or frames. But what if you have to massage the content as it comes in, maybe only keeping part of it?",
      "Well fortunately Ruby on Rails makes that very easy. We simply created a Model object that uses HPricot to pull in remote content and grab the portion we want:",
      "require 'hpricot&';\nrequire 'open-uri'\n\nclass CatalogContent\ncattr_accessor :remote_url\n\ndef self.footer\nurl = remote_url\nraise \"Remote URL was not set\" if url.nil?\n\nputs \"\\nPulling remote content footer from #{url}\\n\"\n\nbegin\ndoc = Hpricot(open(url))\nfooter_content = doc.at(\"//div[@id=footer]\")\nraise \"Didn't find footer content in site #{url}\" if footer_content.nil?\n\nfooter_content\n\nrescue Errno::EHOSTUNREACH\nfooter_content = \"Can't connect to #{url}\"\nrescue Errno::ETIMEDOUT\nfooter_content = \"Can't connect to #{url}\"\nrescue SocketError\nfooter_content = \"Can't connect to #{url}\"\nend\nend\nend",
      "In this case we are loading up a website and pulling out the footer section in the HTML. We have the option here while manipulating the HPricot XML DOM to do any other types of changes we want such as:",
      "Remapping URLS\n  Snipping out bits of content\n  Changing colors or stylesheet classes\n  Conditional inclusion of content",
      "And then, we have a ApplicationHelper.rb method that creates the model object, populates the remote url and displays it:",
      "def remote_footer\nCatalogContent.remote_url = AppConfig.catalog_remote_url\nCatalogContent.footer\nend",
      "Of course, people are going to jump on the fact that this means that every page is making an extra URL call right? Fortunately caching comes to the rescue in our layout:",
      "<div class=\"footer\">\n  < % cache(:controller=> 'remote_content', :action => 'remote_footer') do %><br /> < %= render(:partial=>'/partial/bottom_panel') %><br /> < % end %>\n</div>",
      "Note, however, we don’t actually have a controller called remote_content with an action called remote_footer, you just have to have those bits so the caching support knows what keys to store the cached content under! Could have been:",
      "< % cache(:controller=> 'foo', :action => 'bar') do %>\n< %= render(:partial=>'/partial/bottom_panel') %>\n< % end %>",
      "And in our partial we just have:",
      "< %= remote_footer %>",
      "So, the only downside is that in development we are constantly pulling in the content because the caching is turned off, which slows things down a bit, and that the only way (out of the box) to update the cache is to \"sweep\" out the contents of the /tmp/cache directory.",
      "Also, be aware that if what you are importing has lots of relative paths or references relative JavaScripts and CSS that you are going to have to make those parts work with your app nicely!"
    ],
    "summary_t": ""
  },
  {
    "id": "998ac814222b0e43ae77bf9b26421242",
    "url_s": "https://opensourceconnections.com/blog/2007/08/31/charlottesville-needs-more-nerds-part-deux/",
    "title": "Charlottesville Needs More Nerds, Part Deux",
    "content": [
      "Eric ran into some fellow Darden grads, Adam Healey and Charles Seilheimer yesterday at a local coffee shop because he was lured into a conversation about Ruby, Twitter, and Web 2.0 goodness. The discussion led me to go check out their blog, and I found an excellent article about the ingredients necessary for creating the right environment to foster startups.",
      "We run into the same issue that Adam brings up–there are not enough nerds in this area. While we certainly are comfortable leading in a location independent enterprise (the subject of our talk at OSCON 2006), having most of your people co-located does reduce managerial burden on cat herding. Its not quite as easy to have a look over the shoulder to iron out a gnarly coding problem when your team is spread across three continents as it is when youre sitting next to each other in a bullpen.",
      "There are other items coming up on the horizon which further point out the need for more nerds (besides the need to keep Charlottesvilles coffee shop economy booming):",
      "–The National Ground Intelligence Center is growing. The Global War On Terror is one that focuses on using technology to interdict the bad guys before they can do harmful things to us. The National Ground Intelligence Center is one of the main nexuses of information collection and analysis. Until we beat the bad guys, the NGIC is going to continue to get business, and it needs nerds to help gather that technology.",
      "–The Defense Intelligence Agency is coming to Charlottesville. Theyre bringing about 1,000 jobs to the 29 North corridor, and that doesnt include everything needed to support them (like coffee shops). Just like the National Ground Intelligence Center, the Defense Intelligence Agency relies on nerds to make sense of signal intelligence and human intelligence. There are currently not enough nerds to go around.",
      "–The surplus of nerds from the Value America days has worked itself through the system. Right after Value America closed shop, the programmers from the company had time on their hands. Many of them started up ventures. However, venture capital money dried up, and they were forced to, for the most part, find \"real\" jobs. The days when labor was plentiful are no longer in the Charlottesville region because the Value America refugees have become absorbed by the greater economy.",
      "–Venture capital deals are on the rise. The number of deals is up, but the average per deal is down. This means that startups have to focus more on delivering the goods and less on the posh offices. In order to make this happen, startups will need good, solid nerds to get to market faster and bring in revenues more quickly. Nice chairs do not bring in revenue. The Virginia Piedmont Technology Council – Charlottesville Venture Group is working with the City of Charlottesville to determine a strategy to bring technology-based businesses into the central Virginia region. Technology companies are not going to want to come to a place where there are not enough workers to fulfill their needs. As it is, existing technology employers who are looking to grow are having significant trouble growing organically with hometown growth. The growth of the contractor community around the National Ground Intelligence Center is being pumped by imported talent with people moving in from the D.C. area rather than with people in Charlottesville. When the Defense Intelligence Agency comes down, it will be the same story.",
      "So, how to get more nerds to Charlottesville? Adam touched on one step in his blog article, which was focusing on improving the quality of the engineering school at the University of Virginia. He makes a fine suggestion about making a game-changing move and shelling out millions to bring in the rock star professors to draw more students and improve the quality of output at UVa. While this will work in the long run, it will take time for the trickle down effects to happen. What else can we do?",
      "–Give the incubators some teeth. While Charlottesville has incubators such as the Batten Institute and Spinner Technologies, they really dont provide enough of a runway for entrepreneurs to solidify and implement their ideas. Trying to live on a $1,000 a month stipend while slaving away on your laptop for 25 hours a day simply will not cut it in Charlottesville. The cost of living is too high. Not every great nerd has a burning desire to live on ramen and rice cakes for two years while trying to make his great idea a reality. The reality is that great nerds can get jobs which pay them well, and most of those jobs are not in Charlottesville, Virginia. So, in order to draw in the great nerds to the small businesses and the entrepreneurial ventures which will lead to the next big employment opportunities, the incubators have to put significant investment into the worthy incubator seatholders. This is not so that we can pay nerds tidy sums to do nothing. It is so that we can draw in the motivated great nerds who could otherwise go work for Google. Yes, the incubators will have to blur the line between incubator funding, angel funding, and venture capital funding, but otherwise, the good nerds will go elsewhere, and so will the businesses which employ them and many of their friends.",
      "–Get the local governments in on the action. Universities and foundations shouldnt be the only ones that are in on the incubator game. The city of Charlottesville, Albemarle county, and the surrounding counties have a vested interest in bringing in more nerds. It seems that most people who are interested in the question would rather have Charlottesville turn into Austin than Aspen, and the governments have to take part in solving that equation. Rather than spending money on Art In Place or on a bus station that takes you nowhere, spend it on things which will breed more employment. If I had $11 million to invest in budding entrepreneurial technology-based companies in Charlottesville, I am willing to bet my mortgage that I could grow jobs in the Charlottesville area and grow the citys base of tax revenue. The local governments in the area need to partner together, because this isnt a matter of divvying the pie; its a matter of growing the pie. A growing company based in Charlottesville will have residents who live in Albemarle, Green, Augusta, and other counties, who will use goods and services in those areas as well.–Look for public-private partnerships. What major corporation wouldnt want to have its name attached to the partnership which spat out the next Google? Furthermore, it would get the benefits of the goodwill associated with bringing in new jobs and philanthropy to the region. Everybody wins. The corporation gets to hand-pick the best nerds for its own ventures and gets the positive press that goes along with a public project. The local community gets more money to fund more ventures which bring in more jobs.",
      "–Recruit nerds who live elsewhere. Washington, D.C. is a two hour drive from Charlottesville when the traffic is not abysmally jammed. Naturally, traffic is abysmally jammed all of the time. Nerds (and everyone else) hate traffic jams. Charlottesville does not have them. So, go up to D.C. and talk to the nerds there about the joys and low-stress living that Charlottesville offers. Heres one group and another if you need a start. Were talking to their groups, but we are a drop in the ocean. The incubators and local governments need to wade into the waters. By the way, these nerds who you import can also act as great mentors for the young batch of nerds that we should be growing in the University who came in because of the rock star professors we brought in with the $150 million in allocated funds.",
      "–Tax breaks? How about tax credits? The City of Charlottesville is going in the right direction with a 50% tax break for technology-based companies, but its not enough. Offer tax credits instead. Dont worry about losing out on tax revenues, because youll make it up in spades from the knock-on effects of having larger employers in your region because of the incentives you offered for them to take up shop and hire employees here. Its accelerated supply-side economics working for you. If you need more help understanding this, offer to take Dr. Alan Beckenstein out to lunch. Youll get an amazing return on investment for the $10 that you spend feeding him while he enlightens you.",
      "–Prime the pump with the Piedmont Virginia Community College. For every great nerd we bring in, we need a couple of worker nerds to implement the supernerds great strategy. Grow those worker nerds in our own back yard as well. These people can continue in their day jobs while they become worker nerds at night. PVCC has repeatedly stated its willingness to increase the number of technology-based classes it offers, so take them up on the offer. PVCC is a great resource for helping get more nerds here, but its an underutilized resource.",
      "Growing a nerd population is not going to happen overnight. However, there is still time to start growing that population before the demand spikes, potential employers become frustrated at the lack of supply, and pick up and move elsewhere, leading Charlottesville to become Aspen without the great skiing. Austin did it through both importing the employers and growing them organically, and it had a great educational system at its back pumping out nerds by the bucketload. It offered incentives for companies to take root, and it offered the people to help the companies execute their missions. By combining programs to bring in and grow the nerd population of Charlottesville as well as generating the employment to keep them here instead of watching a mass nerd exodus every May at graduation time, we can build a vibrant technology community. Otherwise, we are always going to be a second-tier player posing as a first-tier location. The saying in Texas is \"all hat, no cattle.\" Nerds are the cattle that will drive Charlottesvilles 21st century economy and keep it from becoming an exclusive enclave where few can afford to live and jobs are scarce."
    ],
    "summary_t": ""
  },
  {
    "id": "65d3513b00b6a7ac6feae2182ee1441b",
    "url_s": "https://opensourceconnections.com/style-guide/",
    "title": "Blog Post Title",
    "content": [
      "",
      "##h2 Heading Style\nThis is a sample paragraph - the first text block will appear as the excerpt in blog archive pages. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum nec felis ac magna auctor scelerisque. Aliquam condimentum aliquam ligula a blandit. Aenean congue tortor sit amet nulla laoreet rhoncus. Donec semper viverra ex in feugiat. Vivamus rhoncus, ipsum non placerat luctus, massa diam ornare risus, eu aliquet arcu nisl id ipsum. Aliquam sit amet nisi risus. Vivamus quis ante vel dolor vulputate bibendum sit amet id nibh. Integer suscipit placerat ligula, sit amet congue dui. Curabitur elementum, lectus non eleifend lacinia, ipsum massa vehicula ipsum, quis vehicula metus purus ac nunc.",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum nec felis ac magna auctor scelerisque. Aliquam condimentum aliquam ligula a blandit. Aenean congue tortor sit amet nulla laoreet rhoncus. Donec semper viverra ex in feugiat. Vivamus rhoncus, ipsum non placerat luctus, massa diam ornare risus, eu aliquet arcu nisl id ipsum. Aliquam sit amet nisi risus. Vivamus quis ante vel dolor vulputate bibendum sit amet id nibh. Integer suscipit placerat ligula, sit amet congue dui. Curabitur elementum, lectus non eleifend lacinia, ipsum massa vehicula ipsum, quis vehicula metus purus ac nunc.",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum nec felis ac magna auctor scelerisque. Aliquam condimentum aliquam ligula a blandit. Aenean congue tortor sit amet nulla laoreet rhoncus. Donec semper viverra ex in feugiat. Vivamus rhoncus, ipsum non placerat luctus, massa diam ornare risus, eu aliquet arcu nisl id ipsum. Aliquam sit amet nisi risus. Vivamus quis ante vel dolor vulputate bibendum sit amet id nibh. Integer suscipit placerat ligula, sit amet congue dui. Curabitur elementum, lectus non eleifend lacinia, ipsum massa vehicula ipsum, quis vehicula metus purus ac nunc.",
      "#h1 Heading Style\n##h2 Heading Style\n###h3 Heading Style\n####h4 Heading Style\n#####h5 Heading Style\n######h6 Heading Style",
      "Candy.\n  Gum.\n  Booze.",
      "Red\n  Green\n  Blue",
      "example link",
      "This is a blockquote.",
      "This is the second paragraph in the blockquote."
    ],
    "summary_t": "Solr and Elasticsearch relevance, performance, and solutions"
  },
  {
    "id": "ceacf436a29a4eabe7c776de7fb7132a",
    "url_s": "https://opensourceconnections.com/blog/2006/08/02/scrum-war-stories-recap/",
    "title": "Scrum War Stories Recap",
    "content": [
      "As I promised at the close of our BoF: Scrum War Stories, I would write up a couple of the notes I made during our discussions. They arent in any specific order, and I only have some of the names of people who spoke written down Unfortunately those name tags at OSCON seemed to really want to flip around! If I ever host a conference, I am putting the names of people on both sides of the badge.",
      "Before I begin I want to thank everybody who showed up! 14 people was a very healthy number of folks. Enough so that we can generate some real ideas, with people who were just learning about it up to people who are certified ScrumMasters and have been using it in the real world.",
      "Mauro Talevi led off with a quick introduction on Scrum. One of the key points he made is that while processes like XP or RUP are \"prescriptive\", Scrum is \"adaptive\". XP says: \"If you take my medicine faithfully and fully, then things will be better\", whereas Scrum says: \"Lets honestly try some principles that are very common sense and see if we can change peoples behavior\". I really appreciated that distinction. Medicine versus Therapy.",
      "He also pointed out that Scrum is a process for developers, but first and foremost is a method of communicating with the rest of the business. And that because it extends outside of the development team, what seems simple is actually incredibly hard to implement. And that lip service to Scrum processes doesnt work.",
      "We then chatted a lot about Jasons situation where he has internal clients at a university. He wondered if you could use Scrum with internal clients. He wants to use it, has been championing it, and it hasnt gone anywhere. After the group asked some more questions about his situation, in many ways Scrum was not a fit. In his situation, the business folks were not onboard with Scrum, and his attempts to gather requirements was failing. The advice was that:",
      "another job was better! A couple folks with university backgrounds had experience frustrations of the type that Jason was feeling.\n  on a more constructive side, the group said he should just get enough requirements to start a prototype, and once the clients had something to look at, the rest of the requirements would become solid. Even if the original prototype was tossed out.\n  Use interview process to elicit requirements. Someone mentioned a tool called DENIM (Denim?) for drawing simple GUIS for use in interviews. Another mention was made of using storyboard and tool that helps with that. Search Google for \"XUL + Berlin\".\n  Scrum doesnt work without a team. If you are a developer of one then you cant do Scrum. I showed how I organize my day to day life with a nice notebook that I scribble in. A notebook with \"what I have to do\" is not a stand up meeting!",
      "The conversation drifted away from Scrum to requirements gathering for a while. The question came up, how do you take a 30 month project and figure out what you can do in 4 months of work. The answer seemed to be \"ask the client, they know at some gut level what the minimum is\", or cancel the project completely, and solve the problem in a different, not IT way.",
      "Aaron Farr then brought the topic back to Scrum as he said he thinks Scrum works even better with internal customers then external clients. A consulting firm with external clients has to justify every billable hours, so there is more of a framework and focus on managing scope and deliverables. Internal customers rarely are directly paying the developers, so they feel more leeway to change things around. This feeds directly back to the usefulness of Scrum. He also pointed out that you should let \"Management be management\". They set the budget and tasking. You as a developer dont. All you set is time.",
      "The improvement in your team, as measured by your Velocity was brought up. Was it a factor of using Scrum? Was it just the team becoming better developers? Or is it that over time you become more familiar with your domain? Some suggestions was that the additional stability provided to your team by Scrum leads to better developers (less stress) and the interactive approach helps build knowledge of the domain.",
      "A scary question was asked: \"Can you lose your job?\" Mauro replied yes, and explained that someone he knew in Scrum circles had \"lost\" their job because their job was to introduce Scrum, and while management had asked for Scrum, they were not willing to change their processes. Lip service was what they were looking for, and the ScrumMaster they brought in wanted to make actual behavior changes in how software development was handled. It was a sobering anecdote that ScrumMasters are often \"Change Agents\" which may ruffle a lot of feathers. And without management support, that can be a disaster.",
      "\"Slack\" by Tom DeMarco was mentioned as a good book for people interested in Scrum. I hadnt heard of it, but added it to my Amazon wish-list!",
      "Lastly, we talked about tools. Tools, always a favorite for software developers! Someone brought up a web based wiki/spreadsheet, and after a bit of confusion we realized it was Dan Bricklins project. Dan and I gave presentations at a symposium once, and I learned a huge amount from him! I also mentioned \"Rugby\", which is almost ready for public consumption. Ill be posting more fully on it later, but it will be available from http://code.google.com/p/rugby/ . There is a Google Group where I will be posting more information.",
      "I was very gratified at the turnout for my BoF, and am looking forward to hosting more at future conferences! If you have any questions, want to talk more about Scrum, please feel free to contact me at [email protected] . Thanks to everyone who showed up!"
    ],
    "summary_t": ""
  },
  {
    "id": "090280ae0ec977f4f9a27325edc7b6ee",
    "url_s": "https://opensourceconnections.com/blog/2007/09/01/javascript-intellisense-documentation-visual-studio-2008-orcas/",
    "title": "Javascript Intellisense, Documentation, & Visual Studio 2008 (Orcas)",
    "content": [
      "I’ve been working inside the quasi evil yet intriguing world of javascript for the last several weeks due to current projects and the web side of my BHAG project (Amplify.Net framework).  I’ve been rethinking a lot things, including spinning the Javascript into its own opensource project that works with the framework and without the framework.  I’ve also had to ask myself, how do I make this easy for developers to leverage/use? And how do go about streamlining  the documentation process for javascript code.",
      "In building this library, since I’m building most of my own controls in order to keep things simple and be able to wire them up to .NET server controls, I’m leveraging the Prototype Library so far, because its not bulky (yet), compared to the Asp.Net Ajax Library.  Also css selectors are not yet available in the Asp.Net Ajax library, but they are planned for a future release.",
      "I’ve also have been testing the new Javascript features of Visual Studio 2008 (Orcas). Don’t get me wrong, when I’m working in ruby, I’ve been digging the e-texteditor, which even has some features the mac version does not, like close all tabs and you can shift + tab code instead of shift + [.  However, intellisense is a compelling feature when it comes to speeding up development, also having inline documentation bundled with intellisense is a win/win for developers.",
      "Granted, Visual Studio 2008 Beta 2 (Orcas) has improved javascript intellisense, but there are still too many gotchas…",
      "Intellisense Gotchas",
      "Gotcha 1.) Visual Studio loses intellisense for all scripts if it is unable to parse one.  If you load a script like prototype.js, that doesn’t follow the rules Visual Studio can handle, it kills the intellisense for all other scripts that in the format Visual Studio can handle. ;",
      "<script type=\"text/javascript\" src=\"../javascripts/prototype.js\"></script>\n<script type=\"text/javascript\" src=\"../javascripts/amplify.js\"></script>",
      "or",
      "/// <reference path=\"prototype.js />\n/// <reference path=\"amplify.js />",
      "This is what happens when its working.",
      "This is what happens when you add a good library that Visual Studio can’t figure out.",
      "Gotcha 2.) Visual Studio crashes when you use dojo functions.  If you have a complex library like dojo and you attempt to do something \"dojo.style\", this crashes visual studio, possibly due to the fact that the dojo style function does both get and set? or its just too much script in the global namespace for visual studio to handle.",
      "Gotcha 3.) Visual Studio is not able to do complex chaining of functions.  I love the inheritance model of Prototype 1.6.0 RC.  When you create a class using prototype, you do some thing like what is shown in the code sample below. Visual Studio doesn’t know what to make of the object \"Amplify.MockWidget\" and ends up killing any intellisense in the script and in any place you reference the script file would have the following code. You can even add xml comments and put in the type and visual studio isn’t currently smart enough to make light of that.",
      "// visual studio can't make out the object type..\nvar Amplify.MockWidget = Class.create(Amplify.UI.Control, {\n  load: function() {\n    this.element = $(this.element);\n    this.element.observe(\n      \"click\",\n      this.onClick.bind(this)\n    );\n  },\n\n  onClick: function() {\n    $log(\"hello world\");\n  }\n}); // should be of type Function, but visual studio does not see that.",
      "So what you end up having to do is creating classes using the Asp.Net Ajax style of coding, which tends be more verbose (and doesn’t seem as elegant to me).",
      "Gotcha 4.) JSON style object declaration breaks intellisense.  Visual Studio Intellisense, won’t allow you to access other properties/methods using the \"this\" keyword. So if you do the following, Visual Studio will not have the object’s functions/properties listed in the drop-down. So in the Define method below, you will not be able to see _specs when you type this.",
      "var Specification = function(name, description, functionality) {\n  this.name = name;\n  this.description = description;\n  this.functionality = functionality;\n}\nvar Spec = {\n  _specs: [],\n  Define: function(specification) {\n  ///<summary> Defines a new part of the specification </summary>\n  ///<param name=\"specification\" type=\"Specification\"> a defined piece of the total spec. </param>\n    this._specs.push(specification);\n  }\n}",
      "Gotcha 5.) Visual Studio Intellisense relies on the Asp.Net Ajax framework concepts for determining object types. So if you want a class to show up as a class, you need to add __class: true or ObjectName.__class = true; or something to that effect for visual studio to recognize the object as a class. So rather than putting that overhead in the scripts, I’m making a second script that has all this type of meta data for intellisense purposes..",
      "Gotcha 6.) Visual Studio finds errors with javascript.  However, because you can’t currently reference a complex javascript file like prototype.js, since it removes all the enhancement of intellisense, Visual Studio finds methods that are not defined that you may use from a library and puts up an error in error panel. It would be nice if Visual Studio would provide a pragma for javascript similar to C#’s pragmas that would allow you to ignore these type of errors.",
      "Documentation Gotchas",
      "Since the great NDoc is no more due to the evils of developer expectations of the Microsoft world when it comes to open source .net projects, you’re pretty left with SandCastle if you go the route of xml comments. Now if you plan to leverage the intellisense of visual studio, you’re pretty much stuck with xml comments. The only other way around this is use something like Aptana which uses JsDoc style of comments. (the YUI library also uses this format.) However, I find JsDoc style of documents ugly and I want to minimize the burden of tweaking the styled pages not to mention I don’t feel like installing and running active pearl and creating yet another ugly dependency on a project.",
      "Gotcha 1.) Parsing XML Comments In Javascript. SandCastle only parses xml documents, it does not go through code and pull out the xml format from the code files such as .js, .cs, .vb files. When you build your dll or exe, thats when visual studio pulls out those xml comments and puts them into one xml file. Right now SandCastle, nor Visual Studio handles parsing xml comments from javascript files.",
      "Enter Bertand Le Roy’s Ajax Doc which parses out the xml comments in javascript files and puts them into one xml document. However… this brings up more gotchas.",
      "Gotcha 2.) Ajax Doc requires extra dependencies. Ajax Doc uses the Asp.Net Ajax Library, which is an extra dependency on the project, however this Ajax Doc is totally written in Javascript, but requires Internet Explorer, probably for writing the xml file or something to that effect.",
      "Gotcha 3.) Ajax Doc relies on Asp.Net Ajax Object Types.  Ajax Doc only documents comments based upon the Asp.Net Ajax Object model. Which means functions who have declared class types of class, interface, enum, etc, are the only ones that get documented. So your classes must have those __class = true;  or __enum = true; defined in your objects otherwise the Ajax Doc will skip your objects.",
      "Gotcha 4.) Ajax Doc relies on the inheritance model of the Asp.Net Ajax library. This means that if you are using libraries like the new Prototype 1.6.0 RC, which has inheritance, Ajax Doc is not smart enough to be able to parse that inheritance model out to put into the xml comments file.",
      "Gotcha 5.) Ajax Doc has limited tag parsing.  Tags like <remarks> are not included, in fact the only thing it really handles at this point is on Bertand’s posting on Javascript xml comments.",
      "In closing, I’m still digging the fact that the visual studio team is putting in better intellisense for javascript, however, I’m hoping one of them reads this in order to provide better support for javascript overall, since the style of javascript is so diverse. Even if you have rely on documenting your scripts to allow for this, I think this would push well documented libraries to evolve, rather than obscure scripts that are impossible to read.",
      "track17 Naruto Bad Situation"
    ],
    "summary_t": ""
  },
  {
    "id": "6f80bc87598fc205fa0e5b97c2969c9c",
    "url_s": "https://opensourceconnections.com/blog/2007/09/07/eric-pugh-will-be-presenting-at-cvreg-sept-11th/",
    "title": "Eric Pugh will be presenting at CVREG Sept 11th",
    "content": [
      "Ill be at the CVREG next week talking about Lego Mindstorms and Ruby. Ill be demoing the acts-as-nxt plugin providing an interface between a Ruby on Rails app and a Lego robot!. As Mel Riffe puts it \"ROBOTS!\"",
      "We are participating in this weekends Rails Rumble contest, so Ill be giving a recap of that!",
      "More details are on Upcoming."
    ],
    "summary_t": ""
  },
  {
    "id": "917e0e2ed661d7da4ffbd8a08ebe90af",
    "url_s": "https://opensourceconnections.com/blog/2007/09/09/lets-get-ready-to-rumble/",
    "title": "Let’s Get Ready to Rumble!!!!",
    "content": [
      "Three of our team members, Eric Pugh, Michael Herndon, and Jim Nist, along with Ashish Tonse from Booz Allen Hamilton, are currently in the midst of throwing together last-minute fixes and tweaks for our entry in the Rails Rumble 2007 contest.",
      "On Wednesday Thursday, voting will begin, and at that time, well unveil our ultra-cool application. For now, sign up to be a judge for the Rumble so that you can let the world know what you think about our application when we make the unveiling.",
      "Cant wait? Come to the Neon Guild meeting Monday, September 10, 2007 at 6:00 in the Inova building and get a sneak peek.",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "d9c605a7dbcb9da2dce2f744f770212a",
    "url_s": "https://opensourceconnections.com/blog/2007/09/12/agile-open-california-oct-23-24/",
    "title": "Agile Open California Oct 23-24",
    "content": [
      "Looking for a great Open Spaces format conference? Agile Open California in San Francisco may be right up your alley! I first heard about this conference from Vandana Shah of Agitar during CITCon this past year. Vendana is leader in implementing better software development practices, and is one of the hosts for Agile Open California. People like her are sure to attract really interesting participants, and make this an exceptional conference. The conference should stay small and intimate since the size is capped at 100 people. I would be there except for the impending arrival of my son on October 23rd!",
      "The website is at http://www.agileopencalifornia.com/ and there is some signup information at Upcoming.org.",
      "Oh, and if the content isnt enough to make you go, it is taking place at Ft Mason in the Golden Gate National Park!"
    ],
    "summary_t": ""
  },
  {
    "id": "2ba0502e3255c5e062d076b8a702aa51",
    "url_s": "https://opensourceconnections.com/blog/2007/09/14/fish4brains/",
    "title": "Fish4Brains",
    "content": [
      "We demonstrated Fish4Brains at the Neon Guild meeting on Monday. The voting on the Rails Rumble website is now live. For those of you who were not at the Neon Guild meeting, the Rails Rumble was a contest where 150 teams of 1-4 developers had 48 hours to create a Ruby on Rails application. 48 hours and 238,053 lines of code later, Fish4Brains was born.",
      "Wed appreciate it if you could go to our website at http://www.fish4brains.com and take a look at our fish and vote on it. Also, if you want to provide feedback on what youd like to see in the updated version, you can send a note to [email protected]",
      "Thanks, and have fun fishing!"
    ],
    "summary_t": ""
  },
  {
    "id": "14a6832147c80f06601afd42d9183206",
    "url_s": "https://opensourceconnections.com/blog/2007/09/21/saic-and-the-armys-pathfinder-intelligence-application/",
    "title": "SAIC and the Armys Pathfinder Intelligence Application",
    "content": [
      "SAIC has been a leading developer of intelligence analysis software for decades. In fact, their Pathfinder application has seen continual usage at a variety of agencies since the early 90s. It has been periodically updated with newer and better technologies as they emerged, and OpenSource Connections was an integral part of one of these latest efforts. Over the course of fourteen months, across two project teams, we helped update both the front and back ends of the application.",
      "The front-end team improved the search functionality by developing a .Net / Service Oriented Architecture-based user interface. To ease the transition to the new interface, old views were ported, and new views were built to maximize the new capabilities, including toolbars, applets, plug-ins etc. The updated interface decreased the technical-know-how required of the analysts performing the queries, thereby allowing them to focus on the information related to the search rather than on the technical parameters required to perform the search.",
      "For full-text searching, the open source Lucene project has singled itself out as an invaluable tool. Its speed and accuracy are second to none. It is also documented to work well with a variety of programming languages, which grants it greater future flexibility. SAIC had previously integrated it into Pathfinder by enhancing its query language and plugging it into their data feeds, but the new implementation needed to be tied directly to the central Oracle database.",
      "To accomplish this, the back-end team indexed the text with Lucene, but only stored the record identity in the index. Searching would take place completely within Lucene, but whenever the actual text was called for it would be retrieved from Oracle. Since the database was already integrated into a great number of data feeds, this instantly added them to Pathfinder and allowed it to leverage the data access security protocols already in place.  This solution not only saves the client money in its lack of licensing fees but offers better forward compatibility.",
      "Finally, we deployed Xen virtualization software for use by both teams as a development and testing platform. This saved additional dollars by effectively turning two servers into twenty and allowed the teams to quickly create whatever server environment their task called for."
    ],
    "summary_t": ""
  },
  {
    "id": "763c027bfb4301068a8f3a830db174aa",
    "url_s": "https://opensourceconnections.com/blog/2007/10/02/mitre-healthstoriesorg/",
    "title": "MITRE–Healthstories.org",
    "content": [
      "To address the growing attention on healthcare, the MITRE FFRDC in Bedford, Massachusetts engaged OpenSource Connections to build a website that would facilitate the exchange of patient health care experiences.  Healthstories.org was born. The website also allows medical professionals to share their experiences.  The ultimate goal of the site is to help identify challenges in the current health system.",
      "",
      ""
    ],
    "summary_t": ""
  },
  {
    "id": "9c92cefbe0dc57117d2aa746f600eda4",
    "url_s": "https://opensourceconnections.com/blog/2007/10/03/eric-pugh-presents-to-rubyjam/",
    "title": "Eric Pugh presents to RubyJam",
    "content": [
      "October 10th at 6 PM Eric Pugh will be presenting to the RubyCodeJam group some cool hackery with of Ruby and Lego Mindstorms, as well as share some lessons in using BackgroundRB with Rails."
    ],
    "summary_t": ""
  },
  {
    "id": "789d965f6df56cf122aaea06ba786094",
    "url_s": "https://opensourceconnections.com/blog/2007/10/04/is-cmmi-overrated/",
    "title": "Is CMMI Overrated?",
    "content": [
      "We are not a CMMI rated company. As a small business, we think it is more important to focus on delivering excellence for our customers and growing top and bottom line numbers than being able to check certain boxes about whether or not we have a laundry list of processes documented and implemented. The causation seems to be backwards. We do things which please our customers. Our customers are pleased; therefore, we seem to be doing the right thing, QED. Having a CMMI certification does not guarantee anyone that we will perform in a manner which will satisfy our customers, only that we will perform in a manner which will ensure we can check boxes.",
      "What is CMMI, you ask? According to the Carnegie Mellon Software Engineering Institute CMMI page:",
      "\"Capability Maturity Model® Integration (CMMI) is a process improvement approach that provides organizations with the essential elements of effective processes. It can be used to guide process improvement across a project, a division, or an entire organization. CMMI helps integrate traditionally separate organizational functions, set process improvement goals and priorities, provide guidance for quality processes, and provide a point of reference for appraising current processes.\"",
      "A sample of the 23 different areas that CMMI measures includes (quoted from the CMMI v 1.2 manual):",
      "· Causal Analysis and Resolution (CAR)",
      "· Configuration Management (CM)",
      "· Decision Analysis and Resolution (DAR)",
      "· Integrated Project Management +IPPD (IPM+IPPD)\n\n  \n\n  [1]",
      "· Measurement and Analysis (MA)",
      "· Organizational Innovation and Deployment (OID)",
      "· Organizational Process Definition +IPPD (OPD+IPPD)",
      "· Organizational Process Focus (OPF)",
      "· Organizational Process Performance (OPP)",
      "· Organizational Training (OT)",
      "For an organization which is not delivering the intended results to its customers, revising and revamping the processes is a good thing. I by no means wish to say that I do not believe in process. Process, within reasonable limits, is good. We are proponents of Agile Development processes. With a complete lack of process, a project will end up with variable timelines, bugs, few (if any) test, and, most likely, a product that suits the developers needs rather than the customers needs.",
      "To me, the question is how many companies who are CMMI-X level certified companies actually implement the CMMI processes fully rather than just showing the process in a project in order to get the certification? As Dr. Rick Hefner states (p.7), \"A CMMI appraisal indicates the organization€™s capacity to perform the next project, but cannot guarantee that each new project will perform in that way.\"",
      "According to this Washington Technology article, the trend in government contracting is to mandate CMMI maturity levels. So, government contractors who wish to do more business with the government look to get CMMI certified so that they can win more contracts. The government thinks that enforcing certain process standards will yield more predictability in results.",
      "Yet, this does not always happen. Government contracting officers and IT managers with whom I have spoken complain that CMMI level 4 and 5 companies are consistently delivering poor code bases, usually over budget (and/or late), and often full of critical bugs that are introduced into live environments.",
      "Where is the disconnect, then? It happens from both sides.",
      "Providers:",
      "Focus on the customer: The prettiest code or the coolest algorithm is worthless if it does not deliver what the customer wants. The scrum master and the development team should be in harmony with the product owner and business customer (if they are not the same person) in terms of vision and outcomes.\n  Test, test, test: Code should not be released into a live environment unless it is unit tested, string tested, integration tested, and user tested. Fixing the nth incremental bug at a negative marginal utility cost is not the goal. Catching the critical bugs early is the goal. If the team focuses on test-driven development, then they will eliminate much of the overhead necessary to deliver working products at the end of the project.\n  Prototype early: By developing form and function prototypes, the development team can give the customer an idea of what is going to be delivered, and the customer can provide intervening feedback to keep the development team pointed in the right direction. Prototyping reduces the amount of intent interpretation necessary, as the customer gets to see what the results will look like rather than hoping that the development team can divine intent.\n  Dont drown in unnecessary documentation: Each report that the team has to deliver reduces the amount of time that the team can spend coding. Stick to burndown charts and product backlogs as much as possible. Scrum masters should push back where possible on excess reporting. Providing periodic performance updates is appropriate; crafting 800 page requirements documents is rarely so. Dont shirk from documenting code, though. Someone has to figure out what the team did long after the team is gone.",
      "Customers:",
      "CMMI certification is not a magic bullet: Dont expect that just because a company has a CMMI certification, theyre going to magically deliver under budget, early, and with more scope than you thought. Expect the appropriate amount of coding rigor to deliver on what you need. Ask where the code tests are. Demand prototypes. Own the product backlog.\n  Keep a gentle, guiding hand on the team, not an oppressing fist: Just as not enough oversight ruins the outcome, so does micromanagement. Expecting TPS reports every week means that youre going to have a team focused on filling out satisfactory paperwork rather than delivering outstanding code. Give guidance, vision, and direction; expect rigor, and get out of the way.\n  Provide performance-based incentives: Periodic payments should keep the lights on with your provider. All of the profit should come at the end when the product delivers what it is supposed to, on time, and meeting the customers intent. Hourly fees are an invitation to print money. Even if the long-term total cost is unknown, set up per sprint requirements and hourly estimates and then hold the provider to those. For example, for a 3 year project, set up monthly (or whatever appropriate interval) sprints and get agreement on sprint deliveries and cost for each sprint, while holding out a percentage of the overall compensation to the end of the project contingent upon mutually agreed upon final deliverables. If those need to change, then negotiate the deliverables through the course of the project.\n  Look at other customers besides those provided by the contractor in the proposal: Requests for proposal are inherently subject to selection bias. As a contractor and proposal provider, we are going to pick our 3 happiest customers and provide them to the requestor. Look deeper into the picture and see if all of the stories are the same. Request a dissatisfied customer in the proposal and dig into why the customer was dissatisfied. Nobody (not even us) has no unhappy customers. Everyone makes mistakes. Find out if the mistakes are systemic or have been corrected before jumping into the long-term relationship.\n  Try before you buy: Do small contracts at first. Build the relationship and the rapport with the potential provider. See if the product is as good as the promise that came in the proposal.\n  Dont be afraid to switch providers: Go with your hunches. If things seem to be going off course, then they probably are. Look at what youve spent as a sunk cost and move on and find the right solution. Staying in a bad marriage is not the answer in life, and its not the answer in business.",
      "In Malcom Gladwells book Blink, he relates the story of a Pentagon exercise conducted in 2002 called Millenium Challenge. The Blue team, the Allies, were to go against the Red team, a Middle Eastern dictatorship. The Blue team, with its superior information, and its abundance of processes, was supposed to destroy the Red team. What they failed to account for is how their processes got in the way of actually doing anything, and while they were busy writing reports and doing prediction scenarios, LTG Paul van Ripers Red team was actually doing things. They were agile. The Blue team was process-obsessed and process-drowned, and they lost. Streamlined processes work. Process for the sake of process makes a team focus only on the process and not on the outcome.",
      "So, having a CMMI certification may be good, but only if done in synch with an organizational focus on serving the customer in the most efficient manner possible. Having a CMMI certification only to check a box or to make sure that boxes are being checked during a project so that the project lead can cover his or her tracks is not. Id rather focus on the customer and getting things right and delivering a solid product. If it means that somewhere along the way, we find that we could be CMMI certified because of the way we do business, then so be it. After all, as Jim Nist pointed out to me, software development is hard."
    ],
    "summary_t": ""
  },
  {
    "id": "c961f234908a3e246808194c9b1d044c",
    "url_s": "https://opensourceconnections.com/blog/2007/10/06/installing-rmagick-on-windows-using-ruby-186/",
    "title": "Installing Rmagick on Windows Using Ruby 1.8.6",
    "content": [
      "Rmagick tends to be tricky to install, but on windows its relatively simple, except for the fact that if you freshly installed ruby 1.8.6, ruby gems needs to be updated. If you install ruby 1.8.6 and then go to ruby.forge and download rmagick for windows. Extract the zip folder, run the .exe, then open a command line, cd (change directory) to the install folder and do â€œgem install rmagick â€\"localâ€. The first time you do this, you might run into the â€œError installing gem RMagick (version) .gem[.gem]: buffer errorâ€. To fix that run â€œgem update â€\"systemâ€ with double hyphens and then run â€œgem install rmagick â€\"localâ€ and everything should now install fine.",
      "currently listening to.. Chevelle Vena Sera Brainiac",
      "Tags: ruby, ruby+1.8.6, windows, rmagick, gems"
    ],
    "summary_t": ""
  }
]